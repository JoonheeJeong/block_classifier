{
    "name": "CNU_Paper_Assistant_2021",
    "create_date": "20211112",
    "documents": [
        {
            "abstractID": "EMNLP_abs-1",
            "text": [
                {
                    "index": "1-0",
                    "sentence": "Finding attackable sentences in an argument is the first step toward successful refutation in argumentation.",
                    "tag": "1"
                },
                {
                    "index": "1-1",
                    "sentence": "We present a first large-scale analysis of sentence attackability in online arguments.",
                    "tag": "1"
                },
                {
                    "index": "1-2",
                    "sentence": "We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences.",
                    "tag": "2"
                },
                {
                    "index": "1-3",
                    "sentence": "We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability.",
                    "tag": "4"
                },
                {
                    "index": "1-4",
                    "sentence": "Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-2",
            "text": [
                {
                    "index": "2-0",
                    "sentence": "Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives.",
                    "tag": "1"
                },
                {
                    "index": "2-1",
                    "sentence": "These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly.",
                    "tag": "1"
                },
                {
                    "index": "2-2",
                    "sentence": "However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation.",
                    "tag": "1"
                },
                {
                    "index": "2-3",
                    "sentence": "In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation.",
                    "tag": "2+3"
                },
                {
                    "index": "2-4",
                    "sentence": "By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models.",
                    "tag": "4"
                },
                {
                    "index": "2-5",
                    "sentence": "Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-3",
            "text": [
                {
                    "index": "3-0",
                    "sentence": "When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence.",
                    "tag": "1"
                },
                {
                    "index": "3-1",
                    "sentence": "Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect.",
                    "tag": "1"
                },
                {
                    "index": "3-2",
                    "sentence": "Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments.",
                    "tag": "1"
                },
                {
                    "index": "3-3",
                    "sentence": "The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert.",
                    "tag": "2"
                },
                {
                    "index": "3-4",
                    "sentence": "Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data.",
                    "tag": "2"
                },
                {
                    "index": "3-5",
                    "sentence": "Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews.",
                    "tag": "3+4"
                },
                {
                    "index": "3-6",
                    "sentence": "An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-4",
            "text": [
                {
                    "index": "4-0",
                    "sentence": "Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions.",
                    "tag": "1"
                },
                {
                    "index": "4-1",
                    "sentence": "In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic.",
                    "tag": "2"
                },
                {
                    "index": "4-2",
                    "sentence": "Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic.",
                    "tag": "3"
                },
                {
                    "index": "4-3",
                    "sentence": "To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences.",
                    "tag": "2"
                },
                {
                    "index": "4-4",
                    "sentence": "We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence.",
                    "tag": "3"
                },
                {
                    "index": "4-5",
                    "sentence": "Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT.",
                    "tag": "4"
                },
                {
                    "index": "4-6",
                    "sentence": "Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-5",
            "text": [
                {
                    "index": "5-0",
                    "sentence": "The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems.",
                    "tag": "1"
                },
                {
                    "index": "5-1",
                    "sentence": "This paper demonstrates that, while choice of metric is important, the nature of the references is also critical.",
                    "tag": "2"
                },
                {
                    "index": "5-2",
                    "sentence": "We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics.",
                    "tag": "3"
                },
                {
                    "index": "5-3",
                    "sentence": "Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias.",
                    "tag": "3"
                },
                {
                    "index": "5-4",
                    "sentence": "Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references.",
                    "tag": "3+4"
                },
                {
                    "index": "5-5",
                    "sentence": "We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-6",
            "text": [
                {
                    "index": "6-0",
                    "sentence": "The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation.",
                    "tag": "1+2"
                },
                {
                    "index": "6-1",
                    "sentence": "Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language.",
                    "tag": "3"
                },
                {
                    "index": "6-2",
                    "sentence": "For this reason we recommend that reverse-created test data be omitted from future machine translation test sets.",
                    "tag": "3"
                },
                {
                    "index": "6-3",
                    "sentence": "In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT.",
                    "tag": "4"
                },
                {
                    "index": "6-4",
                    "sentence": "One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation.",
                    "tag": "4"
                },
                {
                    "index": "6-5",
                    "sentence": "Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test.",
                    "tag": "4"
                },
                {
                    "index": "6-6",
                    "sentence": "We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-7",
            "text": [
                {
                    "index": "7-0",
                    "sentence": "Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings.",
                    "tag": "1"
                },
                {
                    "index": "7-1",
                    "sentence": "We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser’s distribution over possible tokens.",
                    "tag": "2+3"
                },
                {
                    "index": "7-2",
                    "sentence": "We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU.",
                    "tag": "2"
                },
                {
                    "index": "7-3",
                    "sentence": "We also find SMRT is complementary to back-translation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-8",
            "text": [
                {
                    "index": "8-0",
                    "sentence": "We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference.",
                    "tag": "1"
                },
                {
                    "index": "8-1",
                    "sentence": "We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech).",
                    "tag": "2"
                },
                {
                    "index": "8-2",
                    "sentence": "This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference.",
                    "tag": "3"
                },
                {
                    "index": "8-3",
                    "sentence": "Our method is simple and intuitive, and does not require human judgements for training.",
                    "tag": "4"
                },
                {
                    "index": "8-4",
                    "sentence": "Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data).",
                    "tag": "4"
                },
                {
                    "index": "8-5",
                    "sentence": "We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-9",
            "text": [
                {
                    "index": "9-0",
                    "sentence": "Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language.",
                    "tag": "1"
                },
                {
                    "index": "9-1",
                    "sentence": "In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs.",
                    "tag": "1+2"
                },
                {
                    "index": "9-2",
                    "sentence": "Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm.",
                    "tag": "3"
                },
                {
                    "index": "9-3",
                    "sentence": "During inference, a valid proof, satisfying a set of global constraints is generated.",
                    "tag": "3"
                },
                {
                    "index": "9-4",
                    "sentence": "We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance.",
                    "tag": "3"
                },
                {
                    "index": "9-5",
                    "sentence": "First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation).",
                    "tag": "3+4"
                },
                {
                    "index": "9-6",
                    "sentence": "Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement).",
                    "tag": "3+4"
                },
                {
                    "index": "9-7",
                    "sentence": "Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data.",
                    "tag": "3+4"
                },
                {
                    "index": "9-8",
                    "sentence": "However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for “depth 5”, indicating significant scope for future work.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-10",
            "text": [
                {
                    "index": "10-0",
                    "sentence": "The aim of all Question Answering (QA) systems is to generalize to unseen questions.",
                    "tag": "1"
                },
                {
                    "index": "10-1",
                    "sentence": "Current supervised methods are reliant on expensive data annotation.",
                    "tag": "1"
                },
                {
                    "index": "10-2",
                    "sentence": "Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task.",
                    "tag": "1"
                },
                {
                    "index": "10-3",
                    "sentence": "This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs.",
                    "tag": "2"
                },
                {
                    "index": "10-4",
                    "sentence": "We propose heuristics to create synthetic graphs for commonsense and scientific knowledge.",
                    "tag": "2"
                },
                {
                    "index": "10-5",
                    "sentence": "We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-11",
            "text": [
                {
                    "index": "11-0",
                    "sentence": "Deep learning models for linguistic tasks require large training datasets, which are expensive to create.",
                    "tag": "1"
                },
                {
                    "index": "11-1",
                    "sentence": "As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well.",
                    "tag": "2+3"
                },
                {
                    "index": "11-2",
                    "sentence": "Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples.",
                    "tag": "1"
                },
                {
                    "index": "11-3",
                    "sentence": "Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input.",
                    "tag": "1"
                },
                {
                    "index": "11-4",
                    "sentence": "To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch.",
                    "tag": "2"
                },
                {
                    "index": "11-5",
                    "sentence": "We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-12",
            "text": [
                {
                    "index": "12-0",
                    "sentence": "Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "12-1",
                    "sentence": "A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training.",
                    "tag": "1"
                },
                {
                    "index": "12-2",
                    "sentence": "While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms.",
                    "tag": "1"
                },
                {
                    "index": "12-3",
                    "sentence": "Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data.",
                    "tag": "1"
                },
                {
                    "index": "12-4",
                    "sentence": "Can the choice of formalism affect probing results?",
                    "tag": "1"
                },
                {
                    "index": "12-5",
                    "sentence": "To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics.",
                    "tag": "2"
                },
                {
                    "index": "12-6",
                    "sentence": "We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism.",
                    "tag": "2+3"
                },
                {
                    "index": "12-7",
                    "sentence": "Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-13",
            "text": [
                {
                    "index": "13-0",
                    "sentence": "To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations.",
                    "tag": "1"
                },
                {
                    "index": "13-1",
                    "sentence": "Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations.",
                    "tag": "1"
                },
                {
                    "index": "13-2",
                    "sentence": "For example, they do not substantially favour pretrained representations over randomly initialized ones.",
                    "tag": "1"
                },
                {
                    "index": "13-3",
                    "sentence": "Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks.",
                    "tag": "1"
                },
                {
                    "index": "13-4",
                    "sentence": "To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size.",
                    "tag": "1"
                },
                {
                    "index": "13-5",
                    "sentence": "Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL).",
                    "tag": "2"
                },
                {
                    "index": "13-6",
                    "sentence": "With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data.",
                    "tag": "3"
                },
                {
                    "index": "13-7",
                    "sentence": "Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations.",
                    "tag": "3"
                },
                {
                    "index": "13-8",
                    "sentence": "In addition to probe quality, the description length evaluates “the amount of effort” needed to achieve the quality.",
                    "tag": "3"
                },
                {
                    "index": "13-9",
                    "sentence": "This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality.",
                    "tag": "4"
                },
                {
                    "index": "13-10",
                    "sentence": "We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding.",
                    "tag": "4"
                },
                {
                    "index": "13-11",
                    "sentence": "We show that these methods agree in results and are more informative and stable than the standard probes.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-14",
            "text": [
                {
                    "index": "14-0",
                    "sentence": "Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks.",
                    "tag": "1"
                },
                {
                    "index": "14-1",
                    "sentence": "Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it.",
                    "tag": "1"
                },
                {
                    "index": "14-2",
                    "sentence": "In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted.",
                    "tag": "2+3"
                },
                {
                    "index": "14-3",
                    "sentence": "To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal.",
                    "tag": "3"
                },
                {
                    "index": "14-4",
                    "sentence": "We then probe fastText and BERT for various morphosyntactic attributes across 36 languages.",
                    "tag": "3"
                },
                {
                    "index": "14-5",
                    "sentence": "We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-15",
            "text": [
                {
                    "index": "15-0",
                    "sentence": "One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding.",
                    "tag": "1"
                },
                {
                    "index": "15-1",
                    "sentence": "However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning.",
                    "tag": "1"
                },
                {
                    "index": "15-2",
                    "sentence": "With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning.",
                    "tag": "2+3"
                },
                {
                    "index": "15-3",
                    "sentence": "We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE.",
                    "tag": "3"
                },
                {
                    "index": "15-4",
                    "sentence": "We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones.",
                    "tag": "4"
                },
                {
                    "index": "15-5",
                    "sentence": "Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity.",
                    "tag": "5"
                },
                {
                    "index": "15-6",
                    "sentence": "We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-16",
            "text": [
                {
                    "index": "16-0",
                    "sentence": "The neural attention mechanism plays an important role in many natural language processing applications.",
                    "tag": "1"
                },
                {
                    "index": "16-1",
                    "sentence": "In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives.",
                    "tag": "2"
                },
                {
                    "index": "16-2",
                    "sentence": "However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model’s representation power.",
                    "tag": "4"
                },
                {
                    "index": "16-3",
                    "sentence": "In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective.",
                    "tag": "4"
                },
                {
                    "index": "16-4",
                    "sentence": "Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model’s expressiveness.",
                    "tag": "3"
                },
                {
                    "index": "16-5",
                    "sentence": "Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention.",
                    "tag": "4"
                },
                {
                    "index": "16-6",
                    "sentence": "Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-17",
            "text": [
                {
                    "index": "17-0",
                    "sentence": "Syntactic parsers have dominated natural language understanding for decades.",
                    "tag": "1"
                },
                {
                    "index": "17-1",
                    "sentence": "Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners.",
                    "tag": "1"
                },
                {
                    "index": "17-2",
                    "sentence": "In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference.",
                    "tag": "2+3"
                },
                {
                    "index": "17-3",
                    "sentence": "We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-18",
            "text": [
                {
                    "index": "18-0",
                    "sentence": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks.",
                    "tag": "1"
                },
                {
                    "index": "18-1",
                    "sentence": "In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.",
                    "tag": "2"
                },
                {
                    "index": "18-2",
                    "sentence": "To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens.",
                    "tag": "3"
                },
                {
                    "index": "18-3",
                    "sentence": "We also show that combining global-local attention with relative position encodings and a “Contrastive Predictive Coding” (CPC) pre-training objective allows ETC to encode structured inputs.",
                    "tag": "4"
                },
                {
                    "index": "18-4",
                    "sentence": "We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-19",
            "text": [
                {
                    "index": "19-0",
                    "sentence": "We introduce Electric, an energy-based cloze model for representation learning over text.",
                    "tag": "2"
                },
                {
                    "index": "19-1",
                    "sentence": "Like BERT, it is a conditional generative model of tokens given their contexts.",
                    "tag": "1"
                },
                {
                    "index": "19-2",
                    "sentence": "However, Electric does not use masking or output a full distribution over tokens that could occur in a context.",
                    "tag": "1"
                },
                {
                    "index": "19-3",
                    "sentence": "Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.",
                    "tag": "1"
                },
                {
                    "index": "19-4",
                    "sentence": "We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method.",
                    "tag": "2+3"
                },
                {
                    "index": "19-5",
                    "sentence": "Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models.",
                    "tag": "3+4"
                },
                {
                    "index": "19-6",
                    "sentence": "Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-20",
            "text": [
                {
                    "index": "20-0",
                    "sentence": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated.",
                    "tag": "1"
                },
                {
                    "index": "20-1",
                    "sentence": "Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example?",
                    "tag": "1"
                },
                {
                    "index": "20-2",
                    "sentence": "We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning.",
                    "tag": "2"
                },
                {
                    "index": "20-3",
                    "sentence": "For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about.",
                    "tag": "3"
                },
                {
                    "index": "20-4",
                    "sentence": "We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-21",
            "text": [
                {
                    "index": "21-0",
                    "sentence": "Linguistic steganography studies how to hide secret messages in natural language cover texts.",
                    "tag": "1"
                },
                {
                    "index": "21-1",
                    "sentence": "Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification.",
                    "tag": "1"
                },
                {
                    "index": "21-2",
                    "sentence": "Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message.",
                    "tag": "1"
                },
                {
                    "index": "21-3",
                    "sentence": "In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model.",
                    "tag": "2"
                },
                {
                    "index": "21-4",
                    "sentence": "We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively.",
                    "tag": "3"
                },
                {
                    "index": "21-5",
                    "sentence": "Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-22",
            "text": [
                {
                    "index": "22-0",
                    "sentence": "Machine learning models are trained to find patterns in data.",
                    "tag": "1"
                },
                {
                    "index": "22-1",
                    "sentence": "NLP models can inadvertently learn socially undesirable patterns when training on gender biased text.",
                    "tag": "1"
                },
                {
                    "index": "22-2",
                    "sentence": "In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker.",
                    "tag": "2+3"
                },
                {
                    "index": "22-3",
                    "sentence": "Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information.",
                    "tag": "3"
                },
                {
                    "index": "22-4",
                    "sentence": "In addition, we collect a new, crowdsourced evaluation benchmark.",
                    "tag": "3"
                },
                {
                    "index": "22-5",
                    "sentence": "Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers.",
                    "tag": "3"
                },
                {
                    "index": "22-6",
                    "sentence": "We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-23",
            "text": [
                {
                    "index": "23-0",
                    "sentence": "Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets.",
                    "tag": "1"
                },
                {
                    "index": "23-1",
                    "sentence": "These classifiers are thus likely to have undesirable properties.",
                    "tag": "1"
                },
                {
                    "index": "23-2",
                    "sentence": "For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting.",
                    "tag": "1"
                },
                {
                    "index": "23-3",
                    "sentence": "In this paper, we propose FIND – a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features.",
                    "tag": "2+3"
                },
                {
                    "index": "23-4",
                    "sentence": "Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-24",
            "text": [
                {
                    "index": "24-0",
                    "sentence": "A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs.",
                    "tag": "1"
                },
                {
                    "index": "24-1",
                    "sentence": "We study the task of predicting the documents that customer care agents can use to facilitate users’ needs.",
                    "tag": "2"
                },
                {
                    "index": "24-2",
                    "sentence": "We also introduce a new public dataset which supports the aforementioned problem.",
                    "tag": "3"
                },
                {
                    "index": "24-3",
                    "sentence": "Using this dataset and two others, we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task.",
                    "tag": "3"
                },
                {
                    "index": "24-4",
                    "sentence": "Additionally, we analyze the practicality of such systems in terms of inference time complexity.",
                    "tag": "3"
                },
                {
                    "index": "24-5",
                    "sentence": "Our show that an hybrid IR+DL approach provides the best of both worlds.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-25",
            "text": [
                {
                    "index": "25-0",
                    "sentence": "While humans process language incrementally, the best language encoders currently used in NLP do not.",
                    "tag": "1"
                },
                {
                    "index": "25-1",
                    "sentence": "Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers).",
                    "tag": "1"
                },
                {
                    "index": "25-2",
                    "sentence": "We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems.",
                    "tag": "2"
                },
                {
                    "index": "25-3",
                    "sentence": "We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics.",
                    "tag": "1"
                },
                {
                    "index": "25-4",
                    "sentence": "The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality.",
                    "tag": "3"
                },
                {
                    "index": "25-5",
                    "sentence": "The “omni-directional” BERT model, which achieves better non-incremental performance, is impacted more by the incremental access.",
                    "tag": "4"
                },
                {
                    "index": "25-6",
                    "sentence": "This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-26",
            "text": [
                {
                    "index": "26-0",
                    "sentence": "We propose a generative framework for joint sequence labeling and sentence-level classification.",
                    "tag": "2"
                },
                {
                    "index": "26-1",
                    "sentence": "Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space.",
                    "tag": "3"
                },
                {
                    "index": "26-2",
                    "sentence": "Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks.",
                    "tag": "3"
                },
                {
                    "index": "26-3",
                    "sentence": "Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks.",
                    "tag": "4"
                },
                {
                    "index": "26-4",
                    "sentence": "We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "26-5",
                    "sentence": "We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results.",
                    "tag": "4"
                },
                {
                    "index": "26-6",
                    "sentence": "Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics.",
                    "tag": "4"
                },
                {
                    "index": "26-7",
                    "sentence": "We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-27",
            "text": [
                {
                    "index": "27-0",
                    "sentence": "Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses.",
                    "tag": "1"
                },
                {
                    "index": "27-1",
                    "sentence": "However, some human replies are more engaging than others, spawning more followup interactions.",
                    "tag": "1"
                },
                {
                    "index": "27-2",
                    "sentence": "Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging.",
                    "tag": "2"
                },
                {
                    "index": "27-3",
                    "sentence": "We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction.",
                    "tag": "3"
                },
                {
                    "index": "27-4",
                    "sentence": "To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors.",
                    "tag": "4"
                },
                {
                    "index": "27-5",
                    "sentence": "We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines.",
                    "tag": "4"
                },
                {
                    "index": "27-6",
                    "sentence": "Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback.",
                    "tag": "3"
                },
                {
                    "index": "27-7",
                    "sentence": "We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses.",
                    "tag": "3"
                },
                {
                    "index": "27-8",
                    "sentence": "Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-28",
            "text": [
                {
                    "index": "28-0",
                    "sentence": "We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models.",
                    "tag": "2"
                },
                {
                    "index": "28-1",
                    "sentence": "Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases.",
                    "tag": "3"
                },
                {
                    "index": "28-2",
                    "sentence": "At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently.",
                    "tag": "3"
                },
                {
                    "index": "28-3",
                    "sentence": "We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples.",
                    "tag": "3"
                },
                {
                    "index": "28-4",
                    "sentence": "In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed.",
                    "tag": "4"
                },
                {
                    "index": "28-5",
                    "sentence": "Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-29",
            "text": [
                {
                    "index": "29-0",
                    "sentence": "In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering.",
                    "tag": "2"
                },
                {
                    "index": "29-1",
                    "sentence": "Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words.",
                    "tag": "3"
                },
                {
                    "index": "29-2",
                    "sentence": "Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines.",
                    "tag": "4"
                },
                {
                    "index": "29-3",
                    "sentence": "Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-30",
            "text": [
                {
                    "index": "30-0",
                    "sentence": "We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort.",
                    "tag": "2"
                },
                {
                    "index": "30-1",
                    "sentence": "Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations.",
                    "tag": "3"
                },
                {
                    "index": "30-2",
                    "sentence": "It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech.",
                    "tag": "3"
                },
                {
                    "index": "30-3",
                    "sentence": "It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.",
                    "tag": "3"
                },
                {
                    "index": "30-4",
                    "sentence": "We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers.",
                    "tag": "4+5"
                },
                {
                    "index": "30-5",
                    "sentence": "To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset.",
                    "tag": "4"
                },
                {
                    "index": "30-6",
                    "sentence": "AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-31",
            "text": [
                {
                    "index": "31-0",
                    "sentence": "Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents.",
                    "tag": "1"
                },
                {
                    "index": "31-1",
                    "sentence": "In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact.",
                    "tag": "2"
                },
                {
                    "index": "31-2",
                    "sentence": "Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster.",
                    "tag": "3"
                },
                {
                    "index": "31-3",
                    "sentence": "The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation.",
                    "tag": "3"
                },
                {
                    "index": "31-4",
                    "sentence": "According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact.",
                    "tag": "3"
                },
                {
                    "index": "31-5",
                    "sentence": "The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-32",
            "text": [
                {
                    "index": "32-0",
                    "sentence": "Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years.",
                    "tag": "1"
                },
                {
                    "index": "32-1",
                    "sentence": "However, gaps still exist between summaries produced by automatic summarizers and human professionals.",
                    "tag": "1"
                },
                {
                    "index": "32-2",
                    "sentence": "Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually.",
                    "tag": "2+3"
                },
                {
                    "index": "32-3",
                    "sentence": "Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-33",
            "text": [
                {
                    "index": "33-0",
                    "sentence": "Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.",
                    "tag": "1"
                },
                {
                    "index": "33-1",
                    "sentence": "However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.",
                    "tag": "1"
                },
                {
                    "index": "33-2",
                    "sentence": "In this paper, we propose a new approach based on Q-learning with an edit-based summarization.",
                    "tag": "2"
                },
                {
                    "index": "33-3",
                    "sentence": "The method combines two key modules to form an Editorial Agent and Language Model converter (EALM).",
                    "tag": "3"
                },
                {
                    "index": "33-4",
                    "sentence": "The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals.",
                    "tag": "3"
                },
                {
                    "index": "33-5",
                    "sentence": "Q-learning is leveraged to train the agent to produce proper edit actions.",
                    "tag": "3"
                },
                {
                    "index": "33-6",
                    "sentence": "Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set).",
                    "tag": "4"
                },
                {
                    "index": "33-7",
                    "sentence": "Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization.",
                    "tag": "4"
                },
                {
                    "index": "33-8",
                    "sentence": "We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-34",
            "text": [
                {
                    "index": "34-0",
                    "sentence": "Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance.",
                    "tag": "1"
                },
                {
                    "index": "34-1",
                    "sentence": "Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance.",
                    "tag": "2"
                },
                {
                    "index": "34-2",
                    "sentence": "To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules.",
                    "tag": "3"
                },
                {
                    "index": "34-3",
                    "sentence": "TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters.",
                    "tag": "3"
                },
                {
                    "index": "34-4",
                    "sentence": "Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-35",
            "text": [
                {
                    "index": "35-0",
                    "sentence": "Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one.",
                    "tag": "1"
                },
                {
                    "index": "35-1",
                    "sentence": "Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network.",
                    "tag": "1"
                },
                {
                    "index": "35-2",
                    "sentence": "To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective.",
                    "tag": "2+3"
                },
                {
                    "index": "35-3",
                    "sentence": "By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers.",
                    "tag": "3"
                },
                {
                    "index": "35-4",
                    "sentence": "CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-36",
            "text": [
                {
                    "index": "36-0",
                    "sentence": "Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks.",
                    "tag": "1"
                },
                {
                    "index": "36-1",
                    "sentence": "However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices.",
                    "tag": "1"
                },
                {
                    "index": "36-2",
                    "sentence": "In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model.",
                    "tag": "2"
                },
                {
                    "index": "36-3",
                    "sentence": "Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT.",
                    "tag": "3"
                },
                {
                    "index": "36-4",
                    "sentence": "Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process.",
                    "tag": "3"
                },
                {
                    "index": "36-5",
                    "sentence": "Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-37",
            "text": [
                {
                    "index": "37-0",
                    "sentence": "Self-supervised pre-training of transformer models has revolutionized NLP applications.",
                    "tag": "1"
                },
                {
                    "index": "37-1",
                    "sentence": "Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning.",
                    "tag": "1"
                },
                {
                    "index": "37-2",
                    "sentence": "However, fine-tuning is still data inefficient — when there are few labeled examples, accuracy can be low.",
                    "tag": "1"
                },
                {
                    "index": "37-3",
                    "sentence": "Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem.",
                    "tag": "1"
                },
                {
                    "index": "37-4",
                    "sentence": "However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult.",
                    "tag": "2"
                },
                {
                    "index": "37-5",
                    "sentence": "This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text.",
                    "tag": "2+3"
                },
                {
                    "index": "37-6",
                    "sentence": "This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms.",
                    "tag": "3"
                },
                {
                    "index": "37-7",
                    "sentence": "This yields as many unique meta-training tasks as the number of subsets of vocabulary terms.",
                    "tag": "3"
                },
                {
                    "index": "37-8",
                    "sentence": "We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework.",
                    "tag": "3"
                },
                {
                    "index": "37-9",
                    "sentence": "On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning.",
                    "tag": "4"
                },
                {
                    "index": "37-10",
                    "sentence": "Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-38",
            "text": [
                {
                    "index": "38-0",
                    "sentence": "Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning.",
                    "tag": "1"
                },
                {
                    "index": "38-1",
                    "sentence": "State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time.",
                    "tag": "1"
                },
                {
                    "index": "38-2",
                    "sentence": "However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed.",
                    "tag": "1"
                },
                {
                    "index": "38-3",
                    "sentence": "In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion.",
                    "tag": "2"
                },
                {
                    "index": "38-4",
                    "sentence": "To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation.",
                    "tag": "3"
                },
                {
                    "index": "38-5",
                    "sentence": "Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning.",
                    "tag": "4"
                },
                {
                    "index": "38-6",
                    "sentence": "We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-39",
            "text": [
                {
                    "index": "39-0",
                    "sentence": "Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language.",
                    "tag": "1"
                },
                {
                    "index": "39-1",
                    "sentence": "However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers.",
                    "tag": "1"
                },
                {
                    "index": "39-2",
                    "sentence": "We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks.",
                    "tag": "2"
                },
                {
                    "index": "39-3",
                    "sentence": "English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs.",
                    "tag": "4"
                },
                {
                    "index": "39-4",
                    "sentence": "These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R).",
                    "tag": "4"
                },
                {
                    "index": "39-5",
                    "sentence": "We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set.",
                    "tag": "4"
                },
                {
                    "index": "39-6",
                    "sentence": "Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-40",
            "text": [
                {
                    "index": "40-0",
                    "sentence": "We present a novel supervised word alignment method based on cross-language span prediction.",
                    "tag": "2"
                },
                {
                    "index": "40-1",
                    "sentence": "We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence.",
                    "tag": "3"
                },
                {
                    "index": "40-2",
                    "sentence": "Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data.",
                    "tag": "3"
                },
                {
                    "index": "40-3",
                    "sentence": "It is nontrivial to obtain accurate alignment from a set of independently predicted spans.",
                    "tag": "1"
                },
                {
                    "index": "40-4",
                    "sentence": "We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions.",
                    "tag": "4"
                },
                {
                    "index": "40-5",
                    "sentence": "In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining.",
                    "tag": "3+4"
                },
                {
                    "index": "40-6",
                    "sentence": "For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-41",
            "text": [
                {
                    "index": "41-0",
                    "sentence": "Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism.",
                    "tag": "1"
                },
                {
                    "index": "41-1",
                    "sentence": "In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET.",
                    "tag": "2"
                },
                {
                    "index": "41-2",
                    "sentence": "The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work.",
                    "tag": "3"
                },
                {
                    "index": "41-3",
                    "sentence": "Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change.",
                    "tag": "4"
                },
                {
                    "index": "41-4",
                    "sentence": "Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments.",
                    "tag": "3"
                },
                {
                    "index": "41-5",
                    "sentence": "Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-42",
            "text": [
                {
                    "index": "42-0",
                    "sentence": "Cherokee is a highly endangered Native American language spoken by the Cherokee people.",
                    "tag": "1"
                },
                {
                    "index": "42-1",
                    "sentence": "The Cherokee culture is deeply embedded in its language.",
                    "tag": "1"
                },
                {
                    "index": "42-2",
                    "sentence": "However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year.",
                    "tag": "1"
                },
                {
                    "index": "42-3",
                    "sentence": "To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English.",
                    "tag": "2"
                },
                {
                    "index": "42-4",
                    "sentence": "Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total.",
                    "tag": "3"
                },
                {
                    "index": "42-5",
                    "sentence": "We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation.",
                    "tag": "3"
                },
                {
                    "index": "42-6",
                    "sentence": "We also collect 5k Cherokee monolingual data to enable semi-supervised learning.",
                    "tag": "3"
                },
                {
                    "index": "42-7",
                    "sentence": "Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems.",
                    "tag": "3"
                },
                {
                    "index": "42-8",
                    "sentence": "We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages.",
                    "tag": "3"
                },
                {
                    "index": "42-9",
                    "sentence": "Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-43",
            "text": [
                {
                    "index": "43-0",
                    "sentence": "Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable.",
                    "tag": "1"
                },
                {
                    "index": "43-1",
                    "sentence": "We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias.",
                    "tag": "2"
                },
                {
                    "index": "43-2",
                    "sentence": "Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data.",
                    "tag": "3"
                },
                {
                    "index": "43-3",
                    "sentence": "Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning.",
                    "tag": "3"
                },
                {
                    "index": "43-4",
                    "sentence": "Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization.",
                    "tag": "3"
                },
                {
                    "index": "43-5",
                    "sentence": "Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-44",
            "text": [
                {
                    "index": "44-0",
                    "sentence": "Offering condolence is a natural reaction to hearing someone’s distress.",
                    "tag": "1"
                },
                {
                    "index": "44-1",
                    "sentence": "Individuals frequently express distress in social media, where some communities can provide support.",
                    "tag": "2"
                },
                {
                    "index": "44-2",
                    "sentence": "However, not all condolence is equal—trite responses offer little actual support despite their good intentions.",
                    "tag": "3"
                },
                {
                    "index": "44-3",
                    "sentence": "Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online.",
                    "tag": "3"
                },
                {
                    "index": "44-4",
                    "sentence": "Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement.",
                    "tag": "4"
                },
                {
                    "index": "44-5",
                    "sentence": "Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory.",
                    "tag": "4"
                },
                {
                    "index": "44-6",
                    "sentence": "Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-45",
            "text": [
                {
                    "index": "45-0",
                    "sentence": "Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes.",
                    "tag": "1"
                },
                {
                    "index": "45-1",
                    "sentence": "In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting.",
                    "tag": "2"
                },
                {
                    "index": "45-2",
                    "sentence": "Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets.",
                    "tag": "3"
                },
                {
                    "index": "45-3",
                    "sentence": "To illustrate our method, we model legislators’ attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets.",
                    "tag": "3"
                },
                {
                    "index": "45-4",
                    "sentence": "We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018.",
                    "tag": "4"
                },
                {
                    "index": "45-5",
                    "sentence": "We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-46",
            "text": [
                {
                    "index": "46-0",
                    "sentence": "We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text.",
                    "tag": "2"
                },
                {
                    "index": "46-1",
                    "sentence": "We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied.",
                    "tag": "2"
                },
                {
                    "index": "46-2",
                    "sentence": "Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-47",
            "text": [
                {
                    "index": "47-0",
                    "sentence": "Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives.",
                    "tag": "1"
                },
                {
                    "index": "47-1",
                    "sentence": "For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct, such as “It is expected that you report crimes.” We present SOCIAL CHEMISTRY, a new conceptual formalism to study people’s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language.",
                    "tag": "1+2"
                },
                {
                    "index": "47-2",
                    "sentence": "We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units.",
                    "tag": "3"
                },
                {
                    "index": "47-3",
                    "sentence": "Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.",
                    "tag": "3"
                },
                {
                    "index": "47-4",
                    "sentence": "Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction.",
                    "tag": "4"
                },
                {
                    "index": "47-5",
                    "sentence": "Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-48",
            "text": [
                {
                    "index": "48-0",
                    "sentence": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments.",
                    "tag": "1"
                },
                {
                    "index": "48-1",
                    "sentence": "Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation.",
                    "tag": "1"
                },
                {
                    "index": "48-2",
                    "sentence": "To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner.",
                    "tag": "2+3"
                },
                {
                    "index": "48-3",
                    "sentence": "Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-49",
            "text": [
                {
                    "index": "49-0",
                    "sentence": "Event schemas can guide our understanding and ability to make predictions with respect to what might happen next.",
                    "tag": "1"
                },
                {
                    "index": "49-1",
                    "sentence": "We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story.",
                    "tag": "2"
                },
                {
                    "index": "49-2",
                    "sentence": "We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas.",
                    "tag": "3"
                },
                {
                    "index": "49-3",
                    "sentence": "We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph.",
                    "tag": "3"
                },
                {
                    "index": "49-4",
                    "sentence": "Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas.",
                    "tag": "4"
                },
                {
                    "index": "49-5",
                    "sentence": "Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-50",
            "text": [
                {
                    "index": "50-0",
                    "sentence": "Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.",
                    "tag": "1"
                },
                {
                    "index": "50-1",
                    "sentence": "In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.",
                    "tag": "1"
                },
                {
                    "index": "50-2",
                    "sentence": "Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.",
                    "tag": "2+3"
                },
                {
                    "index": "50-3",
                    "sentence": "Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives.",
                    "tag": "3"
                },
                {
                    "index": "50-4",
                    "sentence": "We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.",
                    "tag": "3+4"
                },
                {
                    "index": "50-5",
                    "sentence": "We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-51",
            "text": [
                {
                    "index": "51-0",
                    "sentence": "Conventional approaches to event detection usually require a fixed set of pre-defined event types.",
                    "tag": "1"
                },
                {
                    "index": "51-1",
                    "sentence": "Such a requirement is often challenged in real-world applications, as new events continually occur.",
                    "tag": "1"
                },
                {
                    "index": "51-2",
                    "sentence": "Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive.",
                    "tag": "1"
                },
                {
                    "index": "51-3",
                    "sentence": "We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes.",
                    "tag": "1"
                },
                {
                    "index": "51-4",
                    "sentence": "However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection.",
                    "tag": "1"
                },
                {
                    "index": "51-5",
                    "sentence": "In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues.",
                    "tag": "2"
                },
                {
                    "index": "51-6",
                    "sentence": "Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively.",
                    "tag": "3"
                },
                {
                    "index": "51-7",
                    "sentence": "Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-52",
            "text": [
                {
                    "index": "52-0",
                    "sentence": "Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive.",
                    "tag": "1"
                },
                {
                    "index": "52-1",
                    "sentence": "In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types.",
                    "tag": "2+3"
                },
                {
                    "index": "52-2",
                    "sentence": "We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations.",
                    "tag": "3"
                },
                {
                    "index": "52-3",
                    "sentence": "A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution.",
                    "tag": "3"
                },
                {
                    "index": "52-4",
                    "sentence": "Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-53",
            "text": [
                {
                    "index": "53-0",
                    "sentence": "Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation.",
                    "tag": "1"
                },
                {
                    "index": "53-1",
                    "sentence": "Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph.",
                    "tag": "1"
                },
                {
                    "index": "53-2",
                    "sentence": "We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation.",
                    "tag": "1"
                },
                {
                    "index": "53-3",
                    "sentence": "In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph.",
                    "tag": "2"
                },
                {
                    "index": "53-4",
                    "sentence": "We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge.",
                    "tag": "4"
                },
                {
                    "index": "53-5",
                    "sentence": "We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-54",
            "text": [
                {
                    "index": "54-0",
                    "sentence": "Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs.",
                    "tag": "1"
                },
                {
                    "index": "54-1",
                    "sentence": "However, many existing systems purportedly designed for style transfer inherently warp the input’s meaning through attribute transfer, which changes semantic properties such as sentiment.",
                    "tag": "1"
                },
                {
                    "index": "54-2",
                    "sentence": "In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data.",
                    "tag": "2"
                },
                {
                    "index": "54-3",
                    "sentence": "Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations.",
                    "tag": "4"
                },
                {
                    "index": "54-4",
                    "sentence": "We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants.",
                    "tag": "4"
                },
                {
                    "index": "54-5",
                    "sentence": "Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-55",
            "text": [
                {
                    "index": "55-0",
                    "sentence": "Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation.",
                    "tag": "1"
                },
                {
                    "index": "55-1",
                    "sentence": "While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes.",
                    "tag": "1"
                },
                {
                    "index": "55-2",
                    "sentence": "In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders.",
                    "tag": "2"
                },
                {
                    "index": "55-3",
                    "sentence": "The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized.",
                    "tag": "3"
                },
                {
                    "index": "55-4",
                    "sentence": "The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court’s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model.",
                    "tag": "3"
                },
                {
                    "index": "55-5",
                    "sentence": "Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-56",
            "text": [
                {
                    "index": "56-0",
                    "sentence": "Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content.",
                    "tag": "1"
                },
                {
                    "index": "56-1",
                    "sentence": "In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART.",
                    "tag": "2"
                },
                {
                    "index": "56-2",
                    "sentence": "We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions.",
                    "tag": "3"
                },
                {
                    "index": "56-3",
                    "sentence": "The BART model is employed for generation without modifying its structure.",
                    "tag": "3"
                },
                {
                    "index": "56-4",
                    "sentence": "We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework.",
                    "tag": "3"
                },
                {
                    "index": "56-5",
                    "sentence": "Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements.",
                    "tag": "3"
                },
                {
                    "index": "56-6",
                    "sentence": "In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-57",
            "text": [
                {
                    "index": "57-0",
                    "sentence": "Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future.",
                    "tag": "1"
                },
                {
                    "index": "57-1",
                    "sentence": "However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling.",
                    "tag": "1"
                },
                {
                    "index": "57-2",
                    "sentence": "In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision.",
                    "tag": "2+3"
                },
                {
                    "index": "57-3",
                    "sentence": "The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters.",
                    "tag": "3"
                },
                {
                    "index": "57-4",
                    "sentence": "By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts.",
                    "tag": "3"
                },
                {
                    "index": "57-5",
                    "sentence": "We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-58",
            "text": [
                {
                    "index": "58-0",
                    "sentence": "Learning to fuse vision and language information and representing them is an important research problem with many applications.",
                    "tag": "1"
                },
                {
                    "index": "58-1",
                    "sentence": "Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images.",
                    "tag": "1"
                },
                {
                    "index": "58-2",
                    "sentence": "In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets.",
                    "tag": "2"
                },
                {
                    "index": "58-3",
                    "sentence": "In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded.",
                    "tag": "3"
                },
                {
                    "index": "58-4",
                    "sentence": "This type of generic-to-specific relations can be discovered using linguistic analysis tools.",
                    "tag": "4"
                },
                {
                    "index": "58-5",
                    "sentence": "We propose methods to incorporate such relations into learning representation.",
                    "tag": "4"
                },
                {
                    "index": "58-6",
                    "sentence": "We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations.",
                    "tag": "4"
                },
                {
                    "index": "58-7",
                    "sentence": "The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition.",
                    "tag": "4"
                },
                {
                    "index": "58-8",
                    "sentence": "Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-59",
            "text": [
                {
                    "index": "59-0",
                    "sentence": "Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering.",
                    "tag": "1"
                },
                {
                    "index": "59-1",
                    "sentence": "However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data.",
                    "tag": "1"
                },
                {
                    "index": "59-2",
                    "sentence": "We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task.",
                    "tag": "2"
                },
                {
                    "index": "59-3",
                    "sentence": "This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure.",
                    "tag": "3"
                },
                {
                    "index": "59-4",
                    "sentence": "For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation.",
                    "tag": "4"
                },
                {
                    "index": "59-5",
                    "sentence": "Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions.",
                    "tag": "4"
                },
                {
                    "index": "59-6",
                    "sentence": "We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-60",
            "text": [
                {
                    "index": "60-0",
                    "sentence": "While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d.",
                    "tag": "1"
                },
                {
                    "index": "60-1",
                    "sentence": "setting.",
                    "tag": "1"
                },
                {
                    "index": "60-2",
                    "sentence": "As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization.",
                    "tag": "1"
                },
                {
                    "index": "60-3",
                    "sentence": "In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge.",
                    "tag": "2"
                },
                {
                    "index": "60-4",
                    "sentence": "Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer).",
                    "tag": "3"
                },
                {
                    "index": "60-5",
                    "sentence": "Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions.",
                    "tag": "4"
                },
                {
                    "index": "60-6",
                    "sentence": "MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement.",
                    "tag": "4"
                },
                {
                    "index": "60-7",
                    "sentence": "Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-61",
            "text": [
                {
                    "index": "61-0",
                    "sentence": "Dialogue systems play an increasingly important role in various aspects of our daily life.",
                    "tag": "1"
                },
                {
                    "index": "61-1",
                    "sentence": "It is evident from recent research that dialogue systems trained on human conversation data are biased.",
                    "tag": "2"
                },
                {
                    "index": "61-2",
                    "sentence": "In particular, they can produce responses that reflect people’s gender prejudice.",
                    "tag": "3"
                },
                {
                    "index": "61-3",
                    "sentence": "Many debiasing methods have been developed for various NLP tasks, such as word embedding.",
                    "tag": "3"
                },
                {
                    "index": "61-4",
                    "sentence": "However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders.",
                    "tag": "4"
                },
                {
                    "index": "61-5",
                    "sentence": "This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models.",
                    "tag": "4"
                },
                {
                    "index": "61-6",
                    "sentence": "In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance.",
                    "tag": "5"
                },
                {
                    "index": "61-7",
                    "sentence": "Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-62",
            "text": [
                {
                    "index": "62-0",
                    "sentence": "We explore the task of improving persona consistency of dialogue agents.",
                    "tag": "2"
                },
                {
                    "index": "62-1",
                    "sentence": "Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency.",
                    "tag": "1"
                },
                {
                    "index": "62-2",
                    "sentence": "However, such additional labels and training can be demanding.",
                    "tag": "1"
                },
                {
                    "index": "62-3",
                    "sentence": "Also, we find even the best-performing persona-based agents are insensitive to contradictory words.",
                    "tag": "2"
                },
                {
                    "index": "62-4",
                    "sentence": "Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener.",
                    "tag": "3"
                },
                {
                    "index": "62-5",
                    "sentence": "Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction.",
                    "tag": "3"
                },
                {
                    "index": "62-6",
                    "sentence": "We further extend the framework by learning the distractor selection, which has been usually done manually or randomly.",
                    "tag": "3"
                },
                {
                    "index": "62-7",
                    "sentence": "Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models.",
                    "tag": "4"
                },
                {
                    "index": "62-8",
                    "sentence": "Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-63",
            "text": [
                {
                    "index": "63-0",
                    "sentence": "The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice.",
                    "tag": "1"
                },
                {
                    "index": "63-1",
                    "sentence": "In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling.",
                    "tag": "2"
                },
                {
                    "index": "63-2",
                    "sentence": "To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling.",
                    "tag": "3"
                },
                {
                    "index": "63-3",
                    "sentence": "We propose a contrastive objective function to simulate the response selection task.",
                    "tag": "3"
                },
                {
                    "index": "63-4",
                    "sentence": "Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection.",
                    "tag": "4"
                },
                {
                    "index": "63-5",
                    "sentence": "We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-64",
            "text": [
                {
                    "index": "64-0",
                    "sentence": "Large-scale dialogue datasets have recently become available for training neural dialogue agents.",
                    "tag": "1"
                },
                {
                    "index": "64-1",
                    "sentence": "However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs.",
                    "tag": "1"
                },
                {
                    "index": "64-2",
                    "sentence": "In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness.",
                    "tag": "2"
                },
                {
                    "index": "64-3",
                    "sentence": "The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities.",
                    "tag": "3"
                },
                {
                    "index": "64-4",
                    "sentence": "We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality.",
                    "tag": "4"
                },
                {
                    "index": "64-5",
                    "sentence": "Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality.",
                    "tag": "4"
                },
                {
                    "index": "64-6",
                    "sentence": "We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-65",
            "text": [
                {
                    "index": "65-0",
                    "sentence": "Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model.",
                    "tag": "1"
                },
                {
                    "index": "65-1",
                    "sentence": "Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations.",
                    "tag": "1"
                },
                {
                    "index": "65-2",
                    "sentence": "However, such ancestral populations are hardly interpretable in the context of the tree model.",
                    "tag": "1"
                },
                {
                    "index": "65-3",
                    "sentence": "In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions.",
                    "tag": "2"
                },
                {
                    "index": "65-4",
                    "sentence": "We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions.",
                    "tag": "5"
                },
                {
                    "index": "65-5",
                    "sentence": "Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-66",
            "text": [
                {
                    "index": "66-0",
                    "sentence": "Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories.",
                    "tag": "1"
                },
                {
                    "index": "66-1",
                    "sentence": "Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties.",
                    "tag": "1"
                },
                {
                    "index": "66-2",
                    "sentence": "We propose a principled methodology to explore regularity in word class flexibility.",
                    "tag": "2"
                },
                {
                    "index": "66-3",
                    "sentence": "Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages.",
                    "tag": "3"
                },
                {
                    "index": "66-4",
                    "sentence": "We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages.",
                    "tag": "4"
                },
                {
                    "index": "66-5",
                    "sentence": "Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process.",
                    "tag": "3"
                },
                {
                    "index": "66-6",
                    "sentence": "Our work highlights the utility of deep contextualized models in linguistic typology.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-67",
            "text": [
                {
                    "index": "67-0",
                    "sentence": "Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming.",
                    "tag": "1"
                },
                {
                    "index": "67-1",
                    "sentence": "Moreover, why deep models help NMT is an open question.",
                    "tag": "1"
                },
                {
                    "index": "67-2",
                    "sentence": "In this paper, we investigate the behavior of a well-tuned deep Transformer system.",
                    "tag": "2"
                },
                {
                    "index": "67-3",
                    "sentence": "We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly.",
                    "tag": "4"
                },
                {
                    "index": "67-4",
                    "sentence": "This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models.",
                    "tag": "3"
                },
                {
                    "index": "67-5",
                    "sentence": "In this way, we successfully train a Transformer system with a 54-layer encoder.",
                    "tag": "3"
                },
                {
                    "index": "67-6",
                    "sentence": "Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks.",
                    "tag": "4"
                },
                {
                    "index": "67-7",
                    "sentence": "The code is publicly available at https://github.com/libeineu/SDT-Training.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-68",
            "text": [
                {
                    "index": "68-0",
                    "sentence": "We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space.",
                    "tag": "2"
                },
                {
                    "index": "68-1",
                    "sentence": "Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead.",
                    "tag": "3"
                },
                {
                    "index": "68-2",
                    "sentence": "This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability.",
                    "tag": "3"
                },
                {
                    "index": "68-3",
                    "sentence": "As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space.",
                    "tag": "4"
                },
                {
                    "index": "68-4",
                    "sentence": "We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables.",
                    "tag": "3"
                },
                {
                    "index": "68-5",
                    "sentence": "We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps.",
                    "tag": "3"
                },
                {
                    "index": "68-6",
                    "sentence": "On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-69",
            "text": [
                {
                    "index": "69-0",
                    "sentence": "With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better.",
                    "tag": "1"
                },
                {
                    "index": "69-1",
                    "sentence": "However, they also become harder to deploy on edge devices due to memory constraints.",
                    "tag": "1"
                },
                {
                    "index": "69-2",
                    "sentence": "To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S).",
                    "tag": "1"
                },
                {
                    "index": "69-3",
                    "sentence": "Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative.",
                    "tag": "2+3"
                },
                {
                    "index": "69-4",
                    "sentence": "In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese→English, Turkish→English, and English→German directions.",
                    "tag": "3"
                },
                {
                    "index": "69-5",
                    "sentence": "Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-70",
            "text": [
                {
                    "index": "70-0",
                    "sentence": "While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area.",
                    "tag": "1"
                },
                {
                    "index": "70-1",
                    "sentence": "In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data.",
                    "tag": "2"
                },
                {
                    "index": "70-2",
                    "sentence": "We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets.",
                    "tag": "3"
                },
                {
                    "index": "70-3",
                    "sentence": "We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models.",
                    "tag": "4"
                },
                {
                    "index": "70-4",
                    "sentence": "We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data.",
                    "tag": "4"
                },
                {
                    "index": "70-5",
                    "sentence": "Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-71",
            "text": [
                {
                    "index": "71-0",
                    "sentence": "There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT).",
                    "tag": "1"
                },
                {
                    "index": "71-1",
                    "sentence": "The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution.",
                    "tag": "1"
                },
                {
                    "index": "71-2",
                    "sentence": "However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.",
                    "tag": "1"
                },
                {
                    "index": "71-3",
                    "sentence": "In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.",
                    "tag": "2"
                },
                {
                    "index": "71-4",
                    "sentence": "We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens.",
                    "tag": "3"
                },
                {
                    "index": "71-5",
                    "sentence": "Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively.",
                    "tag": "3"
                },
                {
                    "index": "71-6",
                    "sentence": "Further analyses show that our method can also improve the lexical diversity of translation.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-72",
            "text": [
                {
                    "index": "72-0",
                    "sentence": "Transformer models achieve remarkable success in Neural Machine Translation.",
                    "tag": "1"
                },
                {
                    "index": "72-1",
                    "sentence": "Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention.",
                    "tag": "1"
                },
                {
                    "index": "72-2",
                    "sentence": "In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.",
                    "tag": "2"
                },
                {
                    "index": "72-3",
                    "sentence": "Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.",
                    "tag": "4"
                },
                {
                    "index": "72-4",
                    "sentence": "Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.",
                    "tag": "3"
                },
                {
                    "index": "72-5",
                    "sentence": "Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%).",
                    "tag": "4"
                },
                {
                    "index": "72-6",
                    "sentence": "In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters.",
                    "tag": "4"
                },
                {
                    "index": "72-7",
                    "sentence": "These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-73",
            "text": [
                {
                    "index": "73-0",
                    "sentence": "Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources.",
                    "tag": "1"
                },
                {
                    "index": "73-1",
                    "sentence": "In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance.",
                    "tag": "2"
                },
                {
                    "index": "73-2",
                    "sentence": "Experiments and analyses are systematically conducted on different datasets and NMT architectures.",
                    "tag": "3"
                },
                {
                    "index": "73-3",
                    "sentence": "We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-74",
            "text": [
                {
                    "index": "74-0",
                    "sentence": "In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs.",
                    "tag": "2"
                },
                {
                    "index": "74-1",
                    "sentence": "Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way.",
                    "tag": "3"
                },
                {
                    "index": "74-2",
                    "sentence": "We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence.",
                    "tag": "3"
                },
                {
                    "index": "74-3",
                    "sentence": "We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding.",
                    "tag": "3"
                },
                {
                    "index": "74-4",
                    "sentence": "Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup.",
                    "tag": "4"
                },
                {
                    "index": "74-5",
                    "sentence": "Further analysis indicates that our method reduces repeated translations and performs better at longer sentences.",
                    "tag": "3"
                },
                {
                    "index": "74-6",
                    "sentence": "Our code will be released to the public.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-75",
            "text": [
                {
                    "index": "75-0",
                    "sentence": "Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans.",
                    "tag": "1"
                },
                {
                    "index": "75-1",
                    "sentence": "Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity.",
                    "tag": "1"
                },
                {
                    "index": "75-2",
                    "sentence": "We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.",
                    "tag": "2+3"
                },
                {
                    "index": "75-3",
                    "sentence": "Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-76",
            "text": [
                {
                    "index": "76-0",
                    "sentence": "Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity.",
                    "tag": "1"
                },
                {
                    "index": "76-1",
                    "sentence": "However, few attention is paid to the baseline model.",
                    "tag": "1"
                },
                {
                    "index": "76-2",
                    "sentence": "In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation.",
                    "tag": "2"
                },
                {
                    "index": "76-3",
                    "sentence": "Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors.",
                    "tag": "4"
                },
                {
                    "index": "76-4",
                    "sentence": "We examine our approach on the two publicly available document-level datasets.",
                    "tag": "4"
                },
                {
                    "index": "76-5",
                    "sentence": "We can achieve a strong result in BLEU and capture discourse phenomena.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-77",
            "text": [
                {
                    "index": "77-0",
                    "sentence": "Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation.",
                    "tag": "1"
                },
                {
                    "index": "77-1",
                    "sentence": "In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference.",
                    "tag": "2"
                },
                {
                    "index": "77-2",
                    "sentence": "The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling.",
                    "tag": "3"
                },
                {
                    "index": "77-3",
                    "sentence": "With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled.",
                    "tag": "3"
                },
                {
                    "index": "77-4",
                    "sentence": "We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-78",
            "text": [
                {
                    "index": "78-0",
                    "sentence": "This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming.",
                    "tag": "2+3"
                },
                {
                    "index": "78-1",
                    "sentence": "We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates.",
                    "tag": "3+4"
                },
                {
                    "index": "78-2",
                    "sentence": "In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline.",
                    "tag": "4"
                },
                {
                    "index": "78-3",
                    "sentence": "Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model.",
                    "tag": "4"
                },
                {
                    "index": "78-4",
                    "sentence": "On the competitive WMT’14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps.",
                    "tag": "4"
                },
                {
                    "index": "78-5",
                    "sentence": "This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-79",
            "text": [
                {
                    "index": "79-0",
                    "sentence": "Many extractive question answering models are trained to predict start and end positions of answers.",
                    "tag": "1"
                },
                {
                    "index": "79-1",
                    "sentence": "The choice of predicting answers as positions is mainly due to its simplicity and effectiveness.",
                    "tag": "1"
                },
                {
                    "index": "79-2",
                    "sentence": "In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions.",
                    "tag": "1+2"
                },
                {
                    "index": "79-3",
                    "sentence": "We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT.",
                    "tag": "3"
                },
                {
                    "index": "79-4",
                    "sentence": "To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling.",
                    "tag": "3"
                },
                {
                    "index": "79-5",
                    "sentence": "Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-80",
            "text": [
                {
                    "index": "80-0",
                    "sentence": "BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks.",
                    "tag": "1"
                },
                {
                    "index": "80-1",
                    "sentence": "These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data.",
                    "tag": "1"
                },
                {
                    "index": "80-2",
                    "sentence": "Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance.",
                    "tag": "1"
                },
                {
                    "index": "80-3",
                    "sentence": "While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention.",
                    "tag": "1"
                },
                {
                    "index": "80-4",
                    "sentence": "In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation.",
                    "tag": "2"
                },
                {
                    "index": "80-5",
                    "sentence": "Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data.",
                    "tag": "3"
                },
                {
                    "index": "80-6",
                    "sentence": "By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-81",
            "text": [
                {
                    "index": "81-0",
                    "sentence": "AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph.",
                    "tag": "1"
                },
                {
                    "index": "81-1",
                    "sentence": "Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs.",
                    "tag": "1"
                },
                {
                    "index": "81-2",
                    "sentence": "We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation.",
                    "tag": "2"
                },
                {
                    "index": "81-3",
                    "sentence": "As the result, our outputs can better preserve the input meaning than standard decoders.",
                    "tag": "4"
                },
                {
                    "index": "81-4",
                    "sentence": "Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-82",
            "text": [
                {
                    "index": "82-0",
                    "sentence": "Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence.",
                    "tag": "1"
                },
                {
                    "index": "82-1",
                    "sentence": "In this regard, there is often crucial subtext that is derived from the surrounding contexts.",
                    "tag": "1"
                },
                {
                    "index": "82-2",
                    "sentence": "The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts.",
                    "tag": "1"
                },
                {
                    "index": "82-3",
                    "sentence": "In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images.",
                    "tag": "2+3"
                },
                {
                    "index": "82-4",
                    "sentence": "We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies.",
                    "tag": "4"
                },
                {
                    "index": "82-5",
                    "sentence": "Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts.",
                    "tag": "4"
                },
                {
                    "index": "82-6",
                    "sentence": "We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling.",
                    "tag": "4"
                },
                {
                    "index": "82-7",
                    "sentence": "We also demonstrate the effects of interposing new text with missing images during inference.",
                    "tag": "4"
                },
                {
                    "index": "82-8",
                    "sentence": "The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-83",
            "text": [
                {
                    "index": "83-0",
                    "sentence": "We propose a new task in the area of computational creativity: acrostic poem generation in English.",
                    "tag": "2"
                },
                {
                    "index": "83-1",
                    "sentence": "Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase.",
                    "tag": "1"
                },
                {
                    "index": "83-2",
                    "sentence": "We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme.",
                    "tag": "3"
                },
                {
                    "index": "83-3",
                    "sentence": "We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model.",
                    "tag": "4"
                },
                {
                    "index": "83-4",
                    "sentence": "Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems.",
                    "tag": "3"
                },
                {
                    "index": "83-5",
                    "sentence": "Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints.",
                    "tag": "4"
                },
                {
                    "index": "83-6",
                    "sentence": "Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-84",
            "text": [
                {
                    "index": "84-0",
                    "sentence": "Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data.",
                    "tag": "1"
                },
                {
                    "index": "84-1",
                    "sentence": "In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other.",
                    "tag": "2"
                },
                {
                    "index": "84-2",
                    "sentence": "Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate.",
                    "tag": "3"
                },
                {
                    "index": "84-3",
                    "sentence": "Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning.",
                    "tag": "3"
                },
                {
                    "index": "84-4",
                    "sentence": "We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data.",
                    "tag": "3"
                },
                {
                    "index": "84-5",
                    "sentence": "Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines.",
                    "tag": "4"
                },
                {
                    "index": "84-6",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/LADA",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-85",
            "text": [
                {
                    "index": "85-0",
                    "sentence": "Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks.",
                    "tag": "1"
                },
                {
                    "index": "85-1",
                    "sentence": "A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation.",
                    "tag": "1"
                },
                {
                    "index": "85-2",
                    "sentence": "Prior work has used compositional input embeddings based on surface forms to ameliorate this issue.",
                    "tag": "1"
                },
                {
                    "index": "85-3",
                    "sentence": "In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions.",
                    "tag": "2+3"
                },
                {
                    "index": "85-4",
                    "sentence": "To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary.",
                    "tag": "4"
                },
                {
                    "index": "85-5",
                    "sentence": "We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches.",
                    "tag": "4"
                },
                {
                    "index": "85-6",
                    "sentence": "Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-86",
            "text": [
                {
                    "index": "86-0",
                    "sentence": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples.",
                    "tag": "1"
                },
                {
                    "index": "86-1",
                    "sentence": "Data augmentation is a common method used to prevent overfitting and improve OOD generalization.",
                    "tag": "1"
                },
                {
                    "index": "86-2",
                    "sentence": "However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold.",
                    "tag": "1"
                },
                {
                    "index": "86-3",
                    "sentence": "We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold.",
                    "tag": "2+3"
                },
                {
                    "index": "86-4",
                    "sentence": "We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models.",
                    "tag": "3"
                },
                {
                    "index": "86-5",
                    "sentence": "In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-87",
            "text": [
                {
                    "index": "87-0",
                    "sentence": "For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high.",
                    "tag": "1"
                },
                {
                    "index": "87-1",
                    "sentence": "To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution.",
                    "tag": "2+3"
                },
                {
                    "index": "87-2",
                    "sentence": "We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-88",
            "text": [
                {
                    "index": "88-0",
                    "sentence": "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale.",
                    "tag": "1"
                },
                {
                    "index": "88-1",
                    "sentence": "In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN).",
                    "tag": "2"
                },
                {
                    "index": "88-2",
                    "sentence": "It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs.",
                    "tag": "3"
                },
                {
                    "index": "88-3",
                    "sentence": "The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability.",
                    "tag": "3"
                },
                {
                    "index": "88-4",
                    "sentence": "We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-89",
            "text": [
                {
                    "index": "89-0",
                    "sentence": "This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words.",
                    "tag": "2"
                },
                {
                    "index": "89-1",
                    "sentence": "First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency.",
                    "tag": "3"
                },
                {
                    "index": "89-2",
                    "sentence": "Based on the observation, we further propose two methods to address these two factors, respectively.",
                    "tag": "3"
                },
                {
                    "index": "89-3",
                    "sentence": "The larger issue is hubness.",
                    "tag": "4"
                },
                {
                    "index": "89-4",
                    "sentence": "Addressing that improves induction accuracy significantly, especially for low-frequency words.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-90",
            "text": [
                {
                    "index": "90-0",
                    "sentence": "The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models.",
                    "tag": "1"
                },
                {
                    "index": "90-1",
                    "sentence": "However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable.",
                    "tag": "1"
                },
                {
                    "index": "90-2",
                    "sentence": "This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick.",
                    "tag": "1"
                },
                {
                    "index": "90-3",
                    "sentence": "In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models.",
                    "tag": "2"
                },
                {
                    "index": "90-4",
                    "sentence": "This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-91",
            "text": [
                {
                    "index": "91-0",
                    "sentence": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization.",
                    "tag": "1"
                },
                {
                    "index": "91-1",
                    "sentence": "To mitigate this issue, we propose a regularized fine-tuning method.",
                    "tag": "2"
                },
                {
                    "index": "91-2",
                    "sentence": "Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold.",
                    "tag": "3"
                },
                {
                    "index": "91-3",
                    "sentence": "Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration.",
                    "tag": "4"
                },
                {
                    "index": "91-4",
                    "sentence": "(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.",
                    "tag": "4"
                },
                {
                    "index": "91-5",
                    "sentence": "Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets.",
                    "tag": "4+5"
                },
                {
                    "index": "91-6",
                    "sentence": "Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-92",
            "text": [
                {
                    "index": "92-0",
                    "sentence": "The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure.",
                    "tag": "1"
                },
                {
                    "index": "92-1",
                    "sentence": "However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models.",
                    "tag": "1"
                },
                {
                    "index": "92-2",
                    "sentence": "This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling.",
                    "tag": "1"
                },
                {
                    "index": "92-3",
                    "sentence": "We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization.",
                    "tag": "2"
                },
                {
                    "index": "92-4",
                    "sentence": "Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-93",
            "text": [
                {
                    "index": "93-0",
                    "sentence": "Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs.",
                    "tag": "1"
                },
                {
                    "index": "93-1",
                    "sentence": "We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs.",
                    "tag": "2"
                },
                {
                    "index": "93-2",
                    "sentence": "As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding.",
                    "tag": "3"
                },
                {
                    "index": "93-3",
                    "sentence": "Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords.",
                    "tag": "3"
                },
                {
                    "index": "93-4",
                    "sentence": "We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs.",
                    "tag": "4"
                },
                {
                    "index": "93-5",
                    "sentence": "Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging.",
                    "tag": "4"
                },
                {
                    "index": "93-6",
                    "sentence": "The source code is available at https://github.com/abdulrafae/coding_nmt.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-94",
            "text": [
                {
                    "index": "94-0",
                    "sentence": "Typically, machine learning systems solve new tasks by training on thousands of examples.",
                    "tag": "1"
                },
                {
                    "index": "94-1",
                    "sentence": "In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two.",
                    "tag": "1"
                },
                {
                    "index": "94-2",
                    "sentence": "To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area.",
                    "tag": "2"
                },
                {
                    "index": "94-3",
                    "sentence": "We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks.",
                    "tag": "3"
                },
                {
                    "index": "94-4",
                    "sentence": "Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model’s ability to solve each task.",
                    "tag": "3"
                },
                {
                    "index": "94-5",
                    "sentence": "Moreover, the dataset’s structure tests specific types of systematic generalization.",
                    "tag": "3"
                },
                {
                    "index": "94-6",
                    "sentence": "We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-95",
            "text": [
                {
                    "index": "95-0",
                    "sentence": "Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content.",
                    "tag": "1"
                },
                {
                    "index": "95-1",
                    "sentence": "Semantic augmentation is a potential way to alleviate this problem.",
                    "tag": "1"
                },
                {
                    "index": "95-2",
                    "sentence": "Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation.",
                    "tag": "1"
                },
                {
                    "index": "95-3",
                    "sentence": "In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account.",
                    "tag": "2"
                },
                {
                    "index": "95-4",
                    "sentence": "In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.",
                    "tag": "3"
                },
                {
                    "index": "95-5",
                    "sentence": "Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-96",
            "text": [
                {
                    "index": "96-0",
                    "sentence": "The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV).",
                    "tag": "1"
                },
                {
                    "index": "96-1",
                    "sentence": "A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods.",
                    "tag": "1"
                },
                {
                    "index": "96-2",
                    "sentence": "However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task.",
                    "tag": "1"
                },
                {
                    "index": "96-3",
                    "sentence": "Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads.",
                    "tag": "2+3"
                },
                {
                    "index": "96-4",
                    "sentence": "We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively.",
                    "tag": "2+3"
                },
                {
                    "index": "96-5",
                    "sentence": "Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-97",
            "text": [
                {
                    "index": "97-0",
                    "sentence": "Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval.",
                    "tag": "1"
                },
                {
                    "index": "97-1",
                    "sentence": "However, new challenges related to the reliability and validity of social-media-based estimates arise.",
                    "tag": "1"
                },
                {
                    "index": "97-2",
                    "sentence": "Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on social media.",
                    "tag": "1"
                },
                {
                    "index": "97-3",
                    "sentence": "In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets.",
                    "tag": "2"
                },
                {
                    "index": "97-4",
                    "sentence": "We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data.",
                    "tag": "3"
                },
                {
                    "index": "97-5",
                    "sentence": "We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust.",
                    "tag": "4"
                },
                {
                    "index": "97-6",
                    "sentence": "Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from tweets.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-98",
            "text": [
                {
                    "index": "98-0",
                    "sentence": "Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models.",
                    "tag": "1"
                },
                {
                    "index": "98-1",
                    "sentence": "However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect.",
                    "tag": "1"
                },
                {
                    "index": "98-2",
                    "sentence": "As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China.",
                    "tag": "3"
                },
                {
                    "index": "98-3",
                    "sentence": "It is a challenging medical examination with a passing rate of less than 14.2% in 2018.",
                    "tag": "3"
                },
                {
                    "index": "98-4",
                    "sentence": "Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books).",
                    "tag": "2"
                },
                {
                    "index": "98-5",
                    "sentence": "The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-99",
            "text": [
                {
                    "index": "99-0",
                    "sentence": "Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment.",
                    "tag": "1"
                },
                {
                    "index": "99-1",
                    "sentence": "Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists.",
                    "tag": "1"
                },
                {
                    "index": "99-2",
                    "sentence": "Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain.",
                    "tag": "1"
                },
                {
                    "index": "99-3",
                    "sentence": "In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer.",
                    "tag": "2"
                },
                {
                    "index": "99-4",
                    "sentence": "Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations.",
                    "tag": "4"
                },
                {
                    "index": "99-5",
                    "sentence": "Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge.",
                    "tag": "4"
                },
                {
                    "index": "99-6",
                    "sentence": "Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-100",
            "text": [
                {
                    "index": "100-0",
                    "sentence": "Existing approaches to disfluency detection heavily depend on human-annotated data.",
                    "tag": "1"
                },
                {
                    "index": "100-1",
                    "sentence": "Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data.",
                    "tag": "1"
                },
                {
                    "index": "100-2",
                    "sentence": "However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies.",
                    "tag": "1"
                },
                {
                    "index": "100-3",
                    "sentence": "In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments.",
                    "tag": "2+3"
                },
                {
                    "index": "100-4",
                    "sentence": "We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection.",
                    "tag": "3"
                },
                {
                    "index": "100-5",
                    "sentence": "Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-101",
            "text": [
                {
                    "index": "101-0",
                    "sentence": "Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks.",
                    "tag": "1"
                },
                {
                    "index": "101-1",
                    "sentence": "To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task.",
                    "tag": "2"
                },
                {
                    "index": "101-2",
                    "sentence": "In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population.",
                    "tag": "3"
                },
                {
                    "index": "101-3",
                    "sentence": "While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence.",
                    "tag": "3"
                },
                {
                    "index": "101-4",
                    "sentence": "Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets.",
                    "tag": "3"
                },
                {
                    "index": "101-5",
                    "sentence": "Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1.",
                    "tag": "4"
                },
                {
                    "index": "101-6",
                    "sentence": "Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-102",
            "text": [
                {
                    "index": "102-0",
                    "sentence": "Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians.",
                    "tag": "1"
                },
                {
                    "index": "102-1",
                    "sentence": "Further, free-text medical notes may contain information not immediately available in structured variables.",
                    "tag": "1"
                },
                {
                    "index": "102-2",
                    "sentence": "We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively.",
                    "tag": "2"
                },
                {
                    "index": "102-3",
                    "sentence": "We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis.",
                    "tag": "3"
                },
                {
                    "index": "102-4",
                    "sentence": "Finally, we outline a protocol to evaluate model usability in a clinical decision support context.",
                    "tag": "3"
                },
                {
                    "index": "102-5",
                    "sentence": "From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-103",
            "text": [
                {
                    "index": "103-0",
                    "sentence": "Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "103-1",
                    "sentence": "In this paper, we focus on Chinese medical procedure entity normalization.",
                    "tag": "2"
                },
                {
                    "index": "103-2",
                    "sentence": "However, nonstandard Chinese expressions and combined procedures present challenges in our problem.",
                    "tag": "3"
                },
                {
                    "index": "103-3",
                    "sentence": "The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions.",
                    "tag": "3"
                },
                {
                    "index": "103-4",
                    "sentence": "We propose a sequence generative framework to directly generate all the corresponding medical procedure entities.",
                    "tag": "3"
                },
                {
                    "index": "103-5",
                    "sentence": "we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results.",
                    "tag": "3"
                },
                {
                    "index": "103-6",
                    "sentence": "The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-104",
            "text": [
                {
                    "index": "104-0",
                    "sentence": "The extraction of labels from radiology text reports enables large-scale training of medical imaging models.",
                    "tag": "1"
                },
                {
                    "index": "104-1",
                    "sentence": "Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts.",
                    "tag": "1"
                },
                {
                    "index": "104-2",
                    "sentence": "In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations.",
                    "tag": "2"
                },
                {
                    "index": "104-3",
                    "sentence": "We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation.",
                    "tag": "4"
                },
                {
                    "index": "104-4",
                    "sentence": "We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-105",
            "text": [
                {
                    "index": "105-0",
                    "sentence": "Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories.",
                    "tag": "1"
                },
                {
                    "index": "105-1",
                    "sentence": "Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”.",
                    "tag": "1"
                },
                {
                    "index": "105-2",
                    "sentence": "Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction.",
                    "tag": "1"
                },
                {
                    "index": "105-3",
                    "sentence": "In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes.",
                    "tag": "2+3"
                },
                {
                    "index": "105-4",
                    "sentence": "As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-106",
            "text": [
                {
                    "index": "106-0",
                    "sentence": "We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner.",
                    "tag": "2"
                },
                {
                    "index": "106-1",
                    "sentence": "Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other.",
                    "tag": "1"
                },
                {
                    "index": "106-2",
                    "sentence": "However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them.",
                    "tag": "1"
                },
                {
                    "index": "106-3",
                    "sentence": "To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering.",
                    "tag": "2"
                },
                {
                    "index": "106-4",
                    "sentence": "Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-107",
            "text": [
                {
                    "index": "107-0",
                    "sentence": "Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.",
                    "tag": "1"
                },
                {
                    "index": "107-1",
                    "sentence": "This work improves the prediction and annotation of fine-grained semantic divergences.",
                    "tag": "2"
                },
                {
                    "index": "107-2",
                    "sentence": "We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity.",
                    "tag": "2"
                },
                {
                    "index": "107-3",
                    "sentence": "We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.",
                    "tag": "3"
                },
                {
                    "index": "107-4",
                    "sentence": "Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-108",
            "text": [
                {
                    "index": "108-0",
                    "sentence": "Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences.",
                    "tag": "1"
                },
                {
                    "index": "108-1",
                    "sentence": "Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific.",
                    "tag": "1"
                },
                {
                    "index": "108-2",
                    "sentence": "We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors.",
                    "tag": "2"
                },
                {
                    "index": "108-3",
                    "sentence": "Our proposed approach differs from past work on semantic sentence encoding in two ways.",
                    "tag": "3"
                },
                {
                    "index": "108-4",
                    "sentence": "First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time.",
                    "tag": "3"
                },
                {
                    "index": "108-5",
                    "sentence": "Second, we use high-capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings.",
                    "tag": "3"
                },
                {
                    "index": "108-6",
                    "sentence": "In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations.",
                    "tag": "4"
                },
                {
                    "index": "108-7",
                    "sentence": "Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-109",
            "text": [
                {
                    "index": "109-0",
                    "sentence": "Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them.",
                    "tag": "1"
                },
                {
                    "index": "109-1",
                    "sentence": "Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence.",
                    "tag": "1"
                },
                {
                    "index": "109-2",
                    "sentence": "However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages.",
                    "tag": "1"
                },
                {
                    "index": "109-3",
                    "sentence": "Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair.",
                    "tag": "2+3"
                },
                {
                    "index": "109-4",
                    "sentence": "We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-110",
            "text": [
                {
                    "index": "110-0",
                    "sentence": "BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming.",
                    "tag": "1"
                },
                {
                    "index": "110-1",
                    "sentence": "Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed.",
                    "tag": "1"
                },
                {
                    "index": "110-2",
                    "sentence": "However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce.",
                    "tag": "1"
                },
                {
                    "index": "110-3",
                    "sentence": "In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner.",
                    "tag": "2+3"
                },
                {
                    "index": "110-4",
                    "sentence": "Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus.",
                    "tag": "3"
                },
                {
                    "index": "110-5",
                    "sentence": "Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks.",
                    "tag": "4"
                },
                {
                    "index": "110-6",
                    "sentence": "It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-111",
            "text": [
                {
                    "index": "111-0",
                    "sentence": "Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition.",
                    "tag": "1"
                },
                {
                    "index": "111-1",
                    "sentence": "Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases.",
                    "tag": "1"
                },
                {
                    "index": "111-2",
                    "sentence": "Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice.",
                    "tag": "1"
                },
                {
                    "index": "111-3",
                    "sentence": "We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations.",
                    "tag": "2+3"
                },
                {
                    "index": "111-4",
                    "sentence": "Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments.",
                    "tag": "4"
                },
                {
                    "index": "111-5",
                    "sentence": "Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-112",
            "text": [
                {
                    "index": "112-0",
                    "sentence": "Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning.",
                    "tag": "1"
                },
                {
                    "index": "112-1",
                    "sentence": "Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information.",
                    "tag": "1"
                },
                {
                    "index": "112-2",
                    "sentence": "To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer.",
                    "tag": "2+3"
                },
                {
                    "index": "112-3",
                    "sentence": "A method to combine symbolic and linguistic reasoning is also explored for this task.",
                    "tag": "3"
                },
                {
                    "index": "112-4",
                    "sentence": "Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-113",
            "text": [
                {
                    "index": "113-0",
                    "sentence": "Document-level relation extraction aims to extract relations among entities within a document.",
                    "tag": "1"
                },
                {
                    "index": "113-1",
                    "sentence": "Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs.",
                    "tag": "1"
                },
                {
                    "index": "113-2",
                    "sentence": "In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs.",
                    "tag": "2"
                },
                {
                    "index": "113-3",
                    "sentence": "GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG).",
                    "tag": "3"
                },
                {
                    "index": "113-4",
                    "sentence": "The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities.",
                    "tag": "3"
                },
                {
                    "index": "113-5",
                    "sentence": "Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities.",
                    "tag": "4"
                },
                {
                    "index": "113-6",
                    "sentence": "Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art.",
                    "tag": "4"
                },
                {
                    "index": "113-7",
                    "sentence": "Our code is available at https://github.com/PKUnlp-icler/GAIN.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-114",
            "text": [
                {
                    "index": "114-0",
                    "sentence": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts.",
                    "tag": "1"
                },
                {
                    "index": "114-1",
                    "sentence": "Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem.",
                    "tag": "1"
                },
                {
                    "index": "114-2",
                    "sentence": "In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC).",
                    "tag": "2"
                },
                {
                    "index": "114-3",
                    "sentence": "Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results.",
                    "tag": "1+2"
                },
                {
                    "index": "114-4",
                    "sentence": "This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC.",
                    "tag": "3"
                },
                {
                    "index": "114-5",
                    "sentence": "The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods.",
                    "tag": "4+5"
                },
                {
                    "index": "114-6",
                    "sentence": "ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method.",
                    "tag": "4+5"
                },
                {
                    "index": "114-7",
                    "sentence": "iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-115",
            "text": [
                {
                    "index": "115-0",
                    "sentence": "Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text.",
                    "tag": "1"
                },
                {
                    "index": "115-1",
                    "sentence": "Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity.",
                    "tag": "1"
                },
                {
                    "index": "115-2",
                    "sentence": "Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods.",
                    "tag": "1"
                },
                {
                    "index": "115-3",
                    "sentence": "(2) Low coverage.",
                    "tag": "1"
                },
                {
                    "index": "115-4",
                    "sentence": "Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models.",
                    "tag": "1"
                },
                {
                    "index": "115-5",
                    "sentence": "To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types.",
                    "tag": "2+3"
                },
                {
                    "index": "115-6",
                    "sentence": "MAVEN alleviates the data scarcity problem and covers much more general event types.",
                    "tag": "3"
                },
                {
                    "index": "115-7",
                    "sentence": "We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN.",
                    "tag": "3"
                },
                {
                    "index": "115-8",
                    "sentence": "The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts.",
                    "tag": "4+5"
                },
                {
                    "index": "115-9",
                    "sentence": "We also discuss further directions for general domain ED with empirical analyses.",
                    "tag": "5"
                },
                {
                    "index": "115-10",
                    "sentence": "The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-116",
            "text": [
                {
                    "index": "116-0",
                    "sentence": "Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration.",
                    "tag": "1"
                },
                {
                    "index": "116-1",
                    "sentence": "Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results.",
                    "tag": "1"
                },
                {
                    "index": "116-2",
                    "sentence": "These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations.",
                    "tag": "1"
                },
                {
                    "index": "116-3",
                    "sentence": "Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory.",
                    "tag": "1"
                },
                {
                    "index": "116-4",
                    "sentence": "In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment.",
                    "tag": "2"
                },
                {
                    "index": "116-5",
                    "sentence": "Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities.",
                    "tag": "3"
                },
                {
                    "index": "116-6",
                    "sentence": "To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs.",
                    "tag": "3"
                },
                {
                    "index": "116-7",
                    "sentence": "Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-117",
            "text": [
                {
                    "index": "117-0",
                    "sentence": "Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs.",
                    "tag": "1"
                },
                {
                    "index": "117-1",
                    "sentence": "Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries.",
                    "tag": "1"
                },
                {
                    "index": "117-2",
                    "sentence": "This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations.",
                    "tag": "2"
                },
                {
                    "index": "117-3",
                    "sentence": "Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions.",
                    "tag": "3"
                },
                {
                    "index": "117-4",
                    "sentence": "Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations.",
                    "tag": "3"
                },
                {
                    "index": "117-5",
                    "sentence": "This will be more predictive for knowledge acquisition in the few-shot scenario.",
                    "tag": "3"
                },
                {
                    "index": "117-6",
                    "sentence": "Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes.",
                    "tag": "4+5"
                },
                {
                    "index": "117-7",
                    "sentence": "The source code is available at https://github.com/JiaweiSheng/FAAN.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-118",
            "text": [
                {
                    "index": "118-0",
                    "sentence": "In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task.",
                    "tag": "1+2"
                },
                {
                    "index": "118-1",
                    "sentence": "Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model.",
                    "tag": "3"
                },
                {
                    "index": "118-2",
                    "sentence": "To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs.",
                    "tag": "3"
                },
                {
                    "index": "118-3",
                    "sentence": "In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss.",
                    "tag": "3"
                },
                {
                    "index": "118-4",
                    "sentence": "Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-119",
            "text": [
                {
                    "index": "119-0",
                    "sentence": "Named entity recognition and relation extraction are two important fundamental problems.",
                    "tag": "1"
                },
                {
                    "index": "119-1",
                    "sentence": "Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem.",
                    "tag": "1"
                },
                {
                    "index": "119-2",
                    "sentence": "However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space.",
                    "tag": "1"
                },
                {
                    "index": "119-3",
                    "sentence": "We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.",
                    "tag": "2"
                },
                {
                    "index": "119-4",
                    "sentence": "In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.",
                    "tag": "2+3"
                },
                {
                    "index": "119-5",
                    "sentence": "Our experiments confirm the advantages of having two encoders over one encoder.",
                    "tag": "4"
                },
                {
                    "index": "119-6",
                    "sentence": "On several standard datasets, our model shows significant improvements over existing approaches.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-120",
            "text": [
                {
                    "index": "120-0",
                    "sentence": "Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past.",
                    "tag": "1"
                },
                {
                    "index": "120-1",
                    "sentence": "However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead.",
                    "tag": "1"
                },
                {
                    "index": "120-2",
                    "sentence": "Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet.",
                    "tag": "1"
                },
                {
                    "index": "120-3",
                    "sentence": "In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task.",
                    "tag": "2+3"
                },
                {
                    "index": "120-4",
                    "sentence": "Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-121",
            "text": [
                {
                    "index": "121-0",
                    "sentence": "Topic models are a useful analysis tool to uncover the underlying themes within document collections.",
                    "tag": "1"
                },
                {
                    "index": "121-1",
                    "sentence": "The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words.",
                    "tag": "1+2"
                },
                {
                    "index": "121-2",
                    "sentence": "We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA.",
                    "tag": "3"
                },
                {
                    "index": "121-3",
                    "sentence": "The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-122",
            "text": [
                {
                    "index": "122-0",
                    "sentence": "While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS).",
                    "tag": "1"
                },
                {
                    "index": "122-1",
                    "sentence": "We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle.",
                    "tag": "1"
                },
                {
                    "index": "122-2",
                    "sentence": "To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.",
                    "tag": "2+3"
                },
                {
                    "index": "122-3",
                    "sentence": "RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning.",
                    "tag": "4"
                },
                {
                    "index": "122-4",
                    "sentence": "Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy.",
                    "tag": "4"
                },
                {
                    "index": "122-5",
                    "sentence": "Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets.",
                    "tag": "4"
                },
                {
                    "index": "122-6",
                    "sentence": "In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-123",
            "text": [
                {
                    "index": "123-0",
                    "sentence": "Topic models are often used to identify human-interpretable topics to help make sense of large document collections.",
                    "tag": "1"
                },
                {
                    "index": "123-1",
                    "sentence": "We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers.",
                    "tag": "2+3"
                },
                {
                    "index": "123-2",
                    "sentence": "Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence.",
                    "tag": "4"
                },
                {
                    "index": "123-3",
                    "sentence": "We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-124",
            "text": [
                {
                    "index": "124-0",
                    "sentence": "Topic models have been prevailing for many years on discovering latent semantics while modeling long documents.",
                    "tag": "1"
                },
                {
                    "index": "124-1",
                    "sentence": "However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality.",
                    "tag": "1"
                },
                {
                    "index": "124-2",
                    "sentence": "In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts.",
                    "tag": "2+3"
                },
                {
                    "index": "124-3",
                    "sentence": "Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics.",
                    "tag": "2+3"
                },
                {
                    "index": "124-4",
                    "sentence": "We observe that our model can highly improve short text topic modeling performance.",
                    "tag": "4"
                },
                {
                    "index": "124-5",
                    "sentence": "Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-125",
            "text": [
                {
                    "index": "125-0",
                    "sentence": "Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval.",
                    "tag": "1"
                },
                {
                    "index": "125-1",
                    "sentence": "In contrast to other document types, web page designs are intended for easy navigation and information finding.",
                    "tag": "1"
                },
                {
                    "index": "125-2",
                    "sentence": "Effective designs encode within the layout and formatting signals that point to where the important information can be found.",
                    "tag": "1"
                },
                {
                    "index": "125-3",
                    "sentence": "In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task.",
                    "tag": "2+3"
                },
                {
                    "index": "125-4",
                    "sentence": "In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection.",
                    "tag": "3"
                },
                {
                    "index": "125-5",
                    "sentence": "Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models.",
                    "tag": "4+5"
                },
                {
                    "index": "125-6",
                    "sentence": "A qualitative post-hoc analysis illustrates how these features function within the model.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-126",
            "text": [
                {
                    "index": "126-0",
                    "sentence": "Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice.",
                    "tag": "1"
                },
                {
                    "index": "126-1",
                    "sentence": "There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora.",
                    "tag": "1"
                },
                {
                    "index": "126-2",
                    "sentence": "In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain.",
                    "tag": "2"
                },
                {
                    "index": "126-3",
                    "sentence": "Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training.",
                    "tag": "3"
                },
                {
                    "index": "126-4",
                    "sentence": "Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-127",
            "text": [
                {
                    "index": "127-0",
                    "sentence": "The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language.",
                    "tag": "1"
                },
                {
                    "index": "127-1",
                    "sentence": "Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability.",
                    "tag": "1"
                },
                {
                    "index": "127-2",
                    "sentence": "In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample.",
                    "tag": "2"
                },
                {
                    "index": "127-3",
                    "sentence": "Multimodal routing can identify relative importance of both individual modalities and cross-modality factors.",
                    "tag": "4"
                },
                {
                    "index": "127-4",
                    "sentence": "Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-128",
            "text": [
                {
                    "index": "128-0",
                    "sentence": "Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript).",
                    "tag": "1"
                },
                {
                    "index": "128-1",
                    "sentence": "Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs.",
                    "tag": "1"
                },
                {
                    "index": "128-2",
                    "sentence": "Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise.",
                    "tag": "1"
                },
                {
                    "index": "128-3",
                    "sentence": "To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module.",
                    "tag": "2+3"
                },
                {
                    "index": "128-4",
                    "sentence": "Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance.",
                    "tag": "4"
                },
                {
                    "index": "128-5",
                    "sentence": "Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures.",
                    "tag": "4"
                },
                {
                    "index": "128-6",
                    "sentence": "Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-129",
            "text": [
                {
                    "index": "129-0",
                    "sentence": "Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns.",
                    "tag": "1"
                },
                {
                    "index": "129-1",
                    "sentence": "However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos.",
                    "tag": "1"
                },
                {
                    "index": "129-2",
                    "sentence": "To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues.",
                    "tag": "2"
                },
                {
                    "index": "129-3",
                    "sentence": "Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning.",
                    "tag": "3"
                },
                {
                    "index": "129-4",
                    "sentence": "The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting.",
                    "tag": "3"
                },
                {
                    "index": "129-5",
                    "sentence": "The retrieved visual cues are used as contextual information to construct relevant responses to the users.",
                    "tag": "3"
                },
                {
                    "index": "129-6",
                    "sentence": "Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark.",
                    "tag": "4"
                },
                {
                    "index": "129-7",
                    "sentence": "We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-130",
            "text": [
                {
                    "index": "130-0",
                    "sentence": "Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons.",
                    "tag": "1"
                },
                {
                    "index": "130-1",
                    "sentence": "First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only.",
                    "tag": "1"
                },
                {
                    "index": "130-2",
                    "sentence": "Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users.",
                    "tag": "1"
                },
                {
                    "index": "130-3",
                    "sentence": "Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously.",
                    "tag": "2+3"
                },
                {
                    "index": "130-4",
                    "sentence": "We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-131",
            "text": [
                {
                    "index": "131-0",
                    "sentence": "End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs.",
                    "tag": "1"
                },
                {
                    "index": "131-1",
                    "sentence": "There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history.",
                    "tag": "1"
                },
                {
                    "index": "131-2",
                    "sentence": "In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue.",
                    "tag": "2+3"
                },
                {
                    "index": "131-3",
                    "sentence": "To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs.",
                    "tag": "3"
                },
                {
                    "index": "131-4",
                    "sentence": "To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure.",
                    "tag": "3"
                },
                {
                    "index": "131-5",
                    "sentence": "Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-132",
            "text": [
                {
                    "index": "132-0",
                    "sentence": "Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics.",
                    "tag": "1"
                },
                {
                    "index": "132-1",
                    "sentence": "Advancement made in this area is critical for dialogue system design and discourse analysis.",
                    "tag": "1"
                },
                {
                    "index": "132-2",
                    "sentence": "It can also be extended to solve grammatical inference.",
                    "tag": "1"
                },
                {
                    "index": "132-3",
                    "sentence": "In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion.",
                    "tag": "1+2"
                },
                {
                    "index": "132-4",
                    "sentence": "Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias.",
                    "tag": "4"
                },
                {
                    "index": "132-5",
                    "sentence": "Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus.",
                    "tag": "4"
                },
                {
                    "index": "132-6",
                    "sentence": "While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-133",
            "text": [
                {
                    "index": "133-0",
                    "sentence": "In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation.",
                    "tag": "1"
                },
                {
                    "index": "133-1",
                    "sentence": "While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored.",
                    "tag": "1"
                },
                {
                    "index": "133-2",
                    "sentence": "Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.",
                    "tag": "1"
                },
                {
                    "index": "133-3",
                    "sentence": "In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances’ logical structure simultaneously.",
                    "tag": "1+2"
                },
                {
                    "index": "133-4",
                    "sentence": "Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-134",
            "text": [
                {
                    "index": "134-0",
                    "sentence": "Multi-turn response selection is a task designed for developing dialogue agents.",
                    "tag": "1"
                },
                {
                    "index": "134-1",
                    "sentence": "The performance on this task has a remarkable improvement with pre-trained language models.",
                    "tag": "1"
                },
                {
                    "index": "134-2",
                    "sentence": "However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns.",
                    "tag": "1"
                },
                {
                    "index": "134-3",
                    "sentence": "In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations.",
                    "tag": "2"
                },
                {
                    "index": "134-4",
                    "sentence": "Each thread can be regarded as a self-contained sub-dialogue.",
                    "tag": "3"
                },
                {
                    "index": "134-5",
                    "sentence": "We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer.",
                    "tag": "4"
                },
                {
                    "index": "134-6",
                    "sentence": "The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-135",
            "text": [
                {
                    "index": "135-0",
                    "sentence": "The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models.",
                    "tag": "1"
                },
                {
                    "index": "135-1",
                    "sentence": "In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies.",
                    "tag": "2"
                },
                {
                    "index": "135-2",
                    "sentence": "Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies.",
                    "tag": "3"
                },
                {
                    "index": "135-3",
                    "sentence": "The slot-level context is introduced to extract more expressive features for different slots.",
                    "tag": "4"
                },
                {
                    "index": "135-4",
                    "sentence": "And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances.",
                    "tag": "4"
                },
                {
                    "index": "135-5",
                    "sentence": "Empirical studies demonstrated the superiority of the proposed PIN model.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-136",
            "text": [
                {
                    "index": "136-0",
                    "sentence": "Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system.",
                    "tag": "1"
                },
                {
                    "index": "136-1",
                    "sentence": "In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling.",
                    "tag": "1+2"
                },
                {
                    "index": "136-2",
                    "sentence": "Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model.",
                    "tag": "1+2"
                },
                {
                    "index": "136-3",
                    "sentence": "Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77).",
                    "tag": "4"
                },
                {
                    "index": "136-4",
                    "sentence": "In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-137",
            "text": [
                {
                    "index": "137-0",
                    "sentence": "Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale.",
                    "tag": "1"
                },
                {
                    "index": "137-1",
                    "sentence": "Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context.",
                    "tag": "1"
                },
                {
                    "index": "137-2",
                    "sentence": "In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective.",
                    "tag": "1+2"
                },
                {
                    "index": "137-3",
                    "sentence": "Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences.",
                    "tag": "3"
                },
                {
                    "index": "137-4",
                    "sentence": "Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior.",
                    "tag": "3"
                },
                {
                    "index": "137-5",
                    "sentence": "Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales.",
                    "tag": "4"
                },
                {
                    "index": "137-6",
                    "sentence": "Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-138",
            "text": [
                {
                    "index": "138-0",
                    "sentence": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "138-1",
                    "sentence": "However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations.",
                    "tag": "1"
                },
                {
                    "index": "138-2",
                    "sentence": "To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs).",
                    "tag": "1+2"
                },
                {
                    "index": "138-3",
                    "sentence": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age.",
                    "tag": "3"
                },
                {
                    "index": "138-4",
                    "sentence": "In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.",
                    "tag": "3"
                },
                {
                    "index": "138-5",
                    "sentence": "The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.",
                    "tag": "3"
                },
                {
                    "index": "138-6",
                    "sentence": "We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs.",
                    "tag": "4+5"
                },
                {
                    "index": "138-7",
                    "sentence": "As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-139",
            "text": [
                {
                    "index": "139-0",
                    "sentence": "Machine learning techniques have been widely used in natural language processing (NLP).",
                    "tag": "1"
                },
                {
                    "index": "139-1",
                    "sentence": "However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.",
                    "tag": "1"
                },
                {
                    "index": "139-2",
                    "sentence": "Various metrics have been proposed to quantify biases in model predictions.",
                    "tag": "1"
                },
                {
                    "index": "139-3",
                    "sentence": "In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus.",
                    "tag": "1"
                },
                {
                    "index": "139-4",
                    "sentence": "However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model.",
                    "tag": "2"
                },
                {
                    "index": "139-5",
                    "sentence": "In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region.",
                    "tag": "3"
                },
                {
                    "index": "139-6",
                    "sentence": "To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering.",
                    "tag": "2+3"
                },
                {
                    "index": "139-7",
                    "sentence": "Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-140",
            "text": [
                {
                    "index": "140-0",
                    "sentence": "Recurrent neural networks empirically generate natural language with high syntactic fidelity.",
                    "tag": "1"
                },
                {
                    "index": "140-1",
                    "sentence": "However, their success is not well-understood theoretically.",
                    "tag": "1"
                },
                {
                    "index": "140-2",
                    "sentence": "We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax.",
                    "tag": "2+3"
                },
                {
                    "index": "140-3",
                    "sentence": "We introduce Dyck-(k,m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax.",
                    "tag": "3"
                },
                {
                    "index": "140-4",
                    "sentence": "The best known results use O(km⁄2) memory (hidden units) to generate these languages.",
                    "tag": "3"
                },
                {
                    "index": "140-5",
                    "sentence": "We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction.",
                    "tag": "3+4"
                },
                {
                    "index": "140-6",
                    "sentence": "Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-141",
            "text": [
                {
                    "index": "141-0",
                    "sentence": "We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”.",
                    "tag": "1"
                },
                {
                    "index": "141-1",
                    "sentence": "We find that the same biases exist in recent language models like BERT.",
                    "tag": "4"
                },
                {
                    "index": "141-2",
                    "sentence": "While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models.",
                    "tag": "4"
                },
                {
                    "index": "141-3",
                    "sentence": "We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-142",
            "text": [
                {
                    "index": "142-0",
                    "sentence": "Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems.",
                    "tag": "1"
                },
                {
                    "index": "142-1",
                    "sentence": "To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes.",
                    "tag": "2"
                },
                {
                    "index": "142-2",
                    "sentence": "In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets.",
                    "tag": "3"
                },
                {
                    "index": "142-3",
                    "sentence": "VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions).",
                    "tag": "3"
                },
                {
                    "index": "142-4",
                    "sentence": "To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods.",
                    "tag": "3"
                },
                {
                    "index": "142-5",
                    "sentence": "Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible.",
                    "tag": "4"
                },
                {
                    "index": "142-6",
                    "sentence": "We conduct further ablations and analysis to guide future work.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-143",
            "text": [
                {
                    "index": "143-0",
                    "sentence": "Phrase localization is a task that studies the mapping from textual phrases to regions of an image.",
                    "tag": "1"
                },
                {
                    "index": "143-1",
                    "sentence": "Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision.",
                    "tag": "1"
                },
                {
                    "index": "143-2",
                    "sentence": "We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations.",
                    "tag": "2+3"
                },
                {
                    "index": "143-3",
                    "sentence": "By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios.",
                    "tag": "3+4"
                },
                {
                    "index": "143-4",
                    "sentence": "Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods.",
                    "tag": "3+4"
                },
                {
                    "index": "143-5",
                    "sentence": "With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%.",
                    "tag": "4"
                },
                {
                    "index": "143-6",
                    "sentence": "We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-144",
            "text": [
                {
                    "index": "144-0",
                    "sentence": "Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations.",
                    "tag": "1"
                },
                {
                    "index": "144-1",
                    "sentence": "Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts.",
                    "tag": "1"
                },
                {
                    "index": "144-2",
                    "sentence": "In contrast, unlabeled multi-image, multi-sentence documents are abundant.",
                    "tag": "1"
                },
                {
                    "index": "144-3",
                    "sentence": "Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap?",
                    "tag": "1"
                },
                {
                    "index": "144-4",
                    "sentence": "Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as “kitchen” and “bedroom”, and introduce metrics to assess this document similarity.",
                    "tag": "2+3"
                },
                {
                    "index": "144-5",
                    "sentence": "We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset.",
                    "tag": "3+4"
                },
                {
                    "index": "144-6",
                    "sentence": "The proposed method is particularly effective for local contextual meanings of a word, for example associating “granite” with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-145",
            "text": [
                {
                    "index": "145-0",
                    "sentence": "We present HERO, a novel framework for large-scale video+language omni-representation learning.",
                    "tag": "1+2"
                },
                {
                    "index": "145-1",
                    "sentence": "HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer.",
                    "tag": "3"
                },
                {
                    "index": "145-2",
                    "sentence": "In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames.",
                    "tag": "3"
                },
                {
                    "index": "145-3",
                    "sentence": "HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions.",
                    "tag": "4+5"
                },
                {
                    "index": "145-4",
                    "sentence": "Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains.",
                    "tag": "5"
                },
                {
                    "index": "145-5",
                    "sentence": "We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-146",
            "text": [
                {
                    "index": "146-0",
                    "sentence": "Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world.",
                    "tag": "1"
                },
                {
                    "index": "146-1",
                    "sentence": "Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper.",
                    "tag": "1"
                },
                {
                    "index": "146-2",
                    "sentence": "We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora.",
                    "tag": "1"
                },
                {
                    "index": "146-3",
                    "sentence": "Therefore, we develop a technique named “vokenization” that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call “vokens”).",
                    "tag": "2+3"
                },
                {
                    "index": "146-4",
                    "sentence": "The “vokenizer” is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora.",
                    "tag": "3+4"
                },
                {
                    "index": "146-5",
                    "sentence": "Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-147",
            "text": [
                {
                    "index": "147-0",
                    "sentence": "Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem.",
                    "tag": "1"
                },
                {
                    "index": "147-1",
                    "sentence": "Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism.",
                    "tag": "1"
                },
                {
                    "index": "147-2",
                    "sentence": "While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors.",
                    "tag": "1"
                },
                {
                    "index": "147-3",
                    "sentence": "In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions.",
                    "tag": "2"
                },
                {
                    "index": "147-4",
                    "sentence": "To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset.",
                    "tag": "3"
                },
                {
                    "index": "147-5",
                    "sentence": "Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-148",
            "text": [
                {
                    "index": "148-0",
                    "sentence": "Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on.",
                    "tag": "1"
                },
                {
                    "index": "148-1",
                    "sentence": "Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level.",
                    "tag": "1"
                },
                {
                    "index": "148-2",
                    "sentence": "However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure.",
                    "tag": "1"
                },
                {
                    "index": "148-3",
                    "sentence": "In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes.",
                    "tag": "2+3"
                },
                {
                    "index": "148-4",
                    "sentence": "These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms.",
                    "tag": "5"
                },
                {
                    "index": "148-5",
                    "sentence": "Our proposed model is a general framework and can be combined with almost all sequence taggers.",
                    "tag": "5"
                },
                {
                    "index": "148-6",
                    "sentence": "Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-149",
            "text": [
                {
                    "index": "149-0",
                    "sentence": "Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted.",
                    "tag": "1"
                },
                {
                    "index": "149-1",
                    "sentence": "However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection.",
                    "tag": "1"
                },
                {
                    "index": "149-2",
                    "sentence": "In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged.",
                    "tag": "1+2"
                },
                {
                    "index": "149-3",
                    "sentence": "Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters.",
                    "tag": "3"
                },
                {
                    "index": "149-4",
                    "sentence": "To overcome this bottleneck, we leverage a strategy based on knowledge distillation.",
                    "tag": "3"
                },
                {
                    "index": "149-5",
                    "sentence": "Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters.",
                    "tag": "3"
                },
                {
                    "index": "149-6",
                    "sentence": "Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-150",
            "text": [
                {
                    "index": "150-0",
                    "sentence": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval.",
                    "tag": "1"
                },
                {
                    "index": "150-1",
                    "sentence": "While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications.",
                    "tag": "1"
                },
                {
                    "index": "150-2",
                    "sentence": "In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images.",
                    "tag": "2"
                },
                {
                    "index": "150-3",
                    "sentence": "We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given.",
                    "tag": "3"
                },
                {
                    "index": "150-4",
                    "sentence": "Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values.",
                    "tag": "3"
                },
                {
                    "index": "150-5",
                    "sentence": "Moreover, product images have distinct effects on our tasks for different product attributes and values.",
                    "tag": "4"
                },
                {
                    "index": "150-6",
                    "sentence": "Thus, we selectively draw useful visual information from product images to enhance our model.",
                    "tag": "3+4"
                },
                {
                    "index": "150-7",
                    "sentence": "We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task.",
                    "tag": "3+4"
                },
                {
                    "index": "150-8",
                    "sentence": "Our code and dataset are available at https://github.com/jd-aig/JAVE.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-151",
            "text": [
                {
                    "index": "151-0",
                    "sentence": "Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks.",
                    "tag": "1"
                },
                {
                    "index": "151-1",
                    "sentence": "This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies.",
                    "tag": "2"
                },
                {
                    "index": "151-2",
                    "sentence": "The OIX is an OIE friendly expression of a sentence without information loss.",
                    "tag": "4"
                },
                {
                    "index": "151-3",
                    "sentence": "The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems.",
                    "tag": "4"
                },
                {
                    "index": "151-4",
                    "sentence": "Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased.",
                    "tag": "4"
                },
                {
                    "index": "151-5",
                    "sentence": "This paper focuses on the task of OIX and propose a solution – Open Information Annotation (OIA).",
                    "tag": "4"
                },
                {
                    "index": "151-6",
                    "sentence": "OIA is a predicate-function-argument annotation for sentences.",
                    "tag": "4"
                },
                {
                    "index": "151-7",
                    "sentence": "We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences.",
                    "tag": "4"
                },
                {
                    "index": "151-8",
                    "sentence": "The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-152",
            "text": [
                {
                    "index": "152-0",
                    "sentence": "We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model.",
                    "tag": "1+2"
                },
                {
                    "index": "152-1",
                    "sentence": "A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme.",
                    "tag": "3"
                },
                {
                    "index": "152-2",
                    "sentence": "Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases.",
                    "tag": "4+5"
                },
                {
                    "index": "152-3",
                    "sentence": "By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-153",
            "text": [
                {
                    "index": "153-0",
                    "sentence": "AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text.",
                    "tag": "1"
                },
                {
                    "index": "153-1",
                    "sentence": "A key challenge in this task is to efficiently learn effective graph representations.",
                    "tag": "1"
                },
                {
                    "index": "153-2",
                    "sentence": "Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme.",
                    "tag": "1"
                },
                {
                    "index": "153-3",
                    "sentence": "To account for these issues, larger and deeper GCN models are required to capture more complex interactions.",
                    "tag": "1"
                },
                {
                    "index": "153-4",
                    "sentence": "In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs.",
                    "tag": "2+3"
                },
                {
                    "index": "153-5",
                    "sentence": "We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity.",
                    "tag": "3"
                },
                {
                    "index": "153-6",
                    "sentence": "With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity.",
                    "tag": "4"
                },
                {
                    "index": "153-7",
                    "sentence": "Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-154",
            "text": [
                {
                    "index": "154-0",
                    "sentence": "Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results.",
                    "tag": "1"
                },
                {
                    "index": "154-1",
                    "sentence": "Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate.",
                    "tag": "1"
                },
                {
                    "index": "154-2",
                    "sentence": "This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question?",
                    "tag": "1"
                },
                {
                    "index": "154-3",
                    "sentence": "We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy.",
                    "tag": "1+2"
                },
                {
                    "index": "154-4",
                    "sentence": "We find that beam search enforces uniform information density in text, a property motivated by cognitive science.",
                    "tag": "4"
                },
                {
                    "index": "154-5",
                    "sentence": "We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models.",
                    "tag": "4"
                },
                {
                    "index": "154-6",
                    "sentence": "Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-155",
            "text": [
                {
                    "index": "155-0",
                    "sentence": "Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data.",
                    "tag": "1"
                },
                {
                    "index": "155-1",
                    "sentence": "One challenge with end-to-end training of these models is the argmax operation, which has null gradient.",
                    "tag": "1"
                },
                {
                    "index": "155-2",
                    "sentence": "In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem.",
                    "tag": "2"
                },
                {
                    "index": "155-3",
                    "sentence": "We explore latent structure learning through the angle of pulling back the downstream learning objective.",
                    "tag": "3"
                },
                {
                    "index": "155-4",
                    "sentence": "In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT – a variant of STE for structured models.",
                    "tag": "4"
                },
                {
                    "index": "155-5",
                    "sentence": "Our perspective leads to new algorithms in the same family.",
                    "tag": "4"
                },
                {
                    "index": "155-6",
                    "sentence": "We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-156",
            "text": [
                {
                    "index": "156-0",
                    "sentence": "Recent work raises concerns about the use of standard splits to compare natural language processing models.",
                    "tag": "1"
                },
                {
                    "index": "156-1",
                    "sentence": "We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results.",
                    "tag": "2+3"
                },
                {
                    "index": "156-2",
                    "sentence": "We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-157",
            "text": [
                {
                    "index": "157-0",
                    "sentence": "Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL.",
                    "tag": "1"
                },
                {
                    "index": "157-1",
                    "sentence": "However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks.",
                    "tag": "1"
                },
                {
                    "index": "157-2",
                    "sentence": "Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks.",
                    "tag": "1"
                },
                {
                    "index": "157-3",
                    "sentence": "In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models.",
                    "tag": "2"
                },
                {
                    "index": "157-4",
                    "sentence": "We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors.",
                    "tag": "2"
                },
                {
                    "index": "157-5",
                    "sentence": "In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL.",
                    "tag": "3"
                },
                {
                    "index": "157-6",
                    "sentence": "We conduct experiments on two English datasets and one Chinese dataset.",
                    "tag": "3"
                },
                {
                    "index": "157-7",
                    "sentence": "Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions’ consistency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-158",
            "text": [
                {
                    "index": "158-0",
                    "sentence": "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning.",
                    "tag": "2+3"
                },
                {
                    "index": "158-1",
                    "sentence": "Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred.",
                    "tag": "4+5"
                },
                {
                    "index": "158-2",
                    "sentence": "Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks.",
                    "tag": "4"
                },
                {
                    "index": "158-3",
                    "sentence": "Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy.",
                    "tag": "4"
                },
                {
                    "index": "158-4",
                    "sentence": "This confirms that masking can be utilized as an efficient alternative to finetuning.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-159",
            "text": [
                {
                    "index": "159-0",
                    "sentence": "Document-level neural machine translation has yielded attractive improvements.",
                    "tag": "1"
                },
                {
                    "index": "159-1",
                    "sentence": "However, majority of existing methods roughly use all context sentences in a fixed scope.",
                    "tag": "1"
                },
                {
                    "index": "159-2",
                    "sentence": "They neglect the fact that different source sentences need different sizes of context.",
                    "tag": "1"
                },
                {
                    "index": "159-3",
                    "sentence": "To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations.",
                    "tag": "2+3"
                },
                {
                    "index": "159-4",
                    "sentence": "Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence.",
                    "tag": "3"
                },
                {
                    "index": "159-5",
                    "sentence": "Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module.",
                    "tag": "3"
                },
                {
                    "index": "159-6",
                    "sentence": "We train the two modules end-to-end via reinforcement learning.",
                    "tag": "3"
                },
                {
                    "index": "159-7",
                    "sentence": "A novel reward is proposed to encourage the selection and utilization of dynamic context sentences.",
                    "tag": "3"
                },
                {
                    "index": "159-8",
                    "sentence": "Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-160",
            "text": [
                {
                    "index": "160-0",
                    "sentence": "Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models.",
                    "tag": "1"
                },
                {
                    "index": "160-1",
                    "sentence": "However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.",
                    "tag": "1"
                },
                {
                    "index": "160-2",
                    "sentence": "In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution.",
                    "tag": "2+3"
                },
                {
                    "index": "160-3",
                    "sentence": "We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples.",
                    "tag": "3"
                },
                {
                    "index": "160-4",
                    "sentence": "The proposed framework consists of three phases.",
                    "tag": "3"
                },
                {
                    "index": "160-5",
                    "sentence": "First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities.",
                    "tag": "3"
                },
                {
                    "index": "160-6",
                    "sentence": "Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation.",
                    "tag": "3"
                },
                {
                    "index": "160-7",
                    "sentence": "Finally, the rejuvenated examples and the active examples are combined to train the final NMT model.",
                    "tag": "3"
                },
                {
                    "index": "160-8",
                    "sentence": "Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models.",
                    "tag": "4"
                },
                {
                    "index": "160-9",
                    "sentence": "Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-161",
            "text": [
                {
                    "index": "161-0",
                    "sentence": "Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training.",
                    "tag": "1"
                },
                {
                    "index": "161-1",
                    "sentence": "We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model.",
                    "tag": "2"
                },
                {
                    "index": "161-2",
                    "sentence": "Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data.",
                    "tag": "3+4"
                },
                {
                    "index": "161-3",
                    "sentence": "We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset.",
                    "tag": "4"
                },
                {
                    "index": "161-4",
                    "sentence": "Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation.",
                    "tag": "4"
                },
                {
                    "index": "161-5",
                    "sentence": "We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-162",
            "text": [
                {
                    "index": "162-0",
                    "sentence": "Balancing accuracy and latency is a great challenge for simultaneous translation.",
                    "tag": "1"
                },
                {
                    "index": "162-1",
                    "sentence": "To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency.",
                    "tag": "1"
                },
                {
                    "index": "162-2",
                    "sentence": "However, keeping low latency would probably hurt accuracy.",
                    "tag": "1"
                },
                {
                    "index": "162-3",
                    "sentence": "Therefore, it is essential to segment the ASR output into appropriate units for translation.",
                    "tag": "1"
                },
                {
                    "index": "162-4",
                    "sentence": "Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation.",
                    "tag": "2"
                },
                {
                    "index": "162-5",
                    "sentence": "The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation.",
                    "tag": "4"
                },
                {
                    "index": "162-6",
                    "sentence": "Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-163",
            "text": [
                {
                    "index": "163-0",
                    "sentence": "Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks.",
                    "tag": "1"
                },
                {
                    "index": "163-1",
                    "sentence": "However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "163-2",
                    "sentence": "To address the issues, we propose a meta graph learning (MGL) method.",
                    "tag": "2"
                },
                {
                    "index": "163-3",
                    "sentence": "Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages.",
                    "tag": "4"
                },
                {
                    "index": "163-4",
                    "sentence": "Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer.",
                    "tag": "4"
                },
                {
                    "index": "163-5",
                    "sentence": "Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-164",
            "text": [
                {
                    "index": "164-0",
                    "sentence": "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality.",
                    "tag": "1"
                },
                {
                    "index": "164-1",
                    "sentence": "However, cross-language interference and restrained model capacity remain major obstacles.",
                    "tag": "1"
                },
                {
                    "index": "164-2",
                    "sentence": "To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules.",
                    "tag": "2+3"
                },
                {
                    "index": "164-3",
                    "sentence": "This approach enables to learn adapters via language embeddings while sharing model parameters across languages.",
                    "tag": "4"
                },
                {
                    "index": "164-4",
                    "sentence": "It also allows for an easy but effective integration of existing linguistic typology features into the parsing network.",
                    "tag": "4"
                },
                {
                    "index": "164-5",
                    "sentence": "The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach.",
                    "tag": "5"
                },
                {
                    "index": "164-6",
                    "sentence": "Our in-depth analyses show that soft parameter sharing via typological features is key to this success.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-165",
            "text": [
                {
                    "index": "165-0",
                    "sentence": "Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks.",
                    "tag": "1"
                },
                {
                    "index": "165-1",
                    "sentence": "However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved.",
                    "tag": "1"
                },
                {
                    "index": "165-2",
                    "sentence": "In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient.",
                    "tag": "2"
                },
                {
                    "index": "165-3",
                    "sentence": "A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction.",
                    "tag": "3+4"
                },
                {
                    "index": "165-4",
                    "sentence": "In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation.",
                    "tag": "3+4"
                },
                {
                    "index": "165-5",
                    "sentence": "The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-166",
            "text": [
                {
                    "index": "166-0",
                    "sentence": "Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years.",
                    "tag": "1"
                },
                {
                    "index": "166-1",
                    "sentence": "However, most of the existing approaches focus on classification problems.",
                    "tag": "1"
                },
                {
                    "index": "166-2",
                    "sentence": "In this paper, we investigate attacks and defenses for structured prediction tasks in NLP.",
                    "tag": "2"
                },
                {
                    "index": "166-3",
                    "sentence": "Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input.",
                    "tag": "3"
                },
                {
                    "index": "166-4",
                    "sentence": "To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task.",
                    "tag": "3+4"
                },
                {
                    "index": "166-5",
                    "sentence": "Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate.",
                    "tag": "3"
                },
                {
                    "index": "166-6",
                    "sentence": "We evaluate the proposed framework in dependency parsing and part-of-speech tagging.",
                    "tag": "3"
                },
                {
                    "index": "166-7",
                    "sentence": "Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-167",
            "text": [
                {
                    "index": "167-0",
                    "sentence": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment.",
                    "tag": "1"
                },
                {
                    "index": "167-1",
                    "sentence": "Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.",
                    "tag": "1"
                },
                {
                    "index": "167-2",
                    "sentence": "Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach.",
                    "tag": "1"
                },
                {
                    "index": "167-3",
                    "sentence": "However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question.",
                    "tag": "1"
                },
                {
                    "index": "167-4",
                    "sentence": "In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets.",
                    "tag": "1+2"
                },
                {
                    "index": "167-5",
                    "sentence": "Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches.",
                    "tag": "3+4"
                },
                {
                    "index": "167-6",
                    "sentence": "We also conducted extensive experiments to investigate the model effectiveness and robustness.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-168",
            "text": [
                {
                    "index": "168-0",
                    "sentence": "Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible.",
                    "tag": "1"
                },
                {
                    "index": "168-1",
                    "sentence": "The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation.",
                    "tag": "1"
                },
                {
                    "index": "168-2",
                    "sentence": "In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context.",
                    "tag": "2+3"
                },
                {
                    "index": "168-3",
                    "sentence": "To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks.",
                    "tag": "3"
                },
                {
                    "index": "168-4",
                    "sentence": "Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios.",
                    "tag": "4"
                },
                {
                    "index": "168-5",
                    "sentence": "Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-169",
            "text": [
                {
                    "index": "169-0",
                    "sentence": "In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects.",
                    "tag": "1"
                },
                {
                    "index": "169-1",
                    "sentence": "Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences.",
                    "tag": "1"
                },
                {
                    "index": "169-2",
                    "sentence": "Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "169-3",
                    "sentence": "Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole.",
                    "tag": "2"
                },
                {
                    "index": "169-4",
                    "sentence": "We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer.",
                    "tag": "3+4"
                },
                {
                    "index": "169-5",
                    "sentence": "Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline.",
                    "tag": "5"
                },
                {
                    "index": "169-6",
                    "sentence": "The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-170",
            "text": [
                {
                    "index": "170-0",
                    "sentence": "Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces.",
                    "tag": "1"
                },
                {
                    "index": "170-1",
                    "sentence": "In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT.",
                    "tag": "2"
                },
                {
                    "index": "170-2",
                    "sentence": "We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned.",
                    "tag": "3"
                },
                {
                    "index": "170-3",
                    "sentence": "We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra.",
                    "tag": "3"
                },
                {
                    "index": "170-4",
                    "sentence": "We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret.",
                    "tag": "3+4"
                },
                {
                    "index": "170-5",
                    "sentence": "Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-171",
            "text": [
                {
                    "index": "171-0",
                    "sentence": "Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation.",
                    "tag": "1"
                },
                {
                    "index": "171-1",
                    "sentence": "We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source.",
                    "tag": "2+3"
                },
                {
                    "index": "171-2",
                    "sentence": "By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships.",
                    "tag": "3+4"
                },
                {
                    "index": "171-3",
                    "sentence": "We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer.",
                    "tag": "3+4"
                },
                {
                    "index": "171-4",
                    "sentence": "With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-172",
            "text": [
                {
                    "index": "172-0",
                    "sentence": "Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping.",
                    "tag": "1"
                },
                {
                    "index": "172-1",
                    "sentence": "However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business.",
                    "tag": "1"
                },
                {
                    "index": "172-2",
                    "sentence": "To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums.",
                    "tag": "2"
                },
                {
                    "index": "172-3",
                    "sentence": "Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings.",
                    "tag": "3"
                },
                {
                    "index": "172-4",
                    "sentence": "We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem.",
                    "tag": "3"
                },
                {
                    "index": "172-5",
                    "sentence": "Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-173",
            "text": [
                {
                    "index": "173-0",
                    "sentence": "Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage.",
                    "tag": "1"
                },
                {
                    "index": "173-1",
                    "sentence": "However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question.",
                    "tag": "1"
                },
                {
                    "index": "173-2",
                    "sentence": "This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases.",
                    "tag": "1"
                },
                {
                    "index": "173-3",
                    "sentence": "To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task.",
                    "tag": "2+3"
                },
                {
                    "index": "173-4",
                    "sentence": "With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases.",
                    "tag": "4+5"
                },
                {
                    "index": "173-5",
                    "sentence": "We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-174",
            "text": [
                {
                    "index": "174-0",
                    "sentence": "Document interpretation and dialog understanding are the two major challenges for conversational machine reading.",
                    "tag": "1"
                },
                {
                    "index": "174-1",
                    "sentence": "In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog.",
                    "tag": "1+2"
                },
                {
                    "index": "174-2",
                    "sentence": "Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation.",
                    "tag": "3"
                },
                {
                    "index": "174-3",
                    "sentence": "Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information.",
                    "tag": "3"
                },
                {
                    "index": "174-4",
                    "sentence": "Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation.",
                    "tag": "4"
                },
                {
                    "index": "174-5",
                    "sentence": "Code and models are released at https://github.com/Yifan-Gao/Discern.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-175",
            "text": [
                {
                    "index": "175-0",
                    "sentence": "Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models.",
                    "tag": "1"
                },
                {
                    "index": "175-1",
                    "sentence": "Existing approaches to deepfake detection typically represent documents with coarse-grained representations.",
                    "tag": "1"
                },
                {
                    "index": "175-2",
                    "sentence": "However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis.",
                    "tag": "1"
                },
                {
                    "index": "175-3",
                    "sentence": "To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text.",
                    "tag": "2+3"
                },
                {
                    "index": "175-4",
                    "sentence": "Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network.",
                    "tag": "3"
                },
                {
                    "index": "175-5",
                    "sentence": "Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled.",
                    "tag": "3"
                },
                {
                    "index": "175-6",
                    "sentence": "Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa.",
                    "tag": "4"
                },
                {
                    "index": "175-7",
                    "sentence": "Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-176",
            "text": [
                {
                    "index": "176-0",
                    "sentence": "We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English.",
                    "tag": "2+3"
                },
                {
                    "index": "176-1",
                    "sentence": "We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines.",
                    "tag": "3+4"
                },
                {
                    "index": "176-2",
                    "sentence": "We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains.",
                    "tag": "4"
                },
                {
                    "index": "176-3",
                    "sentence": "In addition, we extensively study how to best combine multiple source domains.",
                    "tag": "4"
                },
                {
                    "index": "176-4",
                    "sentence": "We propose to incorporate self-supervised with supervised multi-task learning on all available source domains.",
                    "tag": "4"
                },
                {
                    "index": "176-5",
                    "sentence": "Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "176-6",
                    "sentence": "Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-177",
            "text": [
                {
                    "index": "177-0",
                    "sentence": "Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph.",
                    "tag": "1"
                },
                {
                    "index": "177-1",
                    "sentence": "It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages.",
                    "tag": "1"
                },
                {
                    "index": "177-2",
                    "sentence": "However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting.",
                    "tag": "1"
                },
                {
                    "index": "177-3",
                    "sentence": "In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR.",
                    "tag": "1+2"
                },
                {
                    "index": "177-4",
                    "sentence": "This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing.",
                    "tag": "3"
                },
                {
                    "index": "177-5",
                    "sentence": "The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish.",
                    "tag": "4"
                },
                {
                    "index": "177-6",
                    "sentence": "Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages.",
                    "tag": "4"
                },
                {
                    "index": "177-7",
                    "sentence": "We release XL-AMR at github.com/SapienzaNLP/xl-amr.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-178",
            "text": [
                {
                    "index": "178-0",
                    "sentence": "In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance.",
                    "tag": "1"
                },
                {
                    "index": "178-1",
                    "sentence": "To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing.",
                    "tag": "1"
                },
                {
                    "index": "178-2",
                    "sentence": "However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing.",
                    "tag": "1"
                },
                {
                    "index": "178-3",
                    "sentence": "In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself.",
                    "tag": "2+3"
                },
                {
                    "index": "178-4",
                    "sentence": "Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models.",
                    "tag": "3+4"
                },
                {
                    "index": "178-5",
                    "sentence": "Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art.",
                    "tag": "4"
                },
                {
                    "index": "178-6",
                    "sentence": "The result is very encouraging since we achieve this with seq2seq models rather than complex models.",
                    "tag": "5"
                },
                {
                    "index": "178-7",
                    "sentence": "We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-179",
            "text": [
                {
                    "index": "179-0",
                    "sentence": "The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion.",
                    "tag": "1"
                },
                {
                    "index": "179-1",
                    "sentence": "Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics.",
                    "tag": "1"
                },
                {
                    "index": "179-2",
                    "sentence": "Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task.",
                    "tag": "1"
                },
                {
                    "index": "179-3",
                    "sentence": "In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available.",
                    "tag": "2+3"
                },
                {
                    "index": "179-4",
                    "sentence": "We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-180",
            "text": [
                {
                    "index": "180-0",
                    "sentence": "Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data.",
                    "tag": "1"
                },
                {
                    "index": "180-1",
                    "sentence": "We examine selection bias in hate speech in a language and label independent fashion.",
                    "tag": "2"
                },
                {
                    "index": "180-2",
                    "sentence": "We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora.",
                    "tag": "3"
                },
                {
                    "index": "180-3",
                    "sentence": "We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-181",
            "text": [
                {
                    "index": "181-0",
                    "sentence": "In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions.",
                    "tag": "1"
                },
                {
                    "index": "181-1",
                    "sentence": "Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying.",
                    "tag": "1"
                },
                {
                    "index": "181-2",
                    "sentence": "In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection.",
                    "tag": "2"
                },
                {
                    "index": "181-3",
                    "sentence": "HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors.",
                    "tag": "3"
                },
                {
                    "index": "181-4",
                    "sentence": "Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-182",
            "text": [
                {
                    "index": "182-0",
                    "sentence": "Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data.",
                    "tag": "1"
                },
                {
                    "index": "182-1",
                    "sentence": "We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques.",
                    "tag": "2+3"
                },
                {
                    "index": "182-2",
                    "sentence": "We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features.",
                    "tag": "3"
                },
                {
                    "index": "182-3",
                    "sentence": "The dataset is expected to advance sarcasm detection research.",
                    "tag": "5"
                },
                {
                    "index": "182-4",
                    "sentence": "Our method can be adapted to other affective computing domains, thus opening up new research opportunities.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-183",
            "text": [
                {
                    "index": "183-0",
                    "sentence": "Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle.",
                    "tag": "1"
                },
                {
                    "index": "183-1",
                    "sentence": "In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training.",
                    "tag": "2"
                },
                {
                    "index": "183-2",
                    "sentence": "We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum.",
                    "tag": "4"
                },
                {
                    "index": "183-3",
                    "sentence": "We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance.",
                    "tag": "4"
                },
                {
                    "index": "183-4",
                    "sentence": "We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-184",
            "text": [
                {
                    "index": "184-0",
                    "sentence": "Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train.",
                    "tag": "1"
                },
                {
                    "index": "184-1",
                    "sentence": "These problems can be partially overcome by incorporating a segmentation into tokens in the model.",
                    "tag": "1"
                },
                {
                    "index": "184-2",
                    "sentence": "We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation.",
                    "tag": "2+3"
                },
                {
                    "index": "184-3",
                    "sentence": "We use only the vanilla 6-layer Transformer Base architecture.",
                    "tag": "3"
                },
                {
                    "index": "184-4",
                    "sentence": "Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality.",
                    "tag": "4"
                },
                {
                    "index": "184-5",
                    "sentence": "Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-185",
            "text": [
                {
                    "index": "185-0",
                    "sentence": "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages.",
                    "tag": "1"
                },
                {
                    "index": "185-1",
                    "sentence": "However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios.",
                    "tag": "1"
                },
                {
                    "index": "185-2",
                    "sentence": "In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification.",
                    "tag": "2"
                },
                {
                    "index": "185-3",
                    "sentence": "We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data.",
                    "tag": "4"
                },
                {
                    "index": "185-4",
                    "sentence": "However, we also find settings where this does not hold.",
                    "tag": "4"
                },
                {
                    "index": "185-5",
                    "sentence": "Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-186",
            "text": [
                {
                    "index": "186-0",
                    "sentence": "The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations.",
                    "tag": "1"
                },
                {
                    "index": "186-1",
                    "sentence": "Supervised learning of this QE task requires human evaluation of translation quality as training data.",
                    "tag": "1"
                },
                {
                    "index": "186-2",
                    "sentence": "Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations.",
                    "tag": "1"
                },
                {
                    "index": "186-3",
                    "sentence": "In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations.",
                    "tag": "2+3"
                },
                {
                    "index": "186-4",
                    "sentence": "Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models.",
                    "tag": "3"
                },
                {
                    "index": "186-5",
                    "sentence": "We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-187",
            "text": [
                {
                    "index": "187-0",
                    "sentence": "The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system.",
                    "tag": "1"
                },
                {
                    "index": "187-1",
                    "sentence": "These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system.",
                    "tag": "1"
                },
                {
                    "index": "187-2",
                    "sentence": "This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account.",
                    "tag": "1"
                },
                {
                    "index": "187-3",
                    "sentence": "This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk.",
                    "tag": "2"
                },
                {
                    "index": "187-4",
                    "sentence": "An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario.",
                    "tag": "4"
                },
                {
                    "index": "187-5",
                    "sentence": "Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-188",
            "text": [
                {
                    "index": "188-0",
                    "sentence": "Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources.",
                    "tag": "1"
                },
                {
                    "index": "188-1",
                    "sentence": "Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them.",
                    "tag": "1"
                },
                {
                    "index": "188-2",
                    "sentence": "In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering.",
                    "tag": "1+2"
                },
                {
                    "index": "188-3",
                    "sentence": "With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before.",
                    "tag": "3"
                },
                {
                    "index": "188-4",
                    "sentence": "Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation.",
                    "tag": "4"
                },
                {
                    "index": "188-5",
                    "sentence": "We also evaluate on a new test set of 1000 pairs made with extensive quality control.",
                    "tag": "4"
                },
                {
                    "index": "188-6",
                    "sentence": "We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status.",
                    "tag": "4"
                },
                {
                    "index": "188-7",
                    "sentence": "To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation.",
                    "tag": "5"
                },
                {
                    "index": "188-8",
                    "sentence": "We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages.",
                    "tag": "5"
                },
                {
                    "index": "188-9",
                    "sentence": "Our data and code are available at https://github.com/csebuetnlp/banglanmt.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-189",
            "text": [
                {
                    "index": "189-0",
                    "sentence": "This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT).",
                    "tag": "1+2"
                },
                {
                    "index": "189-1",
                    "sentence": "Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language.",
                    "tag": "3"
                },
                {
                    "index": "189-2",
                    "sentence": "Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons.",
                    "tag": "3"
                },
                {
                    "index": "189-3",
                    "sentence": "CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence.",
                    "tag": "3"
                },
                {
                    "index": "189-4",
                    "sentence": "In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus.",
                    "tag": "3"
                },
                {
                    "index": "189-5",
                    "sentence": "Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].",
                    "tag": "3"
                },
                {
                    "index": "189-6",
                    "sentence": "To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT.",
                    "tag": "3"
                },
                {
                    "index": "189-7",
                    "sentence": "Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-190",
            "text": [
                {
                    "index": "190-0",
                    "sentence": "The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male doctors.",
                    "tag": "1"
                },
                {
                    "index": "190-1",
                    "sentence": "We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of ‘the doctor removed his mask’ is not ambiguous between a coreferential reading and a disjoint reading.",
                    "tag": "1"
                },
                {
                    "index": "190-2",
                    "sentence": "Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive.",
                    "tag": "1"
                },
                {
                    "index": "190-3",
                    "sentence": "We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon.",
                    "tag": "2"
                },
                {
                    "index": "190-4",
                    "sentence": "We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-191",
            "text": [
                {
                    "index": "191-0",
                    "sentence": "We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs?",
                    "tag": "1"
                },
                {
                    "index": "191-1",
                    "sentence": "We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model.",
                    "tag": "2"
                },
                {
                    "index": "191-2",
                    "sentence": "Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space.",
                    "tag": "3"
                },
                {
                    "index": "191-3",
                    "sentence": "We pre-train a mRASP model on 32 language pairs jointly with only public datasets.",
                    "tag": "3"
                },
                {
                    "index": "191-4",
                    "sentence": "The model is then fine-tuned on downstream language pairs to obtain specialized MT models.",
                    "tag": "3"
                },
                {
                    "index": "191-5",
                    "sentence": "We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs.",
                    "tag": "3"
                },
                {
                    "index": "191-6",
                    "sentence": "Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs.",
                    "tag": "4"
                },
                {
                    "index": "191-7",
                    "sentence": "It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT.",
                    "tag": "4"
                },
                {
                    "index": "191-8",
                    "sentence": "Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus.",
                    "tag": "5"
                },
                {
                    "index": "191-9",
                    "sentence": "Code, data, and pre-trained models are available at https://github.com/linzehui/mRASP.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-192",
            "text": [
                {
                    "index": "192-0",
                    "sentence": "The attention mechanism is the crucial component of the transformer architecture.",
                    "tag": "1"
                },
                {
                    "index": "192-1",
                    "sentence": "Recent research shows that most attention heads are not confident in their decisions and can be pruned.",
                    "tag": "1"
                },
                {
                    "index": "192-2",
                    "sentence": "However, removing them before training a model results in lower quality.",
                    "tag": "1"
                },
                {
                    "index": "192-3",
                    "sentence": "In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training.",
                    "tag": "1+2"
                },
                {
                    "index": "192-4",
                    "sentence": "Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English.",
                    "tag": "4"
                },
                {
                    "index": "192-5",
                    "sentence": "The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training.",
                    "tag": "4"
                },
                {
                    "index": "192-6",
                    "sentence": "Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-193",
            "text": [
                {
                    "index": "193-0",
                    "sentence": "Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.",
                    "tag": "1"
                },
                {
                    "index": "193-1",
                    "sentence": "Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers.",
                    "tag": "1"
                },
                {
                    "index": "193-2",
                    "sentence": "In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt).",
                    "tag": "1+2"
                },
                {
                    "index": "193-3",
                    "sentence": "During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated.",
                    "tag": "3"
                },
                {
                    "index": "193-4",
                    "sentence": "Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments.",
                    "tag": "3"
                },
                {
                    "index": "193-5",
                    "sentence": "Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-194",
            "text": [
                {
                    "index": "194-0",
                    "sentence": "We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements.",
                    "tag": "1+2"
                },
                {
                    "index": "194-1",
                    "sentence": "Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality.",
                    "tag": "3"
                },
                {
                    "index": "194-2",
                    "sentence": "To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric.",
                    "tag": "3"
                },
                {
                    "index": "194-3",
                    "sentence": "Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-195",
            "text": [
                {
                    "index": "195-0",
                    "sentence": "Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results.",
                    "tag": "1"
                },
                {
                    "index": "195-1",
                    "sentence": "When limited data is available for one language, however, this method leads to poor translations.",
                    "tag": "1"
                },
                {
                    "index": "195-2",
                    "sentence": "We present an effective approach that reuses an LM that is pretrained only on the high-resource language.",
                    "tag": "2"
                },
                {
                    "index": "195-3",
                    "sentence": "The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model.",
                    "tag": "3"
                },
                {
                    "index": "195-4",
                    "sentence": "To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language.",
                    "tag": "3"
                },
                {
                    "index": "195-5",
                    "sentence": "We therefore propose a novel vocabulary extension method.",
                    "tag": "4"
                },
                {
                    "index": "195-6",
                    "sentence": "Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-196",
            "text": [
                {
                    "index": "196-0",
                    "sentence": "Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic).",
                    "tag": "1"
                },
                {
                    "index": "196-1",
                    "sentence": "However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages.",
                    "tag": "1"
                },
                {
                    "index": "196-2",
                    "sentence": "In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI.",
                    "tag": "2"
                },
                {
                    "index": "196-3",
                    "sentence": "Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders.",
                    "tag": "3"
                },
                {
                    "index": "196-4",
                    "sentence": "Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin.",
                    "tag": "4"
                },
                {
                    "index": "196-5",
                    "sentence": "Ablation studies show the importance of different model components and the necessity of non-linear mapping.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-197",
            "text": [
                {
                    "index": "197-0",
                    "sentence": "As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other.",
                    "tag": "1"
                },
                {
                    "index": "197-1",
                    "sentence": "However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference.",
                    "tag": "1"
                },
                {
                    "index": "197-2",
                    "sentence": "This leads to a discrepancy of the data distribution between the training and the inference phases.",
                    "tag": "1"
                },
                {
                    "index": "197-3",
                    "sentence": "To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations.",
                    "tag": "2+3"
                },
                {
                    "index": "197-4",
                    "sentence": "Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-198",
            "text": [
                {
                    "index": "198-0",
                    "sentence": "A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts.",
                    "tag": "1"
                },
                {
                    "index": "198-1",
                    "sentence": "This paper proposes a method of parsing sentences with gapping to recover elided elements.",
                    "tag": "1+2"
                },
                {
                    "index": "198-2",
                    "sentence": "The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements.",
                    "tag": "3"
                },
                {
                    "index": "198-3",
                    "sentence": "Our method outperforms the previous method in terms of F-measure and recall.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-199",
            "text": [
                {
                    "index": "199-0",
                    "sentence": "We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures.",
                    "tag": "1+2"
                },
                {
                    "index": "199-1",
                    "sentence": "In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3).",
                    "tag": "2"
                },
                {
                    "index": "199-2",
                    "sentence": "The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers.",
                    "tag": "4"
                },
                {
                    "index": "199-3",
                    "sentence": "We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting.",
                    "tag": "4"
                },
                {
                    "index": "199-4",
                    "sentence": "We also experiment with pre-trained word embeddings and Bert-based neural networks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-200",
            "text": [
                {
                    "index": "200-0",
                    "sentence": "Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences.",
                    "tag": "1"
                },
                {
                    "index": "200-1",
                    "sentence": "We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data?",
                    "tag": "1"
                },
                {
                    "index": "200-2",
                    "sentence": "We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-201",
            "text": [
                {
                    "index": "201-0",
                    "sentence": "This paper reduces discontinuous parsing to sequence labeling.",
                    "tag": "1+2"
                },
                {
                    "index": "201-1",
                    "sentence": "It first shows that existing reductions for constituent parsing as labeling do not support discontinuities.",
                    "tag": "3"
                },
                {
                    "index": "201-2",
                    "sentence": "Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence.",
                    "tag": "3"
                },
                {
                    "index": "201-3",
                    "sentence": "Third, it studies whether such discontinuous representations are learnable.",
                    "tag": "3"
                },
                {
                    "index": "201-4",
                    "sentence": "The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-202",
            "text": [
                {
                    "index": "202-0",
                    "sentence": "This paper focuses on tree-based modeling for the sentence classification task.",
                    "tag": "2"
                },
                {
                    "index": "202-1",
                    "sentence": "In existing works, aggregating on a syntax tree usually considers local information of sub-trees.",
                    "tag": "3"
                },
                {
                    "index": "202-2",
                    "sentence": "In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees.",
                    "tag": "3"
                },
                {
                    "index": "202-3",
                    "sentence": "In MSNN, each node of a syntax tree is modeled by a label-related syntax module.",
                    "tag": "3"
                },
                {
                    "index": "202-4",
                    "sentence": "Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation.",
                    "tag": "3"
                },
                {
                    "index": "202-5",
                    "sentence": "We design a tree-parallel mini-batch strategy for efficient training and predicting.",
                    "tag": "3"
                },
                {
                    "index": "202-6",
                    "sentence": "Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-203",
            "text": [
                {
                    "index": "203-0",
                    "sentence": "This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays.",
                    "tag": "1+2"
                },
                {
                    "index": "203-1",
                    "sentence": "Specifically, we focus on two issues.",
                    "tag": "2"
                },
                {
                    "index": "203-2",
                    "sentence": "First, we propose structural sentence positional encodings to explicitly represent sentence positions.",
                    "tag": "2"
                },
                {
                    "index": "203-3",
                    "sentence": "Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation.",
                    "tag": "2+3"
                },
                {
                    "index": "203-4",
                    "sentence": "We conduct experiments on two datasets: a Chinese dataset and an English dataset.",
                    "tag": "3"
                },
                {
                    "index": "203-5",
                    "sentence": "We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-204",
            "text": [
                {
                    "index": "204-0",
                    "sentence": "Existing pre-trained large language models have shown unparalleled generative capabilities.",
                    "tag": "1"
                },
                {
                    "index": "204-1",
                    "sentence": "However, they are not controllable.",
                    "tag": "1"
                },
                {
                    "index": "204-2",
                    "sentence": "In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base.",
                    "tag": "2+3"
                },
                {
                    "index": "204-3",
                    "sentence": "Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator.",
                    "tag": "3"
                },
                {
                    "index": "204-4",
                    "sentence": "As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding.",
                    "tag": "3"
                },
                {
                    "index": "204-5",
                    "sentence": "The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset.",
                    "tag": "4"
                },
                {
                    "index": "204-6",
                    "sentence": "We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process.",
                    "tag": "4"
                },
                {
                    "index": "204-7",
                    "sentence": "Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords.",
                    "tag": "4"
                },
                {
                    "index": "204-8",
                    "sentence": "Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-205",
            "text": [
                {
                    "index": "205-0",
                    "sentence": "Recent years the task of incomplete utterance rewriting has raised a large attention.",
                    "tag": "1"
                },
                {
                    "index": "205-1",
                    "sentence": "Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism.",
                    "tag": "1"
                },
                {
                    "index": "205-2",
                    "sentence": "In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task.",
                    "tag": "2"
                },
                {
                    "index": "205-3",
                    "sentence": "Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix.",
                    "tag": "3"
                },
                {
                    "index": "205-4",
                    "sentence": "Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets.",
                    "tag": "4"
                },
                {
                    "index": "205-5",
                    "sentence": "Furthermore, our approach is four times faster than the standard approach in inference.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-206",
            "text": [
                {
                    "index": "206-0",
                    "sentence": "A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one.",
                    "tag": "1"
                },
                {
                    "index": "206-1",
                    "sentence": "However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand.",
                    "tag": "1"
                },
                {
                    "index": "206-2",
                    "sentence": "We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set.",
                    "tag": "2+3"
                },
                {
                    "index": "206-3",
                    "sentence": "Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-207",
            "text": [
                {
                    "index": "207-0",
                    "sentence": "Punning is a creative way to make conversation enjoyable and literary writing elegant.",
                    "tag": "1"
                },
                {
                    "index": "207-1",
                    "sentence": "In this paper, we focus on the task of generating a pun sentence given a pair of homophones.",
                    "tag": "2"
                },
                {
                    "index": "207-2",
                    "sentence": "We first find the constraint words supporting the semantic incongruity for a sentence.",
                    "tag": "3"
                },
                {
                    "index": "207-3",
                    "sentence": "Then we rewrite the sentence with explicit positive and negative constraints.",
                    "tag": "3"
                },
                {
                    "index": "207-4",
                    "sentence": "Our model achieves the state-of-the-art results in both automatic and human evaluations.",
                    "tag": "4"
                },
                {
                    "index": "207-5",
                    "sentence": "We further make an error analysis and discuss the challenges for the computational pun models.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-208",
            "text": [
                {
                    "index": "208-0",
                    "sentence": "Neural Natural Language Generation (NLG) systems are well known for their unreliability.",
                    "tag": "1"
                },
                {
                    "index": "208-1",
                    "sentence": "To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability.",
                    "tag": "2"
                },
                {
                    "index": "208-2",
                    "sentence": "While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task.",
                    "tag": "4"
                },
                {
                    "index": "208-3",
                    "sentence": "The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-209",
            "text": [
                {
                    "index": "209-0",
                    "sentence": "Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output.",
                    "tag": "1"
                },
                {
                    "index": "209-1",
                    "sentence": "Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties.",
                    "tag": "1"
                },
                {
                    "index": "209-2",
                    "sentence": "In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English.",
                    "tag": "2"
                },
                {
                    "index": "209-3",
                    "sentence": "We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages.",
                    "tag": "3"
                },
                {
                    "index": "209-4",
                    "sentence": "Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics.",
                    "tag": "4"
                },
                {
                    "index": "209-5",
                    "sentence": "We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-210",
            "text": [
                {
                    "index": "210-0",
                    "sentence": "Bolukbasi et al.(2016) presents one of the first gender bias mitigation techniques for word embeddings.",
                    "tag": "1"
                },
                {
                    "index": "210-1",
                    "sentence": "Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings.",
                    "tag": "1"
                },
                {
                    "index": "210-2",
                    "sentence": "As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings.",
                    "tag": "1"
                },
                {
                    "index": "210-3",
                    "sentence": "However, an implicit and untested assumption of their method is that the bias subspace is actually linear.",
                    "tag": "1"
                },
                {
                    "index": "210-4",
                    "sentence": "In this work, we generalize their method to a kernelized, non-linear version.",
                    "tag": "2"
                },
                {
                    "index": "210-5",
                    "sentence": "We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique.",
                    "tag": "4"
                },
                {
                    "index": "210-6",
                    "sentence": "We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear.",
                    "tag": "4"
                },
                {
                    "index": "210-7",
                    "sentence": "Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al.(2016).",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-211",
            "text": [
                {
                    "index": "211-0",
                    "sentence": "It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts.",
                    "tag": "1"
                },
                {
                    "index": "211-1",
                    "sentence": "To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation.",
                    "tag": "2"
                },
                {
                    "index": "211-2",
                    "sentence": "Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation.",
                    "tag": "3"
                },
                {
                    "index": "211-3",
                    "sentence": "Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge.",
                    "tag": "4"
                },
                {
                    "index": "211-4",
                    "sentence": "Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-212",
            "text": [
                {
                    "index": "212-0",
                    "sentence": "To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems.",
                    "tag": "1"
                },
                {
                    "index": "212-1",
                    "sentence": "In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model.",
                    "tag": "2"
                },
                {
                    "index": "212-2",
                    "sentence": "Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model.",
                    "tag": "3"
                },
                {
                    "index": "212-3",
                    "sentence": "This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient.",
                    "tag": "3"
                },
                {
                    "index": "212-4",
                    "sentence": "We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-213",
            "text": [
                {
                    "index": "213-0",
                    "sentence": "Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples.",
                    "tag": "1"
                },
                {
                    "index": "213-1",
                    "sentence": "It becomes more difficult in multi-label classification, where each instance is labelled with more than one class.",
                    "tag": "1"
                },
                {
                    "index": "213-2",
                    "sentence": "In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification.",
                    "tag": "1+2"
                },
                {
                    "index": "213-3",
                    "sentence": "The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations.",
                    "tag": "3"
                },
                {
                    "index": "213-4",
                    "sentence": "Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-214",
            "text": [
                {
                    "index": "214-0",
                    "sentence": "One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment.",
                    "tag": "1"
                },
                {
                    "index": "214-1",
                    "sentence": "Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors.",
                    "tag": "1"
                },
                {
                    "index": "214-2",
                    "sentence": "We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity.",
                    "tag": "2"
                },
                {
                    "index": "214-3",
                    "sentence": "However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance.",
                    "tag": "3"
                },
                {
                    "index": "214-4",
                    "sentence": "Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover’s distance (optimal transport), which we refer to as word rotator’s distance.",
                    "tag": "4"
                },
                {
                    "index": "214-5",
                    "sentence": "Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method.",
                    "tag": "5"
                },
                {
                    "index": "214-6",
                    "sentence": "On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines.",
                    "tag": "5"
                },
                {
                    "index": "214-7",
                    "sentence": "The source code is avaliable at https://github.com/eumesy/wrd",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-215",
            "text": [
                {
                    "index": "215-0",
                    "sentence": "Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data.",
                    "tag": "1"
                },
                {
                    "index": "215-1",
                    "sentence": "However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data.",
                    "tag": "1"
                },
                {
                    "index": "215-2",
                    "sentence": "To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge.",
                    "tag": "2"
                },
                {
                    "index": "215-3",
                    "sentence": "Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human’s ability to learn procedural knowledge.",
                    "tag": "2+3"
                },
                {
                    "index": "215-4",
                    "sentence": "The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",
                    "tag": "4"
                },
                {
                    "index": "215-5",
                    "sentence": "The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-216",
            "text": [
                {
                    "index": "216-0",
                    "sentence": "Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations.",
                    "tag": "1"
                },
                {
                    "index": "216-1",
                    "sentence": "However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance.",
                    "tag": "1"
                },
                {
                    "index": "216-2",
                    "sentence": "In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment.",
                    "tag": "2"
                },
                {
                    "index": "216-3",
                    "sentence": "We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively.",
                    "tag": "3"
                },
                {
                    "index": "216-4",
                    "sentence": "Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models.",
                    "tag": "3"
                },
                {
                    "index": "216-5",
                    "sentence": "Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport.",
                    "tag": "3"
                },
                {
                    "index": "216-6",
                    "sentence": "Experimental results on MUSE and VecMap datasets show significant improvement of our models.",
                    "tag": "4"
                },
                {
                    "index": "216-7",
                    "sentence": "Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance.",
                    "tag": "5"
                },
                {
                    "index": "216-8",
                    "sentence": "Results on distant language pairs further illustrate the advantage and robustness of our proposed method.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-217",
            "text": [
                {
                    "index": "217-0",
                    "sentence": "One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned.",
                    "tag": "1"
                },
                {
                    "index": "217-1",
                    "sentence": "In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable.",
                    "tag": "1"
                },
                {
                    "index": "217-2",
                    "sentence": "The phenomenon, however, is often overlooked in existing matching models.",
                    "tag": "1"
                },
                {
                    "index": "217-3",
                    "sentence": "As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions.",
                    "tag": "1"
                },
                {
                    "index": "217-4",
                    "sentence": "In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match.",
                    "tag": "2+3"
                },
                {
                    "index": "217-5",
                    "sentence": "In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains.",
                    "tag": "3"
                },
                {
                    "index": "217-6",
                    "sentence": "As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated.",
                    "tag": "3"
                },
                {
                    "index": "217-7",
                    "sentence": "The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance.",
                    "tag": "4"
                },
                {
                    "index": "217-8",
                    "sentence": "WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model.",
                    "tag": "3"
                },
                {
                    "index": "217-9",
                    "sentence": "Four popular text matching methods have been exploited in the paper.",
                    "tag": "3"
                },
                {
                    "index": "217-10",
                    "sentence": "Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-218",
            "text": [
                {
                    "index": "218-0",
                    "sentence": "Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages.",
                    "tag": "1"
                },
                {
                    "index": "218-1",
                    "sentence": "A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space.",
                    "tag": "1"
                },
                {
                    "index": "218-2",
                    "sentence": "In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques.",
                    "tag": "2+3"
                },
                {
                    "index": "218-3",
                    "sentence": "We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing.",
                    "tag": "4"
                },
                {
                    "index": "218-4",
                    "sentence": "When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-219",
            "text": [
                {
                    "index": "219-0",
                    "sentence": "We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes.",
                    "tag": "1"
                },
                {
                    "index": "219-1",
                    "sentence": "To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization.",
                    "tag": "2+3"
                },
                {
                    "index": "219-2",
                    "sentence": "Our method first trains a base model using Q-learning, which typically overfits the training games.",
                    "tag": "3"
                },
                {
                    "index": "219-3",
                    "sentence": "The base model’s action token distribution is used to perform observation pruning that removes irrelevant tokens.",
                    "tag": "3"
                },
                {
                    "index": "219-4",
                    "sentence": "A second bootstrapped model is then retrained on the pruned observation text.",
                    "tag": "3"
                },
                {
                    "index": "219-5",
                    "sentence": "Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-220",
            "text": [
                {
                    "index": "220-0",
                    "sentence": "Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks.",
                    "tag": "1"
                },
                {
                    "index": "220-1",
                    "sentence": "However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices.",
                    "tag": "1"
                },
                {
                    "index": "220-2",
                    "sentence": "In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers.",
                    "tag": "2"
                },
                {
                    "index": "220-3",
                    "sentence": "In this way, our model can learn from different teacher layers adaptively for different NLP tasks.",
                    "tag": "3"
                },
                {
                    "index": "220-4",
                    "sentence": "In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network.",
                    "tag": "3"
                },
                {
                    "index": "220-5",
                    "sentence": "EMD enables effective matching for the many-to-many layer mapping.",
                    "tag": "4"
                },
                {
                    "index": "220-6",
                    "sentence": "Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model’s performance and accelerate convergence time.",
                    "tag": "2+3"
                },
                {
                    "index": "220-7",
                    "sentence": "Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-221",
            "text": [
                {
                    "index": "221-0",
                    "sentence": "Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST).",
                    "tag": "1"
                },
                {
                    "index": "221-1",
                    "sentence": "Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence.",
                    "tag": "1"
                },
                {
                    "index": "221-2",
                    "sentence": "In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN.",
                    "tag": "2"
                },
                {
                    "index": "221-3",
                    "sentence": "Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value.",
                    "tag": "2+3"
                },
                {
                    "index": "221-4",
                    "sentence": "SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span.",
                    "tag": "3"
                },
                {
                    "index": "221-5",
                    "sentence": "VN is designed specifically for the use of ontology, which can convert supporting spans to the values.",
                    "tag": "3"
                },
                {
                    "index": "221-6",
                    "sentence": "Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1.",
                    "tag": "4"
                },
                {
                    "index": "221-7",
                    "sentence": "Besides, we evaluate VN with incomplete ontology.",
                    "tag": "4"
                },
                {
                    "index": "221-8",
                    "sentence": "The results show that even if only 30% ontology is used, VN can also contribute to our model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-222",
            "text": [
                {
                    "index": "222-0",
                    "sentence": "Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer.",
                    "tag": "1"
                },
                {
                    "index": "222-1",
                    "sentence": "Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader.",
                    "tag": "1"
                },
                {
                    "index": "222-2",
                    "sentence": "However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost.",
                    "tag": "1"
                },
                {
                    "index": "222-3",
                    "sentence": "To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read.",
                    "tag": "2+3"
                },
                {
                    "index": "222-4",
                    "sentence": "We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability.",
                    "tag": "3"
                },
                {
                    "index": "222-5",
                    "sentence": "We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning.",
                    "tag": "3"
                },
                {
                    "index": "222-6",
                    "sentence": "Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-223",
            "text": [
                {
                    "index": "223-0",
                    "sentence": "Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives.",
                    "tag": "1"
                },
                {
                    "index": "223-1",
                    "sentence": "Prior work has largely tried to do this either symbolically or with black-box transformers.",
                    "tag": "1"
                },
                {
                    "index": "223-2",
                    "sentence": "We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning.",
                    "tag": "2"
                },
                {
                    "index": "223-3",
                    "sentence": "This model first finds relevant sentences in the context and then chains them together using neural modules.",
                    "tag": "3"
                },
                {
                    "index": "223-4",
                    "sentence": "Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-224",
            "text": [
                {
                    "index": "224-0",
                    "sentence": "State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive.",
                    "tag": "1"
                },
                {
                    "index": "224-1",
                    "sentence": "For this reason, customizing QA systems is challenging.",
                    "tag": "1"
                },
                {
                    "index": "224-2",
                    "sentence": "As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme.",
                    "tag": "2+3"
                },
                {
                    "index": "224-3",
                    "sentence": "The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations.",
                    "tag": "2"
                },
                {
                    "index": "224-4",
                    "sentence": "Human annotators then simply provide binary feedback on these candidates.",
                    "tag": "3"
                },
                {
                    "index": "224-5",
                    "sentence": "Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost.",
                    "tag": "3"
                },
                {
                    "index": "224-6",
                    "sentence": "To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost.",
                    "tag": "5"
                },
                {
                    "index": "224-7",
                    "sentence": "We compare our framework against traditional manual annotations in an extensive set of experiments.",
                    "tag": "3"
                },
                {
                    "index": "224-8",
                    "sentence": "We find that our approach can reduce up to 21.1% of the annotation cost.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-225",
            "text": [
                {
                    "index": "225-0",
                    "sentence": "This paper focuses on machine reading comprehension for narrative passages.",
                    "tag": "1"
                },
                {
                    "index": "225-1",
                    "sentence": "Narrative passages usually describe a chain of events.",
                    "tag": "1"
                },
                {
                    "index": "225-2",
                    "sentence": "When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively.",
                    "tag": "1"
                },
                {
                    "index": "225-3",
                    "sentence": "Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension.",
                    "tag": "2"
                },
                {
                    "index": "225-4",
                    "sentence": "Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph.",
                    "tag": "2+3"
                },
                {
                    "index": "225-5",
                    "sentence": "We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice.",
                    "tag": "3"
                },
                {
                    "index": "225-6",
                    "sentence": "Our method achieves state-of-the-art.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-226",
            "text": [
                {
                    "index": "226-0",
                    "sentence": "Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.",
                    "tag": "1"
                },
                {
                    "index": "226-1",
                    "sentence": "However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.",
                    "tag": "1"
                },
                {
                    "index": "226-2",
                    "sentence": "Naturally, models that return single spans cannot answer these questions.",
                    "tag": "1"
                },
                {
                    "index": "226-3",
                    "sentence": "In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.",
                    "tag": "2+3"
                },
                {
                    "index": "226-4",
                    "sentence": "Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-227",
            "text": [
                {
                    "index": "227-0",
                    "sentence": "The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets.",
                    "tag": "1"
                },
                {
                    "index": "227-1",
                    "sentence": "In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks.",
                    "tag": "1"
                },
                {
                    "index": "227-2",
                    "sentence": "In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks.",
                    "tag": "2+3"
                },
                {
                    "index": "227-3",
                    "sentence": "Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-228",
            "text": [
                {
                    "index": "228-0",
                    "sentence": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets.",
                    "tag": "1"
                },
                {
                    "index": "228-1",
                    "sentence": "During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced.",
                    "tag": "1"
                },
                {
                    "index": "228-2",
                    "sentence": "In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models.",
                    "tag": "2"
                },
                {
                    "index": "228-3",
                    "sentence": "Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge.",
                    "tag": "3"
                },
                {
                    "index": "228-4",
                    "sentence": "It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.",
                    "tag": "3"
                },
                {
                    "index": "228-5",
                    "sentence": "After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability.",
                    "tag": "3"
                },
                {
                    "index": "228-6",
                    "sentence": "We implement MFT upon BERT to solve several multi-domain text mining tasks.",
                    "tag": "3"
                },
                {
                    "index": "228-7",
                    "sentence": "Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-229",
            "text": [
                {
                    "index": "229-0",
                    "sentence": "Generative neural networks have been shown effective on query suggestion.",
                    "tag": "1"
                },
                {
                    "index": "229-1",
                    "sentence": "Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time.",
                    "tag": "2"
                },
                {
                    "index": "229-2",
                    "sentence": "User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns.",
                    "tag": "3"
                },
                {
                    "index": "229-3",
                    "sentence": "This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice.",
                    "tag": "2+3"
                },
                {
                    "index": "229-4",
                    "sentence": "Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-230",
            "text": [
                {
                    "index": "230-0",
                    "sentence": "The causal relationships between emotions and causes in text have recently received a lot of attention.",
                    "tag": "1"
                },
                {
                    "index": "230-1",
                    "sentence": "Most of the existing works focus on the extraction of the causally related clauses from documents.",
                    "tag": "1"
                },
                {
                    "index": "230-2",
                    "sentence": "However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related.",
                    "tag": "1"
                },
                {
                    "index": "230-3",
                    "sentence": "To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset.",
                    "tag": "2+3"
                },
                {
                    "index": "230-4",
                    "sentence": "Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses.",
                    "tag": "3"
                },
                {
                    "index": "230-5",
                    "sentence": "Experiments demonstrate the effectiveness and generality of our aggregation module.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-231",
            "text": [
                {
                    "index": "231-0",
                    "sentence": "The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention.",
                    "tag": "1"
                },
                {
                    "index": "231-1",
                    "sentence": "In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume.",
                    "tag": "1"
                },
                {
                    "index": "231-2",
                    "sentence": "To measure complexity, we present a number of parametric and non-parametric metrics.",
                    "tag": "2"
                },
                {
                    "index": "231-3",
                    "sentence": "Our experiments with such metrics show that probe’s performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones).",
                    "tag": "4"
                },
                {
                    "index": "231-4",
                    "sentence": "These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations.",
                    "tag": "4"
                },
                {
                    "index": "231-5",
                    "sentence": "We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume.",
                    "tag": "3"
                },
                {
                    "index": "231-6",
                    "sentence": "In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-232",
            "text": [
                {
                    "index": "232-0",
                    "sentence": "To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input.",
                    "tag": "2"
                },
                {
                    "index": "232-1",
                    "sentence": "Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations.",
                    "tag": "1"
                },
                {
                    "index": "232-2",
                    "sentence": "In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out.",
                    "tag": "2+3"
                },
                {
                    "index": "232-3",
                    "sentence": "We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-233",
            "text": [
                {
                    "index": "233-0",
                    "sentence": "Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic.",
                    "tag": "1"
                },
                {
                    "index": "233-1",
                    "sentence": "As a result, they perform poorly or fail completely on non-isomorphic spaces.",
                    "tag": "1"
                },
                {
                    "index": "233-2",
                    "sentence": "Such non-isomorphism has been hypothesised to result from typological differences between languages.",
                    "tag": "1"
                },
                {
                    "index": "233-3",
                    "sentence": "In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces.",
                    "tag": "2"
                },
                {
                    "index": "233-4",
                    "sentence": "We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g.“under-training”).",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-234",
            "text": [
                {
                    "index": "234-0",
                    "sentence": "Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable.",
                    "tag": "1"
                },
                {
                    "index": "234-1",
                    "sentence": "On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios.",
                    "tag": "1"
                },
                {
                    "index": "234-2",
                    "sentence": "In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules.",
                    "tag": "2+3"
                },
                {
                    "index": "234-3",
                    "sentence": "An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios.",
                    "tag": "5"
                },
                {
                    "index": "234-4",
                    "sentence": "It can also utilize labeled data for training to achieve improved prediction accuracy.",
                    "tag": "5"
                },
                {
                    "index": "234-5",
                    "sentence": "After training, an FA-RNN often remains interpretable and can be converted back into regular expressions.",
                    "tag": "5"
                },
                {
                    "index": "234-6",
                    "sentence": "We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-235",
            "text": [
                {
                    "index": "235-0",
                    "sentence": "Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers.",
                    "tag": "1"
                },
                {
                    "index": "235-1",
                    "sentence": "We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning.",
                    "tag": "2+3"
                },
                {
                    "index": "235-2",
                    "sentence": "For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse.",
                    "tag": "3+4"
                },
                {
                    "index": "235-3",
                    "sentence": "Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful.",
                    "tag": "5"
                },
                {
                    "index": "235-4",
                    "sentence": "We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-236",
            "text": [
                {
                    "index": "236-0",
                    "sentence": "Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency.",
                    "tag": "1"
                },
                {
                    "index": "236-1",
                    "sentence": "Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads.",
                    "tag": "2"
                },
                {
                    "index": "236-2",
                    "sentence": "We evaluate this on Transformer and BERT models on multiple NLP tasks.",
                    "tag": "3"
                },
                {
                    "index": "236-3",
                    "sentence": "Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy.",
                    "tag": "4"
                },
                {
                    "index": "236-4",
                    "sentence": "Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head.",
                    "tag": "4"
                },
                {
                    "index": "236-5",
                    "sentence": "On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance.",
                    "tag": "4"
                },
                {
                    "index": "236-6",
                    "sentence": "However, strategies that avoid pruning middle layers and consecutive layers perform better.",
                    "tag": "4"
                },
                {
                    "index": "236-7",
                    "sentence": "Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads.",
                    "tag": "4"
                },
                {
                    "index": "236-8",
                    "sentence": "Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-237",
            "text": [
                {
                    "index": "237-0",
                    "sentence": "BERT and its variants have achieved state-of-the-art performance in various NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "237-1",
                    "sentence": "Since then, various works have been proposed to analyze the linguistic information being captured in BERT.",
                    "tag": "1"
                },
                {
                    "index": "237-2",
                    "sentence": "However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering.",
                    "tag": "1"
                },
                {
                    "index": "237-3",
                    "sentence": "In this work, we attempt to interpret BERT for RCQA.",
                    "tag": "2"
                },
                {
                    "index": "237-4",
                    "sentence": "Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients.",
                    "tag": "3"
                },
                {
                    "index": "237-5",
                    "sentence": "Based on the defined roles, we perform a preliminary analysis across all layers.",
                    "tag": "3"
                },
                {
                    "index": "237-6",
                    "sentence": "We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction.",
                    "tag": "4"
                },
                {
                    "index": "237-7",
                    "sentence": "Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly.",
                    "tag": "4"
                },
                {
                    "index": "237-8",
                    "sentence": "The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-238",
            "text": [
                {
                    "index": "238-0",
                    "sentence": "Attribution methods assess the contribution of inputs to the model prediction.",
                    "tag": "1"
                },
                {
                    "index": "238-1",
                    "sentence": "One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction.",
                    "tag": "1"
                },
                {
                    "index": "238-2",
                    "sentence": "Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models.",
                    "tag": "1"
                },
                {
                    "index": "238-3",
                    "sentence": "Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped.",
                    "tag": "1"
                },
                {
                    "index": "238-4",
                    "sentence": "The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction.",
                    "tag": "1"
                },
                {
                    "index": "238-5",
                    "sentence": "To deal with these challenges, we introduce Differentiable Masking.",
                    "tag": "2"
                },
                {
                    "index": "238-6",
                    "sentence": "DiffMask learns to mask-out subsets of the input while maintaining differentiability.",
                    "tag": "3"
                },
                {
                    "index": "238-7",
                    "sentence": "The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model.",
                    "tag": "3"
                },
                {
                    "index": "238-8",
                    "sentence": "First, this makes the approach efficient because we predict rather than search.",
                    "tag": "4"
                },
                {
                    "index": "238-9",
                    "sentence": "Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers.",
                    "tag": "4"
                },
                {
                    "index": "238-10",
                    "sentence": "This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers.",
                    "tag": "1"
                },
                {
                    "index": "238-11",
                    "sentence": "We use DiffMask to study BERT models on sentiment classification and question answering.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-239",
            "text": [
                {
                    "index": "239-0",
                    "sentence": "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity.",
                    "tag": "1"
                },
                {
                    "index": "239-1",
                    "sentence": "Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques.",
                    "tag": "1"
                },
                {
                    "index": "239-2",
                    "sentence": "Provided with an already trained model, they compute saliency scores for the words of an input instance.",
                    "tag": "1"
                },
                {
                    "index": "239-3",
                    "sentence": "However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique.",
                    "tag": "1"
                },
                {
                    "index": "239-4",
                    "sentence": "In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques.",
                    "tag": "2"
                },
                {
                    "index": "239-5",
                    "sentence": "We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures.",
                    "tag": "3"
                },
                {
                    "index": "239-6",
                    "sentence": "We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones.",
                    "tag": "3"
                },
                {
                    "index": "239-7",
                    "sentence": "Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-240",
            "text": [
                {
                    "index": "240-0",
                    "sentence": "Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image.",
                    "tag": "1"
                },
                {
                    "index": "240-1",
                    "sentence": "Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure.",
                    "tag": "1"
                },
                {
                    "index": "240-2",
                    "sentence": "We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach.",
                    "tag": "2+3"
                },
                {
                    "index": "240-3",
                    "sentence": "We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types.",
                    "tag": "3"
                },
                {
                    "index": "240-4",
                    "sentence": "The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset.",
                    "tag": "4"
                },
                {
                    "index": "240-5",
                    "sentence": "We also demonstrate interpretability while examining different components in the inference pipeline.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-241",
            "text": [
                {
                    "index": "241-0",
                    "sentence": "In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data.",
                    "tag": "1"
                },
                {
                    "index": "241-1",
                    "sentence": "Some methods of generating counterfactual samples have been proposed to alleviate this problem.",
                    "tag": "1"
                },
                {
                    "index": "241-2",
                    "sentence": "However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized.",
                    "tag": "1"
                },
                {
                    "index": "241-3",
                    "sentence": "Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples.",
                    "tag": "2+3"
                },
                {
                    "index": "241-4",
                    "sentence": "With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly.",
                    "tag": "3+4"
                },
                {
                    "index": "241-5",
                    "sentence": "We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-242",
            "text": [
                {
                    "index": "242-0",
                    "sentence": "Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction.",
                    "tag": "1"
                },
                {
                    "index": "242-1",
                    "sentence": "Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization.",
                    "tag": "1"
                },
                {
                    "index": "242-2",
                    "sentence": "In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples.",
                    "tag": "2+3"
                },
                {
                    "index": "242-3",
                    "sentence": "Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small.",
                    "tag": "3"
                },
                {
                    "index": "242-4",
                    "sentence": "To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships.",
                    "tag": "3"
                },
                {
                    "index": "242-5",
                    "sentence": "We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-243",
            "text": [
                {
                    "index": "243-0",
                    "sentence": "Social media produces large amounts of contents every day.",
                    "tag": "1"
                },
                {
                    "index": "243-1",
                    "sentence": "To help users quickly capture what they need, keyphrase prediction is receiving a growing attention.",
                    "tag": "1"
                },
                {
                    "index": "243-2",
                    "sentence": "Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images.",
                    "tag": "1"
                },
                {
                    "index": "243-3",
                    "sentence": "In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post.",
                    "tag": "2"
                },
                {
                    "index": "243-4",
                    "sentence": "To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities.",
                    "tag": "2+3"
                },
                {
                    "index": "243-5",
                    "sentence": "Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages.",
                    "tag": "3"
                },
                {
                    "index": "243-6",
                    "sentence": "Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms.",
                    "tag": "4"
                },
                {
                    "index": "243-7",
                    "sentence": "Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-244",
            "text": [
                {
                    "index": "244-0",
                    "sentence": "Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history.",
                    "tag": "1"
                },
                {
                    "index": "244-1",
                    "sentence": "Prior work has mostly focused on various attention mechanisms to model such intricate interactions.",
                    "tag": "1"
                },
                {
                    "index": "244-2",
                    "sentence": "By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "244-3",
                    "sentence": "The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture.",
                    "tag": "3"
                },
                {
                    "index": "244-4",
                    "sentence": "More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training.",
                    "tag": "3"
                },
                {
                    "index": "244-5",
                    "sentence": "Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.",
                    "tag": "3+4"
                },
                {
                    "index": "244-6",
                    "sentence": "Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-245",
            "text": [
                {
                    "index": "245-0",
                    "sentence": "In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language.",
                    "tag": "2"
                },
                {
                    "index": "245-1",
                    "sentence": "We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use.",
                    "tag": "2"
                },
                {
                    "index": "245-2",
                    "sentence": "Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language.",
                    "tag": "4"
                },
                {
                    "index": "245-3",
                    "sentence": "We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-246",
            "text": [
                {
                    "index": "246-0",
                    "sentence": "Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions.",
                    "tag": "1"
                },
                {
                    "index": "246-1",
                    "sentence": "Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences.",
                    "tag": "1"
                },
                {
                    "index": "246-2",
                    "sentence": "Meanwhile, due to the lack of intermediate supervision, the agent’s performance at following each part of the instruction cannot be assessed during navigation.",
                    "tag": "1"
                },
                {
                    "index": "246-3",
                    "sentence": "In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction.",
                    "tag": "2"
                },
                {
                    "index": "246-4",
                    "sentence": "We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time.",
                    "tag": "4"
                },
                {
                    "index": "246-5",
                    "sentence": "We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths.",
                    "tag": "3"
                },
                {
                    "index": "246-6",
                    "sentence": "To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step.",
                    "tag": "3"
                },
                {
                    "index": "246-7",
                    "sentence": "We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.",
                    "tag": "3+4"
                },
                {
                    "index": "246-8",
                    "sentence": "We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-247",
            "text": [
                {
                    "index": "247-0",
                    "sentence": "We study knowledge-grounded dialogue generation with pre-trained language models.",
                    "tag": "2+3"
                },
                {
                    "index": "247-1",
                    "sentence": "To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues.",
                    "tag": "3"
                },
                {
                    "index": "247-2",
                    "sentence": "Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-248",
            "text": [
                {
                    "index": "248-0",
                    "sentence": "In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data.",
                    "tag": "1+2"
                },
                {
                    "index": "248-1",
                    "sentence": "MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.",
                    "tag": "1"
                },
                {
                    "index": "248-2",
                    "sentence": "Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length.",
                    "tag": "2+3"
                },
                {
                    "index": "248-3",
                    "sentence": "We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ.",
                    "tag": "3"
                },
                {
                    "index": "248-4",
                    "sentence": "Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-249",
            "text": [
                {
                    "index": "249-0",
                    "sentence": "Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "249-1",
                    "sentence": "In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs.",
                    "tag": "1"
                },
                {
                    "index": "249-2",
                    "sentence": "Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features.",
                    "tag": "1"
                },
                {
                    "index": "249-3",
                    "sentence": "We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals.",
                    "tag": "1"
                },
                {
                    "index": "249-4",
                    "sentence": "The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces.",
                    "tag": "2+3"
                },
                {
                    "index": "249-5",
                    "sentence": "To overcome training issues that arise from training complex variational models, we propose appropriate training strategies.",
                    "tag": "3"
                },
                {
                    "index": "249-6",
                    "sentence": "Experiments on various dialog datasets show that our model improves the downstream dialog trackers’ robustness via generative data augmentation.",
                    "tag": "5"
                },
                {
                    "index": "249-7",
                    "sentence": "We also discover additional benefits of our unified approach to modeling goal-oriented dialogs – dialog response generation and user simulation, where our model outperforms previous strong baselines.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-250",
            "text": [
                {
                    "index": "250-0",
                    "sentence": "Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge.",
                    "tag": "1"
                },
                {
                    "index": "250-1",
                    "sentence": "Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance.",
                    "tag": "1"
                },
                {
                    "index": "250-2",
                    "sentence": "However, these models suffer from a huge gap between prior and posterior knowledge selection.",
                    "tag": "1"
                },
                {
                    "index": "250-3",
                    "sentence": "Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information.",
                    "tag": "1"
                },
                {
                    "index": "250-4",
                    "sentence": "Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference.",
                    "tag": "1"
                },
                {
                    "index": "250-5",
                    "sentence": "Here, we deal with these issues on two aspects: ",
                    "tag": "2"
                },
                {
                    "index": "250-6",
                    "sentence": "(1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); ",
                    "tag": "2+3"
                },
                {
                    "index": "250-7",
                    "sentence": "(2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection.",
                    "tag": "2+3"
                },
                {
                    "index": "250-8",
                    "sentence": "Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-251",
            "text": [
                {
                    "index": "251-0",
                    "sentence": "Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses.",
                    "tag": "1"
                },
                {
                    "index": "251-1",
                    "sentence": "In this paper, we propose to explore potential responses by counterfactual reasoning.",
                    "tag": "2"
                },
                {
                    "index": "251-2",
                    "sentence": "Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken.",
                    "tag": "1"
                },
                {
                    "index": "251-3",
                    "sentence": "The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch.",
                    "tag": "4"
                },
                {
                    "index": "251-4",
                    "sentence": "Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space.",
                    "tag": "5"
                },
                {
                    "index": "251-5",
                    "sentence": "An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-252",
            "text": [
                {
                    "index": "252-0",
                    "sentence": "Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data.",
                    "tag": "1"
                },
                {
                    "index": "252-1",
                    "sentence": "However, collecting large-scale dialogue data is usually time-consuming and labor-intensive.",
                    "tag": "1"
                },
                {
                    "index": "252-2",
                    "sentence": "To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data.",
                    "tag": "2+3"
                },
                {
                    "index": "252-3",
                    "sentence": "Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data.",
                    "tag": "3"
                },
                {
                    "index": "252-4",
                    "sentence": "A ranking module is employed to filter out low-quality dialogues.",
                    "tag": "3"
                },
                {
                    "index": "252-5",
                    "sentence": "Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data.",
                    "tag": "3"
                },
                {
                    "index": "252-6",
                    "sentence": "Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-253",
            "text": [
                {
                    "index": "253-0",
                    "sentence": "We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning.",
                    "tag": "1+2"
                },
                {
                    "index": "253-1",
                    "sentence": "Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors.",
                    "tag": "1"
                },
                {
                    "index": "253-2",
                    "sentence": "Such idea arises naturally in human behaviors, e.g. predicting others’ responses and then deciding our own actions.",
                    "tag": "1"
                },
                {
                    "index": "253-3",
                    "sentence": "In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly.",
                    "tag": "4"
                },
                {
                    "index": "253-4",
                    "sentence": "We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-254",
            "text": [
                {
                    "index": "254-0",
                    "sentence": "We study multi-turn response generation for open-domain dialogues.",
                    "tag": "2"
                },
                {
                    "index": "254-1",
                    "sentence": "The existing state-of-the-art addresses the problem with deep neural architectures.",
                    "tag": "1"
                },
                {
                    "index": "254-2",
                    "sentence": "While these models improved response quality, their complexity also hinders the application of the models in real systems.",
                    "tag": "1"
                },
                {
                    "index": "254-3",
                    "sentence": "In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation.",
                    "tag": "3"
                },
                {
                    "index": "254-4",
                    "sentence": "To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation.",
                    "tag": "2+3"
                },
                {
                    "index": "254-5",
                    "sentence": "By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum.",
                    "tag": "3+4"
                },
                {
                    "index": "254-6",
                    "sentence": "Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-255",
            "text": [
                {
                    "index": "255-0",
                    "sentence": "Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response.",
                    "tag": "1"
                },
                {
                    "index": "255-1",
                    "sentence": "Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance.",
                    "tag": "1"
                },
                {
                    "index": "255-2",
                    "sentence": "To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows.",
                    "tag": "2"
                },
                {
                    "index": "255-3",
                    "sentence": "Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context.",
                    "tag": "3"
                },
                {
                    "index": "255-4",
                    "sentence": "Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset.",
                    "tag": "4"
                },
                {
                    "index": "255-5",
                    "sentence": "Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-256",
            "text": [
                {
                    "index": "256-0",
                    "sentence": "The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention.",
                    "tag": "1"
                },
                {
                    "index": "256-1",
                    "sentence": "In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously.",
                    "tag": "2"
                },
                {
                    "index": "256-2",
                    "sentence": "TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network).",
                    "tag": "3"
                },
                {
                    "index": "256-3",
                    "sentence": "Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network.",
                    "tag": "3"
                },
                {
                    "index": "256-4",
                    "sentence": "Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student.",
                    "tag": "3"
                },
                {
                    "index": "256-5",
                    "sentence": "The usage of discriminators relaxes the rigid coupling between the student and teachers.",
                    "tag": "4"
                },
                {
                    "index": "256-6",
                    "sentence": "Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-257",
            "text": [
                {
                    "index": "257-0",
                    "sentence": "Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks.",
                    "tag": "1"
                },
                {
                    "index": "257-1",
                    "sentence": "However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains.",
                    "tag": "1"
                },
                {
                    "index": "257-2",
                    "sentence": "Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem.",
                    "tag": "1"
                },
                {
                    "index": "257-3",
                    "sentence": "In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings.",
                    "tag": "2"
                },
                {
                    "index": "257-4",
                    "sentence": "We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-258",
            "text": [
                {
                    "index": "258-0",
                    "sentence": "State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models.",
                    "tag": "1"
                },
                {
                    "index": "258-1",
                    "sentence": "This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet).",
                    "tag": "1"
                },
                {
                    "index": "258-2",
                    "sentence": "At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora.",
                    "tag": "1"
                },
                {
                    "index": "258-3",
                    "sentence": "In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus.",
                    "tag": "2"
                },
                {
                    "index": "258-4",
                    "sentence": "We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-259",
            "text": [
                {
                    "index": "259-0",
                    "sentence": "We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words.",
                    "tag": "2"
                },
                {
                    "index": "259-1",
                    "sentence": "Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation.",
                    "tag": "3"
                },
                {
                    "index": "259-2",
                    "sentence": "We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-260",
            "text": [
                {
                    "index": "260-0",
                    "sentence": "Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information.",
                    "tag": "1"
                },
                {
                    "index": "260-1",
                    "sentence": "However, it is still hard to link them to structured sources of knowledge.",
                    "tag": "1"
                },
                {
                    "index": "260-2",
                    "sentence": "In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors.",
                    "tag": "2+3"
                },
                {
                    "index": "260-3",
                    "sentence": "ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only.",
                    "tag": "3"
                },
                {
                    "index": "260-4",
                    "sentence": "We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures.",
                    "tag": "3"
                },
                {
                    "index": "260-5",
                    "sentence": "ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-261",
            "text": [
                {
                    "index": "261-0",
                    "sentence": "The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence.",
                    "tag": "1"
                },
                {
                    "index": "261-1",
                    "sentence": "While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like “nothing special”.",
                    "tag": "1"
                },
                {
                    "index": "261-2",
                    "sentence": "Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation “food-was” is treated equally as an adjectival complement relation “was-okay” in “food was okay”.",
                    "tag": "1"
                },
                {
                    "index": "261-3",
                    "sentence": "To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs.",
                    "tag": "2"
                },
                {
                    "index": "261-4",
                    "sentence": "Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information.",
                    "tag": "3"
                },
                {
                    "index": "261-5",
                    "sentence": "Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs.",
                    "tag": "3"
                },
                {
                    "index": "261-6",
                    "sentence": "Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs.",
                    "tag": "3"
                },
                {
                    "index": "261-7",
                    "sentence": "Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-262",
            "text": [
                {
                    "index": "262-0",
                    "sentence": "Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories.",
                    "tag": "2"
                },
                {
                    "index": "262-1",
                    "sentence": "To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation.",
                    "tag": "3"
                },
                {
                    "index": "262-2",
                    "sentence": "These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance.",
                    "tag": "3"
                },
                {
                    "index": "262-3",
                    "sentence": "In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category.",
                    "tag": "2+3"
                },
                {
                    "index": "262-4",
                    "sentence": "Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments.",
                    "tag": "3"
                },
                {
                    "index": "262-5",
                    "sentence": "Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-263",
            "text": [
                {
                    "index": "263-0",
                    "sentence": "Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention.",
                    "tag": "1"
                },
                {
                    "index": "263-1",
                    "sentence": "Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification.",
                    "tag": "1"
                },
                {
                    "index": "263-2",
                    "sentence": "However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans.",
                    "tag": "1"
                },
                {
                    "index": "263-3",
                    "sentence": "In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs.",
                    "tag": "2"
                },
                {
                    "index": "263-4",
                    "sentence": "Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features.",
                    "tag": "5"
                },
                {
                    "index": "263-5",
                    "sentence": "The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-264",
            "text": [
                {
                    "index": "264-0",
                    "sentence": "Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document.",
                    "tag": "1"
                },
                {
                    "index": "264-1",
                    "sentence": "The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering.",
                    "tag": "1"
                },
                {
                    "index": "264-2",
                    "sentence": "However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step.",
                    "tag": "1"
                },
                {
                    "index": "264-3",
                    "sentence": "To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL).",
                    "tag": "2+3"
                },
                {
                    "index": "264-4",
                    "sentence": "The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move.",
                    "tag": "3"
                },
                {
                    "index": "264-5",
                    "sentence": "Finally, CMLL and EMLL are integrated to obtain the final result.",
                    "tag": "3"
                },
                {
                    "index": "264-6",
                    "sentence": "We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-265",
            "text": [
                {
                    "index": "265-0",
                    "sentence": "As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years.",
                    "tag": "1"
                },
                {
                    "index": "265-1",
                    "sentence": "However, almost all existing studies focus on one modality (e.g., textual modality).",
                    "tag": "1"
                },
                {
                    "index": "265-2",
                    "sentence": "In this paper, we focus on multi-label emotion detection in a multi-modal scenario.",
                    "tag": "2"
                },
                {
                    "index": "265-3",
                    "sentence": "In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence).",
                    "tag": "3"
                },
                {
                    "index": "265-4",
                    "sentence": "Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection.",
                    "tag": "3"
                },
                {
                    "index": "265-5",
                    "sentence": "The detailed evaluation demonstrates the effectiveness of our approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-266",
            "text": [
                {
                    "index": "266-0",
                    "sentence": "Modeling content importance is an essential yet challenging task for summarization.",
                    "tag": "1"
                },
                {
                    "index": "266-1",
                    "sentence": "Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance.",
                    "tag": "1"
                },
                {
                    "index": "266-2",
                    "sentence": "It is thus hard for these methods to generalize to semantic units of longer text spans.",
                    "tag": "1"
                },
                {
                    "index": "266-3",
                    "sentence": "In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount.",
                    "tag": "2+3"
                },
                {
                    "index": "266-4",
                    "sentence": "It considers both the semantics and context when evaluating the importance of each semantic unit.",
                    "tag": "3"
                },
                {
                    "index": "266-5",
                    "sentence": "With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences.",
                    "tag": "3"
                },
                {
                    "index": "266-6",
                    "sentence": "Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-267",
            "text": [
                {
                    "index": "267-0",
                    "sentence": "Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task.",
                    "tag": "1"
                },
                {
                    "index": "267-1",
                    "sentence": "Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary.",
                    "tag": "1"
                },
                {
                    "index": "267-2",
                    "sentence": "In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning.",
                    "tag": "2+3"
                },
                {
                    "index": "267-3",
                    "sentence": "Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT.",
                    "tag": "3"
                },
                {
                    "index": "267-4",
                    "sentence": "To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss.",
                    "tag": "3"
                },
                {
                    "index": "267-5",
                    "sentence": "Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries.",
                    "tag": "4"
                },
                {
                    "index": "267-6",
                    "sentence": "Furthermore, we show that our method is general and transferable across datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-268",
            "text": [
                {
                    "index": "268-0",
                    "sentence": "Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations.",
                    "tag": "1"
                },
                {
                    "index": "268-1",
                    "sentence": "There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods.",
                    "tag": "1"
                },
                {
                    "index": "268-2",
                    "sentence": "Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences.",
                    "tag": "1"
                },
                {
                    "index": "268-3",
                    "sentence": "While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences.",
                    "tag": "1"
                },
                {
                    "index": "268-4",
                    "sentence": "In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences.",
                    "tag": "2+3"
                },
                {
                    "index": "268-5",
                    "sentence": "Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing.",
                    "tag": "3"
                },
                {
                    "index": "268-6",
                    "sentence": "Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-269",
            "text": [
                {
                    "index": "269-0",
                    "sentence": "We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization.",
                    "tag": "1"
                },
                {
                    "index": "269-1",
                    "sentence": "Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries.",
                    "tag": "1"
                },
                {
                    "index": "269-2",
                    "sentence": "We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central.",
                    "tag": "2+3"
                },
                {
                    "index": "269-3",
                    "sentence": "The modules can be independently developed and leverage training data if available.",
                    "tag": "3"
                },
                {
                    "index": "269-4",
                    "sentence": "We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary.",
                    "tag": "3"
                },
                {
                    "index": "269-5",
                    "sentence": "Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-270",
            "text": [
                {
                    "index": "270-0",
                    "sentence": "Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem.",
                    "tag": "1"
                },
                {
                    "index": "270-1",
                    "sentence": "Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging.",
                    "tag": "1"
                },
                {
                    "index": "270-2",
                    "sentence": "This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text.",
                    "tag": "2"
                },
                {
                    "index": "270-3",
                    "sentence": "The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document.",
                    "tag": "3"
                },
                {
                    "index": "270-4",
                    "sentence": "These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task.",
                    "tag": "3"
                },
                {
                    "index": "270-5",
                    "sentence": "Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines.",
                    "tag": "4"
                },
                {
                    "index": "270-6",
                    "sentence": "Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-271",
            "text": [
                {
                    "index": "271-0",
                    "sentence": "Neural models have achieved remarkable success on relation extraction (RE) benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "271-1",
                    "sentence": "However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models.",
                    "tag": "1"
                },
                {
                    "index": "271-2",
                    "sentence": "To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names).",
                    "tag": "1"
                },
                {
                    "index": "271-3",
                    "sentence": "We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "271-4",
                    "sentence": "Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions.",
                    "tag": "2"
                },
                {
                    "index": "271-5",
                    "sentence": "We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios.",
                    "tag": "4"
                },
                {
                    "index": "271-6",
                    "sentence": "All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-272",
            "text": [
                {
                    "index": "272-0",
                    "sentence": "Open relation extraction is the task of extracting open-domain relation facts from natural language sentences.",
                    "tag": "1"
                },
                {
                    "index": "272-1",
                    "sentence": "Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power.",
                    "tag": "1"
                },
                {
                    "index": "272-2",
                    "sentence": "In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification.",
                    "tag": "2+3"
                },
                {
                    "index": "272-3",
                    "sentence": "Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-273",
            "text": [
                {
                    "index": "273-0",
                    "sentence": "Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results.",
                    "tag": "1"
                },
                {
                    "index": "273-1",
                    "sentence": "However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE.",
                    "tag": "1"
                },
                {
                    "index": "273-2",
                    "sentence": "To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks.",
                    "tag": "2"
                },
                {
                    "index": "273-3",
                    "sentence": "The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-274",
            "text": [
                {
                    "index": "274-0",
                    "sentence": "Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work.",
                    "tag": "1"
                },
                {
                    "index": "274-1",
                    "sentence": "In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation.",
                    "tag": "2"
                },
                {
                    "index": "274-2",
                    "sentence": "We then propose a small empirical study to quantify the most common mistake’s impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05.",
                    "tag": "2+3"
                },
                {
                    "index": "274-3",
                    "sentence": "We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER.",
                    "tag": "3"
                },
                {
                    "index": "274-4",
                    "sentence": "This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics.",
                    "tag": "3"
                },
                {
                    "index": "274-5",
                    "sentence": "We finally call for unifying the evaluation setting in end-to-end RE.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-275",
            "text": [
                {
                    "index": "275-0",
                    "sentence": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior.",
                    "tag": "1"
                },
                {
                    "index": "275-1",
                    "sentence": "We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process.",
                    "tag": "2"
                },
                {
                    "index": "275-2",
                    "sentence": "We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior.",
                    "tag": "3"
                },
                {
                    "index": "275-3",
                    "sentence": "Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data.",
                    "tag": "4"
                },
                {
                    "index": "275-4",
                    "sentence": "Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance.",
                    "tag": "4"
                },
                {
                    "index": "275-5",
                    "sentence": "By adding some of the challenge data as training examples, the performance of the model improves.",
                    "tag": "4"
                },
                {
                    "index": "275-6",
                    "sentence": "Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-276",
            "text": [
                {
                    "index": "276-0",
                    "sentence": "Relation extraction (RE) aims to identify the semantic relations between named entities in text.",
                    "tag": "2"
                },
                {
                    "index": "276-1",
                    "sentence": "Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document.",
                    "tag": "2"
                },
                {
                    "index": "276-2",
                    "sentence": "In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations.",
                    "tag": "2"
                },
                {
                    "index": "276-3",
                    "sentence": "Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations.",
                    "tag": "3"
                },
                {
                    "index": "276-4",
                    "sentence": "Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE.",
                    "tag": "5"
                },
                {
                    "index": "276-5",
                    "sentence": "It is particularly effective in extracting relations between entities of long distance and having multiple mentions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-277",
            "text": [
                {
                    "index": "277-0",
                    "sentence": "The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task.",
                    "tag": "1"
                },
                {
                    "index": "277-1",
                    "sentence": "Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction.",
                    "tag": "1"
                },
                {
                    "index": "277-2",
                    "sentence": "However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks.",
                    "tag": "1"
                },
                {
                    "index": "277-3",
                    "sentence": "As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification.",
                    "tag": "2+3"
                },
                {
                    "index": "277-4",
                    "sentence": "Empirical studies on two real-world datasets confirm the superiority of the proposed model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-278",
            "text": [
                {
                    "index": "278-0",
                    "sentence": "Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days.",
                    "tag": "1"
                },
                {
                    "index": "278-1",
                    "sentence": "Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space.",
                    "tag": "2"
                },
                {
                    "index": "278-2",
                    "sentence": "TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks.",
                    "tag": "3+4"
                },
                {
                    "index": "278-3",
                    "sentence": "We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms.",
                    "tag": "4"
                },
                {
                    "index": "278-4",
                    "sentence": "In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-279",
            "text": [
                {
                    "index": "279-0",
                    "sentence": "A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs.",
                    "tag": "1"
                },
                {
                    "index": "279-1",
                    "sentence": "This comes at a significant computational cost.",
                    "tag": "1"
                },
                {
                    "index": "279-2",
                    "sentence": "On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality.",
                    "tag": "1"
                },
                {
                    "index": "279-3",
                    "sentence": "In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster.",
                    "tag": "2+3"
                },
                {
                    "index": "279-4",
                    "sentence": "This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task.",
                    "tag": "3"
                },
                {
                    "index": "279-5",
                    "sentence": "We improve its performance further by applying coverage (soft) constraints on the grid at training time.",
                    "tag": "3"
                },
                {
                    "index": "279-6",
                    "sentence": "Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture.",
                    "tag": "5"
                },
                {
                    "index": "279-7",
                    "sentence": "This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers.",
                    "tag": "3"
                },
                {
                    "index": "279-8",
                    "sentence": "Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-280",
            "text": [
                {
                    "index": "280-0",
                    "sentence": "Detecting public sentiment drift is a challenging task due to sentiment change over time.",
                    "tag": "1"
                },
                {
                    "index": "280-1",
                    "sentence": "Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data.",
                    "tag": "1"
                },
                {
                    "index": "280-2",
                    "sentence": "In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-281",
            "text": [
                {
                    "index": "281-0",
                    "sentence": "Solving algebraic word problems has recently emerged as an important natural language processing task.",
                    "tag": "1"
                },
                {
                    "index": "281-1",
                    "sentence": "To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output.",
                    "tag": "1"
                },
                {
                    "index": "281-2",
                    "sentence": "However, such a neural model suffered two issues: expression fragmentation and operand-context separation.",
                    "tag": "1"
                },
                {
                    "index": "281-3",
                    "sentence": "To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations.",
                    "tag": "2+3"
                },
                {
                    "index": "281-4",
                    "sentence": "The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS.",
                    "tag": "3"
                },
                {
                    "index": "281-5",
                    "sentence": "Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS.",
                    "tag": "4"
                },
                {
                    "index": "281-6",
                    "sentence": "The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation.",
                    "tag": "4"
                },
                {
                    "index": "281-7",
                    "sentence": "(2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-282",
            "text": [
                {
                    "index": "282-0",
                    "sentence": "A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs.",
                    "tag": "1"
                },
                {
                    "index": "282-1",
                    "sentence": "Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly.",
                    "tag": "1+2"
                },
                {
                    "index": "282-2",
                    "sentence": "Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation.",
                    "tag": "2+3"
                },
                {
                    "index": "282-3",
                    "sentence": "Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols’ semantic meanings like human solving MWPs.",
                    "tag": "3"
                },
                {
                    "index": "282-4",
                    "sentence": "Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information.",
                    "tag": "3"
                },
                {
                    "index": "282-5",
                    "sentence": "Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs.",
                    "tag": "4"
                },
                {
                    "index": "282-6",
                    "sentence": "Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-283",
            "text": [
                {
                    "index": "283-0",
                    "sentence": "Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community.",
                    "tag": "1"
                },
                {
                    "index": "283-1",
                    "sentence": "In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph.",
                    "tag": "2"
                },
                {
                    "index": "283-2",
                    "sentence": "Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences.",
                    "tag": "3"
                },
                {
                    "index": "283-3",
                    "sentence": "By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution.",
                    "tag": "3"
                },
                {
                    "index": "283-4",
                    "sentence": "Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-284",
            "text": [
                {
                    "index": "284-0",
                    "sentence": "One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients.",
                    "tag": "1"
                },
                {
                    "index": "284-1",
                    "sentence": "Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible.",
                    "tag": "1"
                },
                {
                    "index": "284-2",
                    "sentence": "In this work, we propose a routing method to dive into the content selection under the internal restrictions.",
                    "tag": "2"
                },
                {
                    "index": "284-3",
                    "sentence": "The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences.",
                    "tag": "3"
                },
                {
                    "index": "284-4",
                    "sentence": "Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-285",
            "text": [
                {
                    "index": "285-0",
                    "sentence": "Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ.",
                    "tag": "1"
                },
                {
                    "index": "285-1",
                    "sentence": "Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways.",
                    "tag": "1"
                },
                {
                    "index": "285-2",
                    "sentence": "We notice that the helpfulness of the learning material would reflect on the learners’ performance.",
                    "tag": "1"
                },
                {
                    "index": "285-3",
                    "sentence": "Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent’s performance.",
                    "tag": "2"
                },
                {
                    "index": "285-4",
                    "sentence": "To enable the agent to behave like a learner, we leverage entailment modeling’s capability of inferring answers from the provided materials.",
                    "tag": "3"
                },
                {
                    "index": "285-5",
                    "sentence": "Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks.",
                    "tag": "4"
                },
                {
                    "index": "285-6",
                    "sentence": "We further conduct a classroom user study with college ESL learners.",
                    "tag": "4"
                },
                {
                    "index": "285-7",
                    "sentence": "The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently.",
                    "tag": "5"
                },
                {
                    "index": "285-8",
                    "sentence": "Compared to other models, the proposed agent improves the score of more than 17% of students after learning.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-286",
            "text": [
                {
                    "index": "286-0",
                    "sentence": "As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention.",
                    "tag": "1"
                },
                {
                    "index": "286-1",
                    "sentence": "Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic.",
                    "tag": "1"
                },
                {
                    "index": "286-2",
                    "sentence": "A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products.",
                    "tag": "1"
                },
                {
                    "index": "286-3",
                    "sentence": "Hence, multi-product AD post generation is meaningful and important.",
                    "tag": "1"
                },
                {
                    "index": "286-4",
                    "sentence": "We propose a novel end-to-end model named S-MG Net to generate the AD post.",
                    "tag": "2"
                },
                {
                    "index": "286-5",
                    "sentence": "Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network).",
                    "tag": "3"
                },
                {
                    "index": "286-6",
                    "sentence": "(2) generate a post including selected products via the MGenNet (Multi-Generator Network).",
                    "tag": "3"
                },
                {
                    "index": "286-7",
                    "sentence": "Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products.",
                    "tag": "3"
                },
                {
                    "index": "286-8",
                    "sentence": "Then, MGenNet generates the description copywriting of each product.",
                    "tag": "3"
                },
                {
                    "index": "286-9",
                    "sentence": "Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-287",
            "text": [
                {
                    "index": "287-0",
                    "sentence": "Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks.",
                    "tag": "1"
                },
                {
                    "index": "287-1",
                    "sentence": "Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms.",
                    "tag": "1"
                },
                {
                    "index": "287-2",
                    "sentence": "To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures.",
                    "tag": "2+3"
                },
                {
                    "index": "287-3",
                    "sentence": "We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms.",
                    "tag": "3"
                },
                {
                    "index": "287-4",
                    "sentence": "To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task.",
                    "tag": "3"
                },
                {
                    "index": "287-5",
                    "sentence": "We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation.",
                    "tag": "3"
                },
                {
                    "index": "287-6",
                    "sentence": "Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines.",
                    "tag": "4"
                },
                {
                    "index": "287-7",
                    "sentence": "Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-288",
            "text": [
                {
                    "index": "288-0",
                    "sentence": "Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent.",
                    "tag": "1"
                },
                {
                    "index": "288-1",
                    "sentence": "Researchers have been relying on transfer learning to adapt an existing model to a new domain.",
                    "tag": "1"
                },
                {
                    "index": "288-2",
                    "sentence": "However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as “black boxes”.",
                    "tag": "1"
                },
                {
                    "index": "288-3",
                    "sentence": "We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation.",
                    "tag": "1+2"
                },
                {
                    "index": "288-4",
                    "sentence": "We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning.",
                    "tag": "3"
                },
                {
                    "index": "288-5",
                    "sentence": "Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-289",
            "text": [
                {
                    "index": "289-0",
                    "sentence": "Can pretrained language models (PLMs) generate derivationally complex words?",
                    "tag": "1"
                },
                {
                    "index": "289-1",
                    "sentence": "We present the first study investigating this question, taking BERT as the example PLM.",
                    "tag": "1+2"
                },
                {
                    "index": "289-2",
                    "sentence": "We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning.",
                    "tag": "3"
                },
                {
                    "index": "289-3",
                    "sentence": "Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG).",
                    "tag": "4"
                },
                {
                    "index": "289-4",
                    "sentence": "Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-290",
            "text": [
                {
                    "index": "290-0",
                    "sentence": "Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model.",
                    "tag": "2+3"
                },
                {
                    "index": "290-1",
                    "sentence": "Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.",
                    "tag": "3"
                },
                {
                    "index": "290-2",
                    "sentence": "With the effective encoder design, our model only needs to take unigram features for scoring.",
                    "tag": "3"
                },
                {
                    "index": "290-3",
                    "sentence": "Our model is evaluated on SIGHAN Bakeoff benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "290-4",
                    "sentence": "The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-291",
            "text": [
                {
                    "index": "291-0",
                    "sentence": "Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity.",
                    "tag": "1"
                },
                {
                    "index": "291-1",
                    "sentence": "Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "291-2",
                    "sentence": "Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora.",
                    "tag": "1"
                },
                {
                    "index": "291-3",
                    "sentence": "However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words.",
                    "tag": "1"
                },
                {
                    "index": "291-4",
                    "sentence": "In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "291-5",
                    "sentence": "Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters.",
                    "tag": "1"
                },
                {
                    "index": "291-6",
                    "sentence": "To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model.",
                    "tag": "2+3"
                },
                {
                    "index": "291-7",
                    "sentence": "Besides, we utilize a transfer learning method to improve the performance of OOV words.",
                    "tag": "3"
                },
                {
                    "index": "291-8",
                    "sentence": "Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010).",
                    "tag": "3"
                },
                {
                    "index": "291-9",
                    "sentence": "Our method achieves the state-of-the-art performances on all datasets.",
                    "tag": "4"
                },
                {
                    "index": "291-10",
                    "sentence": "Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-292",
            "text": [
                {
                    "index": "292-0",
                    "sentence": "Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language.",
                    "tag": "1"
                },
                {
                    "index": "292-1",
                    "sentence": "Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers.",
                    "tag": "1"
                },
                {
                    "index": "292-2",
                    "sentence": "We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus.",
                    "tag": "2"
                },
                {
                    "index": "292-3",
                    "sentence": "The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings.",
                    "tag": "3"
                },
                {
                    "index": "292-4",
                    "sentence": "The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence.",
                    "tag": "3"
                },
                {
                    "index": "292-5",
                    "sentence": "It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language.",
                    "tag": "3"
                },
                {
                    "index": "292-6",
                    "sentence": "Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-293",
            "text": [
                {
                    "index": "293-0",
                    "sentence": "We study the detection of propagandistic text fragments in news articles.",
                    "tag": "2"
                },
                {
                    "index": "293-1",
                    "sentence": "Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques.",
                    "tag": "3"
                },
                {
                    "index": "293-2",
                    "sentence": "Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language.",
                    "tag": "3"
                },
                {
                    "index": "293-3",
                    "sentence": "The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions.",
                    "tag": "3"
                },
                {
                    "index": "293-4",
                    "sentence": "The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters.",
                    "tag": "3"
                },
                {
                    "index": "293-5",
                    "sentence": "We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection.",
                    "tag": "3"
                },
                {
                    "index": "293-6",
                    "sentence": "Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-294",
            "text": [
                {
                    "index": "294-0",
                    "sentence": "Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available.",
                    "tag": "1"
                },
                {
                    "index": "294-1",
                    "sentence": "In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning.",
                    "tag": "1"
                },
                {
                    "index": "294-2",
                    "sentence": "In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages.",
                    "tag": "2"
                },
                {
                    "index": "294-3",
                    "sentence": "We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages.",
                    "tag": "3"
                },
                {
                    "index": "294-4",
                    "sentence": "We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline.",
                    "tag": "3+4"
                },
                {
                    "index": "294-5",
                    "sentence": "Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-295",
            "text": [
                {
                    "index": "295-0",
                    "sentence": "Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles.",
                    "tag": "1"
                },
                {
                    "index": "295-1",
                    "sentence": "Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax.",
                    "tag": "1"
                },
                {
                    "index": "295-2",
                    "sentence": "In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system.",
                    "tag": "2+3"
                },
                {
                    "index": "295-3",
                    "sentence": "Nodes in our SpanGCN correspond to constituents.",
                    "tag": "3"
                },
                {
                    "index": "295-4",
                    "sentence": "The computation is done in 3 stages.",
                    "tag": "3"
                },
                {
                    "index": "295-5",
                    "sentence": "First, initial node representations are produced by ‘composing’ word representations of the first and last words in the constituent.",
                    "tag": "3"
                },
                {
                    "index": "295-6",
                    "sentence": "Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations.",
                    "tag": "3"
                },
                {
                    "index": "295-7",
                    "sentence": "Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier.",
                    "tag": "3"
                },
                {
                    "index": "295-8",
                    "sentence": "We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-296",
            "text": [
                {
                    "index": "296-0",
                    "sentence": "AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks.",
                    "tag": "1"
                },
                {
                    "index": "296-1",
                    "sentence": "It relies on a type system that models semantic valency but makes existing parsers slow.",
                    "tag": "1"
                },
                {
                    "index": "296-2",
                    "sentence": "We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.",
                    "tag": "1+2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-297",
            "text": [
                {
                    "index": "297-0",
                    "sentence": "This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques.",
                    "tag": "2"
                },
                {
                    "index": "297-1",
                    "sentence": "The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes.",
                    "tag": "3"
                },
                {
                    "index": "297-2",
                    "sentence": "The classification is carried out by mapping text embeddings to the word graph embeddings of the classes.",
                    "tag": "3"
                },
                {
                    "index": "297-3",
                    "sentence": "Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved.",
                    "tag": "4"
                },
                {
                    "index": "297-4",
                    "sentence": "In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-298",
            "text": [
                {
                    "index": "298-0",
                    "sentence": "Language drift has been one of the major obstacles to train language models through interaction.",
                    "tag": "1"
                },
                {
                    "index": "298-1",
                    "sentence": "When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language.",
                    "tag": "1"
                },
                {
                    "index": "298-2",
                    "sentence": "In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).",
                    "tag": "1"
                },
                {
                    "index": "298-3",
                    "sentence": "While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring.",
                    "tag": "1"
                },
                {
                    "index": "298-4",
                    "sentence": "In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus.",
                    "tag": "2"
                },
                {
                    "index": "298-5",
                    "sentence": "Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses.",
                    "tag": "3"
                },
                {
                    "index": "298-6",
                    "sentence": "We then show the effectiveness of in the language-drift translation game.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-299",
            "text": [
                {
                    "index": "299-0",
                    "sentence": "The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots).",
                    "tag": "2"
                },
                {
                    "index": "299-1",
                    "sentence": "Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results.",
                    "tag": "2"
                },
                {
                    "index": "299-2",
                    "sentence": "In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots.",
                    "tag": "2"
                },
                {
                    "index": "299-3",
                    "sentence": "Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations).",
                    "tag": "2+3"
                },
                {
                    "index": "299-4",
                    "sentence": "These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans.",
                    "tag": "3"
                },
                {
                    "index": "299-5",
                    "sentence": "Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis.",
                    "tag": "3"
                },
                {
                    "index": "299-6",
                    "sentence": "This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results.",
                    "tag": "4"
                },
                {
                    "index": "299-7",
                    "sentence": "The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle.",
                    "tag": "4"
                },
                {
                    "index": "299-8",
                    "sentence": "We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work.",
                    "tag": "4"
                },
                {
                    "index": "299-9",
                    "sentence": "The framework is released asa ready-to-use tool.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-300",
            "text": [
                {
                    "index": "300-0",
                    "sentence": "How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors?",
                    "tag": "1"
                },
                {
                    "index": "300-1",
                    "sentence": "We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL).",
                    "tag": "2+3"
                },
                {
                    "index": "300-2",
                    "sentence": "We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.",
                    "tag": "3"
                },
                {
                    "index": "300-3",
                    "sentence": "A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward.",
                    "tag": "3+4"
                },
                {
                    "index": "300-4",
                    "sentence": "These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.",
                    "tag": "3+4"
                },
                {
                    "index": "300-5",
                    "sentence": "We solve the challenge by developing a novel class of offline RL algorithms.",
                    "tag": "3"
                },
                {
                    "index": "300-6",
                    "sentence": "These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.",
                    "tag": "4"
                },
                {
                    "index": "300-7",
                    "sentence": "We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches.",
                    "tag": "4"
                },
                {
                    "index": "300-8",
                    "sentence": "The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-301",
            "text": [
                {
                    "index": "301-0",
                    "sentence": "Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules.",
                    "tag": "1"
                },
                {
                    "index": "301-1",
                    "sentence": "While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data.",
                    "tag": "1"
                },
                {
                    "index": "301-2",
                    "sentence": "We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different.",
                    "tag": "2+3"
                },
                {
                    "index": "301-3",
                    "sentence": "We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-302",
            "text": [
                {
                    "index": "302-0",
                    "sentence": "Why do bilinguals switch languages within a sentence?",
                    "tag": "1"
                },
                {
                    "index": "302-1",
                    "sentence": "The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation.",
                    "tag": "1"
                },
                {
                    "index": "302-2",
                    "sentence": "We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese.",
                    "tag": "2+3"
                },
                {
                    "index": "302-3",
                    "sentence": "The model includes known control variables together with word surprisal and word entropy.",
                    "tag": "4"
                },
                {
                    "index": "302-4",
                    "sentence": "We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors.",
                    "tag": "4"
                },
                {
                    "index": "302-5",
                    "sentence": "We also found sentence length to be a significant predictor, which has been related to sentence complexity.",
                    "tag": "5"
                },
                {
                    "index": "302-6",
                    "sentence": "We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language.",
                    "tag": "5"
                },
                {
                    "index": "302-7",
                    "sentence": "We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-303",
            "text": [
                {
                    "index": "303-0",
                    "sentence": "Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages.",
                    "tag": "1"
                },
                {
                    "index": "303-1",
                    "sentence": "In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy.",
                    "tag": "2"
                },
                {
                    "index": "303-2",
                    "sentence": "Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models.",
                    "tag": "3"
                },
                {
                    "index": "303-3",
                    "sentence": "Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks.",
                    "tag": "4"
                },
                {
                    "index": "303-4",
                    "sentence": "Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data.",
                    "tag": "4"
                },
                {
                    "index": "303-5",
                    "sentence": "The results present a paradox: there should be less variation in grammatical performance than is actually observed.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-304",
            "text": [
                {
                    "index": "304-0",
                    "sentence": "It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD).",
                    "tag": "1"
                },
                {
                    "index": "304-1",
                    "sentence": "However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations.",
                    "tag": "1"
                },
                {
                    "index": "304-2",
                    "sentence": "In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation.",
                    "tag": "2"
                },
                {
                    "index": "304-3",
                    "sentence": "Since our approach is language independent, we perform WSD experiments on several languages.",
                    "tag": "3"
                },
                {
                    "index": "304-4",
                    "sentence": "The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD.",
                    "tag": "4"
                },
                {
                    "index": "304-5",
                    "sentence": "To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-305",
            "text": [
                {
                    "index": "305-0",
                    "sentence": "One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics.",
                    "tag": "1"
                },
                {
                    "index": "305-1",
                    "sentence": "In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors.",
                    "tag": "2"
                },
                {
                    "index": "305-2",
                    "sentence": "Our method requires only another pre-trained model and no labeled data is needed.",
                    "tag": "3"
                },
                {
                    "index": "305-3",
                    "sentence": "We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context.",
                    "tag": "4"
                },
                {
                    "index": "305-4",
                    "sentence": "We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-306",
            "text": [
                {
                    "index": "306-0",
                    "sentence": "Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations.",
                    "tag": "1"
                },
                {
                    "index": "306-1",
                    "sentence": "While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users.",
                    "tag": "1"
                },
                {
                    "index": "306-2",
                    "sentence": "We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion).",
                    "tag": "2+3"
                },
                {
                    "index": "306-3",
                    "sentence": "We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations.",
                    "tag": "4"
                },
                {
                    "index": "306-4",
                    "sentence": "We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-307",
            "text": [
                {
                    "index": "307-0",
                    "sentence": "In politics, neologisms are frequently invented for partisan objectives.",
                    "tag": "1"
                },
                {
                    "index": "307-1",
                    "sentence": "For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations.",
                    "tag": "1"
                },
                {
                    "index": "307-2",
                    "sentence": "Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists.",
                    "tag": "1"
                },
                {
                    "index": "307-3",
                    "sentence": "In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation.",
                    "tag": "1"
                },
                {
                    "index": "307-4",
                    "sentence": "In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations.",
                    "tag": "2"
                },
                {
                    "index": "307-5",
                    "sentence": "For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., “immigrants” vs. “aliens”, “estate tax” vs. “death tax”) move closer to each other in denotation space while moving further apart in connotation space.",
                    "tag": "3"
                },
                {
                    "index": "307-6",
                    "sentence": "For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-308",
            "text": [
                {
                    "index": "308-0",
                    "sentence": "Text summarization is one of the most challenging and interesting problems in NLP.",
                    "tag": "1"
                },
                {
                    "index": "308-1",
                    "sentence": "Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated.",
                    "tag": "1"
                },
                {
                    "index": "308-2",
                    "sentence": "This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries.",
                    "tag": "2+3"
                },
                {
                    "index": "308-3",
                    "sentence": "Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment.",
                    "tag": "4"
                },
                {
                    "index": "308-4",
                    "sentence": "We also discussed specific challenges that current approaches faced with this task.",
                    "tag": "4"
                },
                {
                    "index": "308-5",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-309",
            "text": [
                {
                    "index": "309-0",
                    "sentence": "Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product.",
                    "tag": "1"
                },
                {
                    "index": "309-1",
                    "sentence": "The task is practically important and has attracted a lot of attention.",
                    "tag": "1"
                },
                {
                    "index": "309-2",
                    "sentence": "However, due to the high cost of summary production, datasets large enough for training supervised models are lacking.",
                    "tag": "1"
                },
                {
                    "index": "309-3",
                    "sentence": "Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way.",
                    "tag": "1"
                },
                {
                    "index": "309-4",
                    "sentence": "Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion.",
                    "tag": "1"
                },
                {
                    "index": "309-5",
                    "sentence": "However, these models, not being exposed to actual summaries, fail to capture their essential properties.",
                    "tag": "1"
                },
                {
                    "index": "309-6",
                    "sentence": "In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation.",
                    "tag": "1+2"
                },
                {
                    "index": "309-7",
                    "sentence": "We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product.",
                    "tag": "3"
                },
                {
                    "index": "309-8",
                    "sentence": "The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort.",
                    "tag": "3"
                },
                {
                    "index": "309-9",
                    "sentence": "In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries.",
                    "tag": "3"
                },
                {
                    "index": "309-10",
                    "sentence": "This lets us switch the generator to the summarization mode.",
                    "tag": "3"
                },
                {
                    "index": "309-11",
                    "sentence": "We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-310",
            "text": [
                {
                    "index": "310-0",
                    "sentence": "The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts.",
                    "tag": "1"
                },
                {
                    "index": "310-1",
                    "sentence": "However, to date, summarizers can fail on fusing sentences.",
                    "tag": "1"
                },
                {
                    "index": "310-2",
                    "sentence": "They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning.",
                    "tag": "1"
                },
                {
                    "index": "310-3",
                    "sentence": "In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences.",
                    "tag": "2"
                },
                {
                    "index": "310-4",
                    "sentence": "Through extensive experiments, we investigate the effects of different design choices on Transformer’s performance.",
                    "tag": "4"
                },
                {
                    "index": "310-5",
                    "sentence": "Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-311",
            "text": [
                {
                    "index": "311-0",
                    "sentence": "We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers.",
                    "tag": "2+3"
                },
                {
                    "index": "311-1",
                    "sentence": "We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure.",
                    "tag": "3"
                },
                {
                    "index": "311-2",
                    "sentence": "Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks.",
                    "tag": "3"
                },
                {
                    "index": "311-3",
                    "sentence": "When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering.",
                    "tag": "4"
                },
                {
                    "index": "311-4",
                    "sentence": "This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling.",
                    "tag": "3"
                },
                {
                    "index": "311-5",
                    "sentence": "Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-312",
            "text": [
                {
                    "index": "312-0",
                    "sentence": "We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia.",
                    "tag": "2+3"
                },
                {
                    "index": "312-1",
                    "sentence": "CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.",
                    "tag": "3"
                },
                {
                    "index": "312-2",
                    "sentence": "In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date.",
                    "tag": "3"
                },
                {
                    "index": "312-3",
                    "sentence": "This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url].",
                    "tag": "6"
                },
                {
                    "index": "312-4",
                    "sentence": "We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-313",
            "text": [
                {
                    "index": "313-0",
                    "sentence": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus.",
                    "tag": "1"
                },
                {
                    "index": "313-1",
                    "sentence": "Clinicians, researchers, and policy-makers need to be able to search these articles effectively.",
                    "tag": "1"
                },
                {
                    "index": "313-2",
                    "sentence": "In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature.",
                    "tag": "2"
                },
                {
                    "index": "313-3",
                    "sentence": "Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection.",
                    "tag": "3"
                },
                {
                    "index": "313-4",
                    "sentence": "This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments.",
                    "tag": "3"
                },
                {
                    "index": "313-5",
                    "sentence": "Despite not relying on TREC-COVID data, our method outperforms models that do.",
                    "tag": "4"
                },
                {
                    "index": "313-6",
                    "sentence": "As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-314",
            "text": [
                {
                    "index": "314-0",
                    "sentence": "Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval.",
                    "tag": "1"
                },
                {
                    "index": "314-1",
                    "sentence": "However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process.",
                    "tag": "1"
                },
                {
                    "index": "314-2",
                    "sentence": "In this work, we modularize the Transformer ranker into separate modules for text representation and interaction.",
                    "tag": "2"
                },
                {
                    "index": "314-3",
                    "sentence": "We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions.",
                    "tag": "3"
                },
                {
                    "index": "314-4",
                    "sentence": "The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-315",
            "text": [
                {
                    "index": "315-0",
                    "sentence": "We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval.",
                    "tag": "2"
                },
                {
                    "index": "315-1",
                    "sentence": "Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus.",
                    "tag": "3"
                },
                {
                    "index": "315-2",
                    "sentence": "We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers.",
                    "tag": "2"
                },
                {
                    "index": "315-3",
                    "sentence": "We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets.",
                    "tag": "3"
                },
                {
                    "index": "315-4",
                    "sentence": "We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-316",
            "text": [
                {
                    "index": "316-0",
                    "sentence": "We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models.",
                    "tag": "2+3"
                },
                {
                    "index": "316-1",
                    "sentence": "We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions.",
                    "tag": "2+3"
                },
                {
                    "index": "316-2",
                    "sentence": "For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3.",
                    "tag": "3"
                },
                {
                    "index": "316-3",
                    "sentence": "We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations.",
                    "tag": "5"
                },
                {
                    "index": "316-4",
                    "sentence": "Our code is available at https://github.com/csong27/collision-bert.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-317",
            "text": [
                {
                    "index": "317-0",
                    "sentence": "Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world.",
                    "tag": "1"
                },
                {
                    "index": "317-1",
                    "sentence": "We present RuleNN, a neural network architecture for learning transparent models for sentence classification.",
                    "tag": "1+2"
                },
                {
                    "index": "317-2",
                    "sentence": "The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics.",
                    "tag": "3"
                },
                {
                    "index": "317-3",
                    "sentence": "More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding.",
                    "tag": "3"
                },
                {
                    "index": "317-4",
                    "sentence": "Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks.",
                    "tag": "4"
                },
                {
                    "index": "317-5",
                    "sentence": "Our user studies confirm that the learned LEs are explainable and capture domain semantics.",
                    "tag": "5"
                },
                {
                    "index": "317-6",
                    "sentence": "Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-318",
            "text": [
                {
                    "index": "318-0",
                    "sentence": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining.",
                    "tag": "1"
                },
                {
                    "index": "318-1",
                    "sentence": "Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts.",
                    "tag": "1"
                },
                {
                    "index": "318-2",
                    "sentence": "To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search.",
                    "tag": "2+3"
                },
                {
                    "index": "318-3",
                    "sentence": "Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models.",
                    "tag": "3+4"
                },
                {
                    "index": "318-4",
                    "sentence": "We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models.",
                    "tag": "4"
                },
                {
                    "index": "318-5",
                    "sentence": "These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-319",
            "text": [
                {
                    "index": "319-0",
                    "sentence": "To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations.",
                    "tag": "1"
                },
                {
                    "index": "319-1",
                    "sentence": "A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training.",
                    "tag": "1"
                },
                {
                    "index": "319-2",
                    "sentence": "To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions.",
                    "tag": "2+3"
                },
                {
                    "index": "319-3",
                    "sentence": "The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets.",
                    "tag": "3"
                },
                {
                    "index": "319-4",
                    "sentence": "Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-320",
            "text": [
                {
                    "index": "320-0",
                    "sentence": "Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance.",
                    "tag": "1"
                },
                {
                    "index": "320-1",
                    "sentence": "However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling.",
                    "tag": "1"
                },
                {
                    "index": "320-2",
                    "sentence": "This creates a mismatch between training and testing conditions.",
                    "tag": "1"
                },
                {
                    "index": "320-3",
                    "sentence": "In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch.",
                    "tag": "2"
                },
                {
                    "index": "320-4",
                    "sentence": "The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text.",
                    "tag": "4"
                },
                {
                    "index": "320-5",
                    "sentence": "In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: 𝜖-perplexity, sparsemax score, and Jensen-Shannon divergence.",
                    "tag": "2+3"
                },
                {
                    "index": "320-6",
                    "sentence": "Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-321",
            "text": [
                {
                    "index": "321-0",
                    "sentence": "We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline.",
                    "tag": "2+3"
                },
                {
                    "index": "321-1",
                    "sentence": "This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline.",
                    "tag": "3"
                },
                {
                    "index": "321-2",
                    "sentence": "This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story.",
                    "tag": "3"
                },
                {
                    "index": "321-3",
                    "sentence": "We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states.",
                    "tag": "3"
                },
                {
                    "index": "321-4",
                    "sentence": "In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative.",
                    "tag": "3"
                },
                {
                    "index": "321-5",
                    "sentence": "Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-322",
            "text": [
                {
                    "index": "322-0",
                    "sentence": "Autoregressive language models are powerful and relatively easy to train.",
                    "tag": "1"
                },
                {
                    "index": "322-1",
                    "sentence": "However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation.",
                    "tag": "1"
                },
                {
                    "index": "322-2",
                    "sentence": "Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner.",
                    "tag": "1"
                },
                {
                    "index": "322-3",
                    "sentence": "We question this claim.",
                    "tag": "2"
                },
                {
                    "index": "322-4",
                    "sentence": "We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence.",
                    "tag": "3"
                },
                {
                    "index": "322-5",
                    "sentence": "Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness.",
                    "tag": "4"
                },
                {
                    "index": "322-6",
                    "sentence": "To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining.",
                    "tag": "3"
                },
                {
                    "index": "322-7",
                    "sentence": "These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels.",
                    "tag": "3"
                },
                {
                    "index": "322-8",
                    "sentence": "Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-323",
            "text": [
                {
                    "index": "323-0",
                    "sentence": "Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion.",
                    "tag": "1"
                },
                {
                    "index": "323-1",
                    "sentence": "We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation.",
                    "tag": "2"
                },
                {
                    "index": "323-2",
                    "sentence": "We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle’s Poetics.",
                    "tag": "3"
                },
                {
                    "index": "323-3",
                    "sentence": "We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-324",
            "text": [
                {
                    "index": "324-0",
                    "sentence": "Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary.",
                    "tag": "1"
                },
                {
                    "index": "324-1",
                    "sentence": "However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt.",
                    "tag": "1"
                },
                {
                    "index": "324-2",
                    "sentence": "In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses.",
                    "tag": "2"
                },
                {
                    "index": "324-3",
                    "sentence": "To address this issue, we propose an alternative to the end-to-end classification on vocabulary.",
                    "tag": "2"
                },
                {
                    "index": "324-4",
                    "sentence": "We learn the pair relationship between the prompts and responses as a regression task on a latent space instead.",
                    "tag": "3"
                },
                {
                    "index": "324-5",
                    "sentence": "In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space.",
                    "tag": "4"
                },
                {
                    "index": "324-6",
                    "sentence": "Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-325",
            "text": [
                {
                    "index": "325-0",
                    "sentence": "Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness.",
                    "tag": "1"
                },
                {
                    "index": "325-1",
                    "sentence": "Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions.",
                    "tag": "1"
                },
                {
                    "index": "325-2",
                    "sentence": "In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue.",
                    "tag": "2"
                },
                {
                    "index": "325-3",
                    "sentence": "We propose a generation model that produces referring utterances grounded in both the visual and the conversational context.",
                    "tag": "2"
                },
                {
                    "index": "325-4",
                    "sentence": "To assess the referring effectiveness of its output, we also implement a reference resolution system.",
                    "tag": "2"
                },
                {
                    "index": "325-5",
                    "sentence": "Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-326",
            "text": [
                {
                    "index": "326-0",
                    "sentence": "Exploiting visual groundings for language understanding has recently been drawing much attention.",
                    "tag": "1"
                },
                {
                    "index": "326-1",
                    "sentence": "In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings.",
                    "tag": "2"
                },
                {
                    "index": "326-2",
                    "sentence": "Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences.",
                    "tag": "3"
                },
                {
                    "index": "326-3",
                    "sentence": "While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs).",
                    "tag": "3"
                },
                {
                    "index": "326-4",
                    "sentence": "This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy.",
                    "tag": "4"
                },
                {
                    "index": "326-5",
                    "sentence": "We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning.",
                    "tag": "4"
                },
                {
                    "index": "326-6",
                    "sentence": "Additionally, this enables us to complement the image-text alignment loss with a language modeling objective.",
                    "tag": "4"
                },
                {
                    "index": "326-7",
                    "sentence": "On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction.",
                    "tag": "5"
                },
                {
                    "index": "326-8",
                    "sentence": "It also substantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs).",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-327",
            "text": [
                {
                    "index": "327-0",
                    "sentence": "Training a supervised neural network classifier typically requires many annotated training samples.",
                    "tag": "1"
                },
                {
                    "index": "327-1",
                    "sentence": "Collecting and annotating a large number of data points are costly and sometimes even infeasible.",
                    "tag": "1"
                },
                {
                    "index": "327-2",
                    "sentence": "Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information.",
                    "tag": "1"
                },
                {
                    "index": "327-3",
                    "sentence": "We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning.",
                    "tag": "2"
                },
                {
                    "index": "327-4",
                    "sentence": "AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts.",
                    "tag": "3"
                },
                {
                    "index": "327-5",
                    "sentence": "Then it extracts knowledge from these explanations using a semantic parser.",
                    "tag": "3"
                },
                {
                    "index": "327-6",
                    "sentence": "Finally, it incorporates the extracted knowledge through dynamically changing the learning model’s structure.",
                    "tag": "3"
                },
                {
                    "index": "327-7",
                    "sentence": "We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification.",
                    "tag": "3"
                },
                {
                    "index": "327-8",
                    "sentence": "We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data.",
                    "tag": "3+4"
                },
                {
                    "index": "327-9",
                    "sentence": "We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-328",
            "text": [
                {
                    "index": "328-0",
                    "sentence": "Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step.",
                    "tag": "1"
                },
                {
                    "index": "328-1",
                    "sentence": "Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes.",
                    "tag": "1"
                },
                {
                    "index": "328-2",
                    "sentence": "Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair.",
                    "tag": "1"
                },
                {
                    "index": "328-3",
                    "sentence": "Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already.",
                    "tag": "1"
                },
                {
                    "index": "328-4",
                    "sentence": "In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity.",
                    "tag": "2"
                },
                {
                    "index": "328-5",
                    "sentence": "SSCR allows the model to consider out-of-distribution instructions paired with previous images.",
                    "tag": "2"
                },
                {
                    "index": "328-6",
                    "sentence": "With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario.",
                    "tag": "3"
                },
                {
                    "index": "328-7",
                    "sentence": "Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw).",
                    "tag": "4"
                },
                {
                    "index": "328-8",
                    "sentence": "Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-329",
            "text": [
                {
                    "index": "329-0",
                    "sentence": "It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer.",
                    "tag": "1"
                },
                {
                    "index": "329-1",
                    "sentence": "This is surprising given that mBERT does not use any crosslingual signal during training.",
                    "tag": "1"
                },
                {
                    "index": "329-2",
                    "sentence": "While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure.",
                    "tag": "1"
                },
                {
                    "index": "329-3",
                    "sentence": "We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual.",
                    "tag": "2"
                },
                {
                    "index": "329-4",
                    "sentence": "To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data.",
                    "tag": "2"
                },
                {
                    "index": "329-5",
                    "sentence": "Overall, we identify four architectural and two linguistic elements that influence multilinguality.",
                    "tag": "3"
                },
                {
                    "index": "329-6",
                    "sentence": "Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment.",
                    "tag": "3"
                },
                {
                    "index": "329-7",
                    "sentence": "Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-330",
            "text": [
                {
                    "index": "330-0",
                    "sentence": "Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages.",
                    "tag": "1"
                },
                {
                    "index": "330-1",
                    "sentence": "However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference.",
                    "tag": "1"
                },
                {
                    "index": "330-2",
                    "sentence": "In this paper, we present the first systematic study of negative interference.",
                    "tag": "2"
                },
                {
                    "index": "330-3",
                    "sentence": "We show that, contrary to previous belief, negative interference also impacts low-resource languages.",
                    "tag": "4"
                },
                {
                    "index": "330-4",
                    "sentence": "While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference.",
                    "tag": "3"
                },
                {
                    "index": "330-5",
                    "sentence": "Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers’ generalization on all languages.",
                    "tag": "2+3"
                },
                {
                    "index": "330-6",
                    "sentence": "Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-331",
            "text": [
                {
                    "index": "331-0",
                    "sentence": "Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space.",
                    "tag": "1"
                },
                {
                    "index": "331-1",
                    "sentence": "Multi-Word Expressions (MWE) are common in every language.",
                    "tag": "1"
                },
                {
                    "index": "331-2",
                    "sentence": "When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs.",
                    "tag": "1"
                },
                {
                    "index": "331-3",
                    "sentence": "We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings.",
                    "tag": "2+3"
                },
                {
                    "index": "331-4",
                    "sentence": "CWEs are trained on a word-translation task using the dictionaries that only contain single words.",
                    "tag": "3"
                },
                {
                    "index": "331-5",
                    "sentence": "In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language.",
                    "tag": "3"
                },
                {
                    "index": "331-6",
                    "sentence": "We release these dictionaries to the research community.",
                    "tag": "3"
                },
                {
                    "index": "331-7",
                    "sentence": "We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE.",
                    "tag": "4"
                },
                {
                    "index": "331-8",
                    "sentence": "We can translate MWEs at a top-10 precision of 30-60%.",
                    "tag": "4"
                },
                {
                    "index": "331-9",
                    "sentence": "The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-332",
            "text": [
                {
                    "index": "332-0",
                    "sentence": "We propose a novel adapter layer formalism for adapting multilingual models.",
                    "tag": "2"
                },
                {
                    "index": "332-1",
                    "sentence": "They are more parameter-efficient than existing adapter layers while obtaining as good or better performance.",
                    "tag": "4"
                },
                {
                    "index": "332-2",
                    "sentence": "The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs.",
                    "tag": "3+4"
                },
                {
                    "index": "332-3",
                    "sentence": "In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-333",
            "text": [
                {
                    "index": "333-0",
                    "sentence": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation.",
                    "tag": "1"
                },
                {
                    "index": "333-1",
                    "sentence": "Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations.",
                    "tag": "1"
                },
                {
                    "index": "333-2",
                    "sentence": "However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages.",
                    "tag": "1"
                },
                {
                    "index": "333-3",
                    "sentence": "In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection.",
                    "tag": "2"
                },
                {
                    "index": "333-4",
                    "sentence": "Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks.",
                    "tag": "2"
                },
                {
                    "index": "333-5",
                    "sentence": "Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework.",
                    "tag": "4"
                },
                {
                    "index": "333-6",
                    "sentence": "Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training.",
                    "tag": "4"
                },
                {
                    "index": "333-7",
                    "sentence": "These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-334",
            "text": [
                {
                    "index": "334-0",
                    "sentence": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance.",
                    "tag": "1"
                },
                {
                    "index": "334-1",
                    "sentence": "Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages.",
                    "tag": "1"
                },
                {
                    "index": "334-2",
                    "sentence": "In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages.",
                    "tag": "2"
                },
                {
                    "index": "334-3",
                    "sentence": "Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining.",
                    "tag": "3"
                },
                {
                    "index": "334-4",
                    "sentence": "Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-335",
            "text": [
                {
                    "index": "335-0",
                    "sentence": "Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource.",
                    "tag": "1"
                },
                {
                    "index": "335-1",
                    "sentence": "The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity).",
                    "tag": "1"
                },
                {
                    "index": "335-2",
                    "sentence": "In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage.",
                    "tag": "2"
                },
                {
                    "index": "335-3",
                    "sentence": "We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines.",
                    "tag": "4"
                },
                {
                    "index": "335-4",
                    "sentence": "In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-336",
            "text": [
                {
                    "index": "336-0",
                    "sentence": "We present an easy and efficient method to extend existing sentence embedding models to new languages.",
                    "tag": "2"
                },
                {
                    "index": "336-1",
                    "sentence": "This allows to create multilingual versions from previously monolingual models.",
                    "tag": "2"
                },
                {
                    "index": "336-2",
                    "sentence": "The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence.",
                    "tag": "3"
                },
                {
                    "index": "336-3",
                    "sentence": "We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model.",
                    "tag": "3"
                },
                {
                    "index": "336-4",
                    "sentence": "Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower.",
                    "tag": "4"
                },
                {
                    "index": "336-5",
                    "sentence": "We demonstrate the effectiveness of our approach for 50+ languages from various language families.",
                    "tag": "4"
                },
                {
                    "index": "336-6",
                    "sentence": "Code to extend sentence embeddings models to more than 400 languages is publicly available.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-337",
            "text": [
                {
                    "index": "337-0",
                    "sentence": "We propose an efficient batching strategy for variable-length decoding on GPU architectures.",
                    "tag": "2"
                },
                {
                    "index": "337-1",
                    "sentence": "During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates.",
                    "tag": "3"
                },
                {
                    "index": "337-2",
                    "sentence": "We apply our method to variable-width beam search on a state-of-the-art machine translation model.",
                    "tag": "3"
                },
                {
                    "index": "337-3",
                    "sentence": "Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines’ BLEU.",
                    "tag": "4"
                },
                {
                    "index": "337-4",
                    "sentence": "Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-338",
            "text": [
                {
                    "index": "338-0",
                    "sentence": "State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications.",
                    "tag": "1"
                },
                {
                    "index": "338-1",
                    "sentence": "In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies.",
                    "tag": "2+3"
                },
                {
                    "index": "338-2",
                    "sentence": "Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-339",
            "text": [
                {
                    "index": "339-0",
                    "sentence": "Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance.",
                    "tag": "1"
                },
                {
                    "index": "339-1",
                    "sentence": "This is particularly important for multilingual applications, as most languages in the world are under-resourced.",
                    "tag": "1"
                },
                {
                    "index": "339-2",
                    "sentence": "Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English.",
                    "tag": "2"
                },
                {
                    "index": "339-3",
                    "sentence": "We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first.",
                    "tag": "2"
                },
                {
                    "index": "339-4",
                    "sentence": "We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering).",
                    "tag": "3"
                },
                {
                    "index": "339-5",
                    "sentence": "Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages.",
                    "tag": "4"
                },
                {
                    "index": "339-6",
                    "sentence": "We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset).",
                    "tag": "4"
                },
                {
                    "index": "339-7",
                    "sentence": "A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-340",
            "text": [
                {
                    "index": "340-0",
                    "sentence": "We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification.",
                    "tag": "2"
                },
                {
                    "index": "340-1",
                    "sentence": "The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019.",
                    "tag": "3"
                },
                {
                    "index": "340-2",
                    "sentence": "Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.)",
                    "tag": "3"
                },
                {
                    "index": "340-3",
                    "sentence": "The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language.",
                    "tag": "3"
                },
                {
                    "index": "340-4",
                    "sentence": "For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively.",
                    "tag": "3"
                },
                {
                    "index": "340-5",
                    "sentence": "We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data.",
                    "tag": "3"
                },
                {
                    "index": "340-6",
                    "sentence": "We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-341",
            "text": [
                {
                    "index": "341-0",
                    "sentence": "We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing.",
                    "tag": "2+3"
                },
                {
                    "index": "341-1",
                    "sentence": "Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets.",
                    "tag": "3"
                },
                {
                    "index": "341-2",
                    "sentence": "For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings.",
                    "tag": "3"
                },
                {
                    "index": "341-3",
                    "sentence": "A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-342",
            "text": [
                {
                    "index": "342-0",
                    "sentence": "Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment.",
                    "tag": "1"
                },
                {
                    "index": "342-1",
                    "sentence": "This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition.",
                    "tag": "1"
                },
                {
                    "index": "342-2",
                    "sentence": "While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects.",
                    "tag": "2"
                },
                {
                    "index": "342-3",
                    "sentence": "Hence, we integrate BERT with disease knowledge for improving these important tasks.",
                    "tag": "2"
                },
                {
                    "index": "342-4",
                    "sentence": "Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT.",
                    "tag": "2"
                },
                {
                    "index": "342-5",
                    "sentence": "Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion.",
                    "tag": "4"
                },
                {
                    "index": "342-6",
                    "sentence": "For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets.",
                    "tag": "4"
                },
                {
                    "index": "342-7",
                    "sentence": "We make our data and code freely available.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-343",
            "text": [
                {
                    "index": "343-0",
                    "sentence": "Natural language understanding involves reading between the lines with implicit background knowledge.",
                    "tag": "1"
                },
                {
                    "index": "343-1",
                    "sentence": "Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge.",
                    "tag": "1"
                },
                {
                    "index": "343-2",
                    "sentence": "We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks.",
                    "tag": "2"
                },
                {
                    "index": "343-3",
                    "sentence": "Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as “what is the definition of...” to discover additional background knowledge.",
                    "tag": "3"
                },
                {
                    "index": "343-4",
                    "sentence": "Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs.",
                    "tag": "4"
                },
                {
                    "index": "343-5",
                    "sentence": "While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-344",
            "text": [
                {
                    "index": "344-0",
                    "sentence": "We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”).",
                    "tag": "2"
                },
                {
                    "index": "344-1",
                    "sentence": "We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles.",
                    "tag": "2"
                },
                {
                    "index": "344-2",
                    "sentence": "Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance.",
                    "tag": "3"
                },
                {
                    "index": "344-3",
                    "sentence": "Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-345",
            "text": [
                {
                    "index": "345-0",
                    "sentence": "Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts.",
                    "tag": "1"
                },
                {
                    "index": "345-1",
                    "sentence": "We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes.",
                    "tag": "2"
                },
                {
                    "index": "345-2",
                    "sentence": "First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training.",
                    "tag": "3"
                },
                {
                    "index": "345-3",
                    "sentence": "Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence).",
                    "tag": "3"
                },
                {
                    "index": "345-4",
                    "sentence": "We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision.",
                    "tag": "3"
                },
                {
                    "index": "345-5",
                    "sentence": "We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model.",
                    "tag": "4"
                },
                {
                    "index": "345-6",
                    "sentence": "All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-346",
            "text": [
                {
                    "index": "346-0",
                    "sentence": "When speakers describe an image, they tend to look at objects before mentioning them.",
                    "tag": "1"
                },
                {
                    "index": "346-1",
                    "sentence": "In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally.",
                    "tag": "2"
                },
                {
                    "index": "346-2",
                    "sentence": "We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production.",
                    "tag": "2"
                },
                {
                    "index": "346-3",
                    "sentence": "In particular, we propose the first approach to image description generation where visual processing is modelled sequentially.",
                    "tag": "2"
                },
                {
                    "index": "346-4",
                    "sentence": "Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production.",
                    "tag": "4"
                },
                {
                    "index": "346-5",
                    "sentence": "We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-347",
            "text": [
                {
                    "index": "347-0",
                    "sentence": "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language.",
                    "tag": "1"
                },
                {
                    "index": "347-1",
                    "sentence": "In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space).",
                    "tag": "2"
                },
                {
                    "index": "347-2",
                    "sentence": "A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks.",
                    "tag": "3"
                },
                {
                    "index": "347-3",
                    "sentence": "Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors.",
                    "tag": "4"
                },
                {
                    "index": "347-4",
                    "sentence": "Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure.",
                    "tag": "4"
                },
                {
                    "index": "347-5",
                    "sentence": "Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus.",
                    "tag": "4"
                },
                {
                    "index": "347-6",
                    "sentence": "It achieves new state-of-the-art on VAE language modeling benchmarks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-348",
            "text": [
                {
                    "index": "348-0",
                    "sentence": "There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books.",
                    "tag": "1"
                },
                {
                    "index": "348-1",
                    "sentence": "Yet, most works do not study the factors affecting each domain language application deeply.",
                    "tag": "1"
                },
                {
                    "index": "348-2",
                    "sentence": "Additionally, the study of model size on domain-specific models has been mostly missing.",
                    "tag": "1"
                },
                {
                    "index": "348-3",
                    "sentence": "We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer.",
                    "tag": "2"
                },
                {
                    "index": "348-4",
                    "sentence": "We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications.",
                    "tag": "4"
                },
                {
                    "index": "348-5",
                    "sentence": "We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction.",
                    "tag": "4"
                },
                {
                    "index": "348-6",
                    "sentence": "Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-349",
            "text": [
                {
                    "index": "349-0",
                    "sentence": "Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization.",
                    "tag": "1"
                },
                {
                    "index": "349-1",
                    "sentence": "In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets.",
                    "tag": "2"
                },
                {
                    "index": "349-2",
                    "sentence": "We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases.",
                    "tag": "2"
                },
                {
                    "index": "349-3",
                    "sentence": "We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-350",
            "text": [
                {
                    "index": "350-0",
                    "sentence": "Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.",
                    "tag": "1"
                },
                {
                    "index": "350-1",
                    "sentence": "In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning.",
                    "tag": "2"
                },
                {
                    "index": "350-2",
                    "sentence": "We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction.",
                    "tag": "2"
                },
                {
                    "index": "350-3",
                    "sentence": "Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings.",
                    "tag": "4"
                },
                {
                    "index": "350-4",
                    "sentence": "We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-351",
            "text": [
                {
                    "index": "351-0",
                    "sentence": "In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation.",
                    "tag": "2"
                },
                {
                    "index": "351-1",
                    "sentence": "Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion.",
                    "tag": "3"
                },
                {
                    "index": "351-2",
                    "sentence": "Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery.",
                    "tag": "3"
                },
                {
                    "index": "351-3",
                    "sentence": "As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task.",
                    "tag": "4"
                },
                {
                    "index": "351-4",
                    "sentence": "Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-352",
            "text": [
                {
                    "index": "352-0",
                    "sentence": "Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text.",
                    "tag": "1"
                },
                {
                    "index": "352-1",
                    "sentence": "This leads to problems with numerical reasoning involving tasks such as question answering.",
                    "tag": "1"
                },
                {
                    "index": "352-2",
                    "sentence": "We propose a new methodology to assign and learn embeddings for numbers.",
                    "tag": "2"
                },
                {
                    "index": "352-3",
                    "sentence": "Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line.",
                    "tag": "3"
                },
                {
                    "index": "352-4",
                    "sentence": "DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition.",
                    "tag": "4"
                },
                {
                    "index": "352-5",
                    "sentence": "We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction.",
                    "tag": "3"
                },
                {
                    "index": "352-6",
                    "sentence": "We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-353",
            "text": [
                {
                    "index": "353-0",
                    "sentence": "We conduct a large scale empirical investigation of contextualized number prediction in running text.",
                    "tag": "2"
                },
                {
                    "index": "353-1",
                    "sentence": "Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value within a sentence.",
                    "tag": "3"
                },
                {
                    "index": "353-2",
                    "sentence": "We experiment with novel combinations of contextual encoders and output distributions over the real number line.",
                    "tag": "3"
                },
                {
                    "index": "353-3",
                    "sentence": "Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures.",
                    "tag": "2+3"
                },
                {
                    "index": "353-4",
                    "sentence": "We evaluate these models on two numeric datasets in the financial and scientific domain.",
                    "tag": "3"
                },
                {
                    "index": "353-5",
                    "sentence": "Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection.",
                    "tag": "4"
                },
                {
                    "index": "353-6",
                    "sentence": "We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-354",
            "text": [
                {
                    "index": "354-0",
                    "sentence": "Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors.",
                    "tag": "1"
                },
                {
                    "index": "354-1",
                    "sentence": "Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive.",
                    "tag": "1"
                },
                {
                    "index": "354-2",
                    "sentence": "To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production.",
                    "tag": "2"
                },
                {
                    "index": "354-3",
                    "sentence": "Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character’s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach.",
                    "tag": "3"
                },
                {
                    "index": "354-4",
                    "sentence": "Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-355",
            "text": [
                {
                    "index": "355-0",
                    "sentence": "Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures.",
                    "tag": "1"
                },
                {
                    "index": "355-1",
                    "sentence": "We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit.",
                    "tag": "2"
                },
                {
                    "index": "355-2",
                    "sentence": "Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways.",
                    "tag": "3"
                },
                {
                    "index": "355-3",
                    "sentence": "First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference.",
                    "tag": "3"
                },
                {
                    "index": "355-4",
                    "sentence": "Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit.",
                    "tag": "3"
                },
                {
                    "index": "355-5",
                    "sentence": "Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers.",
                    "tag": "4"
                },
                {
                    "index": "355-6",
                    "sentence": "We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-356",
            "text": [
                {
                    "index": "356-0",
                    "sentence": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test.",
                    "tag": "2"
                },
                {
                    "index": "356-1",
                    "sentence": "One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical).",
                    "tag": "3"
                },
                {
                    "index": "356-2",
                    "sentence": "Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.",
                    "tag": "3"
                },
                {
                    "index": "356-3",
                    "sentence": "To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score.",
                    "tag": "3"
                },
                {
                    "index": "356-4",
                    "sentence": "While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model.",
                    "tag": "3"
                },
                {
                    "index": "356-5",
                    "sentence": "The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-357",
            "text": [
                {
                    "index": "357-0",
                    "sentence": "The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers.",
                    "tag": "1"
                },
                {
                    "index": "357-1",
                    "sentence": "However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree.",
                    "tag": "1"
                },
                {
                    "index": "357-2",
                    "sentence": "We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.",
                    "tag": "2"
                },
                {
                    "index": "357-3",
                    "sentence": "In fact, the worst constraint-violation rate we observe is 24%.",
                    "tag": "4"
                },
                {
                    "index": "357-4",
                    "sentence": "Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime.",
                    "tag": "1"
                },
                {
                    "index": "357-5",
                    "sentence": "We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-358",
            "text": [
                {
                    "index": "358-0",
                    "sentence": "We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario.",
                    "tag": "2"
                },
                {
                    "index": "358-1",
                    "sentence": "We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available.",
                    "tag": "3"
                },
                {
                    "index": "358-2",
                    "sentence": "We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages.",
                    "tag": "3"
                },
                {
                    "index": "358-3",
                    "sentence": "Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology.",
                    "tag": "3"
                },
                {
                    "index": "358-4",
                    "sentence": "In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work.",
                    "tag": "4"
                },
                {
                    "index": "358-5",
                    "sentence": "In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-359",
            "text": [
                {
                    "index": "359-0",
                    "sentence": "The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*.",
                    "tag": "1"
                },
                {
                    "index": "359-1",
                    "sentence": "In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing.",
                    "tag": "2+3"
                },
                {
                    "index": "359-2",
                    "sentence": "To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart.",
                    "tag": "3"
                },
                {
                    "index": "359-3",
                    "sentence": "Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-360",
            "text": [
                {
                    "index": "360-0",
                    "sentence": "Text classification is a critical research topic with broad applications in natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "360-1",
                    "sentence": "Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task.",
                    "tag": "1"
                },
                {
                    "index": "360-2",
                    "sentence": "Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents.",
                    "tag": "1"
                },
                {
                    "index": "360-3",
                    "sentence": "To address those issues, in this paper, we propose a principled model – hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning.",
                    "tag": "1+2"
                },
                {
                    "index": "360-4",
                    "sentence": "Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-361",
            "text": [
                {
                    "index": "361-0",
                    "sentence": "We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model.",
                    "tag": "1"
                },
                {
                    "index": "361-1",
                    "sentence": "We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text.",
                    "tag": "2"
                },
                {
                    "index": "361-2",
                    "sentence": "Unlike previous efforts to integrate entity knowledge into sequence models, EaE’s entity representations are learned directly from text.",
                    "tag": "3"
                },
                {
                    "index": "361-3",
                    "sentence": "We show that EaE’s learned representations capture sufficient knowledge to answer TriviaQA questions such as “Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?”, outperforming an encoder-generator Transformer model with 10x the parameters on this task.",
                    "tag": "4"
                },
                {
                    "index": "361-4",
                    "sentence": "According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE’s performance.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-362",
            "text": [
                {
                    "index": "362-0",
                    "sentence": "Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising.",
                    "tag": "1"
                },
                {
                    "index": "362-1",
                    "sentence": "However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs.",
                    "tag": "1"
                },
                {
                    "index": "362-2",
                    "sentence": "Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence.",
                    "tag": "2"
                },
                {
                    "index": "362-3",
                    "sentence": "Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar.",
                    "tag": "2"
                },
                {
                    "index": "362-4",
                    "sentence": "Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-363",
            "text": [
                {
                    "index": "363-0",
                    "sentence": "Most recent improvements in NLP come from changes to the neural network architectures modeling the text input.",
                    "tag": "1"
                },
                {
                    "index": "363-1",
                    "sentence": "Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging.",
                    "tag": "1"
                },
                {
                    "index": "363-2",
                    "sentence": "More expressive graphical models are rarely used due to their prohibitive computational cost.",
                    "tag": "1"
                },
                {
                    "index": "363-3",
                    "sentence": "In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling.",
                    "tag": "2"
                },
                {
                    "index": "363-4",
                    "sentence": "Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging.",
                    "tag": "2"
                },
                {
                    "index": "363-5",
                    "sentence": "We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical.",
                    "tag": "2"
                },
                {
                    "index": "363-6",
                    "sentence": "The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03.",
                    "tag": "2"
                },
                {
                    "index": "363-7",
                    "sentence": "We obtain new state-of-the-art results on Dutch.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-364",
            "text": [
                {
                    "index": "364-0",
                    "sentence": "Text alignment finds application in tasks such as citation recommendation and plagiarism detection.",
                    "tag": "1"
                },
                {
                    "index": "364-1",
                    "sentence": "Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels.",
                    "tag": "1"
                },
                {
                    "index": "364-2",
                    "sentence": "We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document).",
                    "tag": "2"
                },
                {
                    "index": "364-3",
                    "sentence": "Our component is weakly supervised from document pairs and can align at multiple levels.",
                    "tag": "4"
                },
                {
                    "index": "364-4",
                    "sentence": "Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-365",
            "text": [
                {
                    "index": "365-0",
                    "sentence": "This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks.",
                    "tag": "2"
                },
                {
                    "index": "365-1",
                    "sentence": "We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe.",
                    "tag": "3"
                },
                {
                    "index": "365-2",
                    "sentence": "We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way.",
                    "tag": "3"
                },
                {
                    "index": "365-3",
                    "sentence": "Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering.",
                    "tag": "3"
                },
                {
                    "index": "365-4",
                    "sentence": "The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-366",
            "text": [
                {
                    "index": "366-0",
                    "sentence": "Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks.",
                    "tag": "1"
                },
                {
                    "index": "366-1",
                    "sentence": "Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors.",
                    "tag": "1"
                },
                {
                    "index": "366-2",
                    "sentence": "In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer.",
                    "tag": "2"
                },
                {
                    "index": "366-3",
                    "sentence": "We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus.",
                    "tag": "2"
                },
                {
                    "index": "366-4",
                    "sentence": "Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time.",
                    "tag": "4"
                },
                {
                    "index": "366-5",
                    "sentence": "We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-367",
            "text": [
                {
                    "index": "367-0",
                    "sentence": "Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill.",
                    "tag": "1"
                },
                {
                    "index": "367-1",
                    "sentence": "Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging.",
                    "tag": "1"
                },
                {
                    "index": "367-2",
                    "sentence": "In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention.",
                    "tag": "2"
                },
                {
                    "index": "367-3",
                    "sentence": "Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input.",
                    "tag": "3"
                },
                {
                    "index": "367-4",
                    "sentence": "We propose to boost the discriminative ability by transferring a natural language inference (NLI) model.",
                    "tag": "2"
                },
                {
                    "index": "367-5",
                    "sentence": "Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches.",
                    "tag": "4"
                },
                {
                    "index": "367-6",
                    "sentence": "More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-368",
            "text": [
                {
                    "index": "368-0",
                    "sentence": "The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking.",
                    "tag": "1"
                },
                {
                    "index": "368-1",
                    "sentence": "However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain.",
                    "tag": "1"
                },
                {
                    "index": "368-2",
                    "sentence": "In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data.",
                    "tag": "2"
                },
                {
                    "index": "368-3",
                    "sentence": "We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model.",
                    "tag": "2"
                },
                {
                    "index": "368-4",
                    "sentence": "Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers.",
                    "tag": "3"
                },
                {
                    "index": "368-5",
                    "sentence": "Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-369",
            "text": [
                {
                    "index": "369-0",
                    "sentence": "Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.).",
                    "tag": "1"
                },
                {
                    "index": "369-1",
                    "sentence": "Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).",
                    "tag": "1"
                },
                {
                    "index": "369-2",
                    "sentence": "In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction.",
                    "tag": "2"
                },
                {
                    "index": "369-3",
                    "sentence": "In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques.",
                    "tag": "3"
                },
                {
                    "index": "369-4",
                    "sentence": "Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work.",
                    "tag": "3"
                },
                {
                    "index": "369-5",
                    "sentence": "Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains.",
                    "tag": "3"
                },
                {
                    "index": "369-6",
                    "sentence": "This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-370",
            "text": [
                {
                    "index": "370-0",
                    "sentence": "We introduce a new task of rephrasing for a more natural virtual assistant.",
                    "tag": "1"
                },
                {
                    "index": "370-1",
                    "sentence": "Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine.",
                    "tag": "1"
                },
                {
                    "index": "370-2",
                    "sentence": "However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user.",
                    "tag": "1"
                },
                {
                    "index": "370-3",
                    "sentence": "For example, for queries like ‘ask my wife if she can pick up the kids’ or ‘remind me to take my pills’, we need to rephrase the content to ‘can you pick up the kids’ and ‘take your pills’.",
                    "tag": "1"
                },
                {
                    "index": "370-4",
                    "sentence": "In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query.",
                    "tag": "2"
                },
                {
                    "index": "370-5",
                    "sentence": "We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it.",
                    "tag": "4"
                },
                {
                    "index": "370-6",
                    "sentence": "We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-371",
            "text": [
                {
                    "index": "371-0",
                    "sentence": "Sentence simplification aims to make sentences easier to read and understand.",
                    "tag": "1"
                },
                {
                    "index": "371-1",
                    "sentence": "Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.",
                    "tag": "1"
                },
                {
                    "index": "371-2",
                    "sentence": "We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks.",
                    "tag": "2"
                },
                {
                    "index": "371-3",
                    "sentence": "A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification).",
                    "tag": "3"
                },
                {
                    "index": "371-4",
                    "sentence": "Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-372",
            "text": [
                {
                    "index": "372-0",
                    "sentence": "Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.",
                    "tag": "1"
                },
                {
                    "index": "372-1",
                    "sentence": "This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances.",
                    "tag": "1"
                },
                {
                    "index": "372-2",
                    "sentence": "In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance.",
                    "tag": "2"
                },
                {
                    "index": "372-3",
                    "sentence": "As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance.",
                    "tag": "2+3"
                },
                {
                    "index": "372-4",
                    "sentence": "We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-373",
            "text": [
                {
                    "index": "373-0",
                    "sentence": "NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input.",
                    "tag": "1"
                },
                {
                    "index": "373-1",
                    "sentence": "In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels.",
                    "tag": "2"
                },
                {
                    "index": "373-2",
                    "sentence": "For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews.",
                    "tag": "3"
                },
                {
                    "index": "373-3",
                    "sentence": "Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches.",
                    "tag": "4"
                },
                {
                    "index": "373-4",
                    "sentence": "We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-374",
            "text": [
                {
                    "index": "374-0",
                    "sentence": "We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts.",
                    "tag": "2"
                },
                {
                    "index": "374-1",
                    "sentence": "In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged.",
                    "tag": "3"
                },
                {
                    "index": "374-2",
                    "sentence": "We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board.",
                    "tag": "4"
                },
                {
                    "index": "374-3",
                    "sentence": "For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens.",
                    "tag": "5"
                },
                {
                    "index": "374-4",
                    "sentence": "For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-375",
            "text": [
                {
                    "index": "375-0",
                    "sentence": "We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.",
                    "tag": "2"
                },
                {
                    "index": "375-1",
                    "sentence": "Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness.",
                    "tag": "3"
                },
                {
                    "index": "375-2",
                    "sentence": "Additionally, we evaluate how a phrase-based data augmentation method can improve performance.",
                    "tag": "3"
                },
                {
                    "index": "375-3",
                    "sentence": "We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.",
                    "tag": "4"
                },
                {
                    "index": "375-4",
                    "sentence": "Data augmentation further improves control on difficult, randomly generated utterance plans.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-376",
            "text": [
                {
                    "index": "376-0",
                    "sentence": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks.",
                    "tag": "2"
                },
                {
                    "index": "376-1",
                    "sentence": "The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks.",
                    "tag": "3"
                },
                {
                    "index": "376-2",
                    "sentence": "The model can start from a single blank or partially completed text with blanks at specified locations.",
                    "tag": "3"
                },
                {
                    "index": "376-3",
                    "sentence": "It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill.",
                    "tag": "3"
                },
                {
                    "index": "376-4",
                    "sentence": "BLM can be efficiently trained using a lower bound of the marginal data likelihood.",
                    "tag": "3"
                },
                {
                    "index": "376-5",
                    "sentence": "On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency.",
                    "tag": "4"
                },
                {
                    "index": "376-6",
                    "sentence": "Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-377",
            "text": [
                {
                    "index": "377-0",
                    "sentence": "We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models.",
                    "tag": "2"
                },
                {
                    "index": "377-1",
                    "sentence": "Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks.",
                    "tag": "3"
                },
                {
                    "index": "377-2",
                    "sentence": "Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity.",
                    "tag": "3"
                },
                {
                    "index": "377-3",
                    "sentence": "Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects.",
                    "tag": "3"
                },
                {
                    "index": "377-4",
                    "sentence": "We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-378",
            "text": [
                {
                    "index": "378-0",
                    "sentence": "Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation.",
                    "tag": "1"
                },
                {
                    "index": "378-1",
                    "sentence": "However, at the same time it is a tedious, time-consuming task.",
                    "tag": "1"
                },
                {
                    "index": "378-2",
                    "sentence": "In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format.",
                    "tag": "2"
                },
                {
                    "index": "378-3",
                    "sentence": "We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages.",
                    "tag": "3"
                },
                {
                    "index": "378-4",
                    "sentence": "We apply our framework to all languages included in the Universal Dependencies project, with promising results.",
                    "tag": "3"
                },
                {
                    "index": "378-5",
                    "sentence": "Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data.",
                    "tag": "3"
                },
                {
                    "index": "378-6",
                    "sentence": "We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%.",
                    "tag": "4"
                },
                {
                    "index": "378-7",
                    "sentence": "We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-379",
            "text": [
                {
                    "index": "379-0",
                    "sentence": "An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech.",
                    "tag": "1"
                },
                {
                    "index": "379-1",
                    "sentence": "From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to complete inflectional paradigm tables.",
                    "tag": "1"
                },
                {
                    "index": "379-2",
                    "sentence": "To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs).",
                    "tag": "1"
                },
                {
                    "index": "379-3",
                    "sentence": "We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P).",
                    "tag": "2"
                },
                {
                    "index": "379-4",
                    "sentence": "IGT2P generates entire morphological paradigms from IGT input.",
                    "tag": "3"
                },
                {
                    "index": "379-5",
                    "sentence": "We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language.",
                    "tag": "4"
                },
                {
                    "index": "379-6",
                    "sentence": "We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-380",
            "text": [
                {
                    "index": "380-0",
                    "sentence": "Empathy is critical to successful mental health support.",
                    "tag": "1"
                },
                {
                    "index": "380-1",
                    "sentence": "Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts.",
                    "tag": "1"
                },
                {
                    "index": "380-2",
                    "sentence": "Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial.",
                    "tag": "1"
                },
                {
                    "index": "380-3",
                    "sentence": "In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms.",
                    "tag": "2"
                },
                {
                    "index": "380-4",
                    "sentence": "We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations.",
                    "tag": "2+3"
                },
                {
                    "index": "380-5",
                    "sentence": "We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales).",
                    "tag": "3"
                },
                {
                    "index": "380-6",
                    "sentence": "We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions.",
                    "tag": "3"
                },
                {
                    "index": "380-7",
                    "sentence": "Experiments demonstrate that our approach can effectively identify empathic conversations.",
                    "tag": "4"
                },
                {
                    "index": "380-8",
                    "sentence": "We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-381",
            "text": [
                {
                    "index": "381-0",
                    "sentence": "Emotions and their evolution play a central role in creating a captivating story.",
                    "tag": "1"
                },
                {
                    "index": "381-1",
                    "sentence": "In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling.",
                    "tag": "2"
                },
                {
                    "index": "381-2",
                    "sentence": "We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist.",
                    "tag": "3"
                },
                {
                    "index": "381-3",
                    "sentence": "Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models.",
                    "tag": "3"
                },
                {
                    "index": "381-4",
                    "sentence": "The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning.",
                    "tag": "3"
                },
                {
                    "index": "381-5",
                    "sentence": "Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-382",
            "text": [
                {
                    "index": "382-0",
                    "sentence": "Intimacy is a fundamental aspect of how we relate to others in social settings.",
                    "tag": "1"
                },
                {
                    "index": "382-1",
                    "sentence": "Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing).",
                    "tag": "1"
                },
                {
                    "index": "382-2",
                    "sentence": "Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87).",
                    "tag": "2"
                },
                {
                    "index": "382-3",
                    "sentence": "Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings.",
                    "tag": "3+4"
                },
                {
                    "index": "382-4",
                    "sentence": "Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology.",
                    "tag": "4"
                },
                {
                    "index": "382-5",
                    "sentence": "Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-383",
            "text": [
                {
                    "index": "383-0",
                    "sentence": "Subevents elaborate an event and widely exist in event descriptions.",
                    "tag": "1"
                },
                {
                    "index": "383-1",
                    "sentence": "Subevent knowledge is useful for discourse analysis and event-centric applications.",
                    "tag": "1"
                },
                {
                    "index": "383-2",
                    "sentence": "Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base.",
                    "tag": "2"
                },
                {
                    "index": "383-3",
                    "sentence": "We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents.",
                    "tag": "3"
                },
                {
                    "index": "383-4",
                    "sentence": "Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs.",
                    "tag": "3"
                },
                {
                    "index": "383-5",
                    "sentence": "The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types.",
                    "tag": "4"
                },
                {
                    "index": "383-6",
                    "sentence": "The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-384",
            "text": [
                {
                    "index": "384-0",
                    "sentence": "We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model.",
                    "tag": "2"
                },
                {
                    "index": "384-1",
                    "sentence": "BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning.",
                    "tag": "3"
                },
                {
                    "index": "384-2",
                    "sentence": "BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools.",
                    "tag": "3"
                },
                {
                    "index": "384-3",
                    "sentence": "BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task.",
                    "tag": "3"
                },
                {
                    "index": "384-4",
                    "sentence": "Importantly, we also provide first results on biomedical event extraction without gold entity information.",
                    "tag": "4"
                },
                {
                    "index": "384-5",
                    "sentence": "Empirical results show that BeeSL’s speed and accuracy makes it a viable approach for large-scale real-world scenarios.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-385",
            "text": [
                {
                    "index": "385-0",
                    "sentence": "Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots.",
                    "tag": "1"
                },
                {
                    "index": "385-1",
                    "sentence": "Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts.",
                    "tag": "1"
                },
                {
                    "index": "385-2",
                    "sentence": "Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples.",
                    "tag": "1"
                },
                {
                    "index": "385-3",
                    "sentence": "This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training.",
                    "tag": "2+3"
                },
                {
                    "index": "385-4",
                    "sentence": "Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-386",
            "text": [
                {
                    "index": "386-0",
                    "sentence": "Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance.",
                    "tag": "1"
                },
                {
                    "index": "386-1",
                    "sentence": "However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction.",
                    "tag": "1"
                },
                {
                    "index": "386-2",
                    "sentence": "In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance.",
                    "tag": "1"
                },
                {
                    "index": "386-3",
                    "sentence": "In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate.",
                    "tag": "2"
                },
                {
                    "index": "386-4",
                    "sentence": "We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.",
                    "tag": "2"
                },
                {
                    "index": "386-5",
                    "sentence": "The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-387",
            "text": [
                {
                    "index": "387-0",
                    "sentence": "In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations.",
                    "tag": "2"
                },
                {
                    "index": "387-1",
                    "sentence": "Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them.",
                    "tag": "3"
                },
                {
                    "index": "387-2",
                    "sentence": "Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques.",
                    "tag": "3"
                },
                {
                    "index": "387-3",
                    "sentence": "Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-388",
            "text": [
                {
                    "index": "388-0",
                    "sentence": "We propose an end-to-end approach for synthetic QA data generation.",
                    "tag": "2"
                },
                {
                    "index": "388-1",
                    "sentence": "Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions.",
                    "tag": "3"
                },
                {
                    "index": "388-2",
                    "sentence": "In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token.",
                    "tag": "3"
                },
                {
                    "index": "388-3",
                    "sentence": "The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model.",
                    "tag": "3"
                },
                {
                    "index": "388-4",
                    "sentence": "Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation.",
                    "tag": "3"
                },
                {
                    "index": "388-5",
                    "sentence": "The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-389",
            "text": [
                {
                    "index": "389-0",
                    "sentence": "Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain.",
                    "tag": "1"
                },
                {
                    "index": "389-1",
                    "sentence": "Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "389-2",
                    "sentence": "We show that extending the vocabulary of the LM with domain-specific terms leads to further gains.",
                    "tag": "2"
                },
                {
                    "index": "389-3",
                    "sentence": "To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "389-4",
                    "sentence": "We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-390",
            "text": [
                {
                    "index": "390-0",
                    "sentence": "Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams.",
                    "tag": "1"
                },
                {
                    "index": "390-1",
                    "sentence": "For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails.",
                    "tag": "2"
                },
                {
                    "index": "390-2",
                    "sentence": "Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling.",
                    "tag": "3"
                },
                {
                    "index": "390-3",
                    "sentence": "We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options.",
                    "tag": "3"
                },
                {
                    "index": "390-4",
                    "sentence": "Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions.",
                    "tag": "4"
                },
                {
                    "index": "390-5",
                    "sentence": "ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-391",
            "text": [
                {
                    "index": "391-0",
                    "sentence": "We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining.",
                    "tag": "2"
                },
                {
                    "index": "391-1",
                    "sentence": "Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled.",
                    "tag": "1"
                },
                {
                    "index": "391-2",
                    "sentence": "Thus they cannot incorporate visual information when encoding plain text alone.",
                    "tag": "1"
                },
                {
                    "index": "391-3",
                    "sentence": "In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network.",
                    "tag": "2"
                },
                {
                    "index": "391-4",
                    "sentence": "MACD forces the decoupled text encoder to represent the visual information via contrastive learning.",
                    "tag": "3"
                },
                {
                    "index": "391-5",
                    "sentence": "Therefore, it embeds visual knowledge even for plain text inference.",
                    "tag": "3"
                },
                {
                    "index": "391-6",
                    "sentence": "We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B).",
                    "tag": "3"
                },
                {
                    "index": "391-7",
                    "sentence": "The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-392",
            "text": [
                {
                    "index": "392-0",
                    "sentence": "In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.",
                    "tag": "2"
                },
                {
                    "index": "392-1",
                    "sentence": "While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech.",
                    "tag": "1"
                },
                {
                    "index": "392-2",
                    "sentence": "We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.",
                    "tag": "2"
                },
                {
                    "index": "392-3",
                    "sentence": "Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another.",
                    "tag": "3+4"
                },
                {
                    "index": "392-4",
                    "sentence": "To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-393",
            "text": [
                {
                    "index": "393-0",
                    "sentence": "Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors.",
                    "tag": "1"
                },
                {
                    "index": "393-1",
                    "sentence": "One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly.",
                    "tag": "1"
                },
                {
                    "index": "393-2",
                    "sentence": "We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats.",
                    "tag": "2"
                },
                {
                    "index": "393-3",
                    "sentence": "We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs.",
                    "tag": "3"
                },
                {
                    "index": "393-4",
                    "sentence": "Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models.",
                    "tag": "3"
                },
                {
                    "index": "393-5",
                    "sentence": "Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs.",
                    "tag": "3"
                },
                {
                    "index": "393-6",
                    "sentence": "We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems.",
                    "tag": "3"
                },
                {
                    "index": "393-7",
                    "sentence": "We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs.",
                    "tag": "3"
                },
                {
                    "index": "393-8",
                    "sentence": "To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models.",
                    "tag": "2+3"
                },
                {
                    "index": "393-9",
                    "sentence": "This defense degrades the adversary’s BLEU score and attack success rate at some cost in the defender’s BLEU and inference speed.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-394",
            "text": [
                {
                    "index": "394-0",
                    "sentence": "Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language.",
                    "tag": "1"
                },
                {
                    "index": "394-1",
                    "sentence": "This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems.",
                    "tag": "2"
                },
                {
                    "index": "394-2",
                    "sentence": "Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set.",
                    "tag": "3"
                },
                {
                    "index": "394-3",
                    "sentence": "We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective.",
                    "tag": "3+4"
                },
                {
                    "index": "394-4",
                    "sentence": "SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines.",
                    "tag": "4"
                },
                {
                    "index": "394-5",
                    "sentence": "On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-395",
            "text": [
                {
                    "index": "395-0",
                    "sentence": "Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition.",
                    "tag": "1"
                },
                {
                    "index": "395-1",
                    "sentence": "We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms.",
                    "tag": "2"
                },
                {
                    "index": "395-2",
                    "sentence": "To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model.",
                    "tag": "2+3"
                },
                {
                    "index": "395-3",
                    "sentence": "We prove that commonly used incomplete decoding algorithms – greedy search, beam search, top-k sampling, and nucleus sampling – are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length.",
                    "tag": "4"
                },
                {
                    "index": "395-4",
                    "sentence": "Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model.",
                    "tag": "2"
                },
                {
                    "index": "395-5",
                    "sentence": "Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-396",
            "text": [
                {
                    "index": "396-0",
                    "sentence": "Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation.",
                    "tag": "1"
                },
                {
                    "index": "396-1",
                    "sentence": "Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic.",
                    "tag": "1"
                },
                {
                    "index": "396-2",
                    "sentence": "In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence.",
                    "tag": "2"
                },
                {
                    "index": "396-3",
                    "sentence": "We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks.",
                    "tag": "3"
                },
                {
                    "index": "396-4",
                    "sentence": "We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models.",
                    "tag": "3"
                },
                {
                    "index": "396-5",
                    "sentence": "We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.",
                    "tag": "3+4"
                },
                {
                    "index": "396-6",
                    "sentence": "We also find high-order energies to help in noisy data conditions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-397",
            "text": [
                {
                    "index": "397-0",
                    "sentence": "Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy.",
                    "tag": "1"
                },
                {
                    "index": "397-1",
                    "sentence": "In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs.",
                    "tag": "1"
                },
                {
                    "index": "397-2",
                    "sentence": "However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available.",
                    "tag": "1"
                },
                {
                    "index": "397-3",
                    "sentence": "In this paper, we study ensemble distillation as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles.",
                    "tag": "2"
                },
                {
                    "index": "397-4",
                    "sentence": "We validate this framework on two tasks: named-entity recognition and machine translation.",
                    "tag": "2"
                },
                {
                    "index": "397-5",
                    "sentence": "We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-398",
            "text": [
                {
                    "index": "398-0",
                    "sentence": "Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment.",
                    "tag": "1"
                },
                {
                    "index": "398-1",
                    "sentence": "Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task.",
                    "tag": "1"
                },
                {
                    "index": "398-2",
                    "sentence": "However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory.",
                    "tag": "1"
                },
                {
                    "index": "398-3",
                    "sentence": "To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs.",
                    "tag": "2"
                },
                {
                    "index": "398-4",
                    "sentence": "We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks.",
                    "tag": "2"
                },
                {
                    "index": "398-5",
                    "sentence": "Our model can complement supervised syntactic features with latent semantic dependencies.",
                    "tag": "3+4"
                },
                {
                    "index": "398-6",
                    "sentence": "Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-399",
            "text": [
                {
                    "index": "399-0",
                    "sentence": "Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events.",
                    "tag": "1"
                },
                {
                    "index": "399-1",
                    "sentence": "Our research introduces new classification models to assign affective polarity to event phrases.",
                    "tag": "2"
                },
                {
                    "index": "399-2",
                    "sentence": "First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base.",
                    "tag": "2"
                },
                {
                    "index": "399-3",
                    "sentence": "Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data.",
                    "tag": "2"
                },
                {
                    "index": "399-4",
                    "sentence": "The key idea is to exploit event phrases that occur with a coreferent sentiment expression.",
                    "tag": "3"
                },
                {
                    "index": "399-5",
                    "sentence": "The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier’s predictions and the polarities of the event’s coreferent sentiment expressions.",
                    "tag": "3"
                },
                {
                    "index": "399-6",
                    "sentence": "Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-400",
            "text": [
                {
                    "index": "400-0",
                    "sentence": "Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables.",
                    "tag": "1"
                },
                {
                    "index": "400-1",
                    "sentence": "On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process.",
                    "tag": "1"
                },
                {
                    "index": "400-2",
                    "sentence": "Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT).",
                    "tag": "1"
                },
                {
                    "index": "400-3",
                    "sentence": "We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework.",
                    "tag": "2"
                },
                {
                    "index": "400-4",
                    "sentence": "The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent.",
                    "tag": "3"
                },
                {
                    "index": "400-5",
                    "sentence": "Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-401",
            "text": [
                {
                    "index": "401-0",
                    "sentence": "This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies.",
                    "tag": "2"
                },
                {
                    "index": "401-1",
                    "sentence": "We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events).",
                    "tag": "2"
                },
                {
                    "index": "401-2",
                    "sentence": "Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses.",
                    "tag": "3+4"
                },
                {
                    "index": "401-3",
                    "sentence": "Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision.",
                    "tag": "4"
                },
                {
                    "index": "401-4",
                    "sentence": "We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-402",
            "text": [
                {
                    "index": "402-0",
                    "sentence": "Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English.",
                    "tag": "1"
                },
                {
                    "index": "402-1",
                    "sentence": "Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust.",
                    "tag": "1"
                },
                {
                    "index": "402-2",
                    "sentence": "We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols.",
                    "tag": "2"
                },
                {
                    "index": "402-3",
                    "sentence": "Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data.",
                    "tag": "3"
                },
                {
                    "index": "402-4",
                    "sentence": "Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE.",
                    "tag": "3"
                },
                {
                    "index": "402-5",
                    "sentence": "Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers.",
                    "tag": "4"
                },
                {
                    "index": "402-6",
                    "sentence": "Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-403",
            "text": [
                {
                    "index": "403-0",
                    "sentence": "A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories.",
                    "tag": "1"
                },
                {
                    "index": "403-1",
                    "sentence": "How similar are these gender systems across languages?",
                    "tag": "1"
                },
                {
                    "index": "403-2",
                    "sentence": "To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation.",
                    "tag": "2+3"
                },
                {
                    "index": "403-3",
                    "sentence": "We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems.",
                    "tag": "3"
                },
                {
                    "index": "403-4",
                    "sentence": "We first validate our metrics, then use them to measure gender system similarity in 20 languages.",
                    "tag": "3"
                },
                {
                    "index": "403-5",
                    "sentence": "We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages.",
                    "tag": "3"
                },
                {
                    "index": "403-6",
                    "sentence": "Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages.",
                    "tag": "3"
                },
                {
                    "index": "403-7",
                    "sentence": "Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-404",
            "text": [
                {
                    "index": "404-0",
                    "sentence": "The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models.",
                    "tag": "1"
                },
                {
                    "index": "404-1",
                    "sentence": "In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task.",
                    "tag": "2"
                },
                {
                    "index": "404-2",
                    "sentence": "Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning.",
                    "tag": "3+4"
                },
                {
                    "index": "404-3",
                    "sentence": "Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research.",
                    "tag": "5"
                },
                {
                    "index": "404-4",
                    "sentence": "We make all codes publicly available and release an interface that can quickly evaluate and diagnose user’s models: https://github.com/neulab/InterpretEval",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-405",
            "text": [
                {
                    "index": "405-0",
                    "sentence": "We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary.",
                    "tag": "2"
                },
                {
                    "index": "405-1",
                    "sentence": "From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations.",
                    "tag": "3"
                },
                {
                    "index": "405-2",
                    "sentence": "Using unsupervised methods, the program effectively deciphers writing into speech.",
                    "tag": "3+4"
                },
                {
                    "index": "405-3",
                    "sentence": "Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-406",
            "text": [
                {
                    "index": "406-0",
                    "sentence": "Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion.",
                    "tag": "1"
                },
                {
                    "index": "406-1",
                    "sentence": "Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning.",
                    "tag": "1"
                },
                {
                    "index": "406-2",
                    "sentence": "On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths.",
                    "tag": "1"
                },
                {
                    "index": "406-3",
                    "sentence": "On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult.",
                    "tag": "1"
                },
                {
                    "index": "406-4",
                    "sentence": "To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs.",
                    "tag": "2+3"
                },
                {
                    "index": "406-5",
                    "sentence": "(2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs.",
                    "tag": "3"
                },
                {
                    "index": "406-6",
                    "sentence": "The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines.",
                    "tag": "4"
                },
                {
                    "index": "406-7",
                    "sentence": "Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-407",
            "text": [
                {
                    "index": "407-0",
                    "sentence": "Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations.",
                    "tag": "1"
                },
                {
                    "index": "407-1",
                    "sentence": "Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs.",
                    "tag": "1"
                },
                {
                    "index": "407-2",
                    "sentence": "They also depend on high embedding dimensions to realize enough expressiveness.",
                    "tag": "1"
                },
                {
                    "index": "407-3",
                    "sentence": "Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association.",
                    "tag": "2+3"
                },
                {
                    "index": "407-4",
                    "sentence": "We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation.",
                    "tag": "2+3"
                },
                {
                    "index": "407-5",
                    "sentence": "Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-408",
            "text": [
                {
                    "index": "408-0",
                    "sentence": "Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding.",
                    "tag": "1"
                },
                {
                    "index": "408-1",
                    "sentence": "Prior systems leverage deep learning and pre-trained language models to improve the performance of the task.",
                    "tag": "1"
                },
                {
                    "index": "408-2",
                    "sentence": "However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data.",
                    "tag": "1"
                },
                {
                    "index": "408-3",
                    "sentence": "To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge.",
                    "tag": "2"
                },
                {
                    "index": "408-4",
                    "sentence": "We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "408-5",
                    "sentence": "Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-409",
            "text": [
                {
                    "index": "409-0",
                    "sentence": "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task.",
                    "tag": "1"
                },
                {
                    "index": "409-1",
                    "sentence": "Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations.",
                    "tag": "1"
                },
                {
                    "index": "409-2",
                    "sentence": "However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions.",
                    "tag": "1"
                },
                {
                    "index": "409-3",
                    "sentence": "Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs.",
                    "tag": "1"
                },
                {
                    "index": "409-4",
                    "sentence": "We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques.",
                    "tag": "2"
                },
                {
                    "index": "409-5",
                    "sentence": "Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "409-6",
                    "sentence": "Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-410",
            "text": [
                {
                    "index": "410-0",
                    "sentence": "Transformers have proved effective in many NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "410-1",
                    "sentence": "However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively).",
                    "tag": "1"
                },
                {
                    "index": "410-2",
                    "sentence": "Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives.",
                    "tag": "2"
                },
                {
                    "index": "410-3",
                    "sentence": "Our analysis reveals that unbalanced gradients are not the root cause of the instability of training.",
                    "tag": "3"
                },
                {
                    "index": "410-4",
                    "sentence": "Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output.",
                    "tag": "3"
                },
                {
                    "index": "410-5",
                    "sentence": "Yet we observe that a light dependency limits the model potential and leads to inferior trained models.",
                    "tag": "4"
                },
                {
                    "index": "410-6",
                    "sentence": "Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage.",
                    "tag": "4"
                },
                {
                    "index": "410-7",
                    "sentence": "Extensive experiments show that Admin is more stable, converges faster, and leads to better performance",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-411",
            "text": [
                {
                    "index": "411-0",
                    "sentence": "In this work, we present an empirical study of generation order for machine translation.",
                    "tag": "2"
                },
                {
                    "index": "411-1",
                    "sentence": "Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies.",
                    "tag": "3"
                },
                {
                    "index": "411-2",
                    "sentence": "We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders.",
                    "tag": "3"
                },
                {
                    "index": "411-3",
                    "sentence": "Curiously, we find that for the WMT’14 English → German and WMT’18 English → Chinese translation tasks, order does not have a substantial impact on output quality.",
                    "tag": "4"
                },
                {
                    "index": "411-4",
                    "sentence": "Moreover, for English → German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-412",
            "text": [
                {
                    "index": "412-0",
                    "sentence": "Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation.",
                    "tag": "1"
                },
                {
                    "index": "412-1",
                    "sentence": "Given a trained CMLM, however, it is not clear what the best inference strategy is.",
                    "tag": "1"
                },
                {
                    "index": "412-2",
                    "sentence": "We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective.",
                    "tag": "2+3"
                },
                {
                    "index": "412-3",
                    "sentence": "We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-413",
            "text": [
                {
                    "index": "413-0",
                    "sentence": "Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer.",
                    "tag": "1"
                },
                {
                    "index": "413-1",
                    "sentence": "In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity.",
                    "tag": "2"
                },
                {
                    "index": "413-2",
                    "sentence": "To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.",
                    "tag": "2+3"
                },
                {
                    "index": "413-3",
                    "sentence": "We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references.",
                    "tag": "4"
                },
                {
                    "index": "413-4",
                    "sentence": "We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort.",
                    "tag": "4+5"
                },
                {
                    "index": "413-5",
                    "sentence": "Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-414",
            "text": [
                {
                    "index": "414-0",
                    "sentence": "In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks.",
                    "tag": "2"
                },
                {
                    "index": "414-1",
                    "sentence": "We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples.",
                    "tag": "2+3"
                },
                {
                    "index": "414-2",
                    "sentence": "CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space.",
                    "tag": "3"
                },
                {
                    "index": "414-3",
                    "sentence": "It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization.",
                    "tag": "3"
                },
                {
                    "index": "414-4",
                    "sentence": "Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data.",
                    "tag": "3"
                },
                {
                    "index": "414-5",
                    "sentence": "Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-415",
            "text": [
                {
                    "index": "415-0",
                    "sentence": "Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB).",
                    "tag": "1"
                },
                {
                    "index": "415-1",
                    "sentence": "However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level.",
                    "tag": "1"
                },
                {
                    "index": "415-2",
                    "sentence": "This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions.",
                    "tag": "2"
                },
                {
                    "index": "415-3",
                    "sentence": "Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data.",
                    "tag": "4"
                },
                {
                    "index": "415-4",
                    "sentence": "The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set.",
                    "tag": "4"
                },
                {
                    "index": "415-5",
                    "sentence": "Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set.",
                    "tag": "5"
                },
                {
                    "index": "415-6",
                    "sentence": "We have released our code at https://github.com/DevinJake/MRL-CQA.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-416",
            "text": [
                {
                    "index": "416-0",
                    "sentence": "Offensive content is pervasive in social media and a reason for concern to companies and government organizations.",
                    "tag": "1"
                },
                {
                    "index": "416-1",
                    "sentence": "Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression).",
                    "tag": "1"
                },
                {
                    "index": "416-2",
                    "sentence": "The clear majority of these studies deal with English partially because most annotated datasets available contain English data.",
                    "tag": "1"
                },
                {
                    "index": "416-3",
                    "sentence": "In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources.",
                    "tag": "2"
                },
                {
                    "index": "416-4",
                    "sentence": "We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish.",
                    "tag": "2"
                },
                {
                    "index": "416-5",
                    "sentence": "Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-417",
            "text": [
                {
                    "index": "417-0",
                    "sentence": "We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.",
                    "tag": "2"
                },
                {
                    "index": "417-1",
                    "sentence": "We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.",
                    "tag": "3"
                },
                {
                    "index": "417-2",
                    "sentence": "We are able to decipher 75.1% of the cipher-word tokens correctly.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-418",
            "text": [
                {
                    "index": "418-0",
                    "sentence": "Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties.",
                    "tag": "1"
                },
                {
                    "index": "418-1",
                    "sentence": "Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message.",
                    "tag": "2"
                },
                {
                    "index": "418-2",
                    "sentence": "For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models.",
                    "tag": "2"
                },
                {
                    "index": "418-3",
                    "sentence": "To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks.",
                    "tag": "2"
                },
                {
                    "index": "418-4",
                    "sentence": "MARBERT predicts micro-dialects with 9.9% F1, 76 better than a majority class baseline.",
                    "tag": "4"
                },
                {
                    "index": "418-5",
                    "sentence": "Our new language model also establishes new state-of-the-art on several external tasks.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-419",
            "text": [
                {
                    "index": "419-0",
                    "sentence": "In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent.",
                    "tag": "1"
                },
                {
                    "index": "419-1",
                    "sentence": "One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method.",
                    "tag": "1"
                },
                {
                    "index": "419-2",
                    "sentence": "However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation.",
                    "tag": "1"
                },
                {
                    "index": "419-3",
                    "sentence": "To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data.",
                    "tag": "2"
                },
                {
                    "index": "419-4",
                    "sentence": "To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly.",
                    "tag": "2+3"
                },
                {
                    "index": "419-5",
                    "sentence": "Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-420",
            "text": [
                {
                    "index": "420-0",
                    "sentence": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance.",
                    "tag": "1"
                },
                {
                    "index": "420-1",
                    "sentence": "Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain.",
                    "tag": "1"
                },
                {
                    "index": "420-2",
                    "sentence": "To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text.",
                    "tag": "2"
                },
                {
                    "index": "420-3",
                    "sentence": "In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models.",
                    "tag": "2"
                },
                {
                    "index": "420-4",
                    "sentence": "In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration.",
                    "tag": "2"
                },
                {
                    "index": "420-5",
                    "sentence": "We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs.",
                    "tag": "3"
                },
                {
                    "index": "420-6",
                    "sentence": "Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-421",
            "text": [
                {
                    "index": "421-0",
                    "sentence": "We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language.",
                    "tag": "2"
                },
                {
                    "index": "421-1",
                    "sentence": "Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.",
                    "tag": "3"
                },
                {
                    "index": "421-2",
                    "sentence": "We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains.",
                    "tag": "3"
                },
                {
                    "index": "421-3",
                    "sentence": "Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set.",
                    "tag": "4"
                },
                {
                    "index": "421-4",
                    "sentence": "We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested.",
                    "tag": "4"
                },
                {
                    "index": "421-5",
                    "sentence": "Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.",
                    "tag": "5"
                },
                {
                    "index": "421-6",
                    "sentence": "Our code is released open-source.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-422",
            "text": [
                {
                    "index": "422-0",
                    "sentence": "Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages.",
                    "tag": "1"
                },
                {
                    "index": "422-1",
                    "sentence": "We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem.",
                    "tag": "2"
                },
                {
                    "index": "422-2",
                    "sentence": "First, CLIME ranks words by their salience to the downstream task.",
                    "tag": "3"
                },
                {
                    "index": "422-3",
                    "sentence": "Then, users mark similarity between keywords and their nearest neighbors in the embedding space.",
                    "tag": "3"
                },
                {
                    "index": "422-4",
                    "sentence": "Finally, CLIME updates the embeddings using the annotations.",
                    "tag": "3"
                },
                {
                    "index": "422-5",
                    "sentence": "We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur.",
                    "tag": "3"
                },
                {
                    "index": "422-6",
                    "sentence": "Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings.",
                    "tag": "4"
                },
                {
                    "index": "422-7",
                    "sentence": "CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-423",
            "text": [
                {
                    "index": "423-0",
                    "sentence": "We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring.",
                    "tag": "2"
                },
                {
                    "index": "423-1",
                    "sentence": "Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task.",
                    "tag": "3+4"
                },
                {
                    "index": "423-2",
                    "sentence": "Our method improves downstream MT performance on web-scraped Sinhala–English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release.",
                    "tag": "3+4"
                },
                {
                    "index": "423-3",
                    "sentence": "It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-424",
            "text": [
                {
                    "index": "424-0",
                    "sentence": "In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "424-1",
                    "sentence": "Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages.",
                    "tag": "3"
                },
                {
                    "index": "424-2",
                    "sentence": "We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline.",
                    "tag": "3"
                },
                {
                    "index": "424-3",
                    "sentence": "We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-425",
            "text": [
                {
                    "index": "425-0",
                    "sentence": "The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches.",
                    "tag": "1"
                },
                {
                    "index": "425-1",
                    "sentence": "Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model.",
                    "tag": "1"
                },
                {
                    "index": "425-2",
                    "sentence": "However, these algorithms require sequential computation that makes parallelization impossible.",
                    "tag": "1"
                },
                {
                    "index": "425-3",
                    "sentence": "In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model.",
                    "tag": "2"
                },
                {
                    "index": "425-4",
                    "sentence": "Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction.",
                    "tag": "2+3"
                },
                {
                    "index": "425-5",
                    "sentence": "The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-426",
            "text": [
                {
                    "index": "426-0",
                    "sentence": "Named Entity Recognition (NER) is a fundamental task in natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "426-1",
                    "sentence": "In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures.",
                    "tag": "1"
                },
                {
                    "index": "426-2",
                    "sentence": "Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity.",
                    "tag": "1"
                },
                {
                    "index": "426-3",
                    "sentence": "To address this issue, we present a novel nested NER model named HIT.",
                    "tag": "2"
                },
                {
                    "index": "426-4",
                    "sentence": "Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary.",
                    "tag": "3"
                },
                {
                    "index": "426-5",
                    "sentence": "Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary.",
                    "tag": "3"
                },
                {
                    "index": "426-6",
                    "sentence": "Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-427",
            "text": [
                {
                    "index": "427-0",
                    "sentence": "Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task.",
                    "tag": "1"
                },
                {
                    "index": "427-1",
                    "sentence": "However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM).",
                    "tag": "1"
                },
                {
                    "index": "427-2",
                    "sentence": "In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information.",
                    "tag": "2"
                },
                {
                    "index": "427-3",
                    "sentence": "Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly.",
                    "tag": "3"
                },
                {
                    "index": "427-4",
                    "sentence": "The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing.",
                    "tag": "4"
                },
                {
                    "index": "427-5",
                    "sentence": "Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-428",
            "text": [
                {
                    "index": "428-0",
                    "sentence": "Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization.",
                    "tag": "1"
                },
                {
                    "index": "428-1",
                    "sentence": "In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences.",
                    "tag": "2"
                },
                {
                    "index": "428-2",
                    "sentence": "Our method is applicable to both supervised and semi-supervised settings.",
                    "tag": "3"
                },
                {
                    "index": "428-3",
                    "sentence": "For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks.",
                    "tag": "3"
                },
                {
                    "index": "428-4",
                    "sentence": "For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base.",
                    "tag": "3"
                },
                {
                    "index": "428-5",
                    "sentence": "The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-429",
            "text": [
                {
                    "index": "429-0",
                    "sentence": "With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits.",
                    "tag": "1"
                },
                {
                    "index": "429-1",
                    "sentence": "Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices.",
                    "tag": "1"
                },
                {
                    "index": "429-2",
                    "sentence": "In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task.",
                    "tag": "1+2"
                },
                {
                    "index": "429-3",
                    "sentence": "The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems.",
                    "tag": "3+4"
                },
                {
                    "index": "429-4",
                    "sentence": "By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-430",
            "text": [
                {
                    "index": "430-0",
                    "sentence": "Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit.",
                    "tag": "1"
                },
                {
                    "index": "430-1",
                    "sentence": "In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context.",
                    "tag": "2"
                },
                {
                    "index": "430-2",
                    "sentence": "We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence.",
                    "tag": "2"
                },
                {
                    "index": "430-3",
                    "sentence": "Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-431",
            "text": [
                {
                    "index": "431-0",
                    "sentence": "Text autoencoders are commonly used for conditional generation tasks such as style transfer.",
                    "tag": "1"
                },
                {
                    "index": "431-1",
                    "sentence": "We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding (Emb2Emb).",
                    "tag": "2+3"
                },
                {
                    "index": "431-2",
                    "sentence": "This reduces the need for labeled training data for the task and makes the training procedure more efficient.",
                    "tag": "2"
                },
                {
                    "index": "431-3",
                    "sentence": "Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors.",
                    "tag": "3"
                },
                {
                    "index": "431-4",
                    "sentence": "Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-432",
            "text": [
                {
                    "index": "432-0",
                    "sentence": "Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns.",
                    "tag": "1"
                },
                {
                    "index": "432-1",
                    "sentence": "A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data.",
                    "tag": "1+2"
                },
                {
                    "index": "432-2",
                    "sentence": "While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives.",
                    "tag": "1"
                },
                {
                    "index": "432-3",
                    "sentence": "In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood.",
                    "tag": "2+3"
                },
                {
                    "index": "432-4",
                    "sentence": "Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-433",
            "text": [
                {
                    "index": "433-0",
                    "sentence": "We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering).",
                    "tag": "1+2"
                },
                {
                    "index": "433-1",
                    "sentence": "Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts.",
                    "tag": "3"
                },
                {
                    "index": "433-2",
                    "sentence": "We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text.",
                    "tag": "3"
                },
                {
                    "index": "433-3",
                    "sentence": "We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-434",
            "text": [
                {
                    "index": "434-0",
                    "sentence": "The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.",
                    "tag": "1"
                },
                {
                    "index": "434-1",
                    "sentence": "However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.",
                    "tag": "1"
                },
                {
                    "index": "434-2",
                    "sentence": "We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.",
                    "tag": "2+3"
                },
                {
                    "index": "434-3",
                    "sentence": "The algorithm is designed to address the exposure bias problem.",
                    "tag": "1+2"
                },
                {
                    "index": "434-4",
                    "sentence": "On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.",
                    "tag": "3+4"
                },
                {
                    "index": "434-5",
                    "sentence": "Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-435",
            "text": [
                {
                    "index": "435-0",
                    "sentence": "Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models.",
                    "tag": "1"
                },
                {
                    "index": "435-1",
                    "sentence": "Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models.",
                    "tag": "1"
                },
                {
                    "index": "435-2",
                    "sentence": "Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable.",
                    "tag": "1"
                },
                {
                    "index": "435-3",
                    "sentence": "To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks.",
                    "tag": "2"
                },
                {
                    "index": "435-4",
                    "sentence": "In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation.",
                    "tag": "3"
                },
                {
                    "index": "435-5",
                    "sentence": "A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level.",
                    "tag": "3"
                },
                {
                    "index": "435-6",
                    "sentence": "We consider two most representative NLP tasks: sentiment analysis and question answering (QA).",
                    "tag": "3"
                },
                {
                    "index": "435-7",
                    "sentence": "Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human.",
                    "tag": "4"
                },
                {
                    "index": "435-8",
                    "sentence": "Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice.",
                    "tag": "4"
                },
                {
                    "index": "435-9",
                    "sentence": "Our work sheds light on an effective and general way to examine the robustness of NLP models.",
                    "tag": "3"
                },
                {
                    "index": "435-10",
                    "sentence": "Our code is publicly available at https://github.com/AI-secure/T3/.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-436",
            "text": [
                {
                    "index": "436-0",
                    "sentence": "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks.",
                    "tag": "1"
                },
                {
                    "index": "436-1",
                    "sentence": "Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large?",
                    "tag": "1"
                },
                {
                    "index": "436-2",
                    "sentence": "We study this question through the lens of model compression.",
                    "tag": "2"
                },
                {
                    "index": "436-3",
                    "sentence": "We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training.",
                    "tag": "3"
                },
                {
                    "index": "436-4",
                    "sentence": "On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference.",
                    "tag": "3+4"
                },
                {
                    "index": "436-5",
                    "sentence": "We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-437",
            "text": [
                {
                    "index": "437-0",
                    "sentence": "Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest.",
                    "tag": "1"
                },
                {
                    "index": "437-1",
                    "sentence": "Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens.",
                    "tag": "1"
                },
                {
                    "index": "437-2",
                    "sentence": "In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed.",
                    "tag": "1"
                },
                {
                    "index": "437-3",
                    "sentence": "Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM.",
                    "tag": "2+3"
                },
                {
                    "index": "437-4",
                    "sentence": "The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming.",
                    "tag": "3"
                },
                {
                    "index": "437-5",
                    "sentence": "On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-438",
            "text": [
                {
                    "index": "438-0",
                    "sentence": "Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model.",
                    "tag": "1"
                },
                {
                    "index": "438-1",
                    "sentence": "Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples.",
                    "tag": "1"
                },
                {
                    "index": "438-2",
                    "sentence": "These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans.",
                    "tag": "1"
                },
                {
                    "index": "438-3",
                    "sentence": "We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model.",
                    "tag": "1+2"
                },
                {
                    "index": "438-4",
                    "sentence": "BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens.",
                    "tag": "3"
                },
                {
                    "index": "438-5",
                    "sentence": "Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-439",
            "text": [
                {
                    "index": "439-0",
                    "sentence": "Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "439-1",
                    "sentence": "In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model.",
                    "tag": "1"
                },
                {
                    "index": "439-2",
                    "sentence": "However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues.",
                    "tag": "1"
                },
                {
                    "index": "439-3",
                    "sentence": "To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT).",
                    "tag": "2+3"
                },
                {
                    "index": "439-4",
                    "sentence": "To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge.",
                    "tag": "3"
                },
                {
                    "index": "439-5",
                    "sentence": "Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner.",
                    "tag": "2+3"
                },
                {
                    "index": "439-6",
                    "sentence": "To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "439-7",
                    "sentence": "We verify the effectiveness of our method on several text classification datasets.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-440",
            "text": [
                {
                    "index": "440-0",
                    "sentence": "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods.",
                    "tag": "1"
                },
                {
                    "index": "440-1",
                    "sentence": "Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency.",
                    "tag": "1"
                },
                {
                    "index": "440-2",
                    "sentence": "In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT.",
                    "tag": "2"
                },
                {
                    "index": "440-3",
                    "sentence": "We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly.",
                    "tag": "2"
                },
                {
                    "index": "440-4",
                    "sentence": "Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved.",
                    "tag": "3+4"
                },
                {
                    "index": "440-5",
                    "sentence": "Also, the cost of calculation is low, thus possible for large-scale generations.",
                    "tag": "2"
                },
                {
                    "index": "440-6",
                    "sentence": "The code is available at https://github.com/LinyangLee/BERT-Attack.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-441",
            "text": [
                {
                    "index": "441-0",
                    "sentence": "Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data.",
                    "tag": "1"
                },
                {
                    "index": "441-1",
                    "sentence": "We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual.",
                    "tag": "2"
                },
                {
                    "index": "441-2",
                    "sentence": "The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages.",
                    "tag": "3+4"
                },
                {
                    "index": "441-3",
                    "sentence": "This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task.",
                    "tag": "3"
                },
                {
                    "index": "441-4",
                    "sentence": "We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-442",
            "text": [
                {
                    "index": "442-0",
                    "sentence": "We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora.",
                    "tag": "1"
                },
                {
                    "index": "442-1",
                    "sentence": "Most conventional approaches to this task have been categorized to be either pattern-based or distributional.",
                    "tag": "1"
                },
                {
                    "index": "442-2",
                    "sentence": "Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved.",
                    "tag": "1"
                },
                {
                    "index": "442-3",
                    "sentence": "However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern.",
                    "tag": "1"
                },
                {
                    "index": "442-4",
                    "sentence": "For the first time, this paper quantifies the non-negligible existence of those specific cases.",
                    "tag": "1+2"
                },
                {
                    "index": "442-5",
                    "sentence": "We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases.",
                    "tag": "4"
                },
                {
                    "index": "442-6",
                    "sentence": "We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer.",
                    "tag": "2+3"
                },
                {
                    "index": "442-7",
                    "sentence": "On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-443",
            "text": [
                {
                    "index": "443-0",
                    "sentence": "This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth).",
                    "tag": "2"
                },
                {
                    "index": "443-1",
                    "sentence": "The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories.",
                    "tag": "1"
                },
                {
                    "index": "443-2",
                    "sentence": "In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method.",
                    "tag": "3"
                },
                {
                    "index": "443-3",
                    "sentence": "It does so at a better trade-off between precision and recall.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-444",
            "text": [
                {
                    "index": "444-0",
                    "sentence": "Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques.",
                    "tag": "1"
                },
                {
                    "index": "444-1",
                    "sentence": "However, these embeddings fail to embed sense knowledge in semantic networks.",
                    "tag": "1"
                },
                {
                    "index": "444-2",
                    "sentence": "In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses.",
                    "tag": "2+3"
                },
                {
                    "index": "444-3",
                    "sentence": "Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure.",
                    "tag": "4"
                },
                {
                    "index": "444-4",
                    "sentence": "When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-445",
            "text": [
                {
                    "index": "445-0",
                    "sentence": "News headline generation aims to produce a short sentence to attract readers to read the news.",
                    "tag": "2"
                },
                {
                    "index": "445-1",
                    "sentence": "One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines.",
                    "tag": "1"
                },
                {
                    "index": "445-2",
                    "sentence": "However, most existing methods focus on the single headline generation.",
                    "tag": "1"
                },
                {
                    "index": "445-3",
                    "sentence": "In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines.",
                    "tag": "2"
                },
                {
                    "index": "445-4",
                    "sentence": "We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines.",
                    "tag": "2+3"
                },
                {
                    "index": "445-5",
                    "sentence": "Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>.",
                    "tag": "2+3"
                },
                {
                    "index": "445-6",
                    "sentence": "Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-446",
            "text": [
                {
                    "index": "446-0",
                    "sentence": "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods.",
                    "tag": "1"
                },
                {
                    "index": "446-1",
                    "sentence": "However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge.",
                    "tag": "1"
                },
                {
                    "index": "446-2",
                    "sentence": "We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries.",
                    "tag": "2"
                },
                {
                    "index": "446-3",
                    "sentence": "The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries.",
                    "tag": "3"
                },
                {
                    "index": "446-4",
                    "sentence": "These transformations are inspired by the error analysis of state-of-the-art summarization model outputs.",
                    "tag": "3"
                },
                {
                    "index": "446-5",
                    "sentence": "Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset.",
                    "tag": "4"
                },
                {
                    "index": "446-6",
                    "sentence": "We also find that transferring from artificial error correction to downstream settings is still very challenging.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-447",
            "text": [
                {
                    "index": "447-0",
                    "sentence": "Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE.",
                    "tag": "1"
                },
                {
                    "index": "447-1",
                    "sentence": "In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience.",
                    "tag": "1+2"
                },
                {
                    "index": "447-2",
                    "sentence": "Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary.",
                    "tag": "3"
                },
                {
                    "index": "447-3",
                    "sentence": "Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied.",
                    "tag": "3"
                },
                {
                    "index": "447-4",
                    "sentence": "When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions.",
                    "tag": "3+4"
                },
                {
                    "index": "447-5",
                    "sentence": "Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-448",
            "text": [
                {
                    "index": "448-0",
                    "sentence": "Amongst the best means to summarize is highlighting.",
                    "tag": "1"
                },
                {
                    "index": "448-1",
                    "sentence": "In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text.",
                    "tag": "2"
                },
                {
                    "index": "448-2",
                    "sentence": "The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short.",
                    "tag": "2+3"
                },
                {
                    "index": "448-3",
                    "sentence": "In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion.",
                    "tag": "2+3"
                },
                {
                    "index": "448-4",
                    "sentence": "Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights.",
                    "tag": "3"
                },
                {
                    "index": "448-5",
                    "sentence": "To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets.",
                    "tag": "3"
                },
                {
                    "index": "448-6",
                    "sentence": "Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-449",
            "text": [
                {
                    "index": "449-0",
                    "sentence": "Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect.",
                    "tag": "1"
                },
                {
                    "index": "449-1",
                    "sentence": "Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics.",
                    "tag": "1"
                },
                {
                    "index": "449-2",
                    "sentence": "In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice.",
                    "tag": "1+2"
                },
                {
                    "index": "449-3",
                    "sentence": "Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia.",
                    "tag": "2+3"
                },
                {
                    "index": "449-4",
                    "sentence": "Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-450",
            "text": [
                {
                    "index": "450-0",
                    "sentence": "In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph.",
                    "tag": "1+2"
                },
                {
                    "index": "450-1",
                    "sentence": "In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences.",
                    "tag": "2+3"
                },
                {
                    "index": "450-2",
                    "sentence": "Extensive evaluations are conducted on six public datasets.",
                    "tag": "3"
                },
                {
                    "index": "450-3",
                    "sentence": "The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-451",
            "text": [
                {
                    "index": "451-0",
                    "sentence": "Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently.",
                    "tag": "1"
                },
                {
                    "index": "451-1",
                    "sentence": "Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages.",
                    "tag": "1"
                },
                {
                    "index": "451-2",
                    "sentence": "Conversation disentanglement aims to separate intermingled messages into detached conversations.",
                    "tag": "1"
                },
                {
                    "index": "451-3",
                    "sentence": "However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability.",
                    "tag": "1"
                },
                {
                    "index": "451-4",
                    "sentence": "In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering.",
                    "tag": "2"
                },
                {
                    "index": "451-5",
                    "sentence": "We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion.",
                    "tag": "2+3"
                },
                {
                    "index": "451-6",
                    "sentence": "We also introduce a joint-learning objective to better capture contextual information.",
                    "tag": "2"
                },
                {
                    "index": "451-7",
                    "sentence": "Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-452",
            "text": [
                {
                    "index": "452-0",
                    "sentence": "In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases.",
                    "tag": "1+2"
                },
                {
                    "index": "452-1",
                    "sentence": "Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way.",
                    "tag": "3"
                },
                {
                    "index": "452-2",
                    "sentence": "To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition.",
                    "tag": "2"
                },
                {
                    "index": "452-3",
                    "sentence": "We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance.",
                    "tag": "2+3"
                },
                {
                    "index": "452-4",
                    "sentence": "Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, “Cambridge” and the first non-English corpus “Robert”, which we release to complement our empirical study.",
                    "tag": "3"
                },
                {
                    "index": "452-5",
                    "sentence": "Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-453",
            "text": [
                {
                    "index": "453-0",
                    "sentence": "More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT.",
                    "tag": "1"
                },
                {
                    "index": "453-1",
                    "sentence": "However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge.",
                    "tag": "1"
                },
                {
                    "index": "453-2",
                    "sentence": "To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models.",
                    "tag": "1+2"
                },
                {
                    "index": "453-3",
                    "sentence": "Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities.",
                    "tag": "3"
                },
                {
                    "index": "453-4",
                    "sentence": "Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities.",
                    "tag": "3"
                },
                {
                    "index": "453-5",
                    "sentence": "Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks.",
                    "tag": "2+3"
                },
                {
                    "index": "453-6",
                    "sentence": "Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-454",
            "text": [
                {
                    "index": "454-0",
                    "sentence": "Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs.",
                    "tag": "2"
                },
                {
                    "index": "454-1",
                    "sentence": "GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples.",
                    "tag": "3"
                },
                {
                    "index": "454-2",
                    "sentence": "However, attribute triples can also provide crucial alignment signal but have not been well explored yet.",
                    "tag": "1"
                },
                {
                    "index": "454-3",
                    "sentence": "In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently.",
                    "tag": "2+3"
                },
                {
                    "index": "454-4",
                    "sentence": "Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets.",
                    "tag": "3"
                },
                {
                    "index": "454-5",
                    "sentence": "To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set.",
                    "tag": "3"
                },
                {
                    "index": "454-6",
                    "sentence": "Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets.",
                    "tag": "4"
                },
                {
                    "index": "454-7",
                    "sentence": "Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method.",
                    "tag": "4"
                },
                {
                    "index": "454-8",
                    "sentence": "Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-455",
            "text": [
                {
                    "index": "455-0",
                    "sentence": "We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference.",
                    "tag": "2+3"
                },
                {
                    "index": "455-1",
                    "sentence": "Our system uses a supervised NER model trained on the source domain, as a feature extractor.",
                    "tag": "3"
                },
                {
                    "index": "455-2",
                    "sentence": "Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches.",
                    "tag": "4"
                },
                {
                    "index": "455-3",
                    "sentence": "We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training.",
                    "tag": "3"
                },
                {
                    "index": "455-4",
                    "sentence": "We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-456",
            "text": [
                {
                    "index": "456-0",
                    "sentence": "Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation.",
                    "tag": "1"
                },
                {
                    "index": "456-1",
                    "sentence": "Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging.",
                    "tag": "1"
                },
                {
                    "index": "456-2",
                    "sentence": "In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem.",
                    "tag": "2+3"
                },
                {
                    "index": "456-3",
                    "sentence": "Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-457",
            "text": [
                {
                    "index": "457-0",
                    "sentence": "Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER.",
                    "tag": "1"
                },
                {
                    "index": "457-1",
                    "sentence": "To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method.",
                    "tag": "2+3"
                },
                {
                    "index": "457-2",
                    "sentence": "In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method.",
                    "tag": "3"
                },
                {
                    "index": "457-3",
                    "sentence": "We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations.",
                    "tag": "3"
                },
                {
                    "index": "457-4",
                    "sentence": "In addition, an entity classification task helps inject the entity information into model parameters in pre-training.",
                    "tag": "3"
                },
                {
                    "index": "457-5",
                    "sentence": "The pre-trained models are used for NER fine-tuning.",
                    "tag": "3"
                },
                {
                    "index": "457-6",
                    "sentence": "Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-458",
            "text": [
                {
                    "index": "458-0",
                    "sentence": "This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off.",
                    "tag": "1"
                },
                {
                    "index": "458-1",
                    "sentence": "We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description.",
                    "tag": "2+3"
                },
                {
                    "index": "458-2",
                    "sentence": "The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions.",
                    "tag": "3"
                },
                {
                    "index": "458-3",
                    "sentence": "Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text.",
                    "tag": "3"
                },
                {
                    "index": "458-4",
                    "sentence": "Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables).",
                    "tag": "4"
                },
                {
                    "index": "458-5",
                    "sentence": "We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation.",
                    "tag": "4"
                },
                {
                    "index": "458-6",
                    "sentence": "Our code and models are available at https://github.com/facebookresearch/BLINK.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-459",
            "text": [
                {
                    "index": "459-0",
                    "sentence": "We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass.",
                    "tag": "2+3"
                },
                {
                    "index": "459-1",
                    "sentence": "Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively.",
                    "tag": "3+4"
                },
                {
                    "index": "459-2",
                    "sentence": "With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems.",
                    "tag": "3"
                },
                {
                    "index": "459-3",
                    "sentence": "In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-460",
            "text": [
                {
                    "index": "460-0",
                    "sentence": "Entity representations are useful in natural language tasks involving entities.",
                    "tag": "1"
                },
                {
                    "index": "460-1",
                    "sentence": "In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer.",
                    "tag": "2"
                },
                {
                    "index": "460-2",
                    "sentence": "The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.",
                    "tag": "3"
                },
                {
                    "index": "460-3",
                    "sentence": "Our model is trained using a new pretraining task based on the masked language model of BERT.",
                    "tag": "3"
                },
                {
                    "index": "460-4",
                    "sentence": "The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.",
                    "tag": "3"
                },
                {
                    "index": "460-5",
                    "sentence": "We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.",
                    "tag": "3"
                },
                {
                    "index": "460-6",
                    "sentence": "The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.",
                    "tag": "4"
                },
                {
                    "index": "460-7",
                    "sentence": "In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).",
                    "tag": "4"
                },
                {
                    "index": "460-8",
                    "sentence": "Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-461",
            "text": [
                {
                    "index": "461-0",
                    "sentence": "Literary tropes, from poetry to stories, are at the crux of human imagination and communication.",
                    "tag": "1"
                },
                {
                    "index": "461-1",
                    "sentence": "Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations.",
                    "tag": "1"
                },
                {
                    "index": "461-2",
                    "sentence": "In this paper, we tackle the problem of simile generation.",
                    "tag": "2"
                },
                {
                    "index": "461-3",
                    "sentence": "Generating a simile requires proper understanding for effective mapping of properties between two concepts.",
                    "tag": "1"
                },
                {
                    "index": "461-4",
                    "sentence": "To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge.",
                    "tag": "3"
                },
                {
                    "index": "461-5",
                    "sentence": "We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence.",
                    "tag": "3"
                },
                {
                    "index": "461-6",
                    "sentence": "Experiments show that our approach generates 88% novel similes that do not share properties with the training data.",
                    "tag": "4"
                },
                {
                    "index": "461-7",
                    "sentence": "Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise.",
                    "tag": "4"
                },
                {
                    "index": "461-8",
                    "sentence": "We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-462",
            "text": [
                {
                    "index": "462-0",
                    "sentence": "Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints.",
                    "tag": "1"
                },
                {
                    "index": "462-1",
                    "sentence": "Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently.",
                    "tag": "1"
                },
                {
                    "index": "462-2",
                    "sentence": "In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint.",
                    "tag": "2+3"
                },
                {
                    "index": "462-3",
                    "sentence": "We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs.",
                    "tag": "2+3"
                },
                {
                    "index": "462-4",
                    "sentence": "Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document.",
                    "tag": "3+4"
                },
                {
                    "index": "462-5",
                    "sentence": "Finally, we analyze our model’s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-463",
            "text": [
                {
                    "index": "463-0",
                    "sentence": "Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language.",
                    "tag": "1"
                },
                {
                    "index": "463-1",
                    "sentence": "In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs.",
                    "tag": "2+3"
                },
                {
                    "index": "463-2",
                    "sentence": "First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language.",
                    "tag": "2+3"
                },
                {
                    "index": "463-3",
                    "sentence": "Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API.",
                    "tag": "3"
                },
                {
                    "index": "463-4",
                    "sentence": "To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance.",
                    "tag": "3"
                },
                {
                    "index": "463-5",
                    "sentence": "A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text.",
                    "tag": "3"
                },
                {
                    "index": "463-6",
                    "sentence": "Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-464",
            "text": [
                {
                    "index": "464-0",
                    "sentence": "Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph).",
                    "tag": "1"
                },
                {
                    "index": "464-1",
                    "sentence": "Humans often make structural decisions on what and how to say about before making utterances.",
                    "tag": "1"
                },
                {
                    "index": "464-2",
                    "sentence": "Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process.",
                    "tag": "1"
                },
                {
                    "index": "464-3",
                    "sentence": "Where can the model learn such high-level coherence?",
                    "tag": "1"
                },
                {
                    "index": "464-4",
                    "sentence": "A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on.",
                    "tag": "1+2"
                },
                {
                    "index": "464-5",
                    "sentence": "Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph.",
                    "tag": "2"
                },
                {
                    "index": "464-6",
                    "sentence": "However, the task suffers from predicting and selecting appropriate topical content with respect to the given context.",
                    "tag": "1"
                },
                {
                    "index": "464-7",
                    "sentence": "To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content.",
                    "tag": "2+3"
                },
                {
                    "index": "464-8",
                    "sentence": "SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation.",
                    "tag": "4"
                },
                {
                    "index": "464-9",
                    "sentence": "We also find that a combination of noun and verb types of keywords is the most effective for content selection.",
                    "tag": "4"
                },
                {
                    "index": "464-10",
                    "sentence": "As more number of content keywords are provided, overall generation quality also increases.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-465",
            "text": [
                {
                    "index": "465-0",
                    "sentence": "Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains.",
                    "tag": "1"
                },
                {
                    "index": "465-1",
                    "sentence": "In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy.",
                    "tag": "1"
                },
                {
                    "index": "465-2",
                    "sentence": "In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations.",
                    "tag": "1+2"
                },
                {
                    "index": "465-3",
                    "sentence": "To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding.",
                    "tag": "2+3"
                },
                {
                    "index": "465-4",
                    "sentence": "Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations.",
                    "tag": "2+3"
                },
                {
                    "index": "465-5",
                    "sentence": "We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset.",
                    "tag": "2+3"
                },
                {
                    "index": "465-6",
                    "sentence": "Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding.",
                    "tag": "2+3"
                },
                {
                    "index": "465-7",
                    "sentence": "Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-466",
            "text": [
                {
                    "index": "466-0",
                    "sentence": "The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets.",
                    "tag": "1"
                },
                {
                    "index": "466-1",
                    "sentence": "Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users.",
                    "tag": "1"
                },
                {
                    "index": "466-2",
                    "sentence": "In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants.",
                    "tag": "2"
                },
                {
                    "index": "466-3",
                    "sentence": "The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation.",
                    "tag": "3"
                },
                {
                    "index": "466-4",
                    "sentence": "In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on.",
                    "tag": "2"
                },
                {
                    "index": "466-5",
                    "sentence": "Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems.",
                    "tag": "3"
                },
                {
                    "index": "466-6",
                    "sentence": "The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model.",
                    "tag": "4"
                },
                {
                    "index": "466-7",
                    "sentence": "In those cases, a significant number of information leaking utterances can be detected by our models with high precision.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-467",
            "text": [
                {
                    "index": "467-0",
                    "sentence": "While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario.",
                    "tag": "1"
                },
                {
                    "index": "467-1",
                    "sentence": "Hence, the prolongation and transition of conversation topics are ignored by current methods.",
                    "tag": "1"
                },
                {
                    "index": "467-2",
                    "sentence": "In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context.",
                    "tag": "1+2"
                },
                {
                    "index": "467-3",
                    "sentence": "With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection.",
                    "tag": "2+3"
                },
                {
                    "index": "467-4",
                    "sentence": "We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning.",
                    "tag": "2+3"
                },
                {
                    "index": "467-5",
                    "sentence": "Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-468",
            "text": [
                {
                    "index": "468-0",
                    "sentence": "Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario.",
                    "tag": "1"
                },
                {
                    "index": "468-1",
                    "sentence": "To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge.",
                    "tag": "2+3"
                },
                {
                    "index": "468-2",
                    "sentence": "More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference.",
                    "tag": "4"
                },
                {
                    "index": "468-3",
                    "sentence": "Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-469",
            "text": [
                {
                    "index": "469-0",
                    "sentence": "Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed.",
                    "tag": "1"
                },
                {
                    "index": "469-1",
                    "sentence": "In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots.",
                    "tag": "1+2"
                },
                {
                    "index": "469-2",
                    "sentence": "We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval.",
                    "tag": "2+3"
                },
                {
                    "index": "469-3",
                    "sentence": "The model is first pretrained on the huge general-domain data, then finetuned on our corpus.",
                    "tag": "3"
                },
                {
                    "index": "469-4",
                    "sentence": "We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules.",
                    "tag": "3+4"
                },
                {
                    "index": "469-5",
                    "sentence": "On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones.",
                    "tag": "4"
                },
                {
                    "index": "469-6",
                    "sentence": "We further analyze the limits of our work and point out potential directions for future work",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-470",
            "text": [
                {
                    "index": "470-0",
                    "sentence": "Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved.",
                    "tag": "1"
                },
                {
                    "index": "470-1",
                    "sentence": "We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.",
                    "tag": "1+2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-471",
            "text": [
                {
                    "index": "471-0",
                    "sentence": "For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance.",
                    "tag": "1"
                },
                {
                    "index": "471-1",
                    "sentence": "Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words.",
                    "tag": "1"
                },
                {
                    "index": "471-2",
                    "sentence": "In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model.",
                    "tag": "2+3"
                },
                {
                    "index": "471-3",
                    "sentence": "Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-472",
            "text": [
                {
                    "index": "472-0",
                    "sentence": "Quotations are crucial for successful explanations and persuasions in interpersonal communications.",
                    "tag": "1"
                },
                {
                    "index": "472-1",
                    "sentence": "However, finding what to quote in a conversation is challenging for both humans and machines.",
                    "tag": "1"
                },
                {
                    "index": "472-2",
                    "sentence": "This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context.",
                    "tag": "2"
                },
                {
                    "index": "472-3",
                    "sentence": "Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn’s existing contents.",
                    "tag": "2"
                },
                {
                    "index": "472-4",
                    "sentence": "Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation.",
                    "tag": "2+3"
                },
                {
                    "index": "472-5",
                    "sentence": "Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models.",
                    "tag": "4"
                },
                {
                    "index": "472-6",
                    "sentence": "Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-473",
            "text": [
                {
                    "index": "473-0",
                    "sentence": "Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction.",
                    "tag": "1"
                },
                {
                    "index": "473-1",
                    "sentence": "However, they do not fully use law article information and most of the current work is only for single label samples.",
                    "tag": "1"
                },
                {
                    "index": "473-2",
                    "sentence": "In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples.",
                    "tag": "2"
                },
                {
                    "index": "473-3",
                    "sentence": "The model uses the labeled elements of law articles to extract fact description features from multiple angles.",
                    "tag": "3"
                },
                {
                    "index": "473-4",
                    "sentence": "It generates multiple representations of a fact for classification.",
                    "tag": "3"
                },
                {
                    "index": "473-5",
                    "sentence": "Every label has a law-aware fact representation to encode more information.",
                    "tag": "4"
                },
                {
                    "index": "473-6",
                    "sentence": "To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations.",
                    "tag": "4"
                },
                {
                    "index": "473-7",
                    "sentence": "Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-474",
            "text": [
                {
                    "index": "474-0",
                    "sentence": "Knowledge graph reasoning is a critical task in natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "474-1",
                    "sentence": "The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp.",
                    "tag": "1"
                },
                {
                    "index": "474-2",
                    "sentence": "Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future.",
                    "tag": "1"
                },
                {
                    "index": "474-3",
                    "sentence": "This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions.",
                    "tag": "1+2"
                },
                {
                    "index": "474-4",
                    "sentence": "The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs.",
                    "tag": "3"
                },
                {
                    "index": "474-5",
                    "sentence": "Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp.",
                    "tag": "3"
                },
                {
                    "index": "474-6",
                    "sentence": "Future facts can then be inferred in a sequential manner based on the two modules.",
                    "tag": "3"
                },
                {
                    "index": "474-7",
                    "sentence": "We evaluate our proposed method via link prediction at future times on five public datasets.",
                    "tag": "3"
                },
                {
                    "index": "474-8",
                    "sentence": "Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-475",
            "text": [
                {
                    "index": "475-0",
                    "sentence": "The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences.",
                    "tag": "1"
                },
                {
                    "index": "475-1",
                    "sentence": "Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets.",
                    "tag": "1"
                },
                {
                    "index": "475-2",
                    "sentence": "In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features.",
                    "tag": "2"
                },
                {
                    "index": "475-3",
                    "sentence": "We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices.",
                    "tag": "3"
                },
                {
                    "index": "475-4",
                    "sentence": "We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features.",
                    "tag": "3"
                },
                {
                    "index": "475-5",
                    "sentence": "Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "475-6",
                    "sentence": "We also combine our model with BERT to further boost the generalization performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-476",
            "text": [
                {
                    "index": "476-0",
                    "sentence": "The data imbalance problem is a crucial issue for the multi-label text classification.",
                    "tag": "1"
                },
                {
                    "index": "476-1",
                    "sentence": "Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data.",
                    "tag": "1"
                },
                {
                    "index": "476-2",
                    "sentence": "We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories.",
                    "tag": "2+3"
                },
                {
                    "index": "476-3",
                    "sentence": "We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN.",
                    "tag": "2+3"
                },
                {
                    "index": "476-4",
                    "sentence": "The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-477",
            "text": [
                {
                    "index": "477-0",
                    "sentence": "This paper proposes a pre-training based automated Chinese essay scoring method.",
                    "tag": "2"
                },
                {
                    "index": "477-1",
                    "sentence": "The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning.",
                    "tag": "3"
                },
                {
                    "index": "477-2",
                    "sentence": "An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision.",
                    "tag": "3"
                },
                {
                    "index": "477-3",
                    "sentence": "The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision.",
                    "tag": "3+4"
                },
                {
                    "index": "477-4",
                    "sentence": "At last, the scorer is fine-tuned on the target-prompt training data.",
                    "tag": "3"
                },
                {
                    "index": "477-5",
                    "sentence": "The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-478",
            "text": [
                {
                    "index": "478-0",
                    "sentence": "Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions.",
                    "tag": "1"
                },
                {
                    "index": "478-1",
                    "sentence": "In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries.",
                    "tag": "1+2"
                },
                {
                    "index": "478-2",
                    "sentence": "Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer.",
                    "tag": "3"
                },
                {
                    "index": "478-3",
                    "sentence": "A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives.",
                    "tag": "3"
                },
                {
                    "index": "478-4",
                    "sentence": "Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-479",
            "text": [
                {
                    "index": "479-0",
                    "sentence": "We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation.",
                    "tag": "1"
                },
                {
                    "index": "479-1",
                    "sentence": "Existing works ignore the complicated reasoning process and solve it with a one-step “black box” model.",
                    "tag": "1"
                },
                {
                    "index": "479-2",
                    "sentence": "Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules.",
                    "tag": "2+3"
                },
                {
                    "index": "479-3",
                    "sentence": "In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model.",
                    "tag": "3"
                },
                {
                    "index": "479-4",
                    "sentence": "Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-480",
            "text": [
                {
                    "index": "480-0",
                    "sentence": "Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation.",
                    "tag": "1"
                },
                {
                    "index": "480-1",
                    "sentence": "To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph.",
                    "tag": "2"
                },
                {
                    "index": "480-2",
                    "sentence": "Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-481",
            "text": [
                {
                    "index": "481-0",
                    "sentence": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method.",
                    "tag": "1"
                },
                {
                    "index": "481-1",
                    "sentence": "In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.",
                    "tag": "2+3"
                },
                {
                    "index": "481-2",
                    "sentence": "When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-482",
            "text": [
                {
                    "index": "482-0",
                    "sentence": "There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text.",
                    "tag": "1"
                },
                {
                    "index": "482-1",
                    "sentence": "However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text.",
                    "tag": "1"
                },
                {
                    "index": "482-2",
                    "sentence": "In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance.",
                    "tag": "1+2"
                },
                {
                    "index": "482-3",
                    "sentence": "We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation.",
                    "tag": "3"
                },
                {
                    "index": "482-4",
                    "sentence": "To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN.",
                    "tag": "2+3"
                },
                {
                    "index": "482-5",
                    "sentence": "We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-483",
            "text": [
                {
                    "index": "483-0",
                    "sentence": "The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure?",
                    "tag": "1"
                },
                {
                    "index": "483-1",
                    "sentence": "If so, how is this structure encoded?",
                    "tag": "1"
                },
                {
                    "index": "483-2",
                    "sentence": "To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs.",
                    "tag": "2+3"
                },
                {
                    "index": "483-3",
                    "sentence": "Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form.",
                    "tag": "3+4"
                },
                {
                    "index": "483-4",
                    "sentence": "In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments.",
                    "tag": "3+4"
                },
                {
                    "index": "483-5",
                    "sentence": "Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-484",
            "text": [
                {
                    "index": "484-0",
                    "sentence": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied.",
                    "tag": "1"
                },
                {
                    "index": "484-1",
                    "sentence": "We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model.",
                    "tag": "1+2"
                },
                {
                    "index": "484-2",
                    "sentence": "Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining.",
                    "tag": "4"
                },
                {
                    "index": "484-3",
                    "sentence": "We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance.",
                    "tag": "4"
                },
                {
                    "index": "484-4",
                    "sentence": "These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge.",
                    "tag": "4+5"
                },
                {
                    "index": "484-5",
                    "sentence": "We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-485",
            "text": [
                {
                    "index": "485-0",
                    "sentence": "We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models.",
                    "tag": "2+3"
                },
                {
                    "index": "485-1",
                    "sentence": "We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language.",
                    "tag": "2+3"
                },
                {
                    "index": "485-2",
                    "sentence": "We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary.",
                    "tag": "4"
                },
                {
                    "index": "485-3",
                    "sentence": "To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion.",
                    "tag": "3"
                },
                {
                    "index": "485-4",
                    "sentence": "Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language.",
                    "tag": "4"
                },
                {
                    "index": "485-5",
                    "sentence": "Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties.",
                    "tag": "4+5"
                },
                {
                    "index": "485-6",
                    "sentence": "Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-486",
            "text": [
                {
                    "index": "486-0",
                    "sentence": "Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models.",
                    "tag": "1"
                },
                {
                    "index": "486-1",
                    "sentence": "We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump).",
                    "tag": "2"
                },
                {
                    "index": "486-2",
                    "sentence": "While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts.",
                    "tag": "4"
                },
                {
                    "index": "486-3",
                    "sentence": "For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment.",
                    "tag": "4"
                },
                {
                    "index": "486-4",
                    "sentence": "We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers.",
                    "tag": "3+4"
                },
                {
                    "index": "486-5",
                    "sentence": "As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-487",
            "text": [
                {
                    "index": "487-0",
                    "sentence": "We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas).",
                    "tag": "2+3"
                },
                {
                    "index": "487-1",
                    "sentence": "GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g.",
                    "tag": "3"
                },
                {
                    "index": "487-2",
                    "sentence": "utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser.",
                    "tag": "3"
                },
                {
                    "index": "487-3",
                    "sentence": "Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution.",
                    "tag": "3"
                },
                {
                    "index": "487-4",
                    "sentence": "On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser.",
                    "tag": "3+4"
                },
                {
                    "index": "487-5",
                    "sentence": "Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-488",
            "text": [
                {
                    "index": "488-0",
                    "sentence": "Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks.",
                    "tag": "1"
                },
                {
                    "index": "488-1",
                    "sentence": "In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users.",
                    "tag": "1+2"
                },
                {
                    "index": "488-2",
                    "sentence": "A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain.",
                    "tag": "1"
                },
                {
                    "index": "488-3",
                    "sentence": "In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions.",
                    "tag": "3"
                },
                {
                    "index": "488-4",
                    "sentence": "To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011).",
                    "tag": "3"
                },
                {
                    "index": "488-5",
                    "sentence": "We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.",
                    "tag": "4+5"
                },
                {
                    "index": "488-6",
                    "sentence": "Code will be available at https://github.com/sunlab-osu/MISP.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-489",
            "text": [
                {
                    "index": "489-0",
                    "sentence": "Context-dependent text-to-SQL task has drawn much attention in recent years.",
                    "tag": "1"
                },
                {
                    "index": "489-1",
                    "sentence": "Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs.",
                    "tag": "1"
                },
                {
                    "index": "489-2",
                    "sentence": "In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items.",
                    "tag": "2+3"
                },
                {
                    "index": "489-3",
                    "sentence": "In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens.",
                    "tag": "3"
                },
                {
                    "index": "489-4",
                    "sentence": "We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets.",
                    "tag": "2"
                },
                {
                    "index": "489-5",
                    "sentence": "Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets.",
                    "tag": "4"
                },
                {
                    "index": "489-6",
                    "sentence": "The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-490",
            "text": [
                {
                    "index": "490-0",
                    "sentence": "In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions.",
                    "tag": "1"
                },
                {
                    "index": "490-1",
                    "sentence": "Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems.",
                    "tag": "1"
                },
                {
                    "index": "490-2",
                    "sentence": "One main reason stems from the difficulty of fully understanding the users’ natural language questions.",
                    "tag": "1"
                },
                {
                    "index": "490-3",
                    "sentence": "In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers.",
                    "tag": "2+3"
                },
                {
                    "index": "490-4",
                    "sentence": "Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers.",
                    "tag": "3"
                },
                {
                    "index": "490-5",
                    "sentence": "These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-491",
            "text": [
                {
                    "index": "491-0",
                    "sentence": "On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots.",
                    "tag": "1"
                },
                {
                    "index": "491-1",
                    "sentence": "Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses.",
                    "tag": "1"
                },
                {
                    "index": "491-2",
                    "sentence": "To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries.",
                    "tag": "2+3"
                },
                {
                    "index": "491-3",
                    "sentence": "Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-492",
            "text": [
                {
                    "index": "492-0",
                    "sentence": "(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories.",
                    "tag": "1"
                },
                {
                    "index": "492-1",
                    "sentence": "Incremental learning on new categories is necessary for (T)ACSA real applications.",
                    "tag": "1"
                },
                {
                    "index": "492-2",
                    "sentence": "Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks.",
                    "tag": "1"
                },
                {
                    "index": "492-3",
                    "sentence": "In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net).",
                    "tag": "2"
                },
                {
                    "index": "492-4",
                    "sentence": "We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem.",
                    "tag": "3"
                },
                {
                    "index": "492-5",
                    "sentence": "Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.",
                    "tag": "3"
                },
                {
                    "index": "492-6",
                    "sentence": "Our model achieved state-of-the-art on two (T)ACSA benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "492-7",
                    "sentence": "Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-493",
            "text": [
                {
                    "index": "493-0",
                    "sentence": "Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "493-1",
                    "sentence": "However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns.",
                    "tag": "1"
                },
                {
                    "index": "493-2",
                    "sentence": "In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning.",
                    "tag": "2+3"
                },
                {
                    "index": "493-3",
                    "sentence": "In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns.",
                    "tag": "2+3"
                },
                {
                    "index": "493-4",
                    "sentence": "Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens.",
                    "tag": "2"
                },
                {
                    "index": "493-5",
                    "sentence": "Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient.",
                    "tag": "4+5"
                },
                {
                    "index": "493-6",
                    "sentence": "The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-494",
            "text": [
                {
                    "index": "494-0",
                    "sentence": "Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "494-1",
                    "sentence": "To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models.",
                    "tag": "2+3"
                },
                {
                    "index": "494-2",
                    "sentence": "We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet.",
                    "tag": "2+3"
                },
                {
                    "index": "494-3",
                    "sentence": "Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation.",
                    "tag": "2+3"
                },
                {
                    "index": "494-4",
                    "sentence": "Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-495",
            "text": [
                {
                    "index": "495-0",
                    "sentence": "Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner.",
                    "tag": "1"
                },
                {
                    "index": "495-1",
                    "sentence": "It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity.",
                    "tag": "1"
                },
                {
                    "index": "495-2",
                    "sentence": "In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples.",
                    "tag": "2+3"
                },
                {
                    "index": "495-3",
                    "sentence": "Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts.",
                    "tag": "3"
                },
                {
                    "index": "495-4",
                    "sentence": "We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data.",
                    "tag": "3"
                },
                {
                    "index": "495-5",
                    "sentence": "Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-496",
            "text": [
                {
                    "index": "496-0",
                    "sentence": "Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments.",
                    "tag": "1"
                },
                {
                    "index": "496-1",
                    "sentence": "However, few works study both of them simultaneously.",
                    "tag": "1"
                },
                {
                    "index": "496-2",
                    "sentence": "In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them.",
                    "tag": "1+2"
                },
                {
                    "index": "496-3",
                    "sentence": "We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task.",
                    "tag": "3"
                },
                {
                    "index": "496-4",
                    "sentence": "To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task.",
                    "tag": "3"
                },
                {
                    "index": "496-5",
                    "sentence": "Thus, we propose a multitask learning framework based on hierarchical LSTM networks.",
                    "tag": "4"
                },
                {
                    "index": "496-6",
                    "sentence": "Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-497",
            "text": [
                {
                    "index": "497-0",
                    "sentence": "Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious.",
                    "tag": "1"
                },
                {
                    "index": "497-1",
                    "sentence": "As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations.",
                    "tag": "1"
                },
                {
                    "index": "497-2",
                    "sentence": "To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision.",
                    "tag": "2+3"
                },
                {
                    "index": "497-3",
                    "sentence": "Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision.",
                    "tag": "3"
                },
                {
                    "index": "497-4",
                    "sentence": "Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training.",
                    "tag": "3"
                },
                {
                    "index": "497-5",
                    "sentence": "Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment.",
                    "tag": "3"
                },
                {
                    "index": "497-6",
                    "sentence": "Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-498",
            "text": [
                {
                    "index": "498-0",
                    "sentence": "While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community.",
                    "tag": "1"
                },
                {
                    "index": "498-1",
                    "sentence": "We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-499",
            "text": [
                {
                    "index": "499-0",
                    "sentence": "The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data.",
                    "tag": "1"
                },
                {
                    "index": "499-1",
                    "sentence": "However, fine-grained labeled data are scarce for the ABSA task.",
                    "tag": "1"
                },
                {
                    "index": "499-2",
                    "sentence": "To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation.",
                    "tag": "1"
                },
                {
                    "index": "499-3",
                    "sentence": "To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper.",
                    "tag": "1+2"
                },
                {
                    "index": "499-4",
                    "sentence": "Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling.",
                    "tag": "3"
                },
                {
                    "index": "499-5",
                    "sentence": "Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-500",
            "text": [
                {
                    "index": "500-0",
                    "sentence": "Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected.",
                    "tag": "1"
                },
                {
                    "index": "500-1",
                    "sentence": "The goal is to enable users to assess the correctness of the system and understand its reasoning process.",
                    "tag": "1"
                },
                {
                    "index": "500-2",
                    "sentence": "However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience.",
                    "tag": "1"
                },
                {
                    "index": "500-3",
                    "sentence": "As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling.",
                    "tag": "2"
                },
                {
                    "index": "500-4",
                    "sentence": "We conduct experiments on the HOTPOTQA benchmark data set and perform a user study.",
                    "tag": "2+3"
                },
                {
                    "index": "500-5",
                    "sentence": "The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users.",
                    "tag": "4"
                },
                {
                    "index": "500-6",
                    "sentence": "Our scores are better aligned with user experience, making them promising candidates for model selection.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-501",
            "text": [
                {
                    "index": "501-0",
                    "sentence": "Knowledge graphs (KGs) can vary greatly from one domain to another.",
                    "tag": "1"
                },
                {
                    "index": "501-1",
                    "sentence": "Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations.",
                    "tag": "1"
                },
                {
                    "index": "501-2",
                    "sentence": "This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains.",
                    "tag": "1"
                },
                {
                    "index": "501-3",
                    "sentence": "To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing.",
                    "tag": "2+3"
                },
                {
                    "index": "501-4",
                    "sentence": "We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome.",
                    "tag": "3"
                },
                {
                    "index": "501-5",
                    "sentence": "Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other.",
                    "tag": "4"
                },
                {
                    "index": "501-6",
                    "sentence": "In additional experiments, we investigate the impact of using different unsupervised objectives.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-502",
            "text": [
                {
                    "index": "502-0",
                    "sentence": "We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer.",
                    "tag": "2+3"
                },
                {
                    "index": "502-1",
                    "sentence": "Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training.",
                    "tag": "3"
                },
                {
                    "index": "502-2",
                    "sentence": "Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-503",
            "text": [
                {
                    "index": "503-0",
                    "sentence": "With the advancements in natural language processing tasks, math word problem solving has received increasing attention.",
                    "tag": "1"
                },
                {
                    "index": "503-1",
                    "sentence": "Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem.",
                    "tag": "1"
                },
                {
                    "index": "503-2",
                    "sentence": "In addition, during generation, they focus on local features while neglecting global information.",
                    "tag": "1"
                },
                {
                    "index": "503-3",
                    "sentence": "To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph.",
                    "tag": "2+3"
                },
                {
                    "index": "503-4",
                    "sentence": "Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations.",
                    "tag": "2"
                },
                {
                    "index": "503-5",
                    "sentence": "Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information.",
                    "tag": "2"
                },
                {
                    "index": "503-6",
                    "sentence": "Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-504",
            "text": [
                {
                    "index": "504-0",
                    "sentence": "We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC).",
                    "tag": "2+3"
                },
                {
                    "index": "504-1",
                    "sentence": "ESD identifies grammatically incorrect text spans with an efficient sequence tagging model.",
                    "tag": "3"
                },
                {
                    "index": "504-2",
                    "sentence": "Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans.",
                    "tag": "3"
                },
                {
                    "index": "504-3",
                    "sentence": "Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-505",
            "text": [
                {
                    "index": "505-0",
                    "sentence": "Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning.",
                    "tag": "1"
                },
                {
                    "index": "505-1",
                    "sentence": "However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse.",
                    "tag": "1"
                },
                {
                    "index": "505-2",
                    "sentence": "To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context.",
                    "tag": "2"
                },
                {
                    "index": "505-3",
                    "sentence": "The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks.",
                    "tag": "4"
                },
                {
                    "index": "505-4",
                    "sentence": "The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-506",
            "text": [
                {
                    "index": "506-0",
                    "sentence": "Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory.",
                    "tag": "1"
                },
                {
                    "index": "506-1",
                    "sentence": "In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions.",
                    "tag": "2"
                },
                {
                    "index": "506-2",
                    "sentence": "In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses.",
                    "tag": "2+3"
                },
                {
                    "index": "506-3",
                    "sentence": "We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context.",
                    "tag": "4"
                },
                {
                    "index": "506-4",
                    "sentence": "Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases.",
                    "tag": "4+5"
                },
                {
                    "index": "506-5",
                    "sentence": "The software and reproduction materials are available at http://generationary.org.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-507",
            "text": [
                {
                    "index": "507-0",
                    "sentence": "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture.",
                    "tag": "1"
                },
                {
                    "index": "507-1",
                    "sentence": "While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context.",
                    "tag": "1"
                },
                {
                    "index": "507-2",
                    "sentence": "In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance?",
                    "tag": "2+3"
                },
                {
                    "index": "507-3",
                    "sentence": "How consistent are the observed effects across tasks and languages?",
                    "tag": "2"
                },
                {
                    "index": "507-4",
                    "sentence": "2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network?",
                    "tag": "3"
                },
                {
                    "index": "507-5",
                    "sentence": "3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities?",
                    "tag": "3"
                },
                {
                    "index": "507-6",
                    "sentence": "Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks.",
                    "tag": "4"
                },
                {
                    "index": "507-7",
                    "sentence": "Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-508",
            "text": [
                {
                    "index": "508-0",
                    "sentence": "Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal.",
                    "tag": "1"
                },
                {
                    "index": "508-1",
                    "sentence": "To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource.",
                    "tag": "2+3"
                },
                {
                    "index": "508-2",
                    "sentence": "First, we regularize the representation of user utterances based on their corresponding labels.",
                    "tag": "3"
                },
                {
                    "index": "508-3",
                    "sentence": "Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables.",
                    "tag": "3"
                },
                {
                    "index": "508-4",
                    "sentence": "Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-509",
            "text": [
                {
                    "index": "509-0",
                    "sentence": "Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data.",
                    "tag": "1"
                },
                {
                    "index": "509-1",
                    "sentence": "However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive.",
                    "tag": "1"
                },
                {
                    "index": "509-2",
                    "sentence": "In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective.",
                    "tag": "2+3"
                },
                {
                    "index": "509-3",
                    "sentence": "Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset.",
                    "tag": "2+3"
                },
                {
                    "index": "509-4",
                    "sentence": "Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples.",
                    "tag": "3+4"
                },
                {
                    "index": "509-5",
                    "sentence": "Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels.",
                    "tag": "4"
                },
                {
                    "index": "509-6",
                    "sentence": "We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented.",
                    "tag": "2"
                },
                {
                    "index": "509-7",
                    "sentence": "Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation.",
                    "tag": "4"
                },
                {
                    "index": "509-8",
                    "sentence": "As a result, our method eliminates part of the spurious correlations between context representation and output labels.",
                    "tag": "4"
                },
                {
                    "index": "509-9",
                    "sentence": "The code is available at https://github.com/xijiz/cfgen.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-510",
            "text": [
                {
                    "index": "510-0",
                    "sentence": "The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process.",
                    "tag": "1"
                },
                {
                    "index": "510-1",
                    "sentence": "Here, the key is to track how the entities interact with each other and how their states are changing along the procedure.",
                    "tag": "1"
                },
                {
                    "index": "510-2",
                    "sentence": "Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned.",
                    "tag": "1"
                },
                {
                    "index": "510-3",
                    "sentence": "In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking.",
                    "tag": "2"
                },
                {
                    "index": "510-4",
                    "sentence": "In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions.",
                    "tag": "2+3"
                },
                {
                    "index": "510-5",
                    "sentence": "Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes.",
                    "tag": "2"
                },
                {
                    "index": "510-6",
                    "sentence": "We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-511",
            "text": [
                {
                    "index": "511-0",
                    "sentence": "There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.",
                    "tag": "1"
                },
                {
                    "index": "511-1",
                    "sentence": "Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures.",
                    "tag": "1"
                },
                {
                    "index": "511-2",
                    "sentence": "However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well.",
                    "tag": "1"
                },
                {
                    "index": "511-3",
                    "sentence": "To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data.",
                    "tag": "2+3"
                },
                {
                    "index": "511-4",
                    "sentence": "Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs.",
                    "tag": "3"
                },
                {
                    "index": "511-5",
                    "sentence": "Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp.",
                    "tag": "3"
                },
                {
                    "index": "511-6",
                    "sentence": "We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "511-7",
                    "sentence": "Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-512",
            "text": [
                {
                    "index": "512-0",
                    "sentence": "It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe).",
                    "tag": "1"
                },
                {
                    "index": "512-1",
                    "sentence": "While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings.",
                    "tag": "1"
                },
                {
                    "index": "512-2",
                    "sentence": "We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end.",
                    "tag": "1"
                },
                {
                    "index": "512-3",
                    "sentence": "In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes.",
                    "tag": "1"
                },
                {
                    "index": "512-4",
                    "sentence": "We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm.",
                    "tag": "2+3"
                },
                {
                    "index": "512-5",
                    "sentence": "We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks.",
                    "tag": "4"
                },
                {
                    "index": "512-6",
                    "sentence": "Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-513",
            "text": [
                {
                    "index": "513-0",
                    "sentence": "It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers.",
                    "tag": "1"
                },
                {
                    "index": "513-1",
                    "sentence": "As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases.",
                    "tag": "1"
                },
                {
                    "index": "513-2",
                    "sentence": "Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially.",
                    "tag": "1"
                },
                {
                    "index": "513-3",
                    "sentence": "We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss.",
                    "tag": "2"
                },
                {
                    "index": "513-4",
                    "sentence": "We then add sensitive attributes back on in whitelisted cases.",
                    "tag": "2+3"
                },
                {
                    "index": "513-5",
                    "sentence": "Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-514",
            "text": [
                {
                    "index": "514-0",
                    "sentence": "Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact.",
                    "tag": "1"
                },
                {
                    "index": "514-1",
                    "sentence": "In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs.",
                    "tag": "2"
                },
                {
                    "index": "514-2",
                    "sentence": "Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact.",
                    "tag": "2"
                },
                {
                    "index": "514-3",
                    "sentence": "We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K.",
                    "tag": "3+4"
                },
                {
                    "index": "514-4",
                    "sentence": "Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks.",
                    "tag": "3+4"
                },
                {
                    "index": "514-5",
                    "sentence": "We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-515",
            "text": [
                {
                    "index": "515-0",
                    "sentence": "Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news.",
                    "tag": "1"
                },
                {
                    "index": "515-1",
                    "sentence": "Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account.",
                    "tag": "1"
                },
                {
                    "index": "515-2",
                    "sentence": "In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT).",
                    "tag": "1"
                },
                {
                    "index": "515-3",
                    "sentence": "However, graph-based neural networks do not take sequential information into account.",
                    "tag": "1"
                },
                {
                    "index": "515-4",
                    "sentence": "In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure.",
                    "tag": "2"
                },
                {
                    "index": "515-5",
                    "sentence": "Accordingly, our RGAT model can capture both the speaker dependency and the sequential information.",
                    "tag": "2"
                },
                {
                    "index": "515-6",
                    "sentence": "Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations.",
                    "tag": "4"
                },
                {
                    "index": "515-7",
                    "sentence": "In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-516",
            "text": [
                {
                    "index": "516-0",
                    "sentence": "Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity.",
                    "tag": "1"
                },
                {
                    "index": "516-1",
                    "sentence": "These differences are important for natural language understanding and reasoning.",
                    "tag": "1"
                },
                {
                    "index": "516-2",
                    "sentence": "We propose a novel BERT-based approach to intensity detection for scalar adjectives.",
                    "tag": "2"
                },
                {
                    "index": "516-3",
                    "sentence": "We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives.",
                    "tag": "4"
                },
                {
                    "index": "516-4",
                    "sentence": "We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task.",
                    "tag": "4"
                },
                {
                    "index": "516-5",
                    "sentence": "Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-517",
            "text": [
                {
                    "index": "517-0",
                    "sentence": "Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently.",
                    "tag": "1"
                },
                {
                    "index": "517-1",
                    "sentence": "Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning.",
                    "tag": "1"
                },
                {
                    "index": "517-2",
                    "sentence": "We explore unsupervised domain adaptation (UDA) in this paper.",
                    "tag": "2"
                },
                {
                    "index": "517-3",
                    "sentence": "With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain.",
                    "tag": "3"
                },
                {
                    "index": "517-4",
                    "sentence": "Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training.",
                    "tag": "1"
                },
                {
                    "index": "517-5",
                    "sentence": "However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model.",
                    "tag": "1"
                },
                {
                    "index": "517-6",
                    "sentence": "To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered.",
                    "tag": "2+3"
                },
                {
                    "index": "517-7",
                    "sentence": "We further extend CFd to a cross-language setting, in which language discrepancy is studied.",
                    "tag": "2"
                },
                {
                    "index": "517-8",
                    "sentence": "Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-518",
            "text": [
                {
                    "index": "518-0",
                    "sentence": "In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process.",
                    "tag": "2"
                },
                {
                    "index": "518-1",
                    "sentence": "We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria.",
                    "tag": "3"
                },
                {
                    "index": "518-2",
                    "sentence": "We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function.",
                    "tag": "3"
                },
                {
                    "index": "518-3",
                    "sentence": "Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset.",
                    "tag": "4"
                },
                {
                    "index": "518-4",
                    "sentence": "Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-519",
            "text": [
                {
                    "index": "519-0",
                    "sentence": "Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction.",
                    "tag": "1"
                },
                {
                    "index": "519-1",
                    "sentence": "For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more proactive and powerful (“_He pursues his dream of being a doctor_”).",
                    "tag": "1"
                },
                {
                    "index": "519-2",
                    "sentence": "We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals.",
                    "tag": "1+2"
                },
                {
                    "index": "519-3",
                    "sentence": "We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates.",
                    "tag": "2+3"
                },
                {
                    "index": "519-4",
                    "sentence": "One key challenge of our task is the lack of parallel corpora.",
                    "tag": "1"
                },
                {
                    "index": "519-5",
                    "sentence": "To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models.",
                    "tag": "2+3"
                },
                {
                    "index": "519-6",
                    "sentence": "Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks.",
                    "tag": "4"
                },
                {
                    "index": "519-7",
                    "sentence": "Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-520",
            "text": [
                {
                    "index": "520-0",
                    "sentence": "Previous neural coherence models have focused on identifying semantic relations between adjacent sentences.",
                    "tag": "1"
                },
                {
                    "index": "520-1",
                    "sentence": "However, they do not have the means to exploit structural information.",
                    "tag": "1"
                },
                {
                    "index": "520-2",
                    "sentence": "In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations.",
                    "tag": "2"
                },
                {
                    "index": "520-3",
                    "sentence": "We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments.",
                    "tag": "3"
                },
                {
                    "index": "520-4",
                    "sentence": "Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus.",
                    "tag": "3"
                },
                {
                    "index": "520-5",
                    "sentence": "The model then incorporates this structural information into a structure-aware transformer.",
                    "tag": "3"
                },
                {
                    "index": "520-6",
                    "sentence": "We evaluate our model on two tasks, automated essay scoring and assessing writing quality.",
                    "tag": "3"
                },
                {
                    "index": "520-7",
                    "sentence": "Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks.",
                    "tag": "4"
                },
                {
                    "index": "520-8",
                    "sentence": "We next statistically examine the identified trees of texts assigned to different quality scores.",
                    "tag": "3"
                },
                {
                    "index": "520-9",
                    "sentence": "Finally, we investigate what our model learns in terms of theoretical claims.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-521",
            "text": [
                {
                    "index": "521-0",
                    "sentence": "The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others.",
                    "tag": "1"
                },
                {
                    "index": "521-1",
                    "sentence": "Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction.",
                    "tag": "1"
                },
                {
                    "index": "521-2",
                    "sentence": "Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models.",
                    "tag": "2+3"
                },
                {
                    "index": "521-3",
                    "sentence": "The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations.",
                    "tag": "1"
                },
                {
                    "index": "521-4",
                    "sentence": "Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success).",
                    "tag": "2+3"
                },
                {
                    "index": "521-5",
                    "sentence": "Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-522",
            "text": [
                {
                    "index": "522-0",
                    "sentence": "We present our HABERTOR model for detecting hatespeech in large scale user-generated content.",
                    "tag": "2"
                },
                {
                    "index": "522-1",
                    "sentence": "Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task.",
                    "tag": "2"
                },
                {
                    "index": "522-2",
                    "sentence": "HABERTOR inherits BERT’s architecture, but is different in four aspects: ",
                    "tag": "1"
                },
                {
                    "index": "522-3",
                    "sentence": "(i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; ",
                    "tag": "3"
                },
                {
                    "index": "522-4",
                    "sentence": "(ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; ",
                    "tag": "3"
                },
                {
                    "index": "522-5",
                    "sentence": "(iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness;",
                    "tag": "3"
                },
                {
                    "index": "522-6",
                    "sentence": "and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness.",
                    "tag": "3"
                },
                {
                    "index": "522-7",
                    "sentence": "Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models.",
                    "tag": "4"
                },
                {
                    "index": "522-8",
                    "sentence": "In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words.",
                    "tag": "4"
                },
                {
                    "index": "522-9",
                    "sentence": "Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-523",
            "text": [
                {
                    "index": "523-0",
                    "sentence": "Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges.",
                    "tag": "1"
                },
                {
                    "index": "523-1",
                    "sentence": "First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets.",
                    "tag": "1"
                },
                {
                    "index": "523-2",
                    "sentence": "Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity.",
                    "tag": "1"
                },
                {
                    "index": "523-3",
                    "sentence": "Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization.",
                    "tag": "1"
                },
                {
                    "index": "523-4",
                    "sentence": "Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "523-5",
                    "sentence": "Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains.",
                    "tag": "2+3"
                },
                {
                    "index": "523-6",
                    "sentence": "We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs.",
                    "tag": "2"
                },
                {
                    "index": "523-7",
                    "sentence": "Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN.",
                    "tag": "3"
                },
                {
                    "index": "523-8",
                    "sentence": "Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-524",
            "text": [
                {
                    "index": "524-0",
                    "sentence": "We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision.",
                    "tag": "2"
                },
                {
                    "index": "524-1",
                    "sentence": "To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales.",
                    "tag": "2+3"
                },
                {
                    "index": "524-2",
                    "sentence": "We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news.",
                    "tag": "3+4"
                },
                {
                    "index": "524-3",
                    "sentence": "We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus.",
                    "tag": "4"
                },
                {
                    "index": "524-4",
                    "sentence": "Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge.",
                    "tag": "3+4"
                },
                {
                    "index": "524-5",
                    "sentence": "Data and code for this new task are publicly available at https://github.com/allenai/scifact.",
                    "tag": "6"
                },
                {
                    "index": "524-6",
                    "sentence": "A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-525",
            "text": [
                {
                    "index": "525-0",
                    "sentence": "We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing.",
                    "tag": "2"
                },
                {
                    "index": "525-1",
                    "sentence": "Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data.",
                    "tag": "1"
                },
                {
                    "index": "525-2",
                    "sentence": "Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format.",
                    "tag": "2+3"
                },
                {
                    "index": "525-3",
                    "sentence": "This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art.",
                    "tag": "4"
                },
                {
                    "index": "525-4",
                    "sentence": "Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-526",
            "text": [
                {
                    "index": "526-0",
                    "sentence": "When does a sequence of events define an everyday scenario and how can this knowledge be induced from text?",
                    "tag": "1"
                },
                {
                    "index": "526-1",
                    "sentence": "Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus.",
                    "tag": "1"
                },
                {
                    "index": "526-2",
                    "sentence": "We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions.",
                    "tag": "2+3"
                },
                {
                    "index": "526-3",
                    "sentence": "Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-527",
            "text": [
                {
                    "index": "527-0",
                    "sentence": "NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task.",
                    "tag": "1"
                },
                {
                    "index": "527-1",
                    "sentence": "Recently proposed debiasing methods are shown to be effective in mitigating this tendency.",
                    "tag": "1"
                },
                {
                    "index": "527-2",
                    "sentence": "However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.",
                    "tag": "1"
                },
                {
                    "index": "527-3",
                    "sentence": "In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance.",
                    "tag": "1+2"
                },
                {
                    "index": "527-4",
                    "sentence": "The proposed framework is general and complementary to the existing debiasing methods.",
                    "tag": "3"
                },
                {
                    "index": "527-5",
                    "sentence": "We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases.",
                    "tag": "3+4"
                },
                {
                    "index": "527-6",
                    "sentence": "Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-528",
            "text": [
                {
                    "index": "528-0",
                    "sentence": "The scarcity of large parallel corpora is an important obstacle for neural machine translation.",
                    "tag": "1"
                },
                {
                    "index": "528-1",
                    "sentence": "A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data.",
                    "tag": "1"
                },
                {
                    "index": "528-2",
                    "sentence": "In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM).",
                    "tag": "2"
                },
                {
                    "index": "528-3",
                    "sentence": "Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM.",
                    "tag": "3"
                },
                {
                    "index": "528-4",
                    "sentence": "This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language.",
                    "tag": "2"
                },
                {
                    "index": "528-5",
                    "sentence": "The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference.",
                    "tag": "3"
                },
                {
                    "index": "528-6",
                    "sentence": "We present an analysis of the effects that different methods have on the distributions of the TM.",
                    "tag": "2+3"
                },
                {
                    "index": "528-7",
                    "sentence": "Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-529",
            "text": [
                {
                    "index": "529-0",
                    "sentence": "Word sense disambiguation is a well-known source of translation errors in NMT.",
                    "tag": "1"
                },
                {
                    "index": "529-1",
                    "sentence": "We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text.",
                    "tag": "1"
                },
                {
                    "index": "529-2",
                    "sentence": "We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types.",
                    "tag": "2+3"
                },
                {
                    "index": "529-3",
                    "sentence": "Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models.",
                    "tag": "2+3"
                },
                {
                    "index": "529-4",
                    "sentence": "Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-530",
            "text": [
                {
                    "index": "530-0",
                    "sentence": "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer.",
                    "tag": "1"
                },
                {
                    "index": "530-1",
                    "sentence": "However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training.",
                    "tag": "1"
                },
                {
                    "index": "530-2",
                    "sentence": "We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations.",
                    "tag": "2+3"
                },
                {
                    "index": "530-3",
                    "sentence": "In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language.",
                    "tag": "2"
                },
                {
                    "index": "530-4",
                    "sentence": "MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering.",
                    "tag": "3+4"
                },
                {
                    "index": "530-5",
                    "sentence": "Our code and adapters are available at AdapterHub.ml.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-531",
            "text": [
                {
                    "index": "531-0",
                    "sentence": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique.",
                    "tag": "1"
                },
                {
                    "index": "531-1",
                    "sentence": "In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models.",
                    "tag": "2"
                },
                {
                    "index": "531-2",
                    "sentence": "For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to.",
                    "tag": "1"
                },
                {
                    "index": "531-3",
                    "sentence": "We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon.",
                    "tag": "4"
                },
                {
                    "index": "531-4",
                    "sentence": "Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-532",
            "text": [
                {
                    "index": "532-0",
                    "sentence": "Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings.",
                    "tag": "1"
                },
                {
                    "index": "532-1",
                    "sentence": "Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention.",
                    "tag": "1"
                },
                {
                    "index": "532-2",
                    "sentence": "Suicide ideation is often linked to a history of mental depression.",
                    "tag": "1"
                },
                {
                    "index": "532-3",
                    "sentence": "The emotional spectrum of a user’s historical activity on social media can be indicative of their mental state over time.",
                    "tag": "1"
                },
                {
                    "index": "532-4",
                    "sentence": "In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context.",
                    "tag": "1"
                },
                {
                    "index": "532-5",
                    "sentence": "We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media.",
                    "tag": "2+3"
                },
                {
                    "index": "532-6",
                    "sentence": "STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment.",
                    "tag": "2"
                },
                {
                    "index": "532-7",
                    "sentence": "We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-533",
            "text": [
                {
                    "index": "533-0",
                    "sentence": "In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics.",
                    "tag": "2+3"
                },
                {
                    "index": "533-1",
                    "sentence": "We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way.",
                    "tag": "1+2"
                },
                {
                    "index": "533-2",
                    "sentence": "We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion.",
                    "tag": "2+3"
                },
                {
                    "index": "533-3",
                    "sentence": "We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-534",
            "text": [
                {
                    "index": "534-0",
                    "sentence": "Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media.",
                    "tag": "1"
                },
                {
                    "index": "534-1",
                    "sentence": "These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation.",
                    "tag": "1"
                },
                {
                    "index": "534-2",
                    "sentence": "How can we use fact-checked information to improve users’ consciousness of fake news to which they are exposed?",
                    "tag": "1"
                },
                {
                    "index": "534-3",
                    "sentence": "How can we stop users from spreading fake news?",
                    "tag": "1"
                },
                {
                    "index": "534-4",
                    "sentence": "To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users.",
                    "tag": "1+2"
                },
                {
                    "index": "534-5",
                    "sentence": "The search can directly warn fake news posters and online users (e.g. the posters’ followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media.",
                    "tag": "5"
                },
                {
                    "index": "534-6",
                    "sentence": "Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets.",
                    "tag": "2"
                },
                {
                    "index": "534-7",
                    "sentence": "Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-535",
            "text": [
                {
                    "index": "535-0",
                    "sentence": "Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.",
                    "tag": "1"
                },
                {
                    "index": "535-1",
                    "sentence": "Building a large annotated dataset for such veiled toxicity can be very expensive.",
                    "tag": "1"
                },
                {
                    "index": "535-2",
                    "sentence": "In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.",
                    "tag": "2+3"
                },
                {
                    "index": "535-3",
                    "sentence": "Just a handful of probing examples are used to surface orders of magnitude more disguised offenses.",
                    "tag": "3"
                },
                {
                    "index": "535-4",
                    "sentence": "We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-536",
            "text": [
                {
                    "index": "536-0",
                    "sentence": "Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks.",
                    "tag": "1"
                },
                {
                    "index": "536-1",
                    "sentence": "However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements.",
                    "tag": "1"
                },
                {
                    "index": "536-2",
                    "sentence": "Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly.",
                    "tag": "1"
                },
                {
                    "index": "536-3",
                    "sentence": "While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards.",
                    "tag": "1"
                },
                {
                    "index": "536-4",
                    "sentence": "Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time.",
                    "tag": "1"
                },
                {
                    "index": "536-5",
                    "sentence": "Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains.",
                    "tag": "2+3"
                },
                {
                    "index": "536-6",
                    "sentence": "We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit).",
                    "tag": "3"
                },
                {
                    "index": "536-7",
                    "sentence": "We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation.",
                    "tag": "4"
                },
                {
                    "index": "536-8",
                    "sentence": "Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-537",
            "text": [
                {
                    "index": "537-0",
                    "sentence": "Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines.",
                    "tag": "1"
                },
                {
                    "index": "537-1",
                    "sentence": "The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles.",
                    "tag": "1"
                },
                {
                    "index": "537-2",
                    "sentence": "Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue.",
                    "tag": "1"
                },
                {
                    "index": "537-3",
                    "sentence": "In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task.",
                    "tag": "1+2"
                },
                {
                    "index": "537-4",
                    "sentence": "We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve).",
                    "tag": "4+5"
                },
                {
                    "index": "537-5",
                    "sentence": "Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-538",
            "text": [
                {
                    "index": "538-0",
                    "sentence": "Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim.",
                    "tag": "1"
                },
                {
                    "index": "538-1",
                    "sentence": "Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification.",
                    "tag": "1"
                },
                {
                    "index": "538-2",
                    "sentence": "Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy.",
                    "tag": "2+3"
                },
                {
                    "index": "538-3",
                    "sentence": "Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification.",
                    "tag": "4"
                },
                {
                    "index": "538-4",
                    "sentence": "Our source code is available at https://github.com/ShyamSubramanian/HESM.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-539",
            "text": [
                {
                    "index": "539-0",
                    "sentence": "Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding.",
                    "tag": "1"
                },
                {
                    "index": "539-1",
                    "sentence": "In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models.",
                    "tag": "2"
                },
                {
                    "index": "539-2",
                    "sentence": "Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables.",
                    "tag": "3"
                },
                {
                    "index": "539-3",
                    "sentence": "Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision.",
                    "tag": "3"
                },
                {
                    "index": "539-4",
                    "sentence": "To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results.",
                    "tag": "3+4"
                },
                {
                    "index": "539-5",
                    "sentence": "Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-540",
            "text": [
                {
                    "index": "540-0",
                    "sentence": "Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER.",
                    "tag": "1"
                },
                {
                    "index": "540-1",
                    "sentence": "Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process.",
                    "tag": "1"
                },
                {
                    "index": "540-2",
                    "sentence": "In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence.",
                    "tag": "2"
                },
                {
                    "index": "540-3",
                    "sentence": "We present an extensive evaluation of state-of-the-art verification models under these constraints.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-541",
            "text": [
                {
                    "index": "541-0",
                    "sentence": "We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base.",
                    "tag": "2+3"
                },
                {
                    "index": "541-1",
                    "sentence": "We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities.",
                    "tag": "2+3"
                },
                {
                    "index": "541-2",
                    "sentence": "The model outperforms state-of-the-art results from a far more limited cross-lingual linking task.",
                    "tag": "4"
                },
                {
                    "index": "541-3",
                    "sentence": "Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation.",
                    "tag": "4"
                },
                {
                    "index": "541-4",
                    "sentence": "To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-542",
            "text": [
                {
                    "index": "542-0",
                    "sentence": "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing.",
                    "tag": "2"
                },
                {
                    "index": "542-1",
                    "sentence": "Our approach first divides the original BERT into several modules and builds their compact substitutes.",
                    "tag": "3"
                },
                {
                    "index": "542-2",
                    "sentence": "Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules.",
                    "tag": "3"
                },
                {
                    "index": "542-3",
                    "sentence": "We progressively increase the probability of replacement through the training.",
                    "tag": "3"
                },
                {
                    "index": "542-4",
                    "sentence": "In this way, our approach brings a deeper level of interaction between the original and compact models.",
                    "tag": "4"
                },
                {
                    "index": "542-5",
                    "sentence": "Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function.",
                    "tag": "5"
                },
                {
                    "index": "542-6",
                    "sentence": "Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-543",
            "text": [
                {
                    "index": "543-0",
                    "sentence": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning.",
                    "tag": "1"
                },
                {
                    "index": "543-1",
                    "sentence": "But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance.",
                    "tag": "1"
                },
                {
                    "index": "543-2",
                    "sentence": "To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "543-3",
                    "sentence": "Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually.",
                    "tag": "3"
                },
                {
                    "index": "543-4",
                    "sentence": "Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark.",
                    "tag": "4"
                },
                {
                    "index": "543-5",
                    "sentence": "Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large.",
                    "tag": "4"
                },
                {
                    "index": "543-6",
                    "sentence": "Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-544",
            "text": [
                {
                    "index": "544-0",
                    "sentence": "Active learning strives to reduce annotation costs by choosing the most critical examples to label.",
                    "tag": "1"
                },
                {
                    "index": "544-1",
                    "sentence": "Typically, the active learning strategy is contingent on the classification model.",
                    "tag": "1"
                },
                {
                    "index": "544-2",
                    "sentence": "For instance, uncertainty sampling depends on poorly calibrated model confidence scores.",
                    "tag": "1"
                },
                {
                    "index": "544-3",
                    "sentence": "In the cold-start setting, active learning is impractical because of model instability and data scarcity.",
                    "tag": "1"
                },
                {
                    "index": "544-4",
                    "sentence": "Fortunately, modern NLP provides an additional source of information: pre-trained language models.",
                    "tag": "1"
                },
                {
                    "index": "544-5",
                    "sentence": "The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning.",
                    "tag": "1"
                },
                {
                    "index": "544-6",
                    "sentence": "Therefore, we treat the language modeling loss as a proxy for classification uncertainty.",
                    "tag": "2"
                },
                {
                    "index": "544-7",
                    "sentence": "With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification.",
                    "tag": "3"
                },
                {
                    "index": "544-8",
                    "sentence": "Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-545",
            "text": [
                {
                    "index": "545-0",
                    "sentence": "Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems.",
                    "tag": "1"
                },
                {
                    "index": "545-1",
                    "sentence": "However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time.",
                    "tag": "1"
                },
                {
                    "index": "545-2",
                    "sentence": "To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT.",
                    "tag": "2+3"
                },
                {
                    "index": "545-3",
                    "sentence": "This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations.",
                    "tag": "3"
                },
                {
                    "index": "545-4",
                    "sentence": "It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy.",
                    "tag": "4"
                },
                {
                    "index": "545-5",
                    "sentence": "We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four.",
                    "tag": "4+5"
                },
                {
                    "index": "545-6",
                    "sentence": "Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-546",
            "text": [
                {
                    "index": "546-0",
                    "sentence": "Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast.",
                    "tag": "1"
                },
                {
                    "index": "546-1",
                    "sentence": "In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech.",
                    "tag": "1"
                },
                {
                    "index": "546-2",
                    "sentence": "We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task.",
                    "tag": "2"
                },
                {
                    "index": "546-3",
                    "sentence": "Our model makes greater use of context by using full utterances as input and adding an LSTM layer.",
                    "tag": "3"
                },
                {
                    "index": "546-4",
                    "sentence": "We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result.",
                    "tag": "4+5"
                },
                {
                    "index": "546-5",
                    "sentence": "We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task.",
                    "tag": "4+5"
                },
                {
                    "index": "546-6",
                    "sentence": "Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-547",
            "text": [
                {
                    "index": "547-0",
                    "sentence": "Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting.",
                    "tag": "1"
                },
                {
                    "index": "547-1",
                    "sentence": "Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance.",
                    "tag": "1"
                },
                {
                    "index": "547-2",
                    "sentence": "However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk.",
                    "tag": "1"
                },
                {
                    "index": "547-3",
                    "sentence": "Additionally, most existing approaches ignore the correlations between stocks.",
                    "tag": "1"
                },
                {
                    "index": "547-4",
                    "sentence": "Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation.",
                    "tag": "2+3"
                },
                {
                    "index": "547-5",
                    "sentence": "Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-548",
            "text": [
                {
                    "index": "548-0",
                    "sentence": "Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data.",
                    "tag": "1"
                },
                {
                    "index": "548-1",
                    "sentence": "Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works.",
                    "tag": "1"
                },
                {
                    "index": "548-2",
                    "sentence": "In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-549",
            "text": [
                {
                    "index": "549-0",
                    "sentence": "We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).",
                    "tag": "2+3"
                },
                {
                    "index": "549-1",
                    "sentence": "For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases.",
                    "tag": "3"
                },
                {
                    "index": "549-2",
                    "sentence": "We evaluated this approach on standard benchmark datasets.",
                    "tag": "3"
                },
                {
                    "index": "549-3",
                    "sentence": "We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques.",
                    "tag": "4"
                },
                {
                    "index": "549-4",
                    "sentence": "Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models.",
                    "tag": "5"
                },
                {
                    "index": "549-5",
                    "sentence": "To our knowledge, this is one of the first works that use GANs for keyphrase generation.",
                    "tag": "5"
                },
                {
                    "index": "549-6",
                    "sentence": "We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-550",
            "text": [
                {
                    "index": "550-0",
                    "sentence": "Text classification is a fundamental problem in natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "550-1",
                    "sentence": "Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus.",
                    "tag": "1"
                },
                {
                    "index": "550-2",
                    "sentence": "However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph.",
                    "tag": "1"
                },
                {
                    "index": "550-3",
                    "sentence": "To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer).",
                    "tag": "2"
                },
                {
                    "index": "550-4",
                    "sentence": "Our model learns effective node representations by capturing structure and heterogeneity from the text graph.",
                    "tag": "3"
                },
                {
                    "index": "550-5",
                    "sentence": "We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus.",
                    "tag": "3"
                },
                {
                    "index": "550-6",
                    "sentence": "Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-551",
            "text": [
                {
                    "index": "551-0",
                    "sentence": "Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics.",
                    "tag": "1"
                },
                {
                    "index": "551-1",
                    "sentence": "We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts.",
                    "tag": "2"
                },
                {
                    "index": "551-2",
                    "sentence": "We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task.",
                    "tag": "3+4"
                },
                {
                    "index": "551-3",
                    "sentence": "Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents.",
                    "tag": "3+4"
                },
                {
                    "index": "551-4",
                    "sentence": "Finally, we reveal interesting historical trends in the chapter structure of novels.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-552",
            "text": [
                {
                    "index": "552-0",
                    "sentence": "Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics.",
                    "tag": "1"
                },
                {
                    "index": "552-1",
                    "sentence": "Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content.",
                    "tag": "1"
                },
                {
                    "index": "552-2",
                    "sentence": "This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines.",
                    "tag": "1"
                },
                {
                    "index": "552-3",
                    "sentence": "In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation.",
                    "tag": "2+3"
                },
                {
                    "index": "552-4",
                    "sentence": "We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-553",
            "text": [
                {
                    "index": "553-0",
                    "sentence": "In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task.",
                    "tag": "1"
                },
                {
                    "index": "553-1",
                    "sentence": "Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market.",
                    "tag": "1"
                },
                {
                    "index": "553-2",
                    "sentence": "Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks.",
                    "tag": "1"
                },
                {
                    "index": "553-3",
                    "sentence": "The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting.",
                    "tag": "1"
                },
                {
                    "index": "553-4",
                    "sentence": "We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion.",
                    "tag": "2+3"
                },
                {
                    "index": "553-5",
                    "sentence": "Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-554",
            "text": [
                {
                    "index": "554-0",
                    "sentence": "State of the art research for date-time entity extraction from text is task agnostic.",
                    "tag": "1"
                },
                {
                    "index": "554-1",
                    "sentence": "Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task.",
                    "tag": "1"
                },
                {
                    "index": "554-2",
                    "sentence": "Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time.",
                    "tag": "1"
                },
                {
                    "index": "554-3",
                    "sentence": "We showcase a novel model for extracting task-specific date-time entities along with their negation constraints.",
                    "tag": "2"
                },
                {
                    "index": "554-4",
                    "sentence": "We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant.",
                    "tag": "4"
                },
                {
                    "index": "554-5",
                    "sentence": "Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-555",
            "text": [
                {
                    "index": "555-0",
                    "sentence": "This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates.",
                    "tag": "1+2"
                },
                {
                    "index": "555-1",
                    "sentence": "A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC).",
                    "tag": "3"
                },
                {
                    "index": "555-2",
                    "sentence": "Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines.",
                    "tag": "3"
                },
                {
                    "index": "555-3",
                    "sentence": "As a result, a high Kappa score of 61% is achieved for inter-annotator agreement.",
                    "tag": "4"
                },
                {
                    "index": "555-4",
                    "sentence": "Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2).",
                    "tag": "3"
                },
                {
                    "index": "555-5",
                    "sentence": "Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2.",
                    "tag": "4"
                },
                {
                    "index": "555-6",
                    "sentence": "Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-556",
            "text": [
                {
                    "index": "556-0",
                    "sentence": "Semantic change detection concerns the task of identifying words whose meaning has changed over time.",
                    "tag": "1"
                },
                {
                    "index": "556-1",
                    "sentence": "Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time.",
                    "tag": "1"
                },
                {
                    "index": "556-2",
                    "sentence": "In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time.",
                    "tag": "2+3"
                },
                {
                    "index": "556-3",
                    "sentence": "Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection.",
                    "tag": "4"
                },
                {
                    "index": "556-4",
                    "sentence": "Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-557",
            "text": [
                {
                    "index": "557-0",
                    "sentence": "In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation.",
                    "tag": "2+3"
                },
                {
                    "index": "557-1",
                    "sentence": "Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations.",
                    "tag": "2"
                },
                {
                    "index": "557-2",
                    "sentence": "We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets.",
                    "tag": "3+4"
                },
                {
                    "index": "557-3",
                    "sentence": "We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks.",
                    "tag": "3"
                },
                {
                    "index": "557-4",
                    "sentence": "Our results indicate a significant improvement over the application of the dense word representations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-558",
            "text": [
                {
                    "index": "558-0",
                    "sentence": "Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models.",
                    "tag": "1"
                },
                {
                    "index": "558-1",
                    "sentence": "Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents.",
                    "tag": "1"
                },
                {
                    "index": "558-2",
                    "sentence": "We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document.",
                    "tag": "2"
                },
                {
                    "index": "558-3",
                    "sentence": "We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-559",
            "text": [
                {
                    "index": "559-0",
                    "sentence": "Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering.",
                    "tag": "1"
                },
                {
                    "index": "559-1",
                    "sentence": "It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019).",
                    "tag": "1"
                },
                {
                    "index": "559-2",
                    "sentence": "In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a).",
                    "tag": "2+3"
                },
                {
                    "index": "559-3",
                    "sentence": "We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short.",
                    "tag": "5"
                },
                {
                    "index": "559-4",
                    "sentence": "Source code and resources are made available at https://github.com/bnmin/tdp_ranking.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-560",
            "text": [
                {
                    "index": "560-0",
                    "sentence": "The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>.",
                    "tag": "2"
                },
                {
                    "index": "560-1",
                    "sentence": "For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven, composed, Ode to Joy>.",
                    "tag": "3"
                },
                {
                    "index": "560-2",
                    "sentence": "In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases).",
                    "tag": "3+4"
                },
                {
                    "index": "560-3",
                    "sentence": "Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-561",
            "text": [
                {
                    "index": "561-0",
                    "sentence": "Active learning is an important technique for low-resource sequence labeling tasks.",
                    "tag": "1"
                },
                {
                    "index": "561-1",
                    "sentence": "However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations.",
                    "tag": "1"
                },
                {
                    "index": "561-2",
                    "sentence": "We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling.",
                    "tag": "2+3"
                },
                {
                    "index": "561-3",
                    "sentence": "Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration.",
                    "tag": "3"
                },
                {
                    "index": "561-4",
                    "sentence": "The key difficulty is to generate plausible sequences along with token-level labels.",
                    "tag": "3"
                },
                {
                    "index": "561-5",
                    "sentence": "In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples.",
                    "tag": "3"
                },
                {
                    "index": "561-6",
                    "sentence": "Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not.",
                    "tag": "3"
                },
                {
                    "index": "561-7",
                    "sentence": "Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores.",
                    "tag": "4"
                },
                {
                    "index": "561-8",
                    "sentence": "The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-562",
            "text": [
                {
                    "index": "562-0",
                    "sentence": "In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs).",
                    "tag": "2"
                },
                {
                    "index": "562-1",
                    "sentence": "We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers.",
                    "tag": "3+4"
                },
                {
                    "index": "562-2",
                    "sentence": "We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models.",
                    "tag": "2+3"
                },
                {
                    "index": "562-3",
                    "sentence": "Extensive experimental results show that the proposed method compares very favorably to the existing baselines.",
                    "tag": "4"
                },
                {
                    "index": "562-4",
                    "sentence": "This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-563",
            "text": [
                {
                    "index": "563-0",
                    "sentence": "Data-to-text generation has recently attracted substantial interests due to its wide applications.",
                    "tag": "1"
                },
                {
                    "index": "563-1",
                    "sentence": "Existing methods have shown impressive performance on an array of tasks.",
                    "tag": "1"
                },
                {
                    "index": "563-2",
                    "sentence": "However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains.",
                    "tag": "1"
                },
                {
                    "index": "563-3",
                    "sentence": "In this paper, we propose to leverage pre-training and transfer learning to address this issue.",
                    "tag": "2+3"
                },
                {
                    "index": "563-4",
                    "sentence": "We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text.",
                    "tag": "2+3"
                },
                {
                    "index": "563-5",
                    "sentence": "2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web.",
                    "tag": "3"
                },
                {
                    "index": "563-6",
                    "sentence": "The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text.",
                    "tag": "3"
                },
                {
                    "index": "563-7",
                    "sentence": "We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.",
                    "tag": "3"
                },
                {
                    "index": "563-8",
                    "sentence": "Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines.",
                    "tag": "4"
                },
                {
                    "index": "563-9",
                    "sentence": "Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.",
                    "tag": "4"
                },
                {
                    "index": "563-10",
                    "sentence": "Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models.",
                    "tag": "1"
                },
                {
                    "index": "563-11",
                    "sentence": "These experiments consistently prove the strong generalization ability of our proposed framework.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-564",
            "text": [
                {
                    "index": "564-0",
                    "sentence": "Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation.",
                    "tag": "1"
                },
                {
                    "index": "564-1",
                    "sentence": "However, these models cannot be directly employed to generate text under specified lexical constraints.",
                    "tag": "1"
                },
                {
                    "index": "564-2",
                    "sentence": "To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation.",
                    "tag": "1+2"
                },
                {
                    "index": "564-3",
                    "sentence": "The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner.",
                    "tag": "3"
                },
                {
                    "index": "564-4",
                    "sentence": "This procedure is recursively applied until a sequence is completed.",
                    "tag": "3"
                },
                {
                    "index": "564-5",
                    "sentence": "The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable.",
                    "tag": "4"
                },
                {
                    "index": "564-6",
                    "sentence": "We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks.",
                    "tag": "3"
                },
                {
                    "index": "564-7",
                    "sentence": "Non-autoregressive decoding yields a logarithmic time complexity during inference time.",
                    "tag": "3"
                },
                {
                    "index": "564-8",
                    "sentence": "Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation.",
                    "tag": "4"
                },
                {
                    "index": "564-9",
                    "sentence": "We released the pre-trained models and the source code to facilitate future research.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-565",
            "text": [
                {
                    "index": "565-0",
                    "sentence": "Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation.",
                    "tag": "1"
                },
                {
                    "index": "565-1",
                    "sentence": "Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens.",
                    "tag": "1"
                },
                {
                    "index": "565-2",
                    "sentence": "The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context.",
                    "tag": "1"
                },
                {
                    "index": "565-3",
                    "sentence": "This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context.",
                    "tag": "1+2"
                },
                {
                    "index": "565-4",
                    "sentence": "The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text.",
                    "tag": "4"
                },
                {
                    "index": "565-5",
                    "sentence": "An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-566",
            "text": [
                {
                    "index": "566-0",
                    "sentence": "Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications.",
                    "tag": "1"
                },
                {
                    "index": "566-1",
                    "sentence": "Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation.",
                    "tag": "1"
                },
                {
                    "index": "566-2",
                    "sentence": "However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution.",
                    "tag": "1"
                },
                {
                    "index": "566-3",
                    "sentence": "At the same time, stochastic search methods always cost too many steps to find the correct optimization direction.",
                    "tag": "1"
                },
                {
                    "index": "566-4",
                    "sentence": "In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem.",
                    "tag": "1+2"
                },
                {
                    "index": "566-5",
                    "sentence": "We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word).",
                    "tag": "2+3"
                },
                {
                    "index": "566-6",
                    "sentence": "The word updating process of the inserted/replaced word also benefits from the guidance of gradient.",
                    "tag": "3"
                },
                {
                    "index": "566-7",
                    "sentence": "Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model.",
                    "tag": "3"
                },
                {
                    "index": "566-8",
                    "sentence": "We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation.",
                    "tag": "3"
                },
                {
                    "index": "566-9",
                    "sentence": "The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-567",
            "text": [
                {
                    "index": "567-0",
                    "sentence": "Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps.",
                    "tag": "1"
                },
                {
                    "index": "567-1",
                    "sentence": "Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps.",
                    "tag": "2+3"
                },
                {
                    "index": "567-2",
                    "sentence": "TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup.",
                    "tag": "3"
                },
                {
                    "index": "567-3",
                    "sentence": "Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-568",
            "text": [
                {
                    "index": "568-0",
                    "sentence": "Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces.",
                    "tag": "1"
                },
                {
                    "index": "568-1",
                    "sentence": "In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state.",
                    "tag": "2"
                },
                {
                    "index": "568-2",
                    "sentence": "Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history.",
                    "tag": "2"
                },
                {
                    "index": "568-3",
                    "sentence": "We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards.",
                    "tag": "3"
                },
                {
                    "index": "568-4",
                    "sentence": "We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training.",
                    "tag": "3"
                },
                {
                    "index": "568-5",
                    "sentence": "Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model.",
                    "tag": "4"
                },
                {
                    "index": "568-6",
                    "sentence": "Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions.",
                    "tag": "4+5"
                },
                {
                    "index": "568-7",
                    "sentence": "Code and data are available at https://github.com/princeton-nlp/calm-textgame.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-569",
            "text": [
                {
                    "index": "569-0",
                    "sentence": "Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding.",
                    "tag": "1"
                },
                {
                    "index": "569-1",
                    "sentence": "Recent work has also successfully adapted such models towards the generative task of image captioning.",
                    "tag": "1"
                },
                {
                    "index": "569-2",
                    "sentence": "This begs the question: Can these models go the other way and generate images from pieces of text?",
                    "tag": "1"
                },
                {
                    "index": "569-3",
                    "sentence": "Our analysis of a popular representative from this model family – LXMERT – finds that it is unable to generate rich and semantically meaningful imagery with its current training setup.",
                    "tag": "1"
                },
                {
                    "index": "569-4",
                    "sentence": "We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint.",
                    "tag": "2"
                },
                {
                    "index": "569-5",
                    "sentence": "X-LXMERT’s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT.",
                    "tag": "4"
                },
                {
                    "index": "569-6",
                    "sentence": "Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-570",
            "text": [
                {
                    "index": "570-0",
                    "sentence": "In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering.",
                    "tag": "2"
                },
                {
                    "index": "570-1",
                    "sentence": "To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders.",
                    "tag": "3"
                },
                {
                    "index": "570-2",
                    "sentence": "Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction).",
                    "tag": "3"
                },
                {
                    "index": "570-3",
                    "sentence": "By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously.",
                    "tag": "4"
                },
                {
                    "index": "570-4",
                    "sentence": "Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-571",
            "text": [
                {
                    "index": "571-0",
                    "sentence": "Has there been real progress in multi-hop question-answering?",
                    "tag": "1"
                },
                {
                    "index": "571-1",
                    "sentence": "Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts.",
                    "tag": "1"
                },
                {
                    "index": "571-2",
                    "sentence": "This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets.",
                    "tag": "1"
                },
                {
                    "index": "571-3",
                    "sentence": "We make three contributions towards addressing this.",
                    "tag": "1"
                },
                {
                    "index": "571-4",
                    "sentence": "First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts.",
                    "tag": "2"
                },
                {
                    "index": "571-5",
                    "sentence": "This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning.",
                    "tag": "4"
                },
                {
                    "index": "571-6",
                    "sentence": "Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning.",
                    "tag": "3"
                },
                {
                    "index": "571-7",
                    "sentence": "Third, our experiments suggest that there hasn’t been much progress in multi-hop QA in the reading comprehension setting.",
                    "tag": "3"
                },
                {
                    "index": "571-8",
                    "sentence": "For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline.",
                    "tag": "3"
                },
                {
                    "index": "571-9",
                    "sentence": "Our transformation substantially reduces disconnected reasoning (19 points in answer F1).",
                    "tag": "4"
                },
                {
                    "index": "571-10",
                    "sentence": "It is complementary to adversarial approaches, yielding further reductions in conjunction.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-572",
            "text": [
                {
                    "index": "572-0",
                    "sentence": "We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering.",
                    "tag": "1"
                },
                {
                    "index": "572-1",
                    "sentence": "Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet.",
                    "tag": "1+2"
                },
                {
                    "index": "572-2",
                    "sentence": "Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions.",
                    "tag": "2+3"
                },
                {
                    "index": "572-3",
                    "sentence": "We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer.",
                    "tag": "3"
                },
                {
                    "index": "572-4",
                    "sentence": "We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets.",
                    "tag": "4"
                },
                {
                    "index": "572-5",
                    "sentence": "ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency.",
                    "tag": "4"
                },
                {
                    "index": "572-6",
                    "sentence": "Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-573",
            "text": [
                {
                    "index": "573-0",
                    "sentence": "This work deals with the challenge of learning and reasoning over multi-hop question answering (QA).",
                    "tag": "1"
                },
                {
                    "index": "573-1",
                    "sentence": "We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly.",
                    "tag": "2+3"
                },
                {
                    "index": "573-2",
                    "sentence": "The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges.",
                    "tag": "3"
                },
                {
                    "index": "573-3",
                    "sentence": "Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths.",
                    "tag": "3"
                },
                {
                    "index": "573-4",
                    "sentence": "Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-574",
            "text": [
                {
                    "index": "574-0",
                    "sentence": "Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content.",
                    "tag": "1"
                },
                {
                    "index": "574-1",
                    "sentence": "This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues.",
                    "tag": "2"
                },
                {
                    "index": "574-2",
                    "sentence": "The generated representations are further ensembled and classified using a neural-based early fusion approach.",
                    "tag": "3"
                },
                {
                    "index": "574-3",
                    "sentence": "Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network.",
                    "tag": "2"
                },
                {
                    "index": "574-4",
                    "sentence": "From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts.",
                    "tag": "4"
                },
                {
                    "index": "574-5",
                    "sentence": "Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-575",
            "text": [
                {
                    "index": "575-0",
                    "sentence": "Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence.",
                    "tag": "1"
                },
                {
                    "index": "575-1",
                    "sentence": "Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research.",
                    "tag": "1"
                },
                {
                    "index": "575-2",
                    "sentence": "In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words.",
                    "tag": "2"
                },
                {
                    "index": "575-3",
                    "sentence": "We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE.",
                    "tag": "2"
                },
                {
                    "index": "575-4",
                    "sentence": "The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-576",
            "text": [
                {
                    "index": "576-0",
                    "sentence": "Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly.",
                    "tag": "1"
                },
                {
                    "index": "576-1",
                    "sentence": "We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content.",
                    "tag": "1"
                },
                {
                    "index": "576-2",
                    "sentence": "We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art.",
                    "tag": "1"
                },
                {
                    "index": "576-3",
                    "sentence": "Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work.",
                    "tag": "2"
                },
                {
                    "index": "576-4",
                    "sentence": "We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations.",
                    "tag": "3"
                },
                {
                    "index": "576-5",
                    "sentence": "The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-577",
            "text": [
                {
                    "index": "577-0",
                    "sentence": "In this work, we aim at equipping pre-trained language models with structured knowledge.",
                    "tag": "2"
                },
                {
                    "index": "577-1",
                    "sentence": "We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.",
                    "tag": "3"
                },
                {
                    "index": "577-2",
                    "sentence": "Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text.",
                    "tag": "3"
                },
                {
                    "index": "577-3",
                    "sentence": "This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions.",
                    "tag": "3"
                },
                {
                    "index": "577-4",
                    "sentence": "In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model.",
                    "tag": "3"
                },
                {
                    "index": "577-5",
                    "sentence": "In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text.",
                    "tag": "3"
                },
                {
                    "index": "577-6",
                    "sentence": "It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples.",
                    "tag": "4"
                },
                {
                    "index": "577-7",
                    "sentence": "Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-578",
            "text": [
                {
                    "index": "578-0",
                    "sentence": "Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.",
                    "tag": "1"
                },
                {
                    "index": "578-1",
                    "sentence": "However, existing systems require large amounts of human annotated training data.",
                    "tag": "1"
                },
                {
                    "index": "578-2",
                    "sentence": "Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources.",
                    "tag": "1"
                },
                {
                    "index": "578-3",
                    "sentence": "In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.",
                    "tag": "2"
                },
                {
                    "index": "578-4",
                    "sentence": "We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks.",
                    "tag": "3"
                },
                {
                    "index": "578-5",
                    "sentence": "Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-579",
            "text": [
                {
                    "index": "579-0",
                    "sentence": "Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications.",
                    "tag": "1"
                },
                {
                    "index": "579-1",
                    "sentence": "Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified.",
                    "tag": "1"
                },
                {
                    "index": "579-2",
                    "sentence": "In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents.",
                    "tag": "1+2"
                },
                {
                    "index": "579-3",
                    "sentence": "We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification.",
                    "tag": "3"
                },
                {
                    "index": "579-4",
                    "sentence": "Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training.",
                    "tag": "3"
                },
                {
                    "index": "579-5",
                    "sentence": "We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-580",
            "text": [
                {
                    "index": "580-0",
                    "sentence": "Advances on deep generative models have attracted significant research interest in neural topic modeling.",
                    "tag": "1"
                },
                {
                    "index": "580-1",
                    "sentence": "The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics.",
                    "tag": "1"
                },
                {
                    "index": "580-2",
                    "sentence": "It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels.",
                    "tag": "1"
                },
                {
                    "index": "580-3",
                    "sentence": "To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT.",
                    "tag": "2+3"
                },
                {
                    "index": "580-4",
                    "sentence": "ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.",
                    "tag": "3"
                },
                {
                    "index": "580-5",
                    "sentence": "Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other.",
                    "tag": "3"
                },
                {
                    "index": "580-6",
                    "sentence": "sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics.",
                    "tag": "4"
                },
                {
                    "index": "580-7",
                    "sentence": "The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification.",
                    "tag": "4"
                },
                {
                    "index": "580-8",
                    "sentence": "The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-581",
            "text": [
                {
                    "index": "581-0",
                    "sentence": "In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time.",
                    "tag": "2"
                },
                {
                    "index": "581-1",
                    "sentence": "It is common that the state of an event is dynamically evolving.",
                    "tag": "1"
                },
                {
                    "index": "581-2",
                    "sentence": "However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event’s life cycle.",
                    "tag": "1"
                },
                {
                    "index": "581-3",
                    "sentence": "Such coarse-grained methods failed to capture the event’s unique features in different states.",
                    "tag": "1"
                },
                {
                    "index": "581-4",
                    "sentence": "To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation.",
                    "tag": "2"
                },
                {
                    "index": "581-5",
                    "sentence": "Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events.",
                    "tag": "3"
                },
                {
                    "index": "581-6",
                    "sentence": "For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction.",
                    "tag": "3"
                },
                {
                    "index": "581-7",
                    "sentence": "This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection.",
                    "tag": "4"
                },
                {
                    "index": "581-8",
                    "sentence": "Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems.",
                    "tag": "4"
                },
                {
                    "index": "581-9",
                    "sentence": "We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-582",
            "text": [
                {
                    "index": "582-0",
                    "sentence": "Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding.",
                    "tag": "1"
                },
                {
                    "index": "582-1",
                    "sentence": "Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style.",
                    "tag": "1+2"
                },
                {
                    "index": "582-2",
                    "sentence": "We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized.",
                    "tag": "3+4"
                },
                {
                    "index": "582-3",
                    "sentence": "On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-583",
            "text": [
                {
                    "index": "583-0",
                    "sentence": "Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information.",
                    "tag": "1"
                },
                {
                    "index": "583-1",
                    "sentence": "These models tend to generate irrelevant and uninformative questions.",
                    "tag": "1"
                },
                {
                    "index": "583-2",
                    "sentence": "In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way.",
                    "tag": "2"
                },
                {
                    "index": "583-3",
                    "sentence": "We present a novel task of question generation given a query path in the knowledge graph constructed from the input text.",
                    "tag": "3"
                },
                {
                    "index": "583-4",
                    "sentence": "We divide the task into two steps, namely, query representation learning and query-based question generation.",
                    "tag": "3"
                },
                {
                    "index": "583-5",
                    "sentence": "We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation.",
                    "tag": "3"
                },
                {
                    "index": "583-6",
                    "sentence": "We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework.",
                    "tag": "3"
                },
                {
                    "index": "583-7",
                    "sentence": "We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex.",
                    "tag": "4"
                },
                {
                    "index": "583-8",
                    "sentence": "Human evaluation also proves that our model is able to generate relevant and informative questions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-584",
            "text": [
                {
                    "index": "584-0",
                    "sentence": "Recognizing the flow of time in a story is a crucial aspect of understanding it.",
                    "tag": "1"
                },
                {
                    "index": "584-1",
                    "sentence": "Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases.",
                    "tag": "2+3"
                },
                {
                    "index": "584-2",
                    "sentence": "To do so, we construct a data set of hourly time phrases from 52,183 fictional books.",
                    "tag": "3"
                },
                {
                    "index": "584-3",
                    "sentence": "We then construct a time-of-day classification model that achieves an average error of 2.27 hours.",
                    "tag": "3"
                },
                {
                    "index": "584-4",
                    "sentence": "Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day.",
                    "tag": "3+4"
                },
                {
                    "index": "584-5",
                    "sentence": "This approach improves upon baselines by over two hour.",
                    "tag": "4"
                },
                {
                    "index": "584-6",
                    "sentence": "Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past.",
                    "tag": "4"
                },
                {
                    "index": "584-7",
                    "sentence": "Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-585",
            "text": [
                {
                    "index": "585-0",
                    "sentence": "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts.",
                    "tag": "1"
                },
                {
                    "index": "585-1",
                    "sentence": "To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English.",
                    "tag": "1+2"
                },
                {
                    "index": "585-2",
                    "sentence": "The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures.",
                    "tag": "3"
                },
                {
                    "index": "585-3",
                    "sentence": "In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%).",
                    "tag": "4"
                },
                {
                    "index": "585-4",
                    "sentence": "These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-586",
            "text": [
                {
                    "index": "586-0",
                    "sentence": "Pre-trained contextual representations like BERT have achieved great success in natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "586-1",
                    "sentence": "However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences.",
                    "tag": "1"
                },
                {
                    "index": "586-2",
                    "sentence": "In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited.",
                    "tag": "2"
                },
                {
                    "index": "586-3",
                    "sentence": "We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically.",
                    "tag": "3"
                },
                {
                    "index": "586-4",
                    "sentence": "We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity.",
                    "tag": "4"
                },
                {
                    "index": "586-5",
                    "sentence": "To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective.",
                    "tag": "4"
                },
                {
                    "index": "586-6",
                    "sentence": "Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.",
                    "tag": "4"
                },
                {
                    "index": "586-7",
                    "sentence": "The code is available at https://github.com/bohanli/BERT-flow.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-587",
            "text": [
                {
                    "index": "587-0",
                    "sentence": "Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens.",
                    "tag": "1"
                },
                {
                    "index": "587-1",
                    "sentence": "During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias.",
                    "tag": "1"
                },
                {
                    "index": "587-2",
                    "sentence": "To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes.",
                    "tag": "1+2"
                },
                {
                    "index": "587-3",
                    "sentence": "We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation.",
                    "tag": "3"
                },
                {
                    "index": "587-4",
                    "sentence": "An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences.",
                    "tag": "2+3"
                },
                {
                    "index": "587-5",
                    "sentence": "The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-588",
            "text": [
                {
                    "index": "588-0",
                    "sentence": "Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references.",
                    "tag": "1"
                },
                {
                    "index": "588-1",
                    "sentence": "To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference.",
                    "tag": "2+3"
                },
                {
                    "index": "588-2",
                    "sentence": "Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories.",
                    "tag": "3"
                },
                {
                    "index": "588-3",
                    "sentence": "We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence.",
                    "tag": "2+3"
                },
                {
                    "index": "588-4",
                    "sentence": "Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-589",
            "text": [
                {
                    "index": "589-0",
                    "sentence": "Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive.",
                    "tag": "1"
                },
                {
                    "index": "589-1",
                    "sentence": "We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective.",
                    "tag": "1"
                },
                {
                    "index": "589-2",
                    "sentence": "As a simple yet effective remedy, we propose two novel methods, Fˆ2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution.",
                    "tag": "2+3"
                },
                {
                    "index": "589-3",
                    "sentence": "MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes.",
                    "tag": "3"
                },
                {
                    "index": "589-4",
                    "sentence": "Fˆ2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class.",
                    "tag": "3"
                },
                {
                    "index": "589-5",
                    "sentence": "Models learn more uniform probability distributions because they are confined to subsets of vocabularies.",
                    "tag": "4"
                },
                {
                    "index": "589-6",
                    "sentence": "Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-590",
            "text": [
                {
                    "index": "590-0",
                    "sentence": "The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability.",
                    "tag": "1"
                },
                {
                    "index": "590-1",
                    "sentence": "However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain.",
                    "tag": "1"
                },
                {
                    "index": "590-2",
                    "sentence": "Using partially-aligned data is an alternative way of solving the dataset scarcity problem.",
                    "tag": "1+2"
                },
                {
                    "index": "590-3",
                    "sentence": "This kind of data is much easier to obtain since it can be produced automatically.",
                    "tag": "1+2"
                },
                {
                    "index": "590-4",
                    "sentence": "However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure.",
                    "tag": "3"
                },
                {
                    "index": "590-5",
                    "sentence": "In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains.",
                    "tag": "3"
                },
                {
                    "index": "590-6",
                    "sentence": "To tackle this new task, we propose a novel distant supervision generation framework.",
                    "tag": "2"
                },
                {
                    "index": "590-7",
                    "sentence": "It firstly estimates the input data’s supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively.",
                    "tag": "3"
                },
                {
                    "index": "590-8",
                    "sentence": "We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata.",
                    "tag": "3+4"
                },
                {
                    "index": "590-9",
                    "sentence": "The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-591",
            "text": [
                {
                    "index": "591-0",
                    "sentence": "Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly.",
                    "tag": "1"
                },
                {
                    "index": "591-1",
                    "sentence": "For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break.",
                    "tag": "1"
                },
                {
                    "index": "591-2",
                    "sentence": "In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions.",
                    "tag": "2"
                },
                {
                    "index": "591-3",
                    "sentence": "Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response.",
                    "tag": "3"
                },
                {
                    "index": "591-4",
                    "sentence": "Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions.",
                    "tag": "3"
                },
                {
                    "index": "591-5",
                    "sentence": "Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-592",
            "text": [
                {
                    "index": "592-0",
                    "sentence": "Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems.",
                    "tag": "1"
                },
                {
                    "index": "592-1",
                    "sentence": "However, training belief trackers often requires expensive turn-level annotations of every user utterance.",
                    "tag": "1"
                },
                {
                    "index": "592-2",
                    "sentence": "In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning.",
                    "tag": "2"
                },
                {
                    "index": "592-3",
                    "sentence": "We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs.",
                    "tag": "2+3"
                },
                {
                    "index": "592-4",
                    "sentence": "Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework.",
                    "tag": "4"
                },
                {
                    "index": "592-5",
                    "sentence": "Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES.",
                    "tag": "4"
                },
                {
                    "index": "592-6",
                    "sentence": "In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales.",
                    "tag": "4"
                },
                {
                    "index": "592-7",
                    "sentence": "In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines.",
                    "tag": "4"
                },
                {
                    "index": "592-8",
                    "sentence": "Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-593",
            "text": [
                {
                    "index": "593-0",
                    "sentence": "Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems.",
                    "tag": "1"
                },
                {
                    "index": "593-1",
                    "sentence": "However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows.",
                    "tag": "1"
                },
                {
                    "index": "593-2",
                    "sentence": "Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics.",
                    "tag": "1+2"
                },
                {
                    "index": "593-3",
                    "sentence": "Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation.",
                    "tag": "2+3"
                },
                {
                    "index": "593-4",
                    "sentence": "Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence.",
                    "tag": "3"
                },
                {
                    "index": "593-5",
                    "sentence": "The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights.",
                    "tag": "3"
                },
                {
                    "index": "593-6",
                    "sentence": "Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments.",
                    "tag": "4"
                },
                {
                    "index": "593-7",
                    "sentence": "Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-594",
            "text": [
                {
                    "index": "594-0",
                    "sentence": "Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior.",
                    "tag": "1"
                },
                {
                    "index": "594-1",
                    "sentence": "An open question, however, is how human rationales fare with these automatic metrics.",
                    "tag": "1"
                },
                {
                    "index": "594-2",
                    "sentence": "Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics.",
                    "tag": "1"
                },
                {
                    "index": "594-3",
                    "sentence": "To unpack this finding, we propose improved metrics to account for model-dependent baseline performance.",
                    "tag": "1+2"
                },
                {
                    "index": "594-4",
                    "sentence": "We then propose two methods to further characterize rationale quality, one based on model retraining and one on using “fidelity curves” to reveal properties such as irrelevance and redundancy.",
                    "tag": "3"
                },
                {
                    "index": "594-5",
                    "sentence": "Our work leads to actionable suggestions for evaluating and characterizing rationales.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-595",
            "text": [
                {
                    "index": "595-0",
                    "sentence": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization.",
                    "tag": "2+3"
                },
                {
                    "index": "595-1",
                    "sentence": "We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary.",
                    "tag": "3"
                },
                {
                    "index": "595-2",
                    "sentence": "We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores.",
                    "tag": "3+4"
                },
                {
                    "index": "595-3",
                    "sentence": "We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two.",
                    "tag": "4"
                },
                {
                    "index": "595-4",
                    "sentence": "We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets.",
                    "tag": "4"
                },
                {
                    "index": "595-5",
                    "sentence": "We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts.",
                    "tag": "4"
                },
                {
                    "index": "595-6",
                    "sentence": "We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance.",
                    "tag": "4"
                },
                {
                    "index": "595-7",
                    "sentence": "We hope that these architectures and experiments may serve as strong points of comparison for future work.",
                    "tag": "5"
                },
                {
                    "index": "595-8",
                    "sentence": "Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-596",
            "text": [
                {
                    "index": "596-0",
                    "sentence": "Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE.",
                    "tag": "1"
                },
                {
                    "index": "596-1",
                    "sentence": "However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text.",
                    "tag": "1"
                },
                {
                    "index": "596-2",
                    "sentence": "To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection.",
                    "tag": "2+3"
                },
                {
                    "index": "596-3",
                    "sentence": "Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t.",
                    "tag": "3"
                },
                {
                    "index": "596-4",
                    "sentence": "the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models.",
                    "tag": "3"
                },
                {
                    "index": "596-5",
                    "sentence": "Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-597",
            "text": [
                {
                    "index": "597-0",
                    "sentence": "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents.",
                    "tag": "1"
                },
                {
                    "index": "597-1",
                    "sentence": "We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.",
                    "tag": "2"
                },
                {
                    "index": "597-2",
                    "sentence": "Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it.",
                    "tag": "3"
                },
                {
                    "index": "597-3",
                    "sentence": "Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking.",
                    "tag": "4"
                },
                {
                    "index": "597-4",
                    "sentence": "Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",
                    "tag": "5"
                },
                {
                    "index": "597-5",
                    "sentence": "We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-598",
            "text": [
                {
                    "index": "598-0",
                    "sentence": "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization.",
                    "tag": "1"
                },
                {
                    "index": "598-1",
                    "sentence": "However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers.",
                    "tag": "1"
                },
                {
                    "index": "598-2",
                    "sentence": "In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings.",
                    "tag": "2+3"
                },
                {
                    "index": "598-3",
                    "sentence": "We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.",
                    "tag": "4"
                },
                {
                    "index": "598-4",
                    "sentence": "We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "EMNLP_abs-599",
            "text": [
                {
                    "index": "599-0",
                    "sentence": "A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo.",
                    "tag": "1"
                },
                {
                    "index": "599-1",
                    "sentence": "In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively.",
                    "tag": "1"
                },
                {
                    "index": "599-2",
                    "sentence": "Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem.",
                    "tag": "1+2"
                },
                {
                    "index": "599-3",
                    "sentence": "The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article.",
                    "tag": "2"
                },
                {
                    "index": "599-4",
                    "sentence": "To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator.",
                    "tag": "3"
                },
                {
                    "index": "599-5",
                    "sentence": "In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level.",
                    "tag": "3"
                },
                {
                    "index": "599-6",
                    "sentence": "Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-1",
            "text": [
                {
                    "index": "1-0",
                    "sentence": "We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task.",
                    "tag": "2"
                },
                {
                    "index": "1-1",
                    "sentence": "This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019).",
                    "tag": "1"
                },
                {
                    "index": "1-2",
                    "sentence": "In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.",
                    "tag": "3"
                },
                {
                    "index": "1-3",
                    "sentence": "We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-2",
            "text": [
                {
                    "index": "2-0",
                    "sentence": "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition.",
                    "tag": "1"
                },
                {
                    "index": "2-1",
                    "sentence": "This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain.",
                    "tag": "1+2"
                },
                {
                    "index": "2-2",
                    "sentence": "We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset.",
                    "tag": "4"
                },
                {
                    "index": "2-3",
                    "sentence": "We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset.",
                    "tag": "4"
                },
                {
                    "index": "2-4",
                    "sentence": "We improve the zero-shot learning state of the art on average across domains by 21%.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-3",
            "text": [
                {
                    "index": "3-0",
                    "sentence": "This work proposes a standalone, complete Chinese discourse parser for practical applications.",
                    "tag": "1+2"
                },
                {
                    "index": "3-1",
                    "sentence": "We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies.",
                    "tag": "1+2"
                },
                {
                    "index": "3-2",
                    "sentence": "We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition.",
                    "tag": "3"
                },
                {
                    "index": "3-3",
                    "sentence": "Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-4",
            "text": [
                {
                    "index": "4-0",
                    "sentence": "Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues.",
                    "tag": "1"
                },
                {
                    "index": "4-1",
                    "sentence": "Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal.",
                    "tag": "1"
                },
                {
                    "index": "4-2",
                    "sentence": "Therefore, we propose a novel TransS-driven joint learning architecture to address the issues.",
                    "tag": "2"
                },
                {
                    "index": "4-3",
                    "sentence": "Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task.",
                    "tag": "3"
                },
                {
                    "index": "4-4",
                    "sentence": "Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-5",
            "text": [
                {
                    "index": "5-0",
                    "sentence": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy.",
                    "tag": "1"
                },
                {
                    "index": "5-1",
                    "sentence": "Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS).",
                    "tag": "1"
                },
                {
                    "index": "5-2",
                    "sentence": "With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others.",
                    "tag": "1"
                },
                {
                    "index": "5-3",
                    "sentence": "In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer:",
                    "tag": "2"
                },
                {
                    "index": "5-4",
                    "sentence": "(1) Why NAR models can catch up with AR models in some tasks but not all?",
                    "tag": "2"
                },
                {
                    "index": "5-5",
                    "sentence": "(2) Why techniques like knowledge distillation and source-target alignment can help NAR models.",
                    "tag": "2"
                },
                {
                    "index": "5-6",
                    "sentence": "Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens.",
                    "tag": "1"
                },
                {
                    "index": "5-7",
                    "sentence": "To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks.",
                    "tag": "2"
                },
                {
                    "index": "5-8",
                    "sentence": "We have several interesting findings:",
                    "tag": "4"
                },
                {
                    "index": "5-9",
                    "sentence": "1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least.",
                    "tag": "4"
                },
                {
                    "index": "5-10",
                    "sentence": "2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models.",
                    "tag": "4"
                },
                {
                    "index": "5-11",
                    "sentence": "3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-6",
            "text": [
                {
                    "index": "6-0",
                    "sentence": "Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations.",
                    "tag": "1"
                },
                {
                    "index": "6-1",
                    "sentence": "We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage.",
                    "tag": "2+3"
                },
                {
                    "index": "6-2",
                    "sentence": "We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption.",
                    "tag": "3"
                },
                {
                    "index": "6-3",
                    "sentence": "We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset.",
                    "tag": "4"
                },
                {
                    "index": "6-4",
                    "sentence": "Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-7",
            "text": [
                {
                    "index": "7-0",
                    "sentence": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data.",
                    "tag": "1"
                },
                {
                    "index": "7-1",
                    "sentence": "In this work, we propose the new task of few-shot natural language generation.",
                    "tag": "2"
                },
                {
                    "index": "7-2",
                    "sentence": "Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains.",
                    "tag": "2"
                },
                {
                    "index": "7-3",
                    "sentence": "The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge",
                    "tag": "3"
                },
                {
                    "index": "7-4",
                    "sentence": "With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement.",
                    "tag": "4+5"
                },
                {
                    "index": "7-5",
                    "sentence": "Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-8",
            "text": [
                {
                    "index": "8-0",
                    "sentence": "Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask.",
                    "tag": "1"
                },
                {
                    "index": "8-1",
                    "sentence": "One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents.",
                    "tag": "1"
                },
                {
                    "index": "8-2",
                    "sentence": "In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness.",
                    "tag": "2"
                },
                {
                    "index": "8-3",
                    "sentence": "From a technical perspective, we use data augmentation to generate training data for an end-to-end system.",
                    "tag": "3"
                },
                {
                    "index": "8-4",
                    "sentence": "Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019).",
                    "tag": "3"
                },
                {
                    "index": "8-5",
                    "sentence": "Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses.",
                    "tag": "5"
                },
                {
                    "index": "8-6",
                    "sentence": "We further show our model’s scalability by conducting tests on the CoQA dataset.",
                    "tag": "6"
                },
                {
                    "index": "8-7",
                    "sentence": "The code and data are available at https://github.com/abaheti95/QADialogSystem.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-9",
            "text": [
                {
                    "index": "9-0",
                    "sentence": "One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation.",
                    "tag": "1"
                },
                {
                    "index": "9-1",
                    "sentence": "An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia).",
                    "tag": "1"
                },
                {
                    "index": "9-2",
                    "sentence": "In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency.",
                    "tag": "2"
                },
                {
                    "index": "9-3",
                    "sentence": "We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models.",
                    "tag": "3"
                },
                {
                    "index": "9-4",
                    "sentence": "The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-10",
            "text": [
                {
                    "index": "10-0",
                    "sentence": "Paraphrasing natural language sentences is a multifaceted process:",
                    "tag": "1"
                },
                {
                    "index": "10-1",
                    "sentence": "it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization.",
                    "tag": "1"
                },
                {
                    "index": "10-2",
                    "sentence": "Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner.",
                    "tag": "1"
                },
                {
                    "index": "10-3",
                    "sentence": "Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model.",
                    "tag": "2"
                },
                {
                    "index": "10-4",
                    "sentence": "First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model.",
                    "tag": "3"
                },
                {
                    "index": "10-5",
                    "sentence": "This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks.",
                    "tag": "3"
                },
                {
                    "index": "10-6",
                    "sentence": "Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order.",
                    "tag": "3"
                },
                {
                    "index": "10-7",
                    "sentence": "Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-11",
            "text": [
                {
                    "index": "11-0",
                    "sentence": "Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents.",
                    "tag": "1"
                },
                {
                    "index": "11-1",
                    "sentence": "Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion.",
                    "tag": "1"
                },
                {
                    "index": "11-2",
                    "sentence": "When a new condition added, these techniques require full retraining.",
                    "tag": "1"
                },
                {
                    "index": "11-3",
                    "sentence": "In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation.",
                    "tag": "2"
                },
                {
                    "index": "11-4",
                    "sentence": "PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation.",
                    "tag": "3"
                },
                {
                    "index": "11-5",
                    "sentence": "When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications.",
                    "tag": "4"
                },
                {
                    "index": "11-6",
                    "sentence": "Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-12",
            "text": [
                {
                    "index": "12-0",
                    "sentence": "Masked language model and autoregressive language model are two types of language models.",
                    "tag": "1"
                },
                {
                    "index": "12-1",
                    "sentence": "While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG).",
                    "tag": "1"
                },
                {
                    "index": "12-2",
                    "sentence": "In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM).",
                    "tag": "2"
                },
                {
                    "index": "12-3",
                    "sentence": "We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM.",
                    "tag": "3"
                },
                {
                    "index": "12-4",
                    "sentence": "We prove that u-PMLM is equivalent to an autoregressive permutated language model.",
                    "tag": "4"
                },
                {
                    "index": "12-5",
                    "sentence": "One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation.",
                    "tag": "4"
                },
                {
                    "index": "12-6",
                    "sentence": "Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-13",
            "text": [
                {
                    "index": "13-0",
                    "sentence": "Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways.",
                    "tag": "1"
                },
                {
                    "index": "13-1",
                    "sentence": "It is therefore desirable to develop a deeper understanding of the fundamental properties of such models.",
                    "tag": "1"
                },
                {
                    "index": "13-2",
                    "sentence": "The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area.",
                    "tag": "1"
                },
                {
                    "index": "13-3",
                    "sentence": "To this end, the extent and degree to which these artifacts surface in generated text is still unclear.",
                    "tag": "1"
                },
                {
                    "index": "13-4",
                    "sentence": "In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text.",
                    "tag": "2"
                },
                {
                    "index": "13-5",
                    "sentence": "Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate.",
                    "tag": "3"
                },
                {
                    "index": "13-6",
                    "sentence": "Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone.",
                    "tag": "4"
                },
                {
                    "index": "13-7",
                    "sentence": "This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-14",
            "text": [
                {
                    "index": "14-0",
                    "sentence": "While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need.",
                    "tag": "1"
                },
                {
                    "index": "14-1",
                    "sentence": "We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences.",
                    "tag": "2+3"
                },
                {
                    "index": "14-2",
                    "sentence": "One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences.",
                    "tag": "3"
                },
                {
                    "index": "14-3",
                    "sentence": "To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation.",
                    "tag": "3"
                },
                {
                    "index": "14-4",
                    "sentence": "To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation.",
                    "tag": "3"
                },
                {
                    "index": "14-5",
                    "sentence": "Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-15",
            "text": [
                {
                    "index": "15-0",
                    "sentence": "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc.",
                    "tag": "1"
                },
                {
                    "index": "15-1",
                    "sentence": "However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information.",
                    "tag": "1"
                },
                {
                    "index": "15-2",
                    "sentence": "In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node.",
                    "tag": "2+3"
                },
                {
                    "index": "15-3",
                    "sentence": "Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code.",
                    "tag": "3"
                },
                {
                    "index": "15-4",
                    "sentence": "We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework.",
                    "tag": "3"
                },
                {
                    "index": "15-5",
                    "sentence": "Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-16",
            "text": [
                {
                    "index": "16-0",
                    "sentence": "We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing.",
                    "tag": "1+2"
                },
                {
                    "index": "16-1",
                    "sentence": "We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases.",
                    "tag": "2+3"
                },
                {
                    "index": "16-2",
                    "sentence": "UPSA searches the sentence space towards this objective by performing a sequence of local editing.",
                    "tag": "3"
                },
                {
                    "index": "16-3",
                    "sentence": "We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter.",
                    "tag": "3"
                },
                {
                    "index": "16-4",
                    "sentence": "Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations.",
                    "tag": "4"
                },
                {
                    "index": "16-5",
                    "sentence": "Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-17",
            "text": [
                {
                    "index": "17-0",
                    "sentence": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections.",
                    "tag": "1"
                },
                {
                    "index": "17-1",
                    "sentence": "Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly.",
                    "tag": "1"
                },
                {
                    "index": "17-2",
                    "sentence": "We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments.",
                    "tag": "2"
                },
                {
                    "index": "17-3",
                    "sentence": "In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments.",
                    "tag": "3"
                },
                {
                    "index": "17-4",
                    "sentence": "We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-18",
            "text": [
                {
                    "index": "18-0",
                    "sentence": "Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers.",
                    "tag": "1"
                },
                {
                    "index": "18-1",
                    "sentence": "Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked.",
                    "tag": "1"
                },
                {
                    "index": "18-2",
                    "sentence": "In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification.",
                    "tag": "2"
                },
                {
                    "index": "18-3",
                    "sentence": "Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus.",
                    "tag": "3"
                },
                {
                    "index": "18-4",
                    "sentence": "This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner.",
                    "tag": "3"
                },
                {
                    "index": "18-5",
                    "sentence": "This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized.",
                    "tag": "4"
                },
                {
                    "index": "18-6",
                    "sentence": "Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-19",
            "text": [
                {
                    "index": "19-0",
                    "sentence": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task.",
                    "tag": "1"
                },
                {
                    "index": "19-1",
                    "sentence": "However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words.",
                    "tag": "1"
                },
                {
                    "index": "19-2",
                    "sentence": "Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN.",
                    "tag": "2"
                },
                {
                    "index": "19-3",
                    "sentence": "We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document.",
                    "tag": "2+3"
                },
                {
                    "index": "19-4",
                    "sentence": "Finally, the word nodes are aggregated as the document embedding.",
                    "tag": "3"
                },
                {
                    "index": "19-5",
                    "sentence": "Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-20",
            "text": [
                {
                    "index": "20-0",
                    "sentence": "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA).",
                    "tag": "1"
                },
                {
                    "index": "20-1",
                    "sentence": "However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document.",
                    "tag": "1"
                },
                {
                    "index": "20-2",
                    "sentence": "To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling.",
                    "tag": "2+3"
                },
                {
                    "index": "20-3",
                    "sentence": "The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution.",
                    "tag": "2+3"
                },
                {
                    "index": "20-4",
                    "sentence": "It uses a generator to capture the semantic patterns from texts and an encoder for topic inference.",
                    "tag": "3"
                },
                {
                    "index": "20-5",
                    "sentence": "Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT.",
                    "tag": "3"
                },
                {
                    "index": "20-6",
                    "sentence": "To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments.",
                    "tag": "3"
                },
                {
                    "index": "20-7",
                    "sentence": "The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines.",
                    "tag": "4"
                },
                {
                    "index": "20-8",
                    "sentence": "Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-21",
            "text": [
                {
                    "index": "21-0",
                    "sentence": "Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks.",
                    "tag": "1"
                },
                {
                    "index": "21-1",
                    "sentence": "However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts.",
                    "tag": "1"
                },
                {
                    "index": "21-2",
                    "sentence": "To address this problem, we propose a simple multitask learning model that uses negative supervision.",
                    "tag": "2"
                },
                {
                    "index": "21-3",
                    "sentence": "Specifically, our model encourages texts with different labels to have distinct representations.",
                    "tag": "3"
                },
                {
                    "index": "21-4",
                    "sentence": "Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-22",
            "text": [
                {
                    "index": "22-0",
                    "sentence": "Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word.",
                    "tag": "1"
                },
                {
                    "index": "22-1",
                    "sentence": "However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words).",
                    "tag": "1"
                },
                {
                    "index": "22-2",
                    "sentence": "To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance.",
                    "tag": "2+3"
                },
                {
                    "index": "22-3",
                    "sentence": "Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-23",
            "text": [
                {
                    "index": "23-0",
                    "sentence": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks.",
                    "tag": "1"
                },
                {
                    "index": "23-1",
                    "sentence": "However, few works have adopted this technique in the sequence-to-sequence models.",
                    "tag": "1"
                },
                {
                    "index": "23-2",
                    "sentence": "In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT).",
                    "tag": "2"
                },
                {
                    "index": "23-3",
                    "sentence": "Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality.",
                    "tag": "1"
                },
                {
                    "index": "23-4",
                    "sentence": "Therefore, we propose to train the encoder more rigorously by masking the encoder input while training.",
                    "tag": "2+3"
                },
                {
                    "index": "23-5",
                    "sentence": "As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words.",
                    "tag": "3"
                },
                {
                    "index": "23-6",
                    "sentence": "The two types of masks are applied to the model jointly at the training stage.",
                    "tag": "3"
                },
                {
                    "index": "23-7",
                    "sentence": "We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-24",
            "text": [
                {
                    "index": "24-0",
                    "sentence": "The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT).",
                    "tag": "1"
                },
                {
                    "index": "24-1",
                    "sentence": "Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018).",
                    "tag": "1"
                },
                {
                    "index": "24-2",
                    "sentence": "Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships.",
                    "tag": "1+2"
                },
                {
                    "index": "24-3",
                    "sentence": "In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations.",
                    "tag": "2"
                },
                {
                    "index": "24-4",
                    "sentence": "In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships.",
                    "tag": "4"
                },
                {
                    "index": "24-5",
                    "sentence": "In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach.",
                    "tag": "4"
                },
                {
                    "index": "24-6",
                    "sentence": "Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps.",
                    "tag": "4+5"
                },
                {
                    "index": "24-7",
                    "sentence": "The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-25",
            "text": [
                {
                    "index": "25-0",
                    "sentence": "The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure.",
                    "tag": "1"
                },
                {
                    "index": "25-1",
                    "sentence": "Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge.",
                    "tag": "1"
                },
                {
                    "index": "25-2",
                    "sentence": "In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers.",
                    "tag": "2+3"
                },
                {
                    "index": "25-3",
                    "sentence": "We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.",
                    "tag": "2+3"
                },
                {
                    "index": "25-4",
                    "sentence": "In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers.",
                    "tag": "4"
                },
                {
                    "index": "25-5",
                    "sentence": "In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-26",
            "text": [
                {
                    "index": "26-0",
                    "sentence": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings.",
                    "tag": "1"
                },
                {
                    "index": "26-1",
                    "sentence": "As the NMT architectures become deeper and wider, this issue gets worse and worse.",
                    "tag": "1"
                },
                {
                    "index": "26-2",
                    "sentence": "In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method.",
                    "tag": "2"
                },
                {
                    "index": "26-3",
                    "sentence": "We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence.",
                    "tag": "3"
                },
                {
                    "index": "26-4",
                    "sentence": "The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties.",
                    "tag": "4"
                },
                {
                    "index": "26-5",
                    "sentence": "It is easy to determine and contains learning-dependent features.",
                    "tag": "4"
                },
                {
                    "index": "26-6",
                    "sentence": "The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT.",
                    "tag": "4+5"
                },
                {
                    "index": "26-7",
                    "sentence": "Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-27",
            "text": [
                {
                    "index": "27-0",
                    "sentence": "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently.",
                    "tag": "1"
                },
                {
                    "index": "27-1",
                    "sentence": "Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative.",
                    "tag": "1"
                },
                {
                    "index": "27-2",
                    "sentence": "We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information.",
                    "tag": "2+3"
                },
                {
                    "index": "27-3",
                    "sentence": "At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality.",
                    "tag": "3"
                },
                {
                    "index": "27-4",
                    "sentence": "Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-28",
            "text": [
                {
                    "index": "28-0",
                    "sentence": "We develop a formal hierarchy of the expressive capacity of RNN architectures.",
                    "tag": "2+3"
                },
                {
                    "index": "28-1",
                    "sentence": "The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine.",
                    "tag": "1"
                },
                {
                    "index": "28-2",
                    "sentence": "We place several RNN variants within this hierarchy.",
                    "tag": "3"
                },
                {
                    "index": "28-3",
                    "sentence": "For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016).",
                    "tag": "3"
                },
                {
                    "index": "28-4",
                    "sentence": "We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions.",
                    "tag": "3"
                },
                {
                    "index": "28-5",
                    "sentence": "Our results build on the theory of “saturated” RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy.",
                    "tag": "4+5"
                },
                {
                    "index": "28-6",
                    "sentence": "We provide empirical results to support this conjecture.",
                    "tag": "4"
                },
                {
                    "index": "28-7",
                    "sentence": "Experimental findings from training unsaturated networks on formal languages support this conjecture.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-29",
            "text": [
                {
                    "index": "29-0",
                    "sentence": "We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters.",
                    "tag": "1"
                },
                {
                    "index": "29-1",
                    "sentence": "The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f).",
                    "tag": "1"
                },
                {
                    "index": "29-2",
                    "sentence": "The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0.",
                    "tag": "1"
                },
                {
                    "index": "29-3",
                    "sentence": "We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-30",
            "text": [
                {
                    "index": "30-0",
                    "sentence": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training.",
                    "tag": "1"
                },
                {
                    "index": "30-1",
                    "sentence": "The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.",
                    "tag": "1"
                },
                {
                    "index": "30-2",
                    "sentence": "In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "30-3",
                    "sentence": "Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue.",
                    "tag": "3"
                },
                {
                    "index": "30-4",
                    "sentence": "To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.",
                    "tag": "3"
                },
                {
                    "index": "30-5",
                    "sentence": "Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.",
                    "tag": "4"
                },
                {
                    "index": "30-6",
                    "sentence": "With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks.",
                    "tag": "4"
                },
                {
                    "index": "30-7",
                    "sentence": "Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-31",
            "text": [
                {
                    "index": "31-0",
                    "sentence": "This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance.",
                    "tag": "1+2"
                },
                {
                    "index": "31-1",
                    "sentence": "Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information.",
                    "tag": "3"
                },
                {
                    "index": "31-2",
                    "sentence": "We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-32",
            "text": [
                {
                    "index": "32-0",
                    "sentence": "We examine a methodology using neural language models (LMs) for analyzing the word order of language.",
                    "tag": "1+2"
                },
                {
                    "index": "32-1",
                    "sentence": "This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods.",
                    "tag": "1"
                },
                {
                    "index": "32-2",
                    "sentence": "In this study, we explore whether the LM-based method is valid for analyzing the word order.",
                    "tag": "2"
                },
                {
                    "index": "32-3",
                    "sentence": "As a case study, this study focuses on Japanese due to its complex and flexible word order.",
                    "tag": "1+2"
                },
                {
                    "index": "32-4",
                    "sentence": "To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies.",
                    "tag": "3"
                },
                {
                    "index": "32-5",
                    "sentence": "Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool.",
                    "tag": "4"
                },
                {
                    "index": "32-6",
                    "sentence": "Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-33",
            "text": [
                {
                    "index": "33-0",
                    "sentence": "This paper solves the fake news detection problem under a more realistic scenario on social media.",
                    "tag": "1"
                },
                {
                    "index": "33-1",
                    "sentence": "Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern.",
                    "tag": "2"
                },
                {
                    "index": "33-2",
                    "sentence": "We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal.",
                    "tag": "2+3"
                },
                {
                    "index": "33-3",
                    "sentence": "Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average.",
                    "tag": "4+5"
                },
                {
                    "index": "33-4",
                    "sentence": "In addition, the case studies also show that GCAN can produce reasonable explanations.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-34",
            "text": [
                {
                    "index": "34-0",
                    "sentence": "Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views.",
                    "tag": "1"
                },
                {
                    "index": "34-1",
                    "sentence": "However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set.",
                    "tag": "1"
                },
                {
                    "index": "34-2",
                    "sentence": "To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection.",
                    "tag": "2+3"
                },
                {
                    "index": "34-3",
                    "sentence": "As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically.",
                    "tag": "2+3"
                },
                {
                    "index": "34-4",
                    "sentence": "Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods.",
                    "tag": "4"
                },
                {
                    "index": "34-5",
                    "sentence": "Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-35",
            "text": [
                {
                    "index": "35-0",
                    "sentence": "Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers.",
                    "tag": "1"
                },
                {
                    "index": "35-1",
                    "sentence": "Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly.",
                    "tag": "1"
                },
                {
                    "index": "35-2",
                    "sentence": "In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic.",
                    "tag": "2+3"
                },
                {
                    "index": "35-3",
                    "sentence": "We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-36",
            "text": [
                {
                    "index": "36-0",
                    "sentence": "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science.",
                    "tag": "1"
                },
                {
                    "index": "36-1",
                    "sentence": "This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large.",
                    "tag": "1"
                },
                {
                    "index": "36-2",
                    "sentence": "However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results.",
                    "tag": "1"
                },
                {
                    "index": "36-3",
                    "sentence": "We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word.",
                    "tag": "2+3"
                },
                {
                    "index": "36-4",
                    "sentence": "The method is simple, interpretable and stable.",
                    "tag": "6"
                },
                {
                    "index": "36-5",
                    "sentence": "We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-37",
            "text": [
                {
                    "index": "37-0",
                    "sentence": "Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging.",
                    "tag": "1"
                },
                {
                    "index": "37-1",
                    "sentence": "Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process.",
                    "tag": "1"
                },
                {
                    "index": "37-2",
                    "sentence": "However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified.",
                    "tag": "1+2"
                },
                {
                    "index": "37-3",
                    "sentence": "Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence.",
                    "tag": "1"
                },
                {
                    "index": "37-4",
                    "sentence": "To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively.",
                    "tag": "2+3"
                },
                {
                    "index": "37-5",
                    "sentence": "CDL utilizes two rewards focusing on emotion and content to improve the duality.",
                    "tag": "3"
                },
                {
                    "index": "37-6",
                    "sentence": "Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions.",
                    "tag": "3"
                },
                {
                    "index": "37-7",
                    "sentence": "Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-38",
            "text": [
                {
                    "index": "38-0",
                    "sentence": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches.",
                    "tag": "1"
                },
                {
                    "index": "38-1",
                    "sentence": "However, they are inefficient in that they predict the dialogue state at every turn from scratch.",
                    "tag": "1"
                },
                {
                    "index": "38-2",
                    "sentence": "Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST.",
                    "tag": "2+3"
                },
                {
                    "index": "38-3",
                    "sentence": "This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations.",
                    "tag": "3"
                },
                {
                    "index": "38-4",
                    "sentence": "Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder.",
                    "tag": "3"
                },
                {
                    "index": "38-5",
                    "sentence": "This enhances the effectiveness of training and DST performance.",
                    "tag": "4"
                },
                {
                    "index": "38-6",
                    "sentence": "Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting.",
                    "tag": "5"
                },
                {
                    "index": "38-7",
                    "sentence": "In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-39",
            "text": [
                {
                    "index": "39-0",
                    "sentence": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal.",
                    "tag": "1"
                },
                {
                    "index": "39-1",
                    "sentence": "The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually.",
                    "tag": "1"
                },
                {
                    "index": "39-2",
                    "sentence": "However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system.",
                    "tag": "1"
                },
                {
                    "index": "39-3",
                    "sentence": "On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus.",
                    "tag": "1"
                },
                {
                    "index": "39-4",
                    "sentence": "This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.",
                    "tag": "1"
                },
                {
                    "index": "39-5",
                    "sentence": "In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above.",
                    "tag": "1+2"
                },
                {
                    "index": "39-6",
                    "sentence": "In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-40",
            "text": [
                {
                    "index": "40-0",
                    "sentence": "Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system.",
                    "tag": "1"
                },
                {
                    "index": "40-1",
                    "sentence": "In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training.",
                    "tag": "1"
                },
                {
                    "index": "40-2",
                    "sentence": "In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts.",
                    "tag": "2"
                },
                {
                    "index": "40-3",
                    "sentence": "We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts.",
                    "tag": "3+4"
                },
                {
                    "index": "40-4",
                    "sentence": "Moreover, a new negative sampling method is proposed to augment training data.",
                    "tag": "4"
                },
                {
                    "index": "40-5",
                    "sentence": "Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-41",
            "text": [
                {
                    "index": "41-0",
                    "sentence": "Existing end-to-end dialog systems perform less effectively when data is scarce.",
                    "tag": "1"
                },
                {
                    "index": "41-1",
                    "sentence": "To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems.",
                    "tag": "1"
                },
                {
                    "index": "41-2",
                    "sentence": "In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration.",
                    "tag": "2"
                },
                {
                    "index": "41-3",
                    "sentence": "We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning.",
                    "tag": "3"
                },
                {
                    "index": "41-4",
                    "sentence": "Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-42",
            "text": [
                {
                    "index": "42-0",
                    "sentence": "Neural-based context-aware models for slot tagging have achieved state-of-the-art performance.",
                    "tag": "1"
                },
                {
                    "index": "42-1",
                    "sentence": "However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario.",
                    "tag": "1"
                },
                {
                    "index": "42-2",
                    "sentence": "In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge.",
                    "tag": "2"
                },
                {
                    "index": "42-3",
                    "sentence": "Besides, we use multi-level graph attention to explicitly model lexical relations.",
                    "tag": "3"
                },
                {
                    "index": "42-4",
                    "sentence": "The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-43",
            "text": [
                {
                    "index": "43-0",
                    "sentence": "Many studies have applied reinforcement learning to train a dialog policy and show great promise these years.",
                    "tag": "1"
                },
                {
                    "index": "43-1",
                    "sentence": "One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms.",
                    "tag": "1"
                },
                {
                    "index": "43-2",
                    "sentence": "However, modeling a realistic user simulator is challenging.",
                    "tag": "1"
                },
                {
                    "index": "43-3",
                    "sentence": "A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator.",
                    "tag": "1"
                },
                {
                    "index": "43-4",
                    "sentence": "To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents.",
                    "tag": "1+2"
                },
                {
                    "index": "43-5",
                    "sentence": "Two agents interact with each other and are jointly learned simultaneously.",
                    "tag": "3"
                },
                {
                    "index": "43-6",
                    "sentence": "The method uses the actor-critic framework to facilitate pretraining and improve scalability.",
                    "tag": "3"
                },
                {
                    "index": "43-7",
                    "sentence": "We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog.",
                    "tag": "3"
                },
                {
                    "index": "43-8",
                    "sentence": "Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-44",
            "text": [
                {
                    "index": "44-0",
                    "sentence": "Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set.",
                    "tag": "1"
                },
                {
                    "index": "44-1",
                    "sentence": "However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings.",
                    "tag": "1"
                },
                {
                    "index": "44-2",
                    "sentence": "We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance.",
                    "tag": "2+3"
                },
                {
                    "index": "44-3",
                    "sentence": "We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels.",
                    "tag": "3"
                },
                {
                    "index": "44-4",
                    "sentence": "PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019).",
                    "tag": "4"
                },
                {
                    "index": "44-5",
                    "sentence": "Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ.",
                    "tag": "4"
                },
                {
                    "index": "44-6",
                    "sentence": "PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-45",
            "text": [
                {
                    "index": "45-0",
                    "sentence": "Neural conversation models are known to generate appropriate but non-informative responses in general.",
                    "tag": "1"
                },
                {
                    "index": "45-1",
                    "sentence": "A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document.",
                    "tag": "1"
                },
                {
                    "index": "45-2",
                    "sentence": "In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory.",
                    "tag": "1"
                },
                {
                    "index": "45-3",
                    "sentence": "In this paper, we propose to create the document memory with some anticipated responses in mind.",
                    "tag": "2"
                },
                {
                    "index": "45-4",
                    "sentence": "This is achieved using a teacher-student framework.",
                    "tag": "3"
                },
                {
                    "index": "45-5",
                    "sentence": "The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information.",
                    "tag": "3"
                },
                {
                    "index": "45-6",
                    "sentence": "The student learns to construct a response-anticipated document memory from the first two sources, and teacher’s insight on memory creation.",
                    "tag": "3"
                },
                {
                    "index": "45-7",
                    "sentence": "Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-46",
            "text": [
                {
                    "index": "46-0",
                    "sentence": "Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems.",
                    "tag": "1"
                },
                {
                    "index": "46-1",
                    "sentence": "This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues.",
                    "tag": "1"
                },
                {
                    "index": "46-2",
                    "sentence": "To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards.",
                    "tag": "1"
                },
                {
                    "index": "46-3",
                    "sentence": "This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive.",
                    "tag": "1"
                },
                {
                    "index": "46-4",
                    "sentence": "To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning.",
                    "tag": "2"
                },
                {
                    "index": "46-5",
                    "sentence": "The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations.",
                    "tag": "3"
                },
                {
                    "index": "46-6",
                    "sentence": "The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations.",
                    "tag": "3"
                },
                {
                    "index": "46-7",
                    "sentence": "We further propose to learn action embeddings for a better generalization of the reward function.",
                    "tag": "3"
                },
                {
                    "index": "46-8",
                    "sentence": "The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-47",
            "text": [
                {
                    "index": "47-0",
                    "sentence": "In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations.",
                    "tag": "1"
                },
                {
                    "index": "47-1",
                    "sentence": "However, the dual property between understanding and generation has been rarely explored.",
                    "tag": "1"
                },
                {
                    "index": "47-2",
                    "sentence": "The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework.",
                    "tag": "1"
                },
                {
                    "index": "47-3",
                    "sentence": "However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion.",
                    "tag": "1+2"
                },
                {
                    "index": "47-4",
                    "sentence": "The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.",
                    "tag": "4"
                },
                {
                    "index": "47-5",
                    "sentence": "The source code is available at: https://github.com/MiuLab/DuaLUG.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-48",
            "text": [
                {
                    "index": "48-0",
                    "sentence": "The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research.",
                    "tag": "1"
                },
                {
                    "index": "48-1",
                    "sentence": "Standard language generation metrics have been shown to be ineffective for evaluating dialog models.",
                    "tag": "1"
                },
                {
                    "index": "48-2",
                    "sentence": "To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog.",
                    "tag": "2"
                },
                {
                    "index": "48-3",
                    "sentence": "USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog.",
                    "tag": "3"
                },
                {
                    "index": "48-4",
                    "sentence": "USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0).",
                    "tag": "4"
                },
                {
                    "index": "48-5",
                    "sentence": "USR additionally produces interpretable measures for several desirable properties of dialog.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-49",
            "text": [
                {
                    "index": "49-0",
                    "sentence": "Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts.",
                    "tag": "1"
                },
                {
                    "index": "49-1",
                    "sentence": "However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results.",
                    "tag": "1"
                },
                {
                    "index": "49-2",
                    "sentence": "In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation.",
                    "tag": "2+3"
                },
                {
                    "index": "49-3",
                    "sentence": "Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-50",
            "text": [
                {
                    "index": "50-0",
                    "sentence": "Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss.",
                    "tag": "1"
                },
                {
                    "index": "50-1",
                    "sentence": "While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts).",
                    "tag": "1"
                },
                {
                    "index": "50-2",
                    "sentence": "Even a small fraction of noisy data can degrade the performance of log loss.",
                    "tag": "1"
                },
                {
                    "index": "50-3",
                    "sentence": "As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references.",
                    "tag": "1"
                },
                {
                    "index": "50-4",
                    "sentence": "However, distinguishability has not been used in practice due to challenges in optimization and estimation.",
                    "tag": "1"
                },
                {
                    "index": "50-5",
                    "sentence": "We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability.",
                    "tag": "2+3"
                },
                {
                    "index": "50-6",
                    "sentence": "Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task.",
                    "tag": "4"
                },
                {
                    "index": "50-7",
                    "sentence": "Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-51",
            "text": [
                {
                    "index": "51-0",
                    "sentence": "Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models.",
                    "tag": "1"
                },
                {
                    "index": "51-1",
                    "sentence": "This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR).",
                    "tag": "2+3"
                },
                {
                    "index": "51-2",
                    "sentence": "Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations:",
                    "tag": "1"
                },
                {
                    "index": "51-3",
                    "sentence": "1) The message propagation process in AMR graphs is only guided by the first-order adjacency information.",
                    "tag": "1"
                },
                {
                    "index": "51-4",
                    "sentence": "2) The relationships between labeled edges are not fully considered.",
                    "tag": "1"
                },
                {
                    "index": "51-5",
                    "sentence": "In this work, we propose a novel graph encoding framework which can effectively explore the edge relations.",
                    "tag": "2"
                },
                {
                    "index": "51-6",
                    "sentence": "We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs.",
                    "tag": "3"
                },
                {
                    "index": "51-7",
                    "sentence": "Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "51-8",
                    "sentence": "The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-52",
            "text": [
                {
                    "index": "52-0",
                    "sentence": "Neural text generation has made tremendous progress in various tasks.",
                    "tag": "1"
                },
                {
                    "index": "52-1",
                    "sentence": "One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating.",
                    "tag": "1"
                },
                {
                    "index": "52-2",
                    "sentence": "However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc.",
                    "tag": "1"
                },
                {
                    "index": "52-3",
                    "sentence": "The typical characteristics of these texts are in three folds:",
                    "tag": "1"
                },
                {
                    "index": "52-4",
                    "sentence": "(1) They must comply fully with the rigid predefined formats.",
                    "tag": "1"
                },
                {
                    "index": "52-5",
                    "sentence": "(2) They must obey some rhyming schemes.",
                    "tag": "1"
                },
                {
                    "index": "52-6",
                    "sentence": "(3) Although they are restricted to some formats, the sentence integrity must be guaranteed.",
                    "tag": "1"
                },
                {
                    "index": "52-7",
                    "sentence": "To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated.",
                    "tag": "1"
                },
                {
                    "index": "52-8",
                    "sentence": "Therefore, we propose a simple and elegant framework named SongNet to tackle this problem.",
                    "tag": "2"
                },
                {
                    "index": "52-9",
                    "sentence": "The backbone of the framework is a Transformer-based auto-regressive language model.",
                    "tag": "3"
                },
                {
                    "index": "52-10",
                    "sentence": "Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity.",
                    "tag": "3"
                },
                {
                    "index": "52-11",
                    "sentence": "We improve the attention mechanism to impel the model to capture some future information on the format.",
                    "tag": "3"
                },
                {
                    "index": "52-12",
                    "sentence": "A pre-training and fine-tuning framework is designed to further improve the generation quality.",
                    "tag": "3"
                },
                {
                    "index": "52-13",
                    "sentence": "Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-53",
            "text": [
                {
                    "index": "53-0",
                    "sentence": "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form.",
                    "tag": "1"
                },
                {
                    "index": "53-1",
                    "sentence": "We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs.",
                    "tag": "2+3"
                },
                {
                    "index": "53-2",
                    "sentence": "We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems.",
                    "tag": "3+4"
                },
                {
                    "index": "53-3",
                    "sentence": "In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules.",
                    "tag": "3"
                },
                {
                    "index": "53-4",
                    "sentence": "A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-54",
            "text": [
                {
                    "index": "54-0",
                    "sentence": "Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution.",
                    "tag": "1"
                },
                {
                    "index": "54-1",
                    "sentence": "Existing approaches often exploit short text streams in a batch way.",
                    "tag": "1"
                },
                {
                    "index": "54-2",
                    "sentence": "However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve.",
                    "tag": "1"
                },
                {
                    "index": "54-3",
                    "sentence": "In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering.",
                    "tag": "1"
                },
                {
                    "index": "54-4",
                    "sentence": "Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way.",
                    "tag": "2+3"
                },
                {
                    "index": "54-5",
                    "sentence": "Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-55",
            "text": [
                {
                    "index": "55-0",
                    "sentence": "Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint.",
                    "tag": "1"
                },
                {
                    "index": "55-1",
                    "sentence": "For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes.",
                    "tag": "1"
                },
                {
                    "index": "55-2",
                    "sentence": "From the perspectives of both model representation and code space size, independence is always not the best assumption.",
                    "tag": "1"
                },
                {
                    "index": "55-3",
                    "sentence": "In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior.",
                    "tag": "2+3"
                },
                {
                    "index": "55-4",
                    "sentence": "To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution.",
                    "tag": "3"
                },
                {
                    "index": "55-5",
                    "sentence": "Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO).",
                    "tag": "4"
                },
                {
                    "index": "55-6",
                    "sentence": "With these novel techniques, the entire model can be optimized efficiently.",
                    "tag": "3+4"
                },
                {
                    "index": "55-7",
                    "sentence": "Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-56",
            "text": [
                {
                    "index": "56-0",
                    "sentence": "We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis.",
                    "tag": "1"
                },
                {
                    "index": "56-1",
                    "sentence": "This paper introduces the first formulation of interactive dictionary construction to address this issue.",
                    "tag": "1"
                },
                {
                    "index": "56-2",
                    "sentence": "To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms.",
                    "tag": "2+3"
                },
                {
                    "index": "56-3",
                    "sentence": "Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task.",
                    "tag": "3"
                },
                {
                    "index": "56-4",
                    "sentence": "Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-57",
            "text": [
                {
                    "index": "57-0",
                    "sentence": "This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches.",
                    "tag": "2"
                },
                {
                    "index": "57-1",
                    "sentence": "Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks.",
                    "tag": "3"
                },
                {
                    "index": "57-2",
                    "sentence": "With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010).",
                    "tag": "3"
                },
                {
                    "index": "57-3",
                    "sentence": "This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-58",
            "text": [
                {
                    "index": "58-0",
                    "sentence": "We focus on the task of Frequently Asked Questions (FAQ) retrieval.",
                    "tag": "1"
                },
                {
                    "index": "58-1",
                    "sentence": "A given user query can be matched against the questions and/or the answers in the FAQ.",
                    "tag": "1"
                },
                {
                    "index": "58-2",
                    "sentence": "We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models.",
                    "tag": "2+3"
                },
                {
                    "index": "58-3",
                    "sentence": "The two models match user queries to FAQ answers and questions, respectively.",
                    "tag": "3"
                },
                {
                    "index": "58-4",
                    "sentence": "We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases.",
                    "tag": "3"
                },
                {
                    "index": "58-5",
                    "sentence": "We show that our model is on par and even outperforms supervised models on existing datasets.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-59",
            "text": [
                {
                    "index": "59-0",
                    "sentence": "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems.",
                    "tag": "1"
                },
                {
                    "index": "59-1",
                    "sentence": "Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity.",
                    "tag": "1"
                },
                {
                    "index": "59-2",
                    "sentence": "However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks.",
                    "tag": "1"
                },
                {
                    "index": "59-3",
                    "sentence": "In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence.",
                    "tag": "2+3"
                },
                {
                    "index": "59-4",
                    "sentence": "PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols.",
                    "tag": "2"
                },
                {
                    "index": "59-5",
                    "sentence": "Extensive experiments are conducted on two benchmark datasets.",
                    "tag": "3"
                },
                {
                    "index": "59-6",
                    "sentence": "Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks.",
                    "tag": "4+5"
                },
                {
                    "index": "59-7",
                    "sentence": "In-depth analyses verify the effectiveness and robustness of PCPR.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-60",
            "text": [
                {
                    "index": "60-0",
                    "sentence": "Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations.",
                    "tag": "1"
                },
                {
                    "index": "60-1",
                    "sentence": "To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA).",
                    "tag": "2+3"
                },
                {
                    "index": "60-2",
                    "sentence": "The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT.",
                    "tag": "3"
                },
                {
                    "index": "60-3",
                    "sentence": "In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task.",
                    "tag": "3+4"
                },
                {
                    "index": "60-4",
                    "sentence": "Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks.",
                    "tag": "4+5"
                },
                {
                    "index": "60-5",
                    "sentence": "Code is available at https://github.com/joongbo/tta.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-61",
            "text": [
                {
                    "index": "61-0",
                    "sentence": "Operational risk management is one of the biggest challenges nowadays faced by financial institutions.",
                    "tag": "1"
                },
                {
                    "index": "61-1",
                    "sentence": "There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability.",
                    "tag": "2"
                },
                {
                    "index": "61-2",
                    "sentence": "To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC).",
                    "tag": "3"
                },
                {
                    "index": "61-3",
                    "sentence": "We empirically evaluate the framework on a real-world dataset.",
                    "tag": "3"
                },
                {
                    "index": "61-4",
                    "sentence": "The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations.",
                    "tag": "4"
                },
                {
                    "index": "61-5",
                    "sentence": "SemiORC also outperforms other baseline methods on operational risk classification.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-62",
            "text": [
                {
                    "index": "62-0",
                    "sentence": "Identifying user geolocation in online social networks is an essential task in many location-based applications.",
                    "tag": "1"
                },
                {
                    "index": "62-1",
                    "sentence": "Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior.",
                    "tag": "1"
                },
                {
                    "index": "62-2",
                    "sentence": "In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users.",
                    "tag": "2+3"
                },
                {
                    "index": "62-3",
                    "sentence": "This methodology helps with providing meaningful explanations on prediction results.",
                    "tag": "3"
                },
                {
                    "index": "62-4",
                    "sentence": "Furthermore, it also initiates an attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-63",
            "text": [
                {
                    "index": "63-0",
                    "sentence": "Language modeling is the technique to estimate the probability of a sequence of words.",
                    "tag": "1"
                },
                {
                    "index": "63-1",
                    "sentence": "A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages.",
                    "tag": "1"
                },
                {
                    "index": "63-2",
                    "sentence": "We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency.",
                    "tag": "2+3"
                },
                {
                    "index": "63-3",
                    "sentence": "The attention mechanism learns the bilingual context from a parallel corpus.",
                    "tag": "3"
                },
                {
                    "index": "63-4",
                    "sentence": "BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result.",
                    "tag": "4"
                },
                {
                    "index": "63-5",
                    "sentence": "We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-65",
            "text": [
                {
                    "index": "65-0",
                    "sentence": "Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability.",
                    "tag": "1"
                },
                {
                    "index": "65-1",
                    "sentence": "Without loss of generality we consider Chinese spelling error correction (CSC) in this paper.",
                    "tag": "2"
                },
                {
                    "index": "65-2",
                    "sentence": "A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model.",
                    "tag": "3"
                },
                {
                    "index": "65-3",
                    "sentence": "The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling.",
                    "tag": "3"
                },
                {
                    "index": "65-4",
                    "sentence": "In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique.",
                    "tag": "2+3"
                },
                {
                    "index": "65-5",
                    "sentence": "Our method of using ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems.",
                    "tag": "4"
                },
                {
                    "index": "65-6",
                    "sentence": "Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-66",
            "text": [
                {
                    "index": "66-0",
                    "sentence": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language.",
                    "tag": "1"
                },
                {
                    "index": "66-1",
                    "sentence": "Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters.",
                    "tag": "1"
                },
                {
                    "index": "66-2",
                    "sentence": "However, they take the similarity knowledge as either an external input resource or just heuristic rules.",
                    "tag": "1"
                },
                {
                    "index": "66-3",
                    "sentence": "This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN).",
                    "tag": "2+3"
                },
                {
                    "index": "66-4",
                    "sentence": "The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers.",
                    "tag": "3"
                },
                {
                    "index": "66-5",
                    "sentence": "These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable.",
                    "tag": "3"
                },
                {
                    "index": "66-6",
                    "sentence": "Experiments are conducted on three human-annotated datasets.",
                    "tag": "3"
                },
                {
                    "index": "66-7",
                    "sentence": "Our method achieves superior performance against previous models by a large margin.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-67",
            "text": [
                {
                    "index": "67-0",
                    "sentence": "Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC).",
                    "tag": "1"
                },
                {
                    "index": "67-1",
                    "sentence": "MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge.",
                    "tag": "1"
                },
                {
                    "index": "67-2",
                    "sentence": "To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling.",
                    "tag": "2+3"
                },
                {
                    "index": "67-3",
                    "sentence": "Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema.",
                    "tag": "3"
                },
                {
                    "index": "67-4",
                    "sentence": "Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations.",
                    "tag": "3+4"
                },
                {
                    "index": "67-5",
                    "sentence": "Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-68",
            "text": [
                {
                    "index": "68-0",
                    "sentence": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data.",
                    "tag": "2+3"
                },
                {
                    "index": "68-1",
                    "sentence": "For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT).",
                    "tag": "3"
                },
                {
                    "index": "68-2",
                    "sentence": "This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate.",
                    "tag": "3+4"
                },
                {
                    "index": "68-3",
                    "sentence": "Furthermore, it allows for fine-grained alignment of the tokens to the operations.",
                    "tag": "4"
                },
                {
                    "index": "68-4",
                    "sentence": "Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens.",
                    "tag": "3"
                },
                {
                    "index": "68-5",
                    "sentence": "We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries.",
                    "tag": "3+4"
                },
                {
                    "index": "68-6",
                    "sentence": "Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-69",
            "text": [
                {
                    "index": "69-0",
                    "sentence": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models.",
                    "tag": "1"
                },
                {
                    "index": "69-1",
                    "sentence": "In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc).",
                    "tag": "2"
                },
                {
                    "index": "69-2",
                    "sentence": "Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space.",
                    "tag": "3"
                },
                {
                    "index": "69-3",
                    "sentence": "By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.",
                    "tag": "3+4"
                },
                {
                    "index": "69-4",
                    "sentence": "Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-70",
            "text": [
                {
                    "index": "70-0",
                    "sentence": "Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community.",
                    "tag": "1"
                },
                {
                    "index": "70-1",
                    "sentence": "Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up.",
                    "tag": "1"
                },
                {
                    "index": "70-2",
                    "sentence": "We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model’s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning.",
                    "tag": "2+3"
                },
                {
                    "index": "70-3",
                    "sentence": "We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level.",
                    "tag": "3+4"
                },
                {
                    "index": "70-4",
                    "sentence": "Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-71",
            "text": [
                {
                    "index": "71-0",
                    "sentence": "Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages.",
                    "tag": "2+3"
                },
                {
                    "index": "71-1",
                    "sentence": "However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary.",
                    "tag": "1"
                },
                {
                    "index": "71-2",
                    "sentence": "In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web.",
                    "tag": "2+3"
                },
                {
                    "index": "71-3",
                    "sentence": "Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-72",
            "text": [
                {
                    "index": "72-0",
                    "sentence": "The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions.",
                    "tag": "2"
                },
                {
                    "index": "72-1",
                    "sentence": "Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them.",
                    "tag": "1"
                },
                {
                    "index": "72-2",
                    "sentence": "In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision.",
                    "tag": "2+3"
                },
                {
                    "index": "72-3",
                    "sentence": "Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions.",
                    "tag": "3"
                },
                {
                    "index": "72-4",
                    "sentence": "On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4.",
                    "tag": "4"
                },
                {
                    "index": "72-5",
                    "sentence": "We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows.",
                    "tag": "4"
                },
                {
                    "index": "72-6",
                    "sentence": "Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-73",
            "text": [
                {
                    "index": "73-0",
                    "sentence": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.",
                    "tag": "1"
                },
                {
                    "index": "73-1",
                    "sentence": "However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.",
                    "tag": "1"
                },
                {
                    "index": "73-2",
                    "sentence": "Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.",
                    "tag": "1"
                },
                {
                    "index": "73-3",
                    "sentence": "In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.",
                    "tag": "2+3"
                },
                {
                    "index": "73-4",
                    "sentence": "We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.",
                    "tag": "4"
                },
                {
                    "index": "73-5",
                    "sentence": "Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.",
                    "tag": "4+5"
                },
                {
                    "index": "73-6",
                    "sentence": "Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-74",
            "text": [
                {
                    "index": "74-0",
                    "sentence": "Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions.",
                    "tag": "1"
                },
                {
                    "index": "74-1",
                    "sentence": "Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently.",
                    "tag": "1+2"
                },
                {
                    "index": "74-2",
                    "sentence": "In this paper, we introduce a new follow-up question identification task.",
                    "tag": "1"
                },
                {
                    "index": "74-3",
                    "sentence": "We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question.",
                    "tag": "2+3"
                },
                {
                    "index": "74-4",
                    "sentence": "It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question.",
                    "tag": "3"
                },
                {
                    "index": "74-5",
                    "sentence": "Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-75",
            "text": [
                {
                    "index": "75-0",
                    "sentence": "Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations.",
                    "tag": "1"
                },
                {
                    "index": "75-1",
                    "sentence": "In this paper, we handle both types of complexity at the same time.",
                    "tag": "2"
                },
                {
                    "index": "75-2",
                    "sentence": "Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs.",
                    "tag": "3"
                },
                {
                    "index": "75-3",
                    "sentence": "Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-76",
            "text": [
                {
                    "index": "76-0",
                    "sentence": "Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image.",
                    "tag": "1"
                },
                {
                    "index": "76-1",
                    "sentence": "Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions.",
                    "tag": "1"
                },
                {
                    "index": "76-2",
                    "sentence": "It usually leads to over-penalization and thus a bad correlation to human judgment.",
                    "tag": "1"
                },
                {
                    "index": "76-3",
                    "sentence": "Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance.",
                    "tag": "2+3"
                },
                {
                    "index": "76-4",
                    "sentence": "In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation.",
                    "tag": "2+3"
                },
                {
                    "index": "76-5",
                    "sentence": "The experimental results show that our metric achieves state-of-the-art human judgment correlation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-77",
            "text": [
                {
                    "index": "77-0",
                    "sentence": "Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar.",
                    "tag": "1"
                },
                {
                    "index": "77-1",
                    "sentence": "The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines.",
                    "tag": "1"
                },
                {
                    "index": "77-2",
                    "sentence": "Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work.",
                    "tag": "1"
                },
                {
                    "index": "77-3",
                    "sentence": "In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows.",
                    "tag": "2"
                },
                {
                    "index": "77-4",
                    "sentence": "The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-78",
            "text": [
                {
                    "index": "78-0",
                    "sentence": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training.",
                    "tag": "1"
                },
                {
                    "index": "78-1",
                    "sentence": "We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense.",
                    "tag": "2"
                },
                {
                    "index": "78-2",
                    "sentence": "The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding.",
                    "tag": "3+4"
                },
                {
                    "index": "78-3",
                    "sentence": "Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work.",
                    "tag": "4+5"
                },
                {
                    "index": "78-4",
                    "sentence": "This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-79",
            "text": [
                {
                    "index": "79-0",
                    "sentence": "In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications.",
                    "tag": "2+3"
                },
                {
                    "index": "79-1",
                    "sentence": "In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks.",
                    "tag": "3"
                },
                {
                    "index": "79-2",
                    "sentence": "We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-80",
            "text": [
                {
                    "index": "80-0",
                    "sentence": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized.",
                    "tag": "1"
                },
                {
                    "index": "80-1",
                    "sentence": "However, in these methods, the discovery process of evidence is nontransparent and unexplained.",
                    "tag": "1"
                },
                {
                    "index": "80-2",
                    "sentence": "Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims.",
                    "tag": "1"
                },
                {
                    "index": "80-3",
                    "sentence": "In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification.",
                    "tag": "2"
                },
                {
                    "index": "80-4",
                    "sentence": "Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way.",
                    "tag": "3"
                },
                {
                    "index": "80-5",
                    "sentence": "Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim.",
                    "tag": "3"
                },
                {
                    "index": "80-6",
                    "sentence": "Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-81",
            "text": [
                {
                    "index": "81-0",
                    "sentence": "User intent classification plays a vital role in dialogue systems.",
                    "tag": "1"
                },
                {
                    "index": "81-1",
                    "sentence": "Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun.",
                    "tag": "1"
                },
                {
                    "index": "81-2",
                    "sentence": "This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection.",
                    "tag": "2"
                },
                {
                    "index": "81-3",
                    "sentence": "In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection.",
                    "tag": "1+2"
                },
                {
                    "index": "81-4",
                    "sentence": "Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection.",
                    "tag": "3+4"
                },
                {
                    "index": "81-5",
                    "sentence": "On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance.",
                    "tag": "3"
                },
                {
                    "index": "81-6",
                    "sentence": "A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-82",
            "text": [
                {
                    "index": "82-0",
                    "sentence": "Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions.",
                    "tag": "2"
                },
                {
                    "index": "82-1",
                    "sentence": "Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table.",
                    "tag": "1"
                },
                {
                    "index": "82-2",
                    "sentence": "In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal.",
                    "tag": "2+3"
                },
                {
                    "index": "82-3",
                    "sentence": "The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model.",
                    "tag": "3"
                },
                {
                    "index": "82-4",
                    "sentence": "Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem.",
                    "tag": "3+4"
                },
                {
                    "index": "82-5",
                    "sentence": "We also provide detailed analysis on each component of our model in our experiments.",
                    "tag": "4"
                },
                {
                    "index": "82-6",
                    "sentence": "Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-83",
            "text": [
                {
                    "index": "83-0",
                    "sentence": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification.",
                    "tag": "2"
                },
                {
                    "index": "83-1",
                    "sentence": "The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification.",
                    "tag": "3+4"
                },
                {
                    "index": "83-2",
                    "sentence": "The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning.",
                    "tag": "2+3"
                },
                {
                    "index": "83-3",
                    "sentence": "The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets.",
                    "tag": "4"
                },
                {
                    "index": "83-4",
                    "sentence": "Detailed analysis is further performed to show how the proposed network achieves the new performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-84",
            "text": [
                {
                    "index": "84-0",
                    "sentence": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases.",
                    "tag": "2"
                },
                {
                    "index": "84-1",
                    "sentence": "A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce.",
                    "tag": "1"
                },
                {
                    "index": "84-2",
                    "sentence": "Previous work in this setting employs a sequential decoding process to generate keyphrases.",
                    "tag": "1"
                },
                {
                    "index": "84-3",
                    "sentence": "However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document.",
                    "tag": "1"
                },
                {
                    "index": "84-4",
                    "sentence": "Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources.",
                    "tag": "1"
                },
                {
                    "index": "84-5",
                    "sentence": "To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism.",
                    "tag": "1+2"
                },
                {
                    "index": "84-6",
                    "sentence": "The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set.",
                    "tag": "3"
                },
                {
                    "index": "84-7",
                    "sentence": "Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases.",
                    "tag": "3"
                },
                {
                    "index": "84-8",
                    "sentence": "Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-85",
            "text": [
                {
                    "index": "85-0",
                    "sentence": "Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy.",
                    "tag": "1"
                },
                {
                    "index": "85-1",
                    "sentence": "Existing methods have difficulties in modeling the hierarchical label structure in a global view.",
                    "tag": "1"
                },
                {
                    "index": "85-2",
                    "sentence": "Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space.",
                    "tag": "1"
                },
                {
                    "index": "85-3",
                    "sentence": "In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies.",
                    "tag": "2+3"
                },
                {
                    "index": "85-4",
                    "sentence": "Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants.",
                    "tag": "3"
                },
                {
                    "index": "85-5",
                    "sentence": "A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features.",
                    "tag": "3"
                },
                {
                    "index": "85-6",
                    "sentence": "A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders.",
                    "tag": "3"
                },
                {
                    "index": "85-7",
                    "sentence": "Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-86",
            "text": [
                {
                    "index": "86-0",
                    "sentence": "Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval.",
                    "tag": "1"
                },
                {
                    "index": "86-1",
                    "sentence": "This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models.",
                    "tag": "2+3"
                },
                {
                    "index": "86-2",
                    "sentence": "Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains.",
                    "tag": "2+3"
                },
                {
                    "index": "86-3",
                    "sentence": "Our code is available at https://github.com/boudinfl/ir-using-kg",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-87",
            "text": [
                {
                    "index": "87-0",
                    "sentence": "There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics.",
                    "tag": "1"
                },
                {
                    "index": "87-1",
                    "sentence": "We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation.",
                    "tag": "2+3"
                },
                {
                    "index": "87-2",
                    "sentence": "The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-88",
            "text": [
                {
                    "index": "88-0",
                    "sentence": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis.",
                    "tag": "1"
                },
                {
                    "index": "88-1",
                    "sentence": "Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications.",
                    "tag": "1"
                },
                {
                    "index": "88-2",
                    "sentence": "Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature.",
                    "tag": "1"
                },
                {
                    "index": "88-3",
                    "sentence": "In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language.",
                    "tag": "2+3"
                },
                {
                    "index": "88-4",
                    "sentence": "Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model.",
                    "tag": "3"
                },
                {
                    "index": "88-5",
                    "sentence": "Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each.",
                    "tag": "4"
                },
                {
                    "index": "88-6",
                    "sentence": "We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables.",
                    "tag": "4+5"
                },
                {
                    "index": "88-7",
                    "sentence": "Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-89",
            "text": [
                {
                    "index": "89-0",
                    "sentence": "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem.",
                    "tag": "1"
                },
                {
                    "index": "89-1",
                    "sentence": "One of the main challenges is the fact that multiple outputs can be equally valid.",
                    "tag": "1"
                },
                {
                    "index": "89-2",
                    "sentence": "Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references.",
                    "tag": "1"
                },
                {
                    "index": "89-3",
                    "sentence": "The latter has been shown to significantly improve the performance of evaluation metrics.",
                    "tag": "1"
                },
                {
                    "index": "89-4",
                    "sentence": "However, collecting multiple references is expensive and in practice a single reference is generally used.",
                    "tag": "1"
                },
                {
                    "index": "89-5",
                    "sentence": "In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether.",
                    "tag": "2+3"
                },
                {
                    "index": "89-6",
                    "sentence": "We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-90",
            "text": [
                {
                    "index": "90-0",
                    "sentence": "We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE.",
                    "tag": "2"
                },
                {
                    "index": "90-1",
                    "sentence": "We compare various multimodality integration and fusion strategies.",
                    "tag": "3"
                },
                {
                    "index": "90-2",
                    "sentence": "For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-91",
            "text": [
                {
                    "index": "91-0",
                    "sentence": "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph.",
                    "tag": "2"
                },
                {
                    "index": "91-1",
                    "sentence": "At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept.",
                    "tag": "3"
                },
                {
                    "index": "91-2",
                    "sentence": "We show that the answers to these two questions are mutually causalities.",
                    "tag": "4"
                },
                {
                    "index": "91-3",
                    "sentence": "We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy.",
                    "tag": "2+3"
                },
                {
                    "index": "91-4",
                    "sentence": "Our experimental results significantly outperform all previously reported Smatch scores by large margins.",
                    "tag": "4"
                },
                {
                    "index": "91-5",
                    "sentence": "Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT.",
                    "tag": "4"
                },
                {
                    "index": "91-6",
                    "sentence": "With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-92",
            "text": [
                {
                    "index": "92-0",
                    "sentence": "Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English).",
                    "tag": "1"
                },
                {
                    "index": "92-1",
                    "sentence": "In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary.",
                    "tag": "2"
                },
                {
                    "index": "92-2",
                    "sentence": "We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary.",
                    "tag": "3"
                },
                {
                    "index": "92-3",
                    "sentence": "Specifically, we first employ the encoder-decoder attention distribution to attend to the source words.",
                    "tag": "3"
                },
                {
                    "index": "92-4",
                    "sentence": "Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word.",
                    "tag": "3"
                },
                {
                    "index": "92-5",
                    "sentence": "Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words.",
                    "tag": "3"
                },
                {
                    "index": "92-6",
                    "sentence": "Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-93",
            "text": [
                {
                    "index": "93-0",
                    "sentence": "We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.).",
                    "tag": "2"
                },
                {
                    "index": "93-1",
                    "sentence": "We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques.",
                    "tag": "3"
                },
                {
                    "index": "93-2",
                    "sentence": "Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%.",
                    "tag": "4"
                },
                {
                    "index": "93-3",
                    "sentence": "Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers.",
                    "tag": "3+4"
                },
                {
                    "index": "93-4",
                    "sentence": "All source code is available at https://github.com/yg211/acl20-ref-free-eval.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-94",
            "text": [
                {
                    "index": "94-0",
                    "sentence": "Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary.",
                    "tag": "1"
                },
                {
                    "index": "94-1",
                    "sentence": "Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge.",
                    "tag": "1"
                },
                {
                    "index": "94-2",
                    "sentence": "In this work, we propose a Transformer-based model to enhance the copy mechanism.",
                    "tag": "2"
                },
                {
                    "index": "94-3",
                    "sentence": "Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer.",
                    "tag": "3"
                },
                {
                    "index": "94-4",
                    "sentence": "We use the centrality of each source word to guide the copy process explicitly.",
                    "tag": "3"
                },
                {
                    "index": "94-5",
                    "sentence": "Experimental results show that the self-attention graph provides useful guidance for the copy distribution.",
                    "tag": "4"
                },
                {
                    "index": "94-6",
                    "sentence": "Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-95",
            "text": [
                {
                    "index": "95-0",
                    "sentence": "Open Domain dialog system evaluation is one of the most important challenges in dialog research.",
                    "tag": "1"
                },
                {
                    "index": "95-1",
                    "sentence": "Existing automatic evaluation metrics, such as BLEU are mostly reference-based.",
                    "tag": "1"
                },
                {
                    "index": "95-2",
                    "sentence": "They calculate the difference between the generated response and a limited number of available references.",
                    "tag": "1"
                },
                {
                    "index": "95-3",
                    "sentence": "Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots.",
                    "tag": "1"
                },
                {
                    "index": "95-4",
                    "sentence": "However, self-reported user rating suffers from bias and variance among different users.",
                    "tag": "1"
                },
                {
                    "index": "95-5",
                    "sentence": "To alleviate this problem, we formulate dialog evaluation as a comparison task.",
                    "tag": "2+3"
                },
                {
                    "index": "95-6",
                    "sentence": "We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them.",
                    "tag": "2+3"
                },
                {
                    "index": "95-7",
                    "sentence": "Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples.",
                    "tag": "3"
                },
                {
                    "index": "95-8",
                    "sentence": "Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-96",
            "text": [
                {
                    "index": "96-0",
                    "sentence": "Human conversations contain many types of information, e.g., knowledge, common sense, and language habits.",
                    "tag": "1"
                },
                {
                    "index": "96-1",
                    "sentence": "In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding.",
                    "tag": "2+3"
                },
                {
                    "index": "96-2",
                    "sentence": "Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.",
                    "tag": "1"
                },
                {
                    "index": "96-3",
                    "sentence": "To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.",
                    "tag": "3"
                },
                {
                    "index": "96-4",
                    "sentence": "We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.",
                    "tag": "3"
                },
                {
                    "index": "96-5",
                    "sentence": "The experiment results show that PR-Embedding can improve the quality of the selected response.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-97",
            "text": [
                {
                    "index": "97-0",
                    "sentence": "In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot).",
                    "tag": "2"
                },
                {
                    "index": "97-1",
                    "sentence": "Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels.",
                    "tag": "1"
                },
                {
                    "index": "97-2",
                    "sentence": "But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets.",
                    "tag": "1"
                },
                {
                    "index": "97-3",
                    "sentence": "To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores.",
                    "tag": "2+3"
                },
                {
                    "index": "97-4",
                    "sentence": "In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label.",
                    "tag": "3"
                },
                {
                    "index": "97-5",
                    "sentence": "To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels.",
                    "tag": "2+3"
                },
                {
                    "index": "97-6",
                    "sentence": "Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-98",
            "text": [
                {
                    "index": "98-0",
                    "sentence": "Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems.",
                    "tag": "1"
                },
                {
                    "index": "98-1",
                    "sentence": "Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests.",
                    "tag": "2+3"
                },
                {
                    "index": "98-2",
                    "sentence": "We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators.",
                    "tag": "3"
                },
                {
                    "index": "98-3",
                    "sentence": "We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment.",
                    "tag": "3"
                },
                {
                    "index": "98-4",
                    "sentence": "Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-99",
            "text": [
                {
                    "index": "99-0",
                    "sentence": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors.",
                    "tag": "1"
                },
                {
                    "index": "99-1",
                    "sentence": "The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation.",
                    "tag": "1"
                },
                {
                    "index": "99-2",
                    "sentence": "Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding.",
                    "tag": "2+3"
                },
                {
                    "index": "99-3",
                    "sentence": "Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation.",
                    "tag": "3"
                },
                {
                    "index": "99-4",
                    "sentence": "Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-100",
            "text": [
                {
                    "index": "100-0",
                    "sentence": "Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given.",
                    "tag": "1"
                },
                {
                    "index": "100-1",
                    "sentence": "In this paper, we cast bridging anaphora resolution as question answering based on context.",
                    "tag": "2"
                },
                {
                    "index": "100-2",
                    "sentence": "This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself).",
                    "tag": "4"
                },
                {
                    "index": "100-3",
                    "sentence": "We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning.",
                    "tag": "2"
                },
                {
                    "index": "100-4",
                    "sentence": "Furthermore, we propose a novel method to generate a large amount of “quasi-bridging” training data.",
                    "tag": "2"
                },
                {
                    "index": "100-5",
                    "sentence": "We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-101",
            "text": [
                {
                    "index": "101-0",
                    "sentence": "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels.",
                    "tag": "1"
                },
                {
                    "index": "101-1",
                    "sentence": "It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels.",
                    "tag": "1"
                },
                {
                    "index": "101-2",
                    "sentence": "We address these issues by introducing a novel approach to dialogue coherence assessment.",
                    "tag": "1"
                },
                {
                    "index": "101-3",
                    "sentence": "We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment.",
                    "tag": "2+3"
                },
                {
                    "index": "101-4",
                    "sentence": "Our approach alleviates the need for explicit dialogue act labels during evaluation.",
                    "tag": "4"
                },
                {
                    "index": "101-5",
                    "sentence": "The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.",
                    "tag": "4+5"
                },
                {
                    "index": "101-6",
                    "sentence": "We release our source code.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-102",
            "text": [
                {
                    "index": "102-0",
                    "sentence": "We propose a graph-based method to tackle the dependency tree linearization task.",
                    "tag": "2+3"
                },
                {
                    "index": "102-1",
                    "sentence": "We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs.",
                    "tag": "3"
                },
                {
                    "index": "102-2",
                    "sentence": "We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree.",
                    "tag": "3"
                },
                {
                    "index": "102-3",
                    "sentence": "We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences.",
                    "tag": "3"
                },
                {
                    "index": "102-4",
                    "sentence": "Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-103",
            "text": [
                {
                    "index": "103-0",
                    "sentence": "This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage.",
                    "tag": "2"
                },
                {
                    "index": "103-1",
                    "sentence": "In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN).",
                    "tag": "2+3"
                },
                {
                    "index": "103-2",
                    "sentence": "Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding.",
                    "tag": "3"
                },
                {
                    "index": "103-3",
                    "sentence": "On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance.",
                    "tag": "4+5"
                },
                {
                    "index": "103-4",
                    "sentence": "The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-104",
            "text": [
                {
                    "index": "104-0",
                    "sentence": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training.",
                    "tag": "1"
                },
                {
                    "index": "104-1",
                    "sentence": "When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain.",
                    "tag": "1"
                },
                {
                    "index": "104-2",
                    "sentence": "But what should one do when there is no hand-labelled data for the target domain?",
                    "tag": "1"
                },
                {
                    "index": "104-3",
                    "sentence": "This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision.",
                    "tag": "1+2"
                },
                {
                    "index": "104-4",
                    "sentence": "The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain.",
                    "tag": "3"
                },
                {
                    "index": "104-5",
                    "sentence": "These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions.",
                    "tag": "3"
                },
                {
                    "index": "104-6",
                    "sentence": "A sequence labelling model can finally be trained on the basis of this unified annotation.",
                    "tag": "3"
                },
                {
                    "index": "104-7",
                    "sentence": "We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-105",
            "text": [
                {
                    "index": "105-0",
                    "sentence": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities.",
                    "tag": "1"
                },
                {
                    "index": "105-1",
                    "sentence": "However, effective aggregation of relevant information in the document remains a challenging research question.",
                    "tag": "1"
                },
                {
                    "index": "105-2",
                    "sentence": "Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies.",
                    "tag": "1"
                },
                {
                    "index": "105-3",
                    "sentence": "Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph.",
                    "tag": "2+3"
                },
                {
                    "index": "105-4",
                    "sentence": "We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning.",
                    "tag": "3+4"
                },
                {
                    "index": "105-5",
                    "sentence": "Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset.",
                    "tag": "4+5"
                },
                {
                    "index": "105-6",
                    "sentence": "Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-106",
            "text": [
                {
                    "index": "106-0",
                    "sentence": "In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary.",
                    "tag": "2"
                },
                {
                    "index": "106-1",
                    "sentence": "Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences.",
                    "tag": "3"
                },
                {
                    "index": "106-2",
                    "sentence": "We propose anchored training (AT) to tackle the task.",
                    "tag": "3"
                },
                {
                    "index": "106-3",
                    "sentence": "AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language.",
                    "tag": "3"
                },
                {
                    "index": "106-4",
                    "sentence": "Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT.",
                    "tag": "4"
                },
                {
                    "index": "106-5",
                    "sentence": "On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-107",
            "text": [
                {
                    "index": "107-0",
                    "sentence": "Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models.",
                    "tag": "1"
                },
                {
                    "index": "107-1",
                    "sentence": "Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection.",
                    "tag": "1"
                },
                {
                    "index": "107-2",
                    "sentence": "While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems.",
                    "tag": "1"
                },
                {
                    "index": "107-3",
                    "sentence": "Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training.",
                    "tag": "1"
                },
                {
                    "index": "107-4",
                    "sentence": "We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets.",
                    "tag": "2"
                },
                {
                    "index": "107-5",
                    "sentence": "Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-108",
            "text": [
                {
                    "index": "108-0",
                    "sentence": "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism.",
                    "tag": "1"
                },
                {
                    "index": "108-1",
                    "sentence": "In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios.",
                    "tag": "2+3"
                },
                {
                    "index": "108-2",
                    "sentence": "We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-109",
            "text": [
                {
                    "index": "109-0",
                    "sentence": "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations.",
                    "tag": "1"
                },
                {
                    "index": "109-1",
                    "sentence": "In this paper, we explore ways to improve them.",
                    "tag": "2+3"
                },
                {
                    "index": "109-2",
                    "sentence": "We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures.",
                    "tag": "2"
                },
                {
                    "index": "109-3",
                    "sentence": "We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs.",
                    "tag": "2+3"
                },
                {
                    "index": "109-4",
                    "sentence": "Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-110",
            "text": [
                {
                    "index": "110-0",
                    "sentence": "The performance of neural machine translation systems is commonly evaluated in terms of BLEU.",
                    "tag": "1"
                },
                {
                    "index": "110-1",
                    "sentence": "However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model.",
                    "tag": "1"
                },
                {
                    "index": "110-2",
                    "sentence": "In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models.",
                    "tag": "2+3"
                },
                {
                    "index": "110-3",
                    "sentence": "XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task.",
                    "tag": "3+4"
                },
                {
                    "index": "110-4",
                    "sentence": "We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems.",
                    "tag": "4+5"
                },
                {
                    "index": "110-5",
                    "sentence": "Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-111",
            "text": [
                {
                    "index": "111-0",
                    "sentence": "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages.",
                    "tag": "1"
                },
                {
                    "index": "111-1",
                    "sentence": "However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained.",
                    "tag": "1"
                },
                {
                    "index": "111-2",
                    "sentence": "In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture.",
                    "tag": "2"
                },
                {
                    "index": "111-3",
                    "sentence": "The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair.",
                    "tag": "3"
                },
                {
                    "index": "111-4",
                    "sentence": "Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-112",
            "text": [
                {
                    "index": "112-0",
                    "sentence": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity.",
                    "tag": "1"
                },
                {
                    "index": "112-1",
                    "sentence": "In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders.",
                    "tag": "2+3"
                },
                {
                    "index": "112-2",
                    "sentence": "Reference-free evaluation holds the promise of web-scale comparison of MT systems.",
                    "tag": "1"
                },
                {
                    "index": "112-3",
                    "sentence": "We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER.",
                    "tag": "3"
                },
                {
                    "index": "112-4",
                    "sentence": "We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations.",
                    "tag": "4"
                },
                {
                    "index": "112-5",
                    "sentence": "We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling.",
                    "tag": "4"
                },
                {
                    "index": "112-6",
                    "sentence": "In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-113",
            "text": [
                {
                    "index": "113-0",
                    "sentence": "We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation.",
                    "tag": "2+3"
                },
                {
                    "index": "113-1",
                    "sentence": "Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus.",
                    "tag": "3"
                },
                {
                    "index": "113-2",
                    "sentence": "We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search.",
                    "tag": "2"
                },
                {
                    "index": "113-3",
                    "sentence": "When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-114",
            "text": [
                {
                    "index": "114-0",
                    "sentence": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences.",
                    "tag": "1"
                },
                {
                    "index": "114-1",
                    "sentence": "However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently.",
                    "tag": "1"
                },
                {
                    "index": "114-2",
                    "sentence": "Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem.",
                    "tag": "1"
                },
                {
                    "index": "114-3",
                    "sentence": "In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence.",
                    "tag": "2+3"
                },
                {
                    "index": "114-4",
                    "sentence": "Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments.",
                    "tag": "3"
                },
                {
                    "index": "114-5",
                    "sentence": "Experimental results on WMT’14 English⇒German, WAT’17 Japanese⇒English, and WMT’17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines.",
                    "tag": "4+5"
                },
                {
                    "index": "114-6",
                    "sentence": "Extensive analyses confirm that the performance gains come from the cross-lingual information.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-115",
            "text": [
                {
                    "index": "115-0",
                    "sentence": "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.",
                    "tag": "2+3"
                },
                {
                    "index": "115-1",
                    "sentence": "We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks.",
                    "tag": "3"
                },
                {
                    "index": "115-2",
                    "sentence": "We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia.",
                    "tag": "4"
                },
                {
                    "index": "115-3",
                    "sentence": "They actually equal or improve the current state of the art in tagging and parsing for all five languages.",
                    "tag": "4"
                },
                {
                    "index": "115-4",
                    "sentence": "In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-116",
            "text": [
                {
                    "index": "116-0",
                    "sentence": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge.",
                    "tag": "1"
                },
                {
                    "index": "116-1",
                    "sentence": "Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations.",
                    "tag": "1"
                },
                {
                    "index": "116-2",
                    "sentence": "We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites.",
                    "tag": "2+3"
                },
                {
                    "index": "116-3",
                    "sentence": "We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures.",
                    "tag": "4"
                },
                {
                    "index": "116-4",
                    "sentence": "Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments.",
                    "tag": "4"
                },
                {
                    "index": "116-5",
                    "sentence": "Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-117",
            "text": [
                {
                    "index": "117-0",
                    "sentence": "With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful.",
                    "tag": "1"
                },
                {
                    "index": "117-1",
                    "sentence": "Several testing methodologies have been developed to probe models’ syntactic representations.",
                    "tag": "1"
                },
                {
                    "index": "117-2",
                    "sentence": "One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with different syntax.",
                    "tag": "1"
                },
                {
                    "index": "117-3",
                    "sentence": "We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-118",
            "text": [
                {
                    "index": "118-0",
                    "sentence": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling.",
                    "tag": "1"
                },
                {
                    "index": "118-1",
                    "sentence": "While there is a vast theoretical literature on suspense, it is computationally not well understood.",
                    "tag": "1"
                },
                {
                    "index": "118-2",
                    "sentence": "We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is.",
                    "tag": "1"
                },
                {
                    "index": "118-3",
                    "sentence": "Both can be computed either directly over story representations or over their probability distributions.",
                    "tag": "1"
                },
                {
                    "index": "118-4",
                    "sentence": "We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction.",
                    "tag": "2+3"
                },
                {
                    "index": "118-5",
                    "sentence": "Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy.",
                    "tag": "4"
                },
                {
                    "index": "118-6",
                    "sentence": "We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-119",
            "text": [
                {
                    "index": "119-0",
                    "sentence": "Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time.",
                    "tag": "1"
                },
                {
                    "index": "119-1",
                    "sentence": "However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word).",
                    "tag": "1"
                },
                {
                    "index": "119-2",
                    "sentence": "We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics.",
                    "tag": "2"
                },
                {
                    "index": "119-3",
                    "sentence": "We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants.",
                    "tag": "2+3"
                },
                {
                    "index": "119-4",
                    "sentence": "We then employ a large number of machine learning methods to predict a user’s reading time.",
                    "tag": "3"
                },
                {
                    "index": "119-5",
                    "sentence": "We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-120",
            "text": [
                {
                    "index": "120-0",
                    "sentence": "Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse.",
                    "tag": "1"
                },
                {
                    "index": "120-1",
                    "sentence": "A key to success in either task is parallel training data which is expensive to obtain at a large scale.",
                    "tag": "1"
                },
                {
                    "index": "120-2",
                    "sentence": "In this work, we propose a generative model which couples NLU and NLG through a shared latent variable.",
                    "tag": "2+3"
                },
                {
                    "index": "120-3",
                    "sentence": "This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG.",
                    "tag": "2+3"
                },
                {
                    "index": "120-4",
                    "sentence": "Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations.",
                    "tag": "3+4"
                },
                {
                    "index": "120-5",
                    "sentence": "We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-121",
            "text": [
                {
                    "index": "121-0",
                    "sentence": "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains.",
                    "tag": "1"
                },
                {
                    "index": "121-1",
                    "sentence": "However, this design lacks adaptation to individual domains.",
                    "tag": "1"
                },
                {
                    "index": "121-2",
                    "sentence": "To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing.",
                    "tag": "1"
                },
                {
                    "index": "121-3",
                    "sentence": "We first observe that words in a sentence are often related to multiple domains.",
                    "tag": "2"
                },
                {
                    "index": "121-4",
                    "sentence": "Hence, we assume each word has a domain proportion, which indicates its domain preference.",
                    "tag": "3"
                },
                {
                    "index": "121-5",
                    "sentence": "Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions.",
                    "tag": "3"
                },
                {
                    "index": "121-6",
                    "sentence": "We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions.",
                    "tag": "3+4"
                },
                {
                    "index": "121-7",
                    "sentence": "Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well.",
                    "tag": "4"
                },
                {
                    "index": "121-8",
                    "sentence": "Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-122",
            "text": [
                {
                    "index": "122-0",
                    "sentence": "To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog.",
                    "tag": "1+2"
                },
                {
                    "index": "122-1",
                    "sentence": "To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent “what to say” and “how to say”, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response.",
                    "tag": "3"
                },
                {
                    "index": "122-2",
                    "sentence": "We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation.",
                    "tag": "3"
                },
                {
                    "index": "122-3",
                    "sentence": "In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy.",
                    "tag": "3"
                },
                {
                    "index": "122-4",
                    "sentence": "Results on two benchmark corpora demonstrate the effectiveness of this framework.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-123",
            "text": [
                {
                    "index": "123-0",
                    "sentence": "Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs.",
                    "tag": "1"
                },
                {
                    "index": "123-1",
                    "sentence": "Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only.",
                    "tag": "1"
                },
                {
                    "index": "123-2",
                    "sentence": "In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring.",
                    "tag": "2+3"
                },
                {
                    "index": "123-3",
                    "sentence": "Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures.",
                    "tag": "3+4"
                },
                {
                    "index": "123-4",
                    "sentence": "In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-124",
            "text": [
                {
                    "index": "124-0",
                    "sentence": "We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies.",
                    "tag": "1"
                },
                {
                    "index": "124-1",
                    "sentence": "We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.",
                    "tag": "2+3"
                },
                {
                    "index": "124-2",
                    "sentence": "We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment.",
                    "tag": "3"
                },
                {
                    "index": "124-3",
                    "sentence": "We compare our approach against multiple baselines using both automatic metrics and human evaluation.",
                    "tag": "3"
                },
                {
                    "index": "124-4",
                    "sentence": "Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-125",
            "text": [
                {
                    "index": "125-0",
                    "sentence": "Subword segmentation is widely used to address the open vocabulary problem in machine translation.",
                    "tag": "1"
                },
                {
                    "index": "125-1",
                    "sentence": "The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens.",
                    "tag": "1"
                },
                {
                    "index": "125-2",
                    "sentence": "While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors.",
                    "tag": "1"
                },
                {
                    "index": "125-3",
                    "sentence": "So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018).",
                    "tag": "1"
                },
                {
                    "index": "125-4",
                    "sentence": "In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word.",
                    "tag": "2"
                },
                {
                    "index": "125-5",
                    "sentence": "We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE.",
                    "tag": "2"
                },
                {
                    "index": "125-6",
                    "sentence": "It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework.",
                    "tag": "3"
                },
                {
                    "index": "125-7",
                    "sentence": "Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-126",
            "text": [
                {
                    "index": "126-0",
                    "sentence": "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront.",
                    "tag": "1"
                },
                {
                    "index": "126-1",
                    "sentence": "As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document.",
                    "tag": "1"
                },
                {
                    "index": "126-2",
                    "sentence": "When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient.",
                    "tag": "1"
                },
                {
                    "index": "126-3",
                    "sentence": "In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models.",
                    "tag": "2"
                },
                {
                    "index": "126-4",
                    "sentence": "We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes).",
                    "tag": "3"
                },
                {
                    "index": "126-5",
                    "sentence": "Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-127",
            "text": [
                {
                    "index": "127-0",
                    "sentence": "In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls.",
                    "tag": "1"
                },
                {
                    "index": "127-1",
                    "sentence": "The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.",
                    "tag": "1"
                },
                {
                    "index": "127-2",
                    "sentence": "However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables.",
                    "tag": "1"
                },
                {
                    "index": "127-3",
                    "sentence": "In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.",
                    "tag": "2+3"
                },
                {
                    "index": "127-4",
                    "sentence": "We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-128",
            "text": [
                {
                    "index": "128-0",
                    "sentence": "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear.",
                    "tag": "1"
                },
                {
                    "index": "128-1",
                    "sentence": "There is accumulating evidence that neural models do not learn systematically.",
                    "tag": "1"
                },
                {
                    "index": "128-2",
                    "sentence": "We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour.",
                    "tag": "1"
                },
                {
                    "index": "128-3",
                    "sentence": "We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying.",
                    "tag": "2+3"
                },
                {
                    "index": "128-4",
                    "sentence": "As a case study, we perform a series of experiments in the setting of natural language inference (NLI).",
                    "tag": "3"
                },
                {
                    "index": "128-5",
                    "sentence": "We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-129",
            "text": [
                {
                    "index": "129-0",
                    "sentence": "Human conversations naturally evolve around related concepts and hop to distant concepts.",
                    "tag": "1"
                },
                {
                    "index": "129-1",
                    "sentence": "This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows.",
                    "tag": "2+3"
                },
                {
                    "index": "129-2",
                    "sentence": "By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations.",
                    "tag": "1"
                },
                {
                    "index": "129-3",
                    "sentence": "The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses.",
                    "tag": "3"
                },
                {
                    "index": "129-4",
                    "sentence": "Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures.",
                    "tag": "4+5"
                },
                {
                    "index": "129-5",
                    "sentence": "All source codes of this work are available at https://github.com/thunlp/ConceptFlow.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-130",
            "text": [
                {
                    "index": "130-0",
                    "sentence": "The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST).",
                    "tag": "1"
                },
                {
                    "index": "130-1",
                    "sentence": "This information is typically represented as a semantic frame that captures the and slot-labels provided by the user.",
                    "tag": "1"
                },
                {
                    "index": "130-2",
                    "sentence": "We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains.",
                    "tag": "1"
                },
                {
                    "index": "130-3",
                    "sentence": "We propose a recursive, hierarchical frame-based representation and show how to learn it from data.",
                    "tag": "2+3"
                },
                {
                    "index": "130-4",
                    "sentence": "We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template.",
                    "tag": "3"
                },
                {
                    "index": "130-5",
                    "sentence": "We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end.",
                    "tag": "2+3"
                },
                {
                    "index": "130-6",
                    "sentence": "We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-131",
            "text": [
                {
                    "index": "131-0",
                    "sentence": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications.",
                    "tag": "2"
                },
                {
                    "index": "131-1",
                    "sentence": "It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare.",
                    "tag": "1+2"
                },
                {
                    "index": "131-2",
                    "sentence": "However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods.",
                    "tag": "1"
                },
                {
                    "index": "131-3",
                    "sentence": "In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models.",
                    "tag": "2"
                },
                {
                    "index": "131-4",
                    "sentence": "Our proposed method can be used with any binary class calibration scheme and a neural network model.",
                    "tag": "3"
                },
                {
                    "index": "131-5",
                    "sentence": "Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements.",
                    "tag": "4"
                },
                {
                    "index": "131-6",
                    "sentence": "We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems.",
                    "tag": "4"
                },
                {
                    "index": "131-7",
                    "sentence": "We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "131-8",
                    "sentence": "Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-132",
            "text": [
                {
                    "index": "132-0",
                    "sentence": "Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies.",
                    "tag": "1"
                },
                {
                    "index": "132-1",
                    "sentence": "Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical.",
                    "tag": "1"
                },
                {
                    "index": "132-2",
                    "sentence": "To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance.",
                    "tag": "2+3"
                },
                {
                    "index": "132-3",
                    "sentence": "Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary.",
                    "tag": "3"
                },
                {
                    "index": "132-4",
                    "sentence": "We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-133",
            "text": [
                {
                    "index": "133-0",
                    "sentence": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text.",
                    "tag": "1"
                },
                {
                    "index": "133-1",
                    "sentence": "In this paper, we allow model developers to specify these types of inductive biases as natural language explanations.",
                    "tag": "2+3"
                },
                {
                    "index": "133-2",
                    "sentence": "We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input.",
                    "tag": "3"
                },
                {
                    "index": "133-3",
                    "sentence": "Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-134",
            "text": [
                {
                    "index": "134-0",
                    "sentence": "Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks.",
                    "tag": "1"
                },
                {
                    "index": "134-1",
                    "sentence": "However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples.",
                    "tag": "1"
                },
                {
                    "index": "134-2",
                    "sentence": "In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected.",
                    "tag": "1"
                },
                {
                    "index": "134-3",
                    "sentence": "One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks.",
                    "tag": "1"
                },
                {
                    "index": "134-4",
                    "sentence": "In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting.",
                    "tag": "2+3"
                },
                {
                    "index": "134-5",
                    "sentence": "Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-135",
            "text": [
                {
                    "index": "135-0",
                    "sentence": "Sequence labeling is a fundamental task for a range of natural language processing problems.",
                    "tag": "1"
                },
                {
                    "index": "135-1",
                    "sentence": "When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly.",
                    "tag": "1"
                },
                {
                    "index": "135-2",
                    "sentence": "In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible.",
                    "tag": "1"
                },
                {
                    "index": "135-3",
                    "sentence": "In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data).",
                    "tag": "2"
                },
                {
                    "index": "135-4",
                    "sentence": "It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module.",
                    "tag": "3"
                },
                {
                    "index": "135-5",
                    "sentence": "Finally, it leads to a model reflecting the agreement (consensus) among multiple sources.",
                    "tag": "3"
                },
                {
                    "index": "135-6",
                    "sentence": "We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation.",
                    "tag": "3"
                },
                {
                    "index": "135-7",
                    "sentence": "Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.",
                    "tag": "4"
                },
                {
                    "index": "135-8",
                    "sentence": "We also demonstrate that the method can apply to various tasks and cope with different encoders.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-136",
            "text": [
                {
                    "index": "136-0",
                    "sentence": "This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix.",
                    "tag": "2+3"
                },
                {
                    "index": "136-1",
                    "sentence": "TMix creates a large amount of augmented training samples by interpolating text in hidden space.",
                    "tag": "3"
                },
                {
                    "index": "136-2",
                    "sentence": "Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.",
                    "tag": "3"
                },
                {
                    "index": "136-3",
                    "sentence": "By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "136-4",
                    "sentence": "The improvement is especially prominent when supervision is extremely limited.",
                    "tag": "4"
                },
                {
                    "index": "136-5",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/MixText.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-137",
            "text": [
                {
                    "index": "137-0",
                    "sentence": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters.",
                    "tag": "1"
                },
                {
                    "index": "137-1",
                    "sentence": "However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices.",
                    "tag": "1"
                },
                {
                    "index": "137-2",
                    "sentence": "In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.",
                    "tag": "2+3"
                },
                {
                    "index": "137-3",
                    "sentence": "Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.",
                    "tag": "3"
                },
                {
                    "index": "137-4",
                    "sentence": "Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.",
                    "tag": "3"
                },
                {
                    "index": "137-5",
                    "sentence": "To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model.",
                    "tag": "3"
                },
                {
                    "index": "137-6",
                    "sentence": "Then, we conduct knowledge transfer from this teacher to MobileBERT.",
                    "tag": "3"
                },
                {
                    "index": "137-7",
                    "sentence": "Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "137-8",
                    "sentence": "On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone.",
                    "tag": "4"
                },
                {
                    "index": "137-9",
                    "sentence": "On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-138",
            "text": [
                {
                    "index": "138-0",
                    "sentence": "Transfer learning has fundamentally changed the landscape of natural language processing (NLP).",
                    "tag": "1"
                },
                {
                    "index": "138-1",
                    "sentence": "Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "138-2",
                    "sentence": "However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data.",
                    "tag": "1"
                },
                {
                    "index": "138-3",
                    "sentence": "To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance.",
                    "tag": "2+3"
                },
                {
                    "index": "138-4",
                    "sentence": "The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating.",
                    "tag": "3"
                },
                {
                    "index": "138-5",
                    "sentence": "Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI.",
                    "tag": "4"
                },
                {
                    "index": "138-6",
                    "sentence": "Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-139",
            "text": [
                {
                    "index": "139-0",
                    "sentence": "Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications.",
                    "tag": "1"
                },
                {
                    "index": "139-1",
                    "sentence": "Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks.",
                    "tag": "1"
                },
                {
                    "index": "139-2",
                    "sentence": "However, there has been no attempt to exploit GNN to create taxonomies.",
                    "tag": "1"
                },
                {
                    "index": "139-3",
                    "sentence": "In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task.",
                    "tag": "2+3"
                },
                {
                    "index": "139-4",
                    "sentence": "Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain.",
                    "tag": "5"
                },
                {
                    "index": "139-5",
                    "sentence": "We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction.",
                    "tag": "3"
                },
                {
                    "index": "139-6",
                    "sentence": "Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training.",
                    "tag": "3"
                },
                {
                    "index": "139-7",
                    "sentence": "The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain.",
                    "tag": "3"
                },
                {
                    "index": "139-8",
                    "sentence": "Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-140",
            "text": [
                {
                    "index": "140-0",
                    "sentence": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI).",
                    "tag": "1"
                },
                {
                    "index": "140-1",
                    "sentence": "Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI.",
                    "tag": "1"
                },
                {
                    "index": "140-2",
                    "sentence": "However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary.",
                    "tag": "1"
                },
                {
                    "index": "140-3",
                    "sentence": "We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary.",
                    "tag": "2+3"
                },
                {
                    "index": "140-4",
                    "sentence": "This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy.",
                    "tag": "3"
                },
                {
                    "index": "140-5",
                    "sentence": "We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks.",
                    "tag": "3"
                },
                {
                    "index": "140-6",
                    "sentence": "Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-141",
            "text": [
                {
                    "index": "141-0",
                    "sentence": "Authorship attribution aims to identify the author of a text based on the stylometric analysis.",
                    "tag": "1"
                },
                {
                    "index": "141-1",
                    "sentence": "Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style.",
                    "tag": "1"
                },
                {
                    "index": "141-2",
                    "sentence": "In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model.",
                    "tag": "2"
                },
                {
                    "index": "141-3",
                    "sentence": "An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the adversary interested in authorship attribution.",
                    "tag": "3"
                },
                {
                    "index": "141-4",
                    "sentence": "We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87.",
                    "tag": "4"
                },
                {
                    "index": "141-5",
                    "sentence": "The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner.",
                    "tag": "4"
                },
                {
                    "index": "141-6",
                    "sentence": "Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-142",
            "text": [
                {
                    "index": "142-0",
                    "sentence": "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications.",
                    "tag": "1"
                },
                {
                    "index": "142-1",
                    "sentence": "However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications.",
                    "tag": "1"
                },
                {
                    "index": "142-2",
                    "sentence": "We propose a simple but effective method, DeeBERT, to accelerate BERT inference.",
                    "tag": "2+3"
                },
                {
                    "index": "142-3",
                    "sentence": "Our approach allows samples to exit earlier without passing through the entire model.",
                    "tag": "3"
                },
                {
                    "index": "142-4",
                    "sentence": "Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality.",
                    "tag": "4"
                },
                {
                    "index": "142-5",
                    "sentence": "Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.",
                    "tag": "4"
                },
                {
                    "index": "142-6",
                    "sentence": "Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks.",
                    "tag": "4+5"
                },
                {
                    "index": "142-7",
                    "sentence": "Code is available at https://github.com/castorini/DeeBERT.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-143",
            "text": [
                {
                    "index": "143-0",
                    "sentence": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy.",
                    "tag": "1"
                },
                {
                    "index": "143-1",
                    "sentence": "Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model.",
                    "tag": "1"
                },
                {
                    "index": "143-2",
                    "sentence": "We first define the task as a sequence-to-sequence problem.",
                    "tag": "1"
                },
                {
                    "index": "143-3",
                    "sentence": "Afterwards, we propose an auxiliary synthetic task of bottom-up-classification.",
                    "tag": "2+3"
                },
                {
                    "index": "143-4",
                    "sentence": "Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space.",
                    "tag": "3"
                },
                {
                    "index": "143-5",
                    "sentence": "We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search.",
                    "tag": "3"
                },
                {
                    "index": "143-6",
                    "sentence": "Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy.",
                    "tag": "4"
                },
                {
                    "index": "143-7",
                    "sentence": "With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-144",
            "text": [
                {
                    "index": "144-0",
                    "sentence": "We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts.",
                    "tag": "2+3"
                },
                {
                    "index": "144-1",
                    "sentence": "Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).",
                    "tag": "3"
                },
                {
                    "index": "144-2",
                    "sentence": "We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates.",
                    "tag": "3"
                },
                {
                    "index": "144-3",
                    "sentence": "Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task.",
                    "tag": "4"
                },
                {
                    "index": "144-4",
                    "sentence": "We discuss areas for improvement and potential applications for text-only speech scoring.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-145",
            "text": [
                {
                    "index": "145-0",
                    "sentence": "Representation learning is a critical ingredient for natural language processing systems.",
                    "tag": "1"
                },
                {
                    "index": "145-1",
                    "sentence": "Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.",
                    "tag": "1"
                },
                {
                    "index": "145-2",
                    "sentence": "For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity.",
                    "tag": "1"
                },
                {
                    "index": "145-3",
                    "sentence": "We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.",
                    "tag": "2+3"
                },
                {
                    "index": "145-4",
                    "sentence": "Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning.",
                    "tag": "3"
                },
                {
                    "index": "145-5",
                    "sentence": "Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.",
                    "tag": "3"
                },
                {
                    "index": "145-6",
                    "sentence": "We show that Specter outperforms a variety of competitive baselines on the benchmark.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-146",
            "text": [
                {
                    "index": "146-0",
                    "sentence": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program.",
                    "tag": "2+3"
                },
                {
                    "index": "146-1",
                    "sentence": "By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques.",
                    "tag": "3+4"
                },
                {
                    "index": "146-2",
                    "sentence": "We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases.",
                    "tag": "2+3"
                },
                {
                    "index": "146-3",
                    "sentence": "By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art.",
                    "tag": "4"
                },
                {
                    "index": "146-4",
                    "sentence": "Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-147",
            "text": [
                {
                    "index": "147-0",
                    "sentence": "In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them.",
                    "tag": "1"
                },
                {
                    "index": "147-1",
                    "sentence": "We argue that such data can prove as a testing ground for understanding how we reason about information.",
                    "tag": "1"
                },
                {
                    "index": "147-2",
                    "sentence": "To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.",
                    "tag": "2+3"
                },
                {
                    "index": "147-3",
                    "sentence": "Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning.",
                    "tag": "4"
                },
                {
                    "index": "147-4",
                    "sentence": "Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-148",
            "text": [
                {
                    "index": "148-0",
                    "sentence": "Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA).",
                    "tag": "1"
                },
                {
                    "index": "148-1",
                    "sentence": "We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed.",
                    "tag": "1"
                },
                {
                    "index": "148-2",
                    "sentence": "In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments.",
                    "tag": "2"
                },
                {
                    "index": "148-3",
                    "sentence": "Specifically, we “occlude” the majority of a document’s text and add context-sensitive commands that reveal “glimpses” of the hidden text to a model.",
                    "tag": "3"
                },
                {
                    "index": "148-4",
                    "sentence": "We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.",
                    "tag": "3"
                },
                {
                    "index": "148-5",
                    "sentence": "We believe that this setting can contribute in scaling models to web-level QA scenarios.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-149",
            "text": [
                {
                    "index": "149-0",
                    "sentence": "Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets.",
                    "tag": "1"
                },
                {
                    "index": "149-1",
                    "sentence": "We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage.",
                    "tag": "1"
                },
                {
                    "index": "149-2",
                    "sentence": "We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus.",
                    "tag": "2+3"
                },
                {
                    "index": "149-3",
                    "sentence": "The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set.",
                    "tag": "3+4"
                },
                {
                    "index": "149-4",
                    "sentence": "This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-150",
            "text": [
                {
                    "index": "150-0",
                    "sentence": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech.",
                    "tag": "1"
                },
                {
                    "index": "150-1",
                    "sentence": "One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames.",
                    "tag": "1"
                },
                {
                    "index": "150-2",
                    "sentence": "The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks.",
                    "tag": "1"
                },
                {
                    "index": "150-3",
                    "sentence": "In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions.",
                    "tag": "2"
                },
                {
                    "index": "150-4",
                    "sentence": "We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task.",
                    "tag": "2+3"
                },
                {
                    "index": "150-5",
                    "sentence": "Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-151",
            "text": [
                {
                    "index": "151-0",
                    "sentence": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP.",
                    "tag": "1"
                },
                {
                    "index": "151-1",
                    "sentence": "Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream.",
                    "tag": "1"
                },
                {
                    "index": "151-2",
                    "sentence": "While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication).",
                    "tag": "1"
                },
                {
                    "index": "151-3",
                    "sentence": "More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic.",
                    "tag": "1"
                },
                {
                    "index": "151-4",
                    "sentence": "In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).",
                    "tag": "2"
                },
                {
                    "index": "151-5",
                    "sentence": "MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.",
                    "tag": "3"
                },
                {
                    "index": "151-6",
                    "sentence": "It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities.",
                    "tag": "3"
                },
                {
                    "index": "151-7",
                    "sentence": "In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis.",
                    "tag": "3"
                },
                {
                    "index": "151-8",
                    "sentence": "Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet.",
                    "tag": "3+4"
                },
                {
                    "index": "151-9",
                    "sentence": "On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-152",
            "text": [
                {
                    "index": "152-0",
                    "sentence": "We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers.",
                    "tag": "1+2"
                },
                {
                    "index": "152-1",
                    "sentence": "We propose a novel multimodal approach to real-time sequence labeling in speech.",
                    "tag": "2"
                },
                {
                    "index": "152-2",
                    "sentence": "Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition.",
                    "tag": "3"
                },
                {
                    "index": "152-3",
                    "sentence": "Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data.",
                    "tag": "4"
                },
                {
                    "index": "152-4",
                    "sentence": "The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-153",
            "text": [
                {
                    "index": "153-0",
                    "sentence": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture.",
                    "tag": "2"
                },
                {
                    "index": "153-1",
                    "sentence": "We particularly focus on the scene context provided by the visual information, to ground the ASR.",
                    "tag": "3"
                },
                {
                    "index": "153-2",
                    "sentence": "We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer.",
                    "tag": "3"
                },
                {
                    "index": "153-3",
                    "sentence": "Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.",
                    "tag": "3"
                },
                {
                    "index": "153-4",
                    "sentence": "Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models.",
                    "tag": "4"
                },
                {
                    "index": "153-5",
                    "sentence": "Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.",
                    "tag": "4"
                },
                {
                    "index": "153-6",
                    "sentence": "Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-154",
            "text": [
                {
                    "index": "154-0",
                    "sentence": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation.",
                    "tag": "1"
                },
                {
                    "index": "154-1",
                    "sentence": "Their performance is often assumed to be superior, though in many conditions this is not yet the case.",
                    "tag": "1"
                },
                {
                    "index": "154-2",
                    "sentence": "We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines.",
                    "tag": "2"
                },
                {
                    "index": "154-3",
                    "sentence": "Further, we introduce two methods to incorporate phone features into ST models.",
                    "tag": "3"
                },
                {
                    "index": "154-4",
                    "sentence": "We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-155",
            "text": [
                {
                    "index": "155-0",
                    "sentence": "Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people.",
                    "tag": "1"
                },
                {
                    "index": "155-1",
                    "sentence": "Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication.",
                    "tag": "1"
                },
                {
                    "index": "155-2",
                    "sentence": "Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality.",
                    "tag": "1"
                },
                {
                    "index": "155-3",
                    "sentence": "We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.",
                    "tag": "2+3"
                },
                {
                    "index": "155-4",
                    "sentence": "We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-156",
            "text": [
                {
                    "index": "156-0",
                    "sentence": "To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners.",
                    "tag": "1"
                },
                {
                    "index": "156-1",
                    "sentence": "Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014).",
                    "tag": "1"
                },
                {
                    "index": "156-2",
                    "sentence": "In this work we study large-scale architectures and datasets for this goal.",
                    "tag": "2"
                },
                {
                    "index": "156-3",
                    "sentence": "We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components.",
                    "tag": "3"
                },
                {
                    "index": "156-4",
                    "sentence": "To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019).",
                    "tag": "3"
                },
                {
                    "index": "156-5",
                    "sentence": "Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits.",
                    "tag": "3"
                },
                {
                    "index": "156-6",
                    "sentence": "Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-157",
            "text": [
                {
                    "index": "157-0",
                    "sentence": "Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue.",
                    "tag": "1"
                },
                {
                    "index": "157-1",
                    "sentence": "There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation.",
                    "tag": "1"
                },
                {
                    "index": "157-2",
                    "sentence": "Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them.",
                    "tag": "2+3"
                },
                {
                    "index": "157-3",
                    "sentence": "We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-158",
            "text": [
                {
                    "index": "158-0",
                    "sentence": "The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue.",
                    "tag": "1"
                },
                {
                    "index": "158-1",
                    "sentence": "We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn.",
                    "tag": "2"
                },
                {
                    "index": "158-2",
                    "sentence": "The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS).",
                    "tag": "3"
                },
                {
                    "index": "158-3",
                    "sentence": "We evaluate our models using offline experiments as well as human listening tests.",
                    "tag": "3"
                },
                {
                    "index": "158-4",
                    "sentence": "We show that human listeners consider certain response timings to be more natural based on the dialogue context.",
                    "tag": "4"
                },
                {
                    "index": "158-5",
                    "sentence": "The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-159",
            "text": [
                {
                    "index": "159-0",
                    "sentence": "Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text.",
                    "tag": "1"
                },
                {
                    "index": "159-1",
                    "sentence": "Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task.",
                    "tag": "1"
                },
                {
                    "index": "159-2",
                    "sentence": "Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult.",
                    "tag": "1"
                },
                {
                    "index": "159-3",
                    "sentence": "To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text.",
                    "tag": "1+2"
                },
                {
                    "index": "159-4",
                    "sentence": "Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-160",
            "text": [
                {
                    "index": "160-0",
                    "sentence": "We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.",
                    "tag": "1"
                },
                {
                    "index": "160-1",
                    "sentence": "While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling—a special case of infilling where text is predicted at the end of a document.",
                    "tag": "1"
                },
                {
                    "index": "160-2",
                    "sentence": "In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.",
                    "tag": "2"
                },
                {
                    "index": "160-3",
                    "sentence": "To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.",
                    "tag": "3"
                },
                {
                    "index": "160-4",
                    "sentence": "We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.",
                    "tag": "5"
                },
                {
                    "index": "160-5",
                    "sentence": "Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-161",
            "text": [
                {
                    "index": "161-0",
                    "sentence": "Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion.",
                    "tag": "1"
                },
                {
                    "index": "161-1",
                    "sentence": "This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context.",
                    "tag": "1"
                },
                {
                    "index": "161-2",
                    "sentence": "Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation.",
                    "tag": "1"
                },
                {
                    "index": "161-3",
                    "sentence": "In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.",
                    "tag": "2+3"
                },
                {
                    "index": "161-4",
                    "sentence": "We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-162",
            "text": [
                {
                    "index": "162-0",
                    "sentence": "Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation.",
                    "tag": "1"
                },
                {
                    "index": "162-1",
                    "sentence": "Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply.",
                    "tag": "1"
                },
                {
                    "index": "162-2",
                    "sentence": "We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues.",
                    "tag": "2"
                },
                {
                    "index": "162-3",
                    "sentence": "Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization.",
                    "tag": "2"
                },
                {
                    "index": "162-4",
                    "sentence": "Extensive experiments demonstrate that the proposed method leads to improved performance.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-163",
            "text": [
                {
                    "index": "163-0",
                    "sentence": "Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output.",
                    "tag": "1"
                },
                {
                    "index": "163-1",
                    "sentence": "We propose to extend this framework with a simple and effective post-generation ranking approach.",
                    "tag": "2"
                },
                {
                    "index": "163-2",
                    "sentence": "Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output.",
                    "tag": "3"
                },
                {
                    "index": "163-3",
                    "sentence": "We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies.",
                    "tag": "3+4"
                },
                {
                    "index": "163-4",
                    "sentence": "Experiments on two machine translation (MT) datasets show new state-of-art results.",
                    "tag": "4"
                },
                {
                    "index": "163-5",
                    "sentence": "We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-164",
            "text": [
                {
                    "index": "164-0",
                    "sentence": "Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN).",
                    "tag": "1"
                },
                {
                    "index": "164-1",
                    "sentence": "In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones.",
                    "tag": "2"
                },
                {
                    "index": "164-2",
                    "sentence": "We show that existing state-of-the-art agents do not generalize well.",
                    "tag": "3"
                },
                {
                    "index": "164-3",
                    "sentence": "To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.",
                    "tag": "3"
                },
                {
                    "index": "164-4",
                    "sentence": "A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps.",
                    "tag": "3"
                },
                {
                    "index": "164-5",
                    "sentence": "The learning process is composed of two phases.",
                    "tag": "3"
                },
                {
                    "index": "164-6",
                    "sentence": "In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps.",
                    "tag": "3"
                },
                {
                    "index": "164-7",
                    "sentence": "In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions.",
                    "tag": "3"
                },
                {
                    "index": "164-8",
                    "sentence": "We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability.",
                    "tag": "3"
                },
                {
                    "index": "164-9",
                    "sentence": "Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better.",
                    "tag": "4"
                },
                {
                    "index": "164-10",
                    "sentence": "The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-165",
            "text": [
                {
                    "index": "165-0",
                    "sentence": "We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.",
                    "tag": "2+3"
                },
                {
                    "index": "165-1",
                    "sentence": "We focus on unsupervised and weakly-supervised settings where no action labels are known during training.",
                    "tag": "3"
                },
                {
                    "index": "165-2",
                    "sentence": "Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos.",
                    "tag": "4"
                },
                {
                    "index": "165-3",
                    "sentence": "Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-166",
            "text": [
                {
                    "index": "166-0",
                    "sentence": "Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.",
                    "tag": "1"
                },
                {
                    "index": "166-1",
                    "sentence": "Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture.",
                    "tag": "2+3"
                },
                {
                    "index": "166-2",
                    "sentence": "The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation.",
                    "tag": "3"
                },
                {
                    "index": "166-3",
                    "sentence": "Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-167",
            "text": [
                {
                    "index": "167-0",
                    "sentence": "Visual features are a promising signal for learning bootstrap textual models.",
                    "tag": "1"
                },
                {
                    "index": "167-1",
                    "sentence": "However, blackbox learning models make it difficult to isolate the specific contribution of visual components.",
                    "tag": "1"
                },
                {
                    "index": "167-2",
                    "sentence": "In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal.",
                    "tag": "2"
                },
                {
                    "index": "167-3",
                    "sentence": "By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance.",
                    "tag": "3"
                },
                {
                    "index": "167-4",
                    "sentence": "Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better.",
                    "tag": "4"
                },
                {
                    "index": "167-5",
                    "sentence": "We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-168",
            "text": [
                {
                    "index": "168-0",
                    "sentence": "Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks.",
                    "tag": "1"
                },
                {
                    "index": "168-1",
                    "sentence": "However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”.",
                    "tag": "1"
                },
                {
                    "index": "168-2",
                    "sentence": "Previous approaches consider the Kullback–Leibler divergence (KL) individual for each datapoint.",
                    "tag": "1"
                },
                {
                    "index": "168-3",
                    "sentence": "We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL’s distribution positive.",
                    "tag": "2"
                },
                {
                    "index": "168-4",
                    "sentence": "Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters.",
                    "tag": "3"
                },
                {
                    "index": "168-5",
                    "sentence": "Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently.",
                    "tag": "4"
                },
                {
                    "index": "168-6",
                    "sentence": "We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE).",
                    "tag": "4+5"
                },
                {
                    "index": "168-7",
                    "sentence": "Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-169",
            "text": [
                {
                    "index": "169-0",
                    "sentence": "We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline—random word embeddings—focusing on the impact of the training set size and the linguistic properties of the task.",
                    "tag": "2"
                },
                {
                    "index": "169-1",
                    "sentence": "Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks.",
                    "tag": "4"
                },
                {
                    "index": "169-2",
                    "sentence": "Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-170",
            "text": [
                {
                    "index": "170-0",
                    "sentence": "We study the potential for interaction in natural language classification.",
                    "tag": "2"
                },
                {
                    "index": "170-1",
                    "sentence": "We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions.",
                    "tag": "3"
                },
                {
                    "index": "170-2",
                    "sentence": "At each turn, our system decides between asking the most informative question or making the final classification pre-diction.",
                    "tag": "3"
                },
                {
                    "index": "170-3",
                    "sentence": "The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks.",
                    "tag": "3"
                },
                {
                    "index": "170-4",
                    "sentence": "We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-171",
            "text": [
                {
                    "index": "171-0",
                    "sentence": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications.",
                    "tag": "1"
                },
                {
                    "index": "171-1",
                    "sentence": "With a large KG, the embeddings consume a large amount of storage and memory.",
                    "tag": "1"
                },
                {
                    "index": "171-2",
                    "sentence": "This is problematic and prohibits the deployment of these techniques in many real world settings.",
                    "tag": "1"
                },
                {
                    "index": "171-3",
                    "sentence": "Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes.",
                    "tag": "2+3"
                },
                {
                    "index": "171-4",
                    "sentence": "The approach can be trained end-to-end with simple modifications to any existing KG embedding technique.",
                    "tag": "3"
                },
                {
                    "index": "171-5",
                    "sentence": "We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance.",
                    "tag": "4"
                },
                {
                    "index": "171-6",
                    "sentence": "The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-172",
            "text": [
                {
                    "index": "172-0",
                    "sentence": "This work revisits the task of training sequence tagging models with limited resources using transfer learning.",
                    "tag": "2+3"
                },
                {
                    "index": "172-1",
                    "sentence": "We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings.",
                    "tag": "3"
                },
                {
                    "index": "172-2",
                    "sentence": "Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines.",
                    "tag": "4"
                },
                {
                    "index": "172-3",
                    "sentence": "We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-173",
            "text": [
                {
                    "index": "173-0",
                    "sentence": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "173-1",
                    "sentence": "Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one.",
                    "tag": "1"
                },
                {
                    "index": "173-2",
                    "sentence": "We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks.",
                    "tag": "2"
                },
                {
                    "index": "173-3",
                    "sentence": "By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.",
                    "tag": "3+4"
                },
                {
                    "index": "173-4",
                    "sentence": "We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP).",
                    "tag": "4"
                },
                {
                    "index": "173-5",
                    "sentence": "One can finetune MLMs to give scores without masking, enabling computation in a single inference pass.",
                    "tag": "4"
                },
                {
                    "index": "173-6",
                    "sentence": "In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages.",
                    "tag": "4"
                },
                {
                    "index": "173-7",
                    "sentence": "We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-174",
            "text": [
                {
                    "index": "174-0",
                    "sentence": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE.",
                    "tag": "1"
                },
                {
                    "index": "174-1",
                    "sentence": "However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict.",
                    "tag": "1"
                },
                {
                    "index": "174-2",
                    "sentence": "In this work, we propose a novel distance-based approach for knowledge graph link prediction.",
                    "tag": "2"
                },
                {
                    "index": "174-3",
                    "sentence": "First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations.",
                    "tag": "3"
                },
                {
                    "index": "174-4",
                    "sentence": "The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity.",
                    "tag": "3+4"
                },
                {
                    "index": "174-5",
                    "sentence": "Second, the graph context is integrated into distance scoring functions directly.",
                    "tag": "3"
                },
                {
                    "index": "174-6",
                    "sentence": "Specifically, graph context is explicitly modeled via two directed context representations.",
                    "tag": "3"
                },
                {
                    "index": "174-7",
                    "sentence": "Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively.",
                    "tag": "4"
                },
                {
                    "index": "174-8",
                    "sentence": "The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases.",
                    "tag": "4"
                },
                {
                    "index": "174-9",
                    "sentence": "Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-175",
            "text": [
                {
                    "index": "175-0",
                    "sentence": "Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability.",
                    "tag": "1"
                },
                {
                    "index": "175-1",
                    "sentence": "In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone.",
                    "tag": "1"
                },
                {
                    "index": "175-2",
                    "sentence": "When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications.",
                    "tag": "1"
                },
                {
                    "index": "175-3",
                    "sentence": "Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.",
                    "tag": "2"
                },
                {
                    "index": "175-4",
                    "sentence": "We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives.",
                    "tag": "4"
                },
                {
                    "index": "175-5",
                    "sentence": "Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline.",
                    "tag": "4"
                },
                {
                    "index": "175-6",
                    "sentence": "We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline.",
                    "tag": "4"
                },
                {
                    "index": "175-7",
                    "sentence": "PosCal training can be easily extendable to any types of classification tasks as a form of regularization term.",
                    "tag": "5"
                },
                {
                    "index": "175-8",
                    "sentence": "Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-176",
            "text": [
                {
                    "index": "176-0",
                    "sentence": "Text generation often requires high-precision output that obeys task-specific rules.",
                    "tag": "1"
                },
                {
                    "index": "176-1",
                    "sentence": "This fine-grained control is difficult to enforce with off-the-shelf deep learning models.",
                    "tag": "1"
                },
                {
                    "index": "176-2",
                    "sentence": "In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach.",
                    "tag": "2"
                },
                {
                    "index": "176-3",
                    "sentence": "Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model.",
                    "tag": "4"
                },
                {
                    "index": "176-4",
                    "sentence": "This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models.",
                    "tag": "4"
                },
                {
                    "index": "176-5",
                    "sentence": "Experiments consider applications of this approach for text generation.",
                    "tag": "4"
                },
                {
                    "index": "176-6",
                    "sentence": "We find that this method improves over standard benchmarks, while also providing fine-grained control.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-177",
            "text": [
                {
                    "index": "177-0",
                    "sentence": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.",
                    "tag": "1"
                },
                {
                    "index": "177-1",
                    "sentence": "Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.",
                    "tag": "1"
                },
                {
                    "index": "177-2",
                    "sentence": "In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.",
                    "tag": "1+2"
                },
                {
                    "index": "177-3",
                    "sentence": "The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.",
                    "tag": "3"
                },
                {
                    "index": "177-4",
                    "sentence": "Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks.",
                    "tag": "3"
                },
                {
                    "index": "177-5",
                    "sentence": "We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).",
                    "tag": "3"
                },
                {
                    "index": "177-6",
                    "sentence": "We instantiate RobEn to defend against a large family of adversarial typos.",
                    "tag": "4"
                },
                {
                    "index": "177-7",
                    "sentence": "Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-178",
            "text": [
                {
                    "index": "178-0",
                    "sentence": "BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA).",
                    "tag": "1"
                },
                {
                    "index": "178-1",
                    "sentence": "BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction.",
                    "tag": "1"
                },
                {
                    "index": "178-2",
                    "sentence": "In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding.",
                    "tag": "1+2"
                },
                {
                    "index": "178-3",
                    "sentence": "Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage.",
                    "tag": "3"
                },
                {
                    "index": "178-4",
                    "sentence": "We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets.",
                    "tag": "4"
                },
                {
                    "index": "178-5",
                    "sentence": "Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction.",
                    "tag": "4"
                },
                {
                    "index": "178-6",
                    "sentence": "We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system.",
                    "tag": "4"
                },
                {
                    "index": "178-7",
                    "sentence": "Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-179",
            "text": [
                {
                    "index": "179-0",
                    "sentence": "Recently, NLP has seen a surge in the usage of large pre-trained models.",
                    "tag": "1"
                },
                {
                    "index": "179-1",
                    "sentence": "Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice.",
                    "tag": "1"
                },
                {
                    "index": "179-2",
                    "sentence": "This raises the question of whether downloading untrusted pre-trained weights can pose a security threat.",
                    "tag": "1"
                },
                {
                    "index": "179-3",
                    "sentence": "In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.",
                    "tag": "2+3"
                },
                {
                    "index": "179-4",
                    "sentence": "We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure.",
                    "tag": "3+4"
                },
                {
                    "index": "179-5",
                    "sentence": "Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.",
                    "tag": "4"
                },
                {
                    "index": "179-6",
                    "sentence": "Finally, we outline practical defenses against such attacks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-180",
            "text": [
                {
                    "index": "180-0",
                    "sentence": "Transformers have gradually become a key component for many state-of-the-art natural language representation models.",
                    "tag": "1"
                },
                {
                    "index": "180-1",
                    "sentence": "A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0.",
                    "tag": "1"
                },
                {
                    "index": "180-2",
                    "sentence": "This model however is computationally prohibitive and has a huge number of parameters.",
                    "tag": "1"
                },
                {
                    "index": "180-3",
                    "sentence": "In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.",
                    "tag": "2"
                },
                {
                    "index": "180-4",
                    "sentence": "We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency.",
                    "tag": "3"
                },
                {
                    "index": "180-5",
                    "sentence": "We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers.",
                    "tag": "3+4"
                },
                {
                    "index": "180-6",
                    "sentence": "In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-181",
            "text": [
                {
                    "index": "181-0",
                    "sentence": "We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model.",
                    "tag": "1+2"
                },
                {
                    "index": "181-1",
                    "sentence": "In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy.",
                    "tag": "3"
                },
                {
                    "index": "181-2",
                    "sentence": "This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model.",
                    "tag": "4"
                },
                {
                    "index": "181-3",
                    "sentence": "Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-182",
            "text": [
                {
                    "index": "182-0",
                    "sentence": "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged.",
                    "tag": "1"
                },
                {
                    "index": "182-1",
                    "sentence": "The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT.",
                    "tag": "1"
                },
                {
                    "index": "182-2",
                    "sentence": "The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data.",
                    "tag": "1"
                },
                {
                    "index": "182-3",
                    "sentence": "In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT.",
                    "tag": "2"
                },
                {
                    "index": "182-4",
                    "sentence": "We offer three major results:",
                    "tag": "4"
                },
                {
                    "index": "182-5",
                    "sentence": "(i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models.",
                    "tag": "4"
                },
                {
                    "index": "182-6",
                    "sentence": "(ii) Self-supervision improves zero-shot translation quality in multilingual models.",
                    "tag": "4"
                },
                {
                    "index": "182-7",
                    "sentence": "(iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-183",
            "text": [
                {
                    "index": "183-0",
                    "sentence": "Back-translation is a widely used data augmentation technique which leverages target monolingual data.",
                    "tag": "1"
                },
                {
                    "index": "183-1",
                    "sentence": "However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese.",
                    "tag": "1"
                },
                {
                    "index": "183-2",
                    "sentence": "This is believed to be due to translationese inputs better matching the back-translated training data.",
                    "tag": "1"
                },
                {
                    "index": "183-3",
                    "sentence": "In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators.",
                    "tag": "1+2"
                },
                {
                    "index": "183-4",
                    "sentence": "We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs.",
                    "tag": "4"
                },
                {
                    "index": "183-5",
                    "sentence": "BLEU cannot capture human preferences because references are translationese when source sentences are natural text.",
                    "tag": "4"
                },
                {
                    "index": "183-6",
                    "sentence": "We recommend complementing BLEU with a language model score to measure fluency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-184",
            "text": [
                {
                    "index": "184-0",
                    "sentence": "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information.",
                    "tag": "1"
                },
                {
                    "index": "184-1",
                    "sentence": "But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies.",
                    "tag": "1"
                },
                {
                    "index": "184-2",
                    "sentence": "We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies.",
                    "tag": "2+3"
                },
                {
                    "index": "184-3",
                    "sentence": "Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-185",
            "text": [
                {
                    "index": "185-0",
                    "sentence": "Neural architectures are the current state of the art in Word Sense Disambiguation (WSD).",
                    "tag": "1"
                },
                {
                    "index": "185-1",
                    "sentence": "However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB).",
                    "tag": "1"
                },
                {
                    "index": "185-2",
                    "sentence": "We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.",
                    "tag": "2"
                },
                {
                    "index": "185-3",
                    "sentence": "As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "185-4",
                    "sentence": "On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-186",
            "text": [
                {
                    "index": "186-0",
                    "sentence": "Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus.",
                    "tag": "1"
                },
                {
                    "index": "186-1",
                    "sentence": "We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics.",
                    "tag": "1"
                },
                {
                    "index": "186-2",
                    "sentence": "We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem.",
                    "tag": "2"
                },
                {
                    "index": "186-3",
                    "sentence": "Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios.",
                    "tag": "5"
                },
                {
                    "index": "186-4",
                    "sentence": "Experiments across different applications show the significant effectiveness of our model.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-187",
            "text": [
                {
                    "index": "187-0",
                    "sentence": "We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures.",
                    "tag": "1+2"
                },
                {
                    "index": "187-1",
                    "sentence": "Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.",
                    "tag": "3"
                },
                {
                    "index": "187-2",
                    "sentence": "The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure.",
                    "tag": "4"
                },
                {
                    "index": "187-3",
                    "sentence": "We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity.",
                    "tag": "5"
                },
                {
                    "index": "187-4",
                    "sentence": "The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-188",
            "text": [
                {
                    "index": "188-0",
                    "sentence": "While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied.",
                    "tag": "1"
                },
                {
                    "index": "188-1",
                    "sentence": "We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction.",
                    "tag": "2+3"
                },
                {
                    "index": "188-2",
                    "sentence": "When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.",
                    "tag": "3"
                },
                {
                    "index": "188-3",
                    "sentence": "We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.",
                    "tag": "3"
                },
                {
                    "index": "188-4",
                    "sentence": "Both models outperform previous approaches, with the multi-channel model performing best.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-189",
            "text": [
                {
                    "index": "189-0",
                    "sentence": "Metaphor is a linguistic device in which a concept is expressed by mentioning another.",
                    "tag": "1"
                },
                {
                    "index": "189-1",
                    "sentence": "Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics.",
                    "tag": "1"
                },
                {
                    "index": "189-2",
                    "sentence": "Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models.",
                    "tag": "1"
                },
                {
                    "index": "189-3",
                    "sentence": "This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.",
                    "tag": "1+2"
                },
                {
                    "index": "189-4",
                    "sentence": "To the best of our knowledge, this is the first “MWE-aware” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena.",
                    "tag": "3"
                },
                {
                    "index": "189-5",
                    "sentence": "The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-190",
            "text": [
                {
                    "index": "190-0",
                    "sentence": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language.",
                    "tag": "1"
                },
                {
                    "index": "190-1",
                    "sentence": "These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language.",
                    "tag": "1"
                },
                {
                    "index": "190-2",
                    "sentence": "While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages.",
                    "tag": "1"
                },
                {
                    "index": "190-3",
                    "sentence": "In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications.",
                    "tag": "2"
                },
                {
                    "index": "190-4",
                    "sentence": "We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives.",
                    "tag": "3"
                },
                {
                    "index": "190-5",
                    "sentence": "Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning.",
                    "tag": "4"
                },
                {
                    "index": "190-6",
                    "sentence": "We further provide recommendations for using the multilingual word representations for downstream tasks.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-191",
            "text": [
                {
                    "index": "191-0",
                    "sentence": "Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity).",
                    "tag": "1"
                },
                {
                    "index": "191-1",
                    "sentence": "However, manually annotating a large dataset with a protected attribute is slow and expensive.",
                    "tag": "1"
                },
                {
                    "index": "191-2",
                    "sentence": "Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?",
                    "tag": "1"
                },
                {
                    "index": "191-3",
                    "sentence": "While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias.",
                    "tag": "1"
                },
                {
                    "index": "191-4",
                    "sentence": "In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval.",
                    "tag": "1+2"
                },
                {
                    "index": "191-5",
                    "sentence": "We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias.",
                    "tag": "4"
                },
                {
                    "index": "191-6",
                    "sentence": "In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim.",
                    "tag": "5"
                },
                {
                    "index": "191-7",
                    "sentence": "Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases.",
                    "tag": "5"
                },
                {
                    "index": "191-8",
                    "sentence": "For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences – to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-192",
            "text": [
                {
                    "index": "192-0",
                    "sentence": "We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.",
                    "tag": "1+2"
                },
                {
                    "index": "192-1",
                    "sentence": "We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance.",
                    "tag": "3"
                },
                {
                    "index": "192-2",
                    "sentence": "Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing.",
                    "tag": "3"
                },
                {
                    "index": "192-3",
                    "sentence": "Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures.",
                    "tag": "3"
                },
                {
                    "index": "192-4",
                    "sentence": "We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-193",
            "text": [
                {
                    "index": "193-0",
                    "sentence": "Pooling is an important technique for learning text representations in many neural NLP models.",
                    "tag": "1"
                },
                {
                    "index": "193-1",
                    "sentence": "In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features.",
                    "tag": "1"
                },
                {
                    "index": "193-2",
                    "sentence": "However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks.",
                    "tag": "1"
                },
                {
                    "index": "193-3",
                    "sentence": "In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited.",
                    "tag": "1"
                },
                {
                    "index": "193-4",
                    "sentence": "In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.",
                    "tag": "2"
                },
                {
                    "index": "193-5",
                    "sentence": "Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks.",
                    "tag": "3"
                },
                {
                    "index": "193-6",
                    "sentence": "In addition, we propose two methods to ensure the numerical stability of the model training.",
                    "tag": "3"
                },
                {
                    "index": "193-7",
                    "sentence": "The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion.",
                    "tag": "3"
                },
                {
                    "index": "193-8",
                    "sentence": "The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation.",
                    "tag": "3"
                },
                {
                    "index": "193-9",
                    "sentence": "Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-194",
            "text": [
                {
                    "index": "194-0",
                    "sentence": "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks.",
                    "tag": "1"
                },
                {
                    "index": "194-1",
                    "sentence": "However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach.",
                    "tag": "1"
                },
                {
                    "index": "194-2",
                    "sentence": "We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups.",
                    "tag": "1+2"
                },
                {
                    "index": "194-3",
                    "sentence": "Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel.",
                    "tag": "4"
                },
                {
                    "index": "194-4",
                    "sentence": "Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work.",
                    "tag": "4"
                },
                {
                    "index": "194-5",
                    "sentence": "We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance.",
                    "tag": "4"
                },
                {
                    "index": "194-6",
                    "sentence": "We provide an efficient, open-source implementation.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-195",
            "text": [
                {
                    "index": "195-0",
                    "sentence": "Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words.",
                    "tag": "1"
                },
                {
                    "index": "195-1",
                    "sentence": "However, the underlying reasons for their strong performance have not been well explained.",
                    "tag": "1"
                },
                {
                    "index": "195-2",
                    "sentence": "In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax.",
                    "tag": "2+3"
                },
                {
                    "index": "195-3",
                    "sentence": "Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs.",
                    "tag": "4"
                },
                {
                    "index": "195-4",
                    "sentence": "Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling.",
                    "tag": "4"
                },
                {
                    "index": "195-5",
                    "sentence": "Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-196",
            "text": [
                {
                    "index": "196-0",
                    "sentence": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.",
                    "tag": "1"
                },
                {
                    "index": "196-1",
                    "sentence": "Could ordering the sublayers in a different pattern lead to better performance?",
                    "tag": "1"
                },
                {
                    "index": "196-2",
                    "sentence": "We generate randomly ordered transformers and train them with the language modeling objective.",
                    "tag": "2+3"
                },
                {
                    "index": "196-3",
                    "sentence": "We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.",
                    "tag": "4"
                },
                {
                    "index": "196-4",
                    "sentence": "We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.",
                    "tag": "4"
                },
                {
                    "index": "196-5",
                    "sentence": "However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.",
                    "tag": "4"
                },
                {
                    "index": "196-6",
                    "sentence": "Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-197",
            "text": [
                {
                    "index": "197-0",
                    "sentence": "Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort.",
                    "tag": "1"
                },
                {
                    "index": "197-1",
                    "sentence": "In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.",
                    "tag": "2"
                },
                {
                    "index": "197-2",
                    "sentence": "Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors.",
                    "tag": "3"
                },
                {
                    "index": "197-3",
                    "sentence": "Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-198",
            "text": [
                {
                    "index": "198-0",
                    "sentence": "Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity.",
                    "tag": "1"
                },
                {
                    "index": "198-1",
                    "sentence": "In this situation, transferring from seen classes to unseen classes is extremely hard.",
                    "tag": "1"
                },
                {
                    "index": "198-2",
                    "sentence": "To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data.",
                    "tag": "2"
                },
                {
                    "index": "198-3",
                    "sentence": "Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets.",
                    "tag": "3"
                },
                {
                    "index": "198-4",
                    "sentence": "We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection.",
                    "tag": "3"
                },
                {
                    "index": "198-5",
                    "sentence": "Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-199",
            "text": [
                {
                    "index": "199-0",
                    "sentence": "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.",
                    "tag": "1"
                },
                {
                    "index": "199-1",
                    "sentence": "However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.",
                    "tag": "1"
                },
                {
                    "index": "199-2",
                    "sentence": "To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.",
                    "tag": "2+3"
                },
                {
                    "index": "199-3",
                    "sentence": "Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).",
                    "tag": "3"
                },
                {
                    "index": "199-4",
                    "sentence": "We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.",
                    "tag": "3"
                },
                {
                    "index": "199-5",
                    "sentence": "Finally, these representations provide an attention-based context vector for the decoder.",
                    "tag": "3"
                },
                {
                    "index": "199-6",
                    "sentence": "We evaluate our proposed encoder on the Multi30K datasets.",
                    "tag": "3"
                },
                {
                    "index": "199-7",
                    "sentence": "Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-200",
            "text": [
                {
                    "index": "200-0",
                    "sentence": "Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest.",
                    "tag": "1"
                },
                {
                    "index": "200-1",
                    "sentence": "One of the crucial parts in methods for the BLI task is the matching procedure.",
                    "tag": "1"
                },
                {
                    "index": "200-2",
                    "sentence": "Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings.",
                    "tag": "1"
                },
                {
                    "index": "200-3",
                    "sentence": "Thus We propose a relaxed matching procedure to find a more precise matching between two languages.",
                    "tag": "2"
                },
                {
                    "index": "200-4",
                    "sentence": "We also find that aligning source and target language embedding space bidirectionally will bring significant improvement.",
                    "tag": "3"
                },
                {
                    "index": "200-5",
                    "sentence": "We follow the previous iterative framework to conduct experiments.",
                    "tag": "3"
                },
                {
                    "index": "200-6",
                    "sentence": "Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-201",
            "text": [
                {
                    "index": "201-0",
                    "sentence": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units.",
                    "tag": "2"
                },
                {
                    "index": "201-1",
                    "sentence": "We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference.",
                    "tag": "3"
                },
                {
                    "index": "201-2",
                    "sentence": "A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability.",
                    "tag": "3"
                },
                {
                    "index": "201-3",
                    "sentence": "DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.",
                    "tag": "3"
                },
                {
                    "index": "201-4",
                    "sentence": "Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences.",
                    "tag": "4"
                },
                {
                    "index": "201-5",
                    "sentence": "DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-202",
            "text": [
                {
                    "index": "202-0",
                    "sentence": "We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages.",
                    "tag": "1+2"
                },
                {
                    "index": "202-1",
                    "sentence": "Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices.",
                    "tag": "3"
                },
                {
                    "index": "202-2",
                    "sentence": "This viewpoint arises from the aim to align the second order information of the two language spaces.",
                    "tag": "3"
                },
                {
                    "index": "202-3",
                    "sentence": "The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation.",
                    "tag": "3"
                },
                {
                    "index": "202-4",
                    "sentence": "Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs.",
                    "tag": "4"
                },
                {
                    "index": "202-5",
                    "sentence": "The performance improvement is more significant for distant language pairs.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-203",
            "text": [
                {
                    "index": "203-0",
                    "sentence": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process.",
                    "tag": "1"
                },
                {
                    "index": "203-1",
                    "sentence": "However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing.",
                    "tag": "1"
                },
                {
                    "index": "203-2",
                    "sentence": "To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments.",
                    "tag": "2"
                },
                {
                    "index": "203-3",
                    "sentence": "The segments are generated simultaneously while each segment is predicted token-by-token.",
                    "tag": "3"
                },
                {
                    "index": "203-4",
                    "sentence": "By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors.",
                    "tag": "3"
                },
                {
                    "index": "203-5",
                    "sentence": "Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-204",
            "text": [
                {
                    "index": "204-0",
                    "sentence": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output.",
                    "tag": "1"
                },
                {
                    "index": "204-1",
                    "sentence": "While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference.",
                    "tag": "1"
                },
                {
                    "index": "204-2",
                    "sentence": "By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models.",
                    "tag": "2+3"
                },
                {
                    "index": "204-3",
                    "sentence": "Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-205",
            "text": [
                {
                    "index": "205-0",
                    "sentence": "We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task.",
                    "tag": "1+2"
                },
                {
                    "index": "205-1",
                    "sentence": "A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation.",
                    "tag": "4"
                },
                {
                    "index": "205-2",
                    "sentence": "A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation.",
                    "tag": "4"
                },
                {
                    "index": "205-3",
                    "sentence": "The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task.",
                    "tag": "4+5"
                },
                {
                    "index": "205-4",
                    "sentence": "To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-206",
            "text": [
                {
                    "index": "206-0",
                    "sentence": "Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public.",
                    "tag": "1"
                },
                {
                    "index": "206-1",
                    "sentence": "In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged.",
                    "tag": "1"
                },
                {
                    "index": "206-2",
                    "sentence": "To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems.",
                    "tag": "1"
                },
                {
                    "index": "206-3",
                    "sentence": "In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.",
                    "tag": "1+2"
                },
                {
                    "index": "206-4",
                    "sentence": "To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions.",
                    "tag": "3"
                },
                {
                    "index": "206-5",
                    "sentence": "Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-207",
            "text": [
                {
                    "index": "207-0",
                    "sentence": "Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think.",
                    "tag": "1"
                },
                {
                    "index": "207-1",
                    "sentence": "It is challenging to specify the level of education, experience, relevant skills per the company information and job description.",
                    "tag": "1"
                },
                {
                    "index": "207-2",
                    "sentence": "To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions.",
                    "tag": "1+2"
                },
                {
                    "index": "207-3",
                    "sentence": "To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA.",
                    "tag": "3"
                },
                {
                    "index": "207-4",
                    "sentence": "Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels.",
                    "tag": "3"
                },
                {
                    "index": "207-5",
                    "sentence": "At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results.",
                    "tag": "3"
                },
                {
                    "index": "207-6",
                    "sentence": "The proposed approach is evaluated on real-world job posting data.",
                    "tag": "4"
                },
                {
                    "index": "207-7",
                    "sentence": "Experimental results clearly demonstrate the effectiveness of the proposed method.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-208",
            "text": [
                {
                    "index": "208-0",
                    "sentence": "The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code.",
                    "tag": "1"
                },
                {
                    "index": "208-1",
                    "sentence": "ICD coding aims to assign proper ICD codes to a medical record.",
                    "tag": "1"
                },
                {
                    "index": "208-2",
                    "sentence": "Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.",
                    "tag": "1"
                },
                {
                    "index": "208-3",
                    "sentence": "However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence.",
                    "tag": "1"
                },
                {
                    "index": "208-4",
                    "sentence": "In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem.",
                    "tag": "2"
                },
                {
                    "index": "208-5",
                    "sentence": "Specifically, we propose a hyperbolic representation method to leverage the code hierarchy.",
                    "tag": "2"
                },
                {
                    "index": "208-6",
                    "sentence": "Moreover, we propose a graph convolutional network to utilize the code co-occurrence.",
                    "tag": "2"
                },
                {
                    "index": "208-7",
                    "sentence": "Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-209",
            "text": [
                {
                    "index": "209-0",
                    "sentence": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling).",
                    "tag": "1"
                },
                {
                    "index": "209-1",
                    "sentence": "Such operations limit the performance.",
                    "tag": "1"
                },
                {
                    "index": "209-2",
                    "sentence": "For instance, a multi-label document may contain several concepts.",
                    "tag": "1"
                },
                {
                    "index": "209-3",
                    "sentence": "In this case, one vector can not sufficiently capture its salient and discriminative content.",
                    "tag": "1"
                },
                {
                    "index": "209-4",
                    "sentence": "Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.",
                    "tag": "2"
                },
                {
                    "index": "209-5",
                    "sentence": "First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents.",
                    "tag": "4"
                },
                {
                    "index": "209-6",
                    "sentence": "Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks.",
                    "tag": "4"
                },
                {
                    "index": "209-7",
                    "sentence": "To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing.",
                    "tag": "4"
                },
                {
                    "index": "209-8",
                    "sentence": "Extensive experiments are conducted on four benchmark datasets.",
                    "tag": "4"
                },
                {
                    "index": "209-9",
                    "sentence": "Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-210",
            "text": [
                {
                    "index": "210-0",
                    "sentence": "They typically contain user descriptions of the problem, the setup, and steps for attempted resolution.",
                    "tag": "1"
                },
                {
                    "index": "210-1",
                    "sentence": "Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces.",
                    "tag": "1"
                },
                {
                    "index": "210-2",
                    "sentence": "These elements contain potentially crucial information for problem resolution.",
                    "tag": "1"
                },
                {
                    "index": "210-3",
                    "sentence": "However, they cannot be correctly parsed by tools designed for natural language.",
                    "tag": "1"
                },
                {
                    "index": "210-4",
                    "sentence": "In this paper, we address the problem of segmentation for technical support questions.",
                    "tag": "2"
                },
                {
                    "index": "210-5",
                    "sentence": "We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches.",
                    "tag": "3"
                },
                {
                    "index": "210-6",
                    "sentence": "We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach.",
                    "tag": "3"
                },
                {
                    "index": "210-7",
                    "sentence": "We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model.",
                    "tag": "3+4"
                },
                {
                    "index": "210-8",
                    "sentence": "Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-211",
            "text": [
                {
                    "index": "211-0",
                    "sentence": "The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability.",
                    "tag": "1"
                },
                {
                    "index": "211-1",
                    "sentence": "In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system.",
                    "tag": "2+3"
                },
                {
                    "index": "211-2",
                    "sentence": "The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare.",
                    "tag": "4"
                },
                {
                    "index": "211-3",
                    "sentence": "The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-212",
            "text": [
                {
                    "index": "212-0",
                    "sentence": "In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis.",
                    "tag": "1"
                },
                {
                    "index": "212-1",
                    "sentence": "It aims at extracting the potential pairs of emotions and their corresponding causes in a document.",
                    "tag": "1"
                },
                {
                    "index": "212-2",
                    "sentence": "To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes.",
                    "tag": "1"
                },
                {
                    "index": "212-3",
                    "sentence": "However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step.",
                    "tag": "1"
                },
                {
                    "index": "212-4",
                    "sentence": "To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.",
                    "tag": "2"
                },
                {
                    "index": "212-5",
                    "sentence": "A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs.",
                    "tag": "3"
                },
                {
                    "index": "212-6",
                    "sentence": "The 2D representation, interaction, and prediction are integrated into a joint framework.",
                    "tag": "3"
                },
                {
                    "index": "212-7",
                    "sentence": "In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-213",
            "text": [
                {
                    "index": "213-0",
                    "sentence": "Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document.",
                    "tag": "1"
                },
                {
                    "index": "213-1",
                    "sentence": "Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.",
                    "tag": "1"
                },
                {
                    "index": "213-2",
                    "sentence": "However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well.",
                    "tag": "1"
                },
                {
                    "index": "213-3",
                    "sentence": "In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction.",
                    "tag": "2+3"
                },
                {
                    "index": "213-4",
                    "sentence": "It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking.",
                    "tag": "3"
                },
                {
                    "index": "213-5",
                    "sentence": "Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-214",
            "text": [
                {
                    "index": "214-0",
                    "sentence": "We present a simple but effective method for aspect identification in sentiment analysis.",
                    "tag": "1+2"
                },
                {
                    "index": "214-1",
                    "sentence": "Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages.",
                    "tag": "3"
                },
                {
                    "index": "214-2",
                    "sentence": "We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable.",
                    "tag": "3"
                },
                {
                    "index": "214-3",
                    "sentence": "Previous work relied on syntactic features and complex neural models.",
                    "tag": "3"
                },
                {
                    "index": "214-4",
                    "sentence": "We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed.",
                    "tag": "4"
                },
                {
                    "index": "214-5",
                    "sentence": "The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-215",
            "text": [
                {
                    "index": "215-0",
                    "sentence": "Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target.",
                    "tag": "1"
                },
                {
                    "index": "215-1",
                    "sentence": "Remarkable success has been achieved when sufficient labeled training data is available.",
                    "tag": "1"
                },
                {
                    "index": "215-2",
                    "sentence": "However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets.",
                    "tag": "1"
                },
                {
                    "index": "215-3",
                    "sentence": "In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets.",
                    "tag": "1+2"
                },
                {
                    "index": "215-4",
                    "sentence": "Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags.",
                    "tag": "3"
                },
                {
                    "index": "215-5",
                    "sentence": "Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell.",
                    "tag": "3"
                },
                {
                    "index": "215-6",
                    "sentence": "Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-216",
            "text": [
                {
                    "index": "216-0",
                    "sentence": "Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis.",
                    "tag": "1"
                },
                {
                    "index": "216-1",
                    "sentence": "In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge.",
                    "tag": "2"
                },
                {
                    "index": "216-2",
                    "sentence": "We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts.",
                    "tag": "3"
                },
                {
                    "index": "216-3",
                    "sentence": "These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner.",
                    "tag": "3"
                },
                {
                    "index": "216-4",
                    "sentence": "Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-217",
            "text": [
                {
                    "index": "217-0",
                    "sentence": "The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification.",
                    "tag": "1"
                },
                {
                    "index": "217-1",
                    "sentence": "Rather than considering the tasks separately, we build an end-to-end ABSA solution.",
                    "tag": "1+2"
                },
                {
                    "index": "217-2",
                    "sentence": "Previous works in ABSA tasks did not fully leverage the importance of syntactical information.",
                    "tag": "1"
                },
                {
                    "index": "217-3",
                    "sentence": "Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms.",
                    "tag": "1"
                },
                {
                    "index": "217-4",
                    "sentence": "On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words.",
                    "tag": "1"
                },
                {
                    "index": "217-5",
                    "sentence": "This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning.",
                    "tag": "1+2"
                },
                {
                    "index": "217-6",
                    "sentence": "We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor.",
                    "tag": "3"
                },
                {
                    "index": "217-7",
                    "sentence": "We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms.",
                    "tag": "3"
                },
                {
                    "index": "217-8",
                    "sentence": "This increases the accuracy of the aspect sentiment classifier.",
                    "tag": "4"
                },
                {
                    "index": "217-9",
                    "sentence": "Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-218",
            "text": [
                {
                    "index": "218-0",
                    "sentence": "Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews.",
                    "tag": "1"
                },
                {
                    "index": "218-1",
                    "sentence": "Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words.",
                    "tag": "1"
                },
                {
                    "index": "218-2",
                    "sentence": "However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections.",
                    "tag": "1"
                },
                {
                    "index": "218-3",
                    "sentence": "In this paper, we address this problem by means of effective encoding of syntax information.",
                    "tag": "2+3"
                },
                {
                    "index": "218-4",
                    "sentence": "Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree.",
                    "tag": "3"
                },
                {
                    "index": "218-5",
                    "sentence": "Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.",
                    "tag": "3"
                },
                {
                    "index": "218-6",
                    "sentence": "Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-219",
            "text": [
                {
                    "index": "219-0",
                    "sentence": "Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA).",
                    "tag": "1"
                },
                {
                    "index": "219-1",
                    "sentence": "The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems.",
                    "tag": "1"
                },
                {
                    "index": "219-2",
                    "sentence": "However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms.",
                    "tag": "1"
                },
                {
                    "index": "219-3",
                    "sentence": "Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs.",
                    "tag": "1"
                },
                {
                    "index": "219-4",
                    "sentence": "To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE).",
                    "tag": "1+2"
                },
                {
                    "index": "219-5",
                    "sentence": "Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works.",
                    "tag": "3"
                },
                {
                    "index": "219-6",
                    "sentence": "We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries.",
                    "tag": "3"
                },
                {
                    "index": "219-7",
                    "sentence": "Meanwhile, the pair-wise relations are jointly identified using the span representations.",
                    "tag": "4"
                },
                {
                    "index": "219-8",
                    "sentence": "Extensive experiments show that our model consistently outperforms state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-220",
            "text": [
                {
                    "index": "220-0",
                    "sentence": "Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”.",
                    "tag": "1"
                },
                {
                    "index": "220-1",
                    "sentence": "Due to the scarcity of labeled data, ORL remains challenging for data-driven methods.",
                    "tag": "1"
                },
                {
                    "index": "220-2",
                    "sentence": "In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations.",
                    "tag": "2+3"
                },
                {
                    "index": "220-3",
                    "sentence": "We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels.",
                    "tag": "3"
                },
                {
                    "index": "220-4",
                    "sentence": "In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously.",
                    "tag": "3"
                },
                {
                    "index": "220-5",
                    "sentence": "We verify our methods on the benchmark MPQA corpus.",
                    "tag": "3"
                },
                {
                    "index": "220-6",
                    "sentence": "The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline.",
                    "tag": "4"
                },
                {
                    "index": "220-7",
                    "sentence": "In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT).",
                    "tag": "4"
                },
                {
                    "index": "220-8",
                    "sentence": "Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-221",
            "text": [
                {
                    "index": "221-0",
                    "sentence": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures.",
                    "tag": "1"
                },
                {
                    "index": "221-1",
                    "sentence": "However, the technology for capturing non-tree-structured arguments is still in its infancy.",
                    "tag": "1"
                },
                {
                    "index": "221-2",
                    "sentence": "In this paper, we focus on non-tree argument mining with a neural model.",
                    "tag": "2"
                },
                {
                    "index": "221-3",
                    "sentence": "We jointly predict proposition types and edges between propositions.",
                    "tag": "3"
                },
                {
                    "index": "221-4",
                    "sentence": "Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges.",
                    "tag": "3"
                },
                {
                    "index": "221-5",
                    "sentence": "Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-222",
            "text": [
                {
                    "index": "222-0",
                    "sentence": "We propose a novel linearization of a constituent tree, together with a new locally normalized model.",
                    "tag": "2"
                },
                {
                    "index": "222-1",
                    "sentence": "For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them.",
                    "tag": "3"
                },
                {
                    "index": "222-2",
                    "sentence": "Compared with global models, our model is fast and parallelizable.",
                    "tag": "4"
                },
                {
                    "index": "222-3",
                    "sentence": "Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.",
                    "tag": "4"
                },
                {
                    "index": "222-4",
                    "sentence": "Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-223",
            "text": [
                {
                    "index": "223-0",
                    "sentence": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "223-1",
                    "sentence": "Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span.",
                    "tag": "3"
                },
                {
                    "index": "223-2",
                    "sentence": "Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference.",
                    "tag": "3"
                },
                {
                    "index": "223-3",
                    "sentence": "The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity.",
                    "tag": "4"
                },
                {
                    "index": "223-4",
                    "sentence": "Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-224",
            "text": [
                {
                    "index": "224-0",
                    "sentence": "In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.",
                    "tag": "1"
                },
                {
                    "index": "224-1",
                    "sentence": "As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.",
                    "tag": "1"
                },
                {
                    "index": "224-2",
                    "sentence": "This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.",
                    "tag": "2"
                },
                {
                    "index": "224-3",
                    "sentence": "For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.",
                    "tag": "3"
                },
                {
                    "index": "224-4",
                    "sentence": "To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.",
                    "tag": "3"
                },
                {
                    "index": "224-5",
                    "sentence": "Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.",
                    "tag": "4+5"
                },
                {
                    "index": "224-6",
                    "sentence": "We release our code at https://github.com/yzhangcs/crfpar.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-225",
            "text": [
                {
                    "index": "225-0",
                    "sentence": "Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks.",
                    "tag": "1"
                },
                {
                    "index": "225-1",
                    "sentence": "Such tree-based networks can be provided with a constituency parse, a dependency parse, or both.",
                    "tag": "1"
                },
                {
                    "index": "225-2",
                    "sentence": "We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task.",
                    "tag": "2"
                },
                {
                    "index": "225-3",
                    "sentence": "We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement.",
                    "tag": "4"
                },
                {
                    "index": "225-4",
                    "sentence": "Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-226",
            "text": [
                {
                    "index": "226-0",
                    "sentence": "Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages.",
                    "tag": "1"
                },
                {
                    "index": "226-1",
                    "sentence": "Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages.",
                    "tag": "1"
                },
                {
                    "index": "226-2",
                    "sentence": "However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations.",
                    "tag": "1"
                },
                {
                    "index": "226-3",
                    "sentence": "In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).",
                    "tag": "2+3"
                },
                {
                    "index": "226-4",
                    "sentence": "We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions.",
                    "tag": "3"
                },
                {
                    "index": "226-5",
                    "sentence": "Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-227",
            "text": [
                {
                    "index": "227-0",
                    "sentence": "Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner.",
                    "tag": "1"
                },
                {
                    "index": "227-1",
                    "sentence": "Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests.",
                    "tag": "2"
                },
                {
                    "index": "227-2",
                    "sentence": "While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time.",
                    "tag": "4"
                },
                {
                    "index": "227-3",
                    "sentence": "Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter.",
                    "tag": "3+4"
                },
                {
                    "index": "227-4",
                    "sentence": "We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests.",
                    "tag": "4"
                },
                {
                    "index": "227-5",
                    "sentence": "We further evaluate on handling “cold start”, and observe consistently better performance by our model when considering various degrees of sparsity of user’s chatting history and conversation contexts.",
                    "tag": "3+4"
                },
                {
                    "index": "227-6",
                    "sentence": "Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-228",
            "text": [
                {
                    "index": "228-0",
                    "sentence": "In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.",
                    "tag": "2"
                },
                {
                    "index": "228-1",
                    "sentence": "Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context.",
                    "tag": "1"
                },
                {
                    "index": "228-2",
                    "sentence": "To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations.",
                    "tag": "2"
                },
                {
                    "index": "228-3",
                    "sentence": "To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions.",
                    "tag": "3"
                },
                {
                    "index": "228-4",
                    "sentence": "Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-229",
            "text": [
                {
                    "index": "229-0",
                    "sentence": "Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements.",
                    "tag": "1"
                },
                {
                    "index": "229-1",
                    "sentence": "The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction.",
                    "tag": "1"
                },
                {
                    "index": "229-2",
                    "sentence": "In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding.",
                    "tag": "2+3"
                },
                {
                    "index": "229-3",
                    "sentence": "The stock embedding is acquired with a deep learning framework using both news articles and price history.",
                    "tag": "3"
                },
                {
                    "index": "229-4",
                    "sentence": "Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction.",
                    "tag": "3"
                },
                {
                    "index": "229-5",
                    "sentence": "As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data.",
                    "tag": "4"
                },
                {
                    "index": "229-6",
                    "sentence": "This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-230",
            "text": [
                {
                    "index": "230-0",
                    "sentence": "Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction.",
                    "tag": "1"
                },
                {
                    "index": "230-1",
                    "sentence": "The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically.",
                    "tag": "1"
                },
                {
                    "index": "230-2",
                    "sentence": "Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content.",
                    "tag": "1"
                },
                {
                    "index": "230-3",
                    "sentence": "This makes it possible to detect likely “fake news” the moment they are published, by simply checking the reliability of their source.",
                    "tag": "1"
                },
                {
                    "index": "230-4",
                    "sentence": "From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context.",
                    "tag": "1"
                },
                {
                    "index": "230-5",
                    "sentence": "Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium’s audience on social media).",
                    "tag": "2"
                },
                {
                    "index": "230-6",
                    "sentence": "We further study (iii) what was written about the target medium (in Wikipedia).",
                    "tag": "2"
                },
                {
                    "index": "230-7",
                    "sentence": "The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-231",
            "text": [
                {
                    "index": "231-0",
                    "sentence": "We explore the utilities of explicit negative examples in training neural language models.",
                    "tag": "1"
                },
                {
                    "index": "231-1",
                    "sentence": "Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks.",
                    "tag": "1"
                },
                {
                    "index": "231-2",
                    "sentence": "Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.",
                    "tag": "1"
                },
                {
                    "index": "231-3",
                    "sentence": "In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity.",
                    "tag": "2"
                },
                {
                    "index": "231-4",
                    "sentence": "The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word.",
                    "tag": "3"
                },
                {
                    "index": "231-5",
                    "sentence": "We then provide a detailed analysis of the trained models.",
                    "tag": "3"
                },
                {
                    "index": "231-6",
                    "sentence": "One of our findings is the difficulty of object-relative clauses for RNNs.",
                    "tag": "4"
                },
                {
                    "index": "231-7",
                    "sentence": "We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause.",
                    "tag": "4"
                },
                {
                    "index": "231-8",
                    "sentence": "Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses.",
                    "tag": "4"
                },
                {
                    "index": "231-9",
                    "sentence": "Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-232",
            "text": [
                {
                    "index": "232-0",
                    "sentence": "We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors.",
                    "tag": "2"
                },
                {
                    "index": "232-1",
                    "sentence": "Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data.",
                    "tag": "3"
                },
                {
                    "index": "232-2",
                    "sentence": "We use this approach to facilitate debugging models on downstream applications.",
                    "tag": "4"
                },
                {
                    "index": "232-3",
                    "sentence": "Results confirm that the performance of all tested models is affected but the degree of impact varies.",
                    "tag": "4"
                },
                {
                    "index": "232-4",
                    "sentence": "To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors.",
                    "tag": "4"
                },
                {
                    "index": "232-5",
                    "sentence": "We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions.",
                    "tag": "4"
                },
                {
                    "index": "232-6",
                    "sentence": "We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context.",
                    "tag": "4"
                },
                {
                    "index": "232-7",
                    "sentence": "Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-233",
            "text": [
                {
                    "index": "233-0",
                    "sentence": "Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks.",
                    "tag": "1"
                },
                {
                    "index": "233-1",
                    "sentence": "The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture.",
                    "tag": "1"
                },
                {
                    "index": "233-2",
                    "sentence": "However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model.",
                    "tag": "1"
                },
                {
                    "index": "233-3",
                    "sentence": "For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA).",
                    "tag": "2+3"
                },
                {
                    "index": "233-4",
                    "sentence": "Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-234",
            "text": [
                {
                    "index": "234-0",
                    "sentence": "Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems.",
                    "tag": "1"
                },
                {
                    "index": "234-1",
                    "sentence": "To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples.",
                    "tag": "2+3"
                },
                {
                    "index": "234-2",
                    "sentence": "R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism.",
                    "tag": "3"
                },
                {
                    "index": "234-3",
                    "sentence": "Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector.",
                    "tag": "3"
                },
                {
                    "index": "234-4",
                    "sentence": "Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple.",
                    "tag": "3"
                },
                {
                    "index": "234-5",
                    "sentence": "Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-235",
            "text": [
                {
                    "index": "235-0",
                    "sentence": "It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.",
                    "tag": "1"
                },
                {
                    "index": "235-1",
                    "sentence": "In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem.",
                    "tag": "1"
                },
                {
                    "index": "235-2",
                    "sentence": "In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones.",
                    "tag": "2"
                },
                {
                    "index": "235-3",
                    "sentence": "Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-236",
            "text": [
                {
                    "index": "236-0",
                    "sentence": "Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese.",
                    "tag": "1"
                },
                {
                    "index": "236-1",
                    "sentence": "Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models.",
                    "tag": "1+2"
                },
                {
                    "index": "236-2",
                    "sentence": "Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion.",
                    "tag": "3"
                },
                {
                    "index": "236-3",
                    "sentence": "As a result, word and character information are explicitly integrated at the fine-tuning procedure.",
                    "tag": "3"
                },
                {
                    "index": "236-4",
                    "sentence": "Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-237",
            "text": [
                {
                    "index": "237-0",
                    "sentence": "Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling.",
                    "tag": "1"
                },
                {
                    "index": "237-1",
                    "sentence": "By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold.",
                    "tag": "1"
                },
                {
                    "index": "237-2",
                    "sentence": "We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them.",
                    "tag": "1+2"
                },
                {
                    "index": "237-3",
                    "sentence": "To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.",
                    "tag": "3"
                },
                {
                    "index": "237-4",
                    "sentence": "We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy.",
                    "tag": "3"
                },
                {
                    "index": "237-5",
                    "sentence": "Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space.",
                    "tag": "4"
                },
                {
                    "index": "237-6",
                    "sentence": "We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-238",
            "text": [
                {
                    "index": "238-0",
                    "sentence": "State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution.",
                    "tag": "1"
                },
                {
                    "index": "238-1",
                    "sentence": "For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution.",
                    "tag": "1"
                },
                {
                    "index": "238-2",
                    "sentence": "In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness.",
                    "tag": "2+3"
                },
                {
                    "index": "238-3",
                    "sentence": "Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level).",
                    "tag": "4"
                },
                {
                    "index": "238-4",
                    "sentence": "Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks.",
                    "tag": "4"
                },
                {
                    "index": "238-5",
                    "sentence": "To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-239",
            "text": [
                {
                    "index": "239-0",
                    "sentence": "Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages.",
                    "tag": "1"
                },
                {
                    "index": "239-1",
                    "sentence": "Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages.",
                    "tag": "1"
                },
                {
                    "index": "239-2",
                    "sentence": "However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words.",
                    "tag": "1"
                },
                {
                    "index": "239-3",
                    "sentence": "To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way.",
                    "tag": "2"
                },
                {
                    "index": "239-4",
                    "sentence": "We first build a graph for each language with its vertices representing different words.",
                    "tag": "3"
                },
                {
                    "index": "239-5",
                    "sentence": "Then we extract word cliques from the graphs and map the cliques of two languages.",
                    "tag": "3"
                },
                {
                    "index": "239-6",
                    "sentence": "Based on that, we induce the initial word translation solution with the central words of the aligned cliques.",
                    "tag": "3"
                },
                {
                    "index": "239-7",
                    "sentence": "This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings.",
                    "tag": "4"
                },
                {
                    "index": "239-8",
                    "sentence": "Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons.",
                    "tag": "3"
                },
                {
                    "index": "239-9",
                    "sentence": "Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-240",
            "text": [
                {
                    "index": "240-0",
                    "sentence": "Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance.",
                    "tag": "1"
                },
                {
                    "index": "240-1",
                    "sentence": "Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning.",
                    "tag": "2+3"
                },
                {
                    "index": "240-2",
                    "sentence": "Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture.",
                    "tag": "3"
                },
                {
                    "index": "240-3",
                    "sentence": "We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer.",
                    "tag": "3"
                },
                {
                    "index": "240-4",
                    "sentence": "The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples.",
                    "tag": "4"
                },
                {
                    "index": "240-5",
                    "sentence": "We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-241",
            "text": [
                {
                    "index": "241-0",
                    "sentence": "The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance.",
                    "tag": "1"
                },
                {
                    "index": "241-1",
                    "sentence": "The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance.",
                    "tag": "1"
                },
                {
                    "index": "241-2",
                    "sentence": "In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.",
                    "tag": "1+2"
                },
                {
                    "index": "241-3",
                    "sentence": "We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model.",
                    "tag": "3"
                },
                {
                    "index": "241-4",
                    "sentence": "The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation.",
                    "tag": "3"
                },
                {
                    "index": "241-5",
                    "sentence": "Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.",
                    "tag": "4"
                },
                {
                    "index": "241-6",
                    "sentence": "Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-242",
            "text": [
                {
                    "index": "242-0",
                    "sentence": "Open-domain dialogue generation has gained increasing attention in Natural Language Processing.",
                    "tag": "1"
                },
                {
                    "index": "242-1",
                    "sentence": "Its evaluation requires a holistic means.",
                    "tag": "1"
                },
                {
                    "index": "242-2",
                    "sentence": "Human ratings are deemed as the gold standard.",
                    "tag": "1"
                },
                {
                    "index": "242-3",
                    "sentence": "As human evaluation is inefficient and costly, an automated substitute is highly desirable.",
                    "tag": "1"
                },
                {
                    "index": "242-4",
                    "sentence": "In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.",
                    "tag": "2"
                },
                {
                    "index": "242-5",
                    "sentence": "Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency.",
                    "tag": "3"
                },
                {
                    "index": "242-6",
                    "sentence": "The empirical validity of our metrics is demonstrated by strong correlations with human judgments.",
                    "tag": "4"
                },
                {
                    "index": "242-7",
                    "sentence": "We open source the code and relevant materials.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-243",
            "text": [
                {
                    "index": "243-0",
                    "sentence": "The hypernymy detection task has been addressed under various frameworks.",
                    "tag": "1"
                },
                {
                    "index": "243-1",
                    "sentence": "Previously, the design of unsupervised hypernymy scores has been extensively studied.",
                    "tag": "1"
                },
                {
                    "index": "243-2",
                    "sentence": "In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”.",
                    "tag": "1"
                },
                {
                    "index": "243-3",
                    "sentence": "In this work, we revisit supervised distributional models for hypernymy detection.",
                    "tag": "1+2"
                },
                {
                    "index": "243-4",
                    "sentence": "Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE).",
                    "tag": "3"
                },
                {
                    "index": "243-5",
                    "sentence": "In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations.",
                    "tag": "3"
                },
                {
                    "index": "243-6",
                    "sentence": "A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections.",
                    "tag": "3"
                },
                {
                    "index": "243-7",
                    "sentence": "Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-244",
            "text": [
                {
                    "index": "244-0",
                    "sentence": "Biomedical named entities often play important roles in many biomedical text mining tools.",
                    "tag": "1"
                },
                {
                    "index": "244-1",
                    "sentence": "However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging.",
                    "tag": "1"
                },
                {
                    "index": "244-2",
                    "sentence": "In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities.",
                    "tag": "2"
                },
                {
                    "index": "244-3",
                    "sentence": "To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.",
                    "tag": "3"
                },
                {
                    "index": "244-4",
                    "sentence": "Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves.",
                    "tag": "3"
                },
                {
                    "index": "244-5",
                    "sentence": "In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates.",
                    "tag": "3"
                },
                {
                    "index": "244-6",
                    "sentence": "On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-245",
            "text": [
                {
                    "index": "245-0",
                    "sentence": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks.",
                    "tag": "1"
                },
                {
                    "index": "245-1",
                    "sentence": "Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios.",
                    "tag": "1"
                },
                {
                    "index": "245-2",
                    "sentence": "This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.",
                    "tag": "2"
                },
                {
                    "index": "245-3",
                    "sentence": "We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.",
                    "tag": "3"
                },
                {
                    "index": "245-4",
                    "sentence": "Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-246",
            "text": [
                {
                    "index": "246-0",
                    "sentence": "This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space.",
                    "tag": "2"
                },
                {
                    "index": "246-1",
                    "sentence": "To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model.",
                    "tag": "3"
                },
                {
                    "index": "246-2",
                    "sentence": "Specifically, we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class.",
                    "tag": "3"
                },
                {
                    "index": "246-3",
                    "sentence": "Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution.",
                    "tag": "4"
                },
                {
                    "index": "246-4",
                    "sentence": "We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-247",
            "text": [
                {
                    "index": "247-0",
                    "sentence": "In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC.",
                    "tag": "1"
                },
                {
                    "index": "247-1",
                    "sentence": "In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency).",
                    "tag": "2"
                },
                {
                    "index": "247-2",
                    "sentence": "On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation.",
                    "tag": "3"
                },
                {
                    "index": "247-3",
                    "sentence": "Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference.",
                    "tag": "3"
                },
                {
                    "index": "247-4",
                    "sentence": "Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines.",
                    "tag": "4"
                },
                {
                    "index": "247-5",
                    "sentence": "This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-248",
            "text": [
                {
                    "index": "248-0",
                    "sentence": "The current aspect extraction methods suffer from boundary errors.",
                    "tag": "1"
                },
                {
                    "index": "248-1",
                    "sentence": "In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth.",
                    "tag": "1"
                },
                {
                    "index": "248-2",
                    "sentence": "However, they hurt the performance severely.",
                    "tag": "1"
                },
                {
                    "index": "248-3",
                    "sentence": "In this paper, we propose to utilize a pointer network for repositioning the boundaries.",
                    "tag": "1+2"
                },
                {
                    "index": "248-4",
                    "sentence": "Recycling mechanism is used, which enables the training data to be collected without manual intervention.",
                    "tag": "4"
                },
                {
                    "index": "248-5",
                    "sentence": "We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant.",
                    "tag": "3"
                },
                {
                    "index": "248-6",
                    "sentence": "Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-249",
            "text": [
                {
                    "index": "249-0",
                    "sentence": "Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification.",
                    "tag": "1"
                },
                {
                    "index": "249-1",
                    "sentence": "Most existing studies focused on one of these subtasks only.",
                    "tag": "1"
                },
                {
                    "index": "249-2",
                    "sentence": "Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework.",
                    "tag": "1"
                },
                {
                    "index": "249-3",
                    "sentence": "However, the interactive relations among three subtasks are still under-exploited.",
                    "tag": "1"
                },
                {
                    "index": "249-4",
                    "sentence": "We argue that such relations encode collaborative signals between different subtasks.",
                    "tag": "2"
                },
                {
                    "index": "249-5",
                    "sentence": "For example, when the opinion term is “delicious”, the aspect term must be “food” rather than “place”.",
                    "tag": "3"
                },
                {
                    "index": "249-6",
                    "sentence": "In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network.",
                    "tag": "3"
                },
                {
                    "index": "249-7",
                    "sentence": "Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-250",
            "text": [
                {
                    "index": "250-0",
                    "sentence": "We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.",
                    "tag": "1+2"
                },
                {
                    "index": "250-1",
                    "sentence": "The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition.",
                    "tag": "3"
                },
                {
                    "index": "250-2",
                    "sentence": "Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification.",
                    "tag": "4"
                },
                {
                    "index": "250-3",
                    "sentence": "We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks.",
                    "tag": "5"
                },
                {
                    "index": "250-4",
                    "sentence": "Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT.",
                    "tag": "5"
                },
                {
                    "index": "250-5",
                    "sentence": "We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-251",
            "text": [
                {
                    "index": "251-0",
                    "sentence": "Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text.",
                    "tag": "1"
                },
                {
                    "index": "251-1",
                    "sentence": "Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation.",
                    "tag": "1"
                },
                {
                    "index": "251-2",
                    "sentence": "Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction.",
                    "tag": "1+2"
                },
                {
                    "index": "251-3",
                    "sentence": "The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently.",
                    "tag": "3+4"
                },
                {
                    "index": "251-4",
                    "sentence": "Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-252",
            "text": [
                {
                    "index": "252-0",
                    "sentence": "End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously.",
                    "tag": "1"
                },
                {
                    "index": "252-1",
                    "sentence": "To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features.",
                    "tag": "1"
                },
                {
                    "index": "252-2",
                    "sentence": "However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered.",
                    "tag": "1+2"
                },
                {
                    "index": "252-3",
                    "sentence": "Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.",
                    "tag": "3"
                },
                {
                    "index": "252-4",
                    "sentence": "The difficulty of these courses is gradually increasing.",
                    "tag": "4"
                },
                {
                    "index": "252-5",
                    "sentence": "Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-253",
            "text": [
                {
                    "index": "253-0",
                    "sentence": "In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system.",
                    "tag": "2"
                },
                {
                    "index": "253-1",
                    "sentence": "We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents.",
                    "tag": "3"
                },
                {
                    "index": "253-2",
                    "sentence": "We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers.",
                    "tag": "3"
                },
                {
                    "index": "253-3",
                    "sentence": "We find different accents exhibiting similar trends irrespective of the probing technique used.",
                    "tag": "4"
                },
                {
                    "index": "253-4",
                    "sentence": "We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-254",
            "text": [
                {
                    "index": "254-0",
                    "sentence": "Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts.",
                    "tag": "1"
                },
                {
                    "index": "254-1",
                    "sentence": "Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant.",
                    "tag": "1"
                },
                {
                    "index": "254-2",
                    "sentence": "However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations.",
                    "tag": "2+3"
                },
                {
                    "index": "254-3",
                    "sentence": "We also show that ensembling self-trained parsers provides further gains for disfluency detection.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-255",
            "text": [
                {
                    "index": "255-0",
                    "sentence": "Pre-trained language models have achieved huge improvement on many NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "255-1",
                    "sentence": "However, these methods are usually designed for written text, so they do not consider the properties of spoken language.",
                    "tag": "1"
                },
                {
                    "index": "255-2",
                    "sentence": "Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems.",
                    "tag": "2"
                },
                {
                    "index": "255-3",
                    "sentence": "We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks.",
                    "tag": "3"
                },
                {
                    "index": "255-4",
                    "sentence": "The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency.",
                    "tag": "3+4"
                },
                {
                    "index": "255-5",
                    "sentence": "Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.",
                    "tag": "4"
                },
                {
                    "index": "255-6",
                    "sentence": "The code is available at https://github.com/MiuLab/Lattice-ELMo.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-256",
            "text": [
                {
                    "index": "256-0",
                    "sentence": "An increasing number of people in the world today speak a mixed-language as a result of being multilingual.",
                    "tag": "1"
                },
                {
                    "index": "256-1",
                    "sentence": "However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data.",
                    "tag": "1"
                },
                {
                    "index": "256-2",
                    "sentence": "We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.",
                    "tag": "2+3"
                },
                {
                    "index": "256-3",
                    "sentence": "Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data.",
                    "tag": "3"
                },
                {
                    "index": "256-4",
                    "sentence": "Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-257",
            "text": [
                {
                    "index": "257-0",
                    "sentence": "Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means.",
                    "tag": "1"
                },
                {
                    "index": "257-1",
                    "sentence": "With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms.",
                    "tag": "1"
                },
                {
                    "index": "257-2",
                    "sentence": "In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions.",
                    "tag": "1"
                },
                {
                    "index": "257-3",
                    "sentence": "Thus traditional text-based methods are insufficient to detect multimodal sarcasm.",
                    "tag": "1"
                },
                {
                    "index": "257-4",
                    "sentence": "To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context.",
                    "tag": "2+3"
                },
                {
                    "index": "257-5",
                    "sentence": "Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net).",
                    "tag": "3"
                },
                {
                    "index": "257-6",
                    "sentence": "The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context.",
                    "tag": "3"
                },
                {
                    "index": "257-7",
                    "sentence": "Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-258",
            "text": [
                {
                    "index": "258-0",
                    "sentence": "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently.",
                    "tag": "2"
                },
                {
                    "index": "258-1",
                    "sentence": "SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation.",
                    "tag": "3"
                },
                {
                    "index": "258-2",
                    "sentence": "SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)).",
                    "tag": "4"
                },
                {
                    "index": "258-3",
                    "sentence": "We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech.",
                    "tag": "3"
                },
                {
                    "index": "258-4",
                    "sentence": "Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-259",
            "text": [
                {
                    "index": "259-0",
                    "sentence": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR).",
                    "tag": "1"
                },
                {
                    "index": "259-1",
                    "sentence": "We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding.",
                    "tag": "2"
                },
                {
                    "index": "259-2",
                    "sentence": "Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features.",
                    "tag": "2"
                },
                {
                    "index": "259-3",
                    "sentence": "This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec.",
                    "tag": "3"
                },
                {
                    "index": "259-4",
                    "sentence": "Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels.",
                    "tag": "3"
                },
                {
                    "index": "259-5",
                    "sentence": "We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-260",
            "text": [
                {
                    "index": "260-0",
                    "sentence": "Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context.",
                    "tag": "1"
                },
                {
                    "index": "260-1",
                    "sentence": "In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours.",
                    "tag": "2+3"
                },
                {
                    "index": "260-2",
                    "sentence": "We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context.",
                    "tag": "2+3"
                },
                {
                    "index": "260-3",
                    "sentence": "Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-261",
            "text": [
                {
                    "index": "261-0",
                    "sentence": "Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support.",
                    "tag": "1"
                },
                {
                    "index": "261-1",
                    "sentence": "But trust can be betrayed through deception.",
                    "tag": "1"
                },
                {
                    "index": "261-2",
                    "sentence": "We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other.",
                    "tag": "2+3"
                },
                {
                    "index": "261-3",
                    "sentence": "Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.",
                    "tag": "3"
                },
                {
                    "index": "261-4",
                    "sentence": "Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives.",
                    "tag": "2+3"
                },
                {
                    "index": "261-5",
                    "sentence": "A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-262",
            "text": [
                {
                    "index": "262-0",
                    "sentence": "Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks.",
                    "tag": "1"
                },
                {
                    "index": "262-1",
                    "sentence": "In this paper, we present new GFMN formulations that are effective for sequential data.",
                    "tag": "2"
                },
                {
                    "index": "262-2",
                    "sentence": "Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer.",
                    "tag": "4+5"
                },
                {
                    "index": "262-3",
                    "sentence": "SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-263",
            "text": [
                {
                    "index": "263-0",
                    "sentence": "A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures.",
                    "tag": "1"
                },
                {
                    "index": "263-1",
                    "sentence": "In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "263-2",
                    "sentence": "In this work, we show that this is also the case for text generation from structured and unstructured data.",
                    "tag": "2"
                },
                {
                    "index": "263-3",
                    "sentence": "We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.",
                    "tag": "3"
                },
                {
                    "index": "263-4",
                    "sentence": "Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models.",
                    "tag": "4"
                },
                {
                    "index": "263-5",
                    "sentence": "Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks.",
                    "tag": "4"
                },
                {
                    "index": "263-6",
                    "sentence": "Code is available at https://github.com/h-shahidi/2birds-gen.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-264",
            "text": [
                {
                    "index": "264-0",
                    "sentence": "This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.",
                    "tag": "2"
                },
                {
                    "index": "264-1",
                    "sentence": "BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors.",
                    "tag": "4"
                },
                {
                    "index": "264-2",
                    "sentence": "By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations.",
                    "tag": "4"
                },
                {
                    "index": "264-3",
                    "sentence": "Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.",
                    "tag": "4+5"
                },
                {
                    "index": "264-4",
                    "sentence": "Finally, we further show that BHWR produces better representations for rare words.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-265",
            "text": [
                {
                    "index": "265-0",
                    "sentence": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.",
                    "tag": "1"
                },
                {
                    "index": "265-1",
                    "sentence": "Most of the existing approaches rely on a randomly initialized classifier on top of such networks.",
                    "tag": "1+2"
                },
                {
                    "index": "265-2",
                    "sentence": "We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.",
                    "tag": "1+2"
                },
                {
                    "index": "265-3",
                    "sentence": "In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.",
                    "tag": "2"
                },
                {
                    "index": "265-4",
                    "sentence": "We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.",
                    "tag": "2+3"
                },
                {
                    "index": "265-5",
                    "sentence": "By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches.",
                    "tag": "2+3"
                },
                {
                    "index": "265-6",
                    "sentence": "Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-266",
            "text": [
                {
                    "index": "266-0",
                    "sentence": "In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.",
                    "tag": "1"
                },
                {
                    "index": "266-1",
                    "sentence": "However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.",
                    "tag": "1"
                },
                {
                    "index": "266-2",
                    "sentence": "To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.",
                    "tag": "2"
                },
                {
                    "index": "266-3",
                    "sentence": "Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations.",
                    "tag": "2"
                },
                {
                    "index": "266-4",
                    "sentence": "It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.",
                    "tag": "3"
                },
                {
                    "index": "266-5",
                    "sentence": "Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
                    "tag": "3+4"
                },
                {
                    "index": "266-6",
                    "sentence": "Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-267",
            "text": [
                {
                    "index": "267-0",
                    "sentence": "Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs.",
                    "tag": "1"
                },
                {
                    "index": "267-1",
                    "sentence": "This is relevant both for time-critical and on-device computations using neural networks.",
                    "tag": "1"
                },
                {
                    "index": "267-2",
                    "sentence": "The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model.",
                    "tag": "1"
                },
                {
                    "index": "267-3",
                    "sentence": "On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-268",
            "text": [
                {
                    "index": "268-0",
                    "sentence": "Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor.",
                    "tag": "2"
                },
                {
                    "index": "268-1",
                    "sentence": "The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence.",
                    "tag": "3"
                },
                {
                    "index": "268-2",
                    "sentence": "Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.",
                    "tag": "3"
                },
                {
                    "index": "268-3",
                    "sentence": "To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process.",
                    "tag": "2"
                },
                {
                    "index": "268-4",
                    "sentence": "At each iteration, a base MRC model is trained with golden answers and noisy evidence labels.",
                    "tag": "3"
                },
                {
                    "index": "268-5",
                    "sentence": "The trained model will predict pseudo evidence labels as extra supervision in the next iteration.",
                    "tag": "3"
                },
                {
                    "index": "268-6",
                    "sentence": "We evaluate STM on seven datasets over three MRC tasks.",
                    "tag": "3+4"
                },
                {
                    "index": "268-7",
                    "sentence": "Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-269",
            "text": [
                {
                    "index": "269-0",
                    "sentence": "While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well.",
                    "tag": "1"
                },
                {
                    "index": "269-1",
                    "sentence": "This results in poor quantity representations and incorrect solution expressions.",
                    "tag": "1"
                },
                {
                    "index": "269-2",
                    "sentence": "In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions.",
                    "tag": "2"
                },
                {
                    "index": "269-3",
                    "sentence": "Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs.",
                    "tag": "3"
                },
                {
                    "index": "269-4",
                    "sentence": "We conduct extensive experiments on two available datasets.",
                    "tag": "3"
                },
                {
                    "index": "269-5",
                    "sentence": "Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly.",
                    "tag": "2+3"
                },
                {
                    "index": "269-6",
                    "sentence": "We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-270",
            "text": [
                {
                    "index": "270-0",
                    "sentence": "In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis.",
                    "tag": "1"
                },
                {
                    "index": "270-1",
                    "sentence": "Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes).",
                    "tag": "2+3"
                },
                {
                    "index": "270-2",
                    "sentence": "In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory.",
                    "tag": "2"
                },
                {
                    "index": "270-3",
                    "sentence": "Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously.",
                    "tag": "2+3"
                },
                {
                    "index": "270-4",
                    "sentence": "In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-271",
            "text": [
                {
                    "index": "271-0",
                    "sentence": "Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks.",
                    "tag": "1"
                },
                {
                    "index": "271-1",
                    "sentence": "However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices).",
                    "tag": "1+2"
                },
                {
                    "index": "271-2",
                    "sentence": "In this paper, we propose a novel method to adaptively compress word embeddings.",
                    "tag": "2+3"
                },
                {
                    "index": "271-3",
                    "sentence": "We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4).",
                    "tag": "2+3"
                },
                {
                    "index": "271-4",
                    "sentence": "However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks.",
                    "tag": "3+4"
                },
                {
                    "index": "271-5",
                    "sentence": "The proposed method works in two steps.",
                    "tag": "3"
                },
                {
                    "index": "271-6",
                    "sentence": "First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks.",
                    "tag": "3"
                },
                {
                    "index": "271-7",
                    "sentence": "After selecting the code length, each word learns discrete codes through a neural network with a binary constraint.",
                    "tag": "3"
                },
                {
                    "index": "271-8",
                    "sentence": "To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "271-9",
                    "sentence": "Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy.",
                    "tag": "3"
                },
                {
                    "index": "271-10",
                    "sentence": "Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-272",
            "text": [
                {
                    "index": "272-0",
                    "sentence": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global).",
                    "tag": "1"
                },
                {
                    "index": "272-1",
                    "sentence": "Existing representation learning models do not fully capture these features.",
                    "tag": "1+2"
                },
                {
                    "index": "272-2",
                    "sentence": "To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph.",
                    "tag": "2+3"
                },
                {
                    "index": "272-3",
                    "sentence": "The graph is constructed with topical keywords as nodes and multiple local and global features as edges.",
                    "tag": "4"
                },
                {
                    "index": "272-4",
                    "sentence": "A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them.",
                    "tag": "3"
                },
                {
                    "index": "272-5",
                    "sentence": "Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes.",
                    "tag": "3"
                },
                {
                    "index": "272-6",
                    "sentence": "Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-273",
            "text": [
                {
                    "index": "273-0",
                    "sentence": "Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector.",
                    "tag": "1"
                },
                {
                    "index": "273-1",
                    "sentence": "However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge.",
                    "tag": "1"
                },
                {
                    "index": "273-2",
                    "sentence": "In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.",
                    "tag": "2+3"
                },
                {
                    "index": "273-3",
                    "sentence": "This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-274",
            "text": [
                {
                    "index": "274-0",
                    "sentence": "Pretraining deep language models has led to large performance gains in NLP.",
                    "tag": "1"
                },
                {
                    "index": "274-1",
                    "sentence": "Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words.",
                    "tag": "1"
                },
                {
                    "index": "274-2",
                    "sentence": "For static word embeddings, this problem has been addressed by separately learning representations for rare words.",
                    "tag": "1"
                },
                {
                    "index": "274-3",
                    "sentence": "In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models.",
                    "tag": "2"
                },
                {
                    "index": "274-4",
                    "sentence": "This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture.",
                    "tag": "3"
                },
                {
                    "index": "274-5",
                    "sentence": "Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-275",
            "text": [
                {
                    "index": "275-0",
                    "sentence": "Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly.",
                    "tag": "1+2"
                },
                {
                    "index": "275-1",
                    "sentence": "However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary.",
                    "tag": "1"
                },
                {
                    "index": "275-2",
                    "sentence": "To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences.",
                    "tag": "1+2"
                },
                {
                    "index": "275-3",
                    "sentence": "Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches.",
                    "tag": "2"
                },
                {
                    "index": "275-4",
                    "sentence": "When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models.",
                    "tag": "3"
                },
                {
                    "index": "275-5",
                    "sentence": "Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "275-6",
                    "sentence": "We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-276",
            "text": [
                {
                    "index": "276-0",
                    "sentence": "Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data.",
                    "tag": "1+2"
                },
                {
                    "index": "276-1",
                    "sentence": "It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain.",
                    "tag": "1"
                },
                {
                    "index": "276-2",
                    "sentence": "In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation.",
                    "tag": "2+3"
                },
                {
                    "index": "276-3",
                    "sentence": "Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge.",
                    "tag": "1"
                },
                {
                    "index": "276-4",
                    "sentence": "To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task.",
                    "tag": "2"
                },
                {
                    "index": "276-5",
                    "sentence": "The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way.",
                    "tag": "2+3"
                },
                {
                    "index": "276-6",
                    "sentence": "Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features.",
                    "tag": "2+3"
                },
                {
                    "index": "276-7",
                    "sentence": "Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin.",
                    "tag": "3+4"
                },
                {
                    "index": "276-8",
                    "sentence": "The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-277",
            "text": [
                {
                    "index": "277-0",
                    "sentence": "Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches.",
                    "tag": "1+2"
                },
                {
                    "index": "277-1",
                    "sentence": "However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches.",
                    "tag": "1"
                },
                {
                    "index": "277-2",
                    "sentence": "In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks.",
                    "tag": "2"
                },
                {
                    "index": "277-3",
                    "sentence": "With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation.",
                    "tag": "2+3"
                },
                {
                    "index": "277-4",
                    "sentence": "In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.",
                    "tag": "3"
                },
                {
                    "index": "277-5",
                    "sentence": "Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets.",
                    "tag": "3"
                },
                {
                    "index": "277-6",
                    "sentence": "We release our code at https://github.com/baidu/Senta.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-278",
            "text": [
                {
                    "index": "278-0",
                    "sentence": "Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces.",
                    "tag": "1"
                },
                {
                    "index": "278-1",
                    "sentence": "However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism.",
                    "tag": "1+2"
                },
                {
                    "index": "278-2",
                    "sentence": "In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages.",
                    "tag": "2"
                },
                {
                    "index": "278-3",
                    "sentence": "We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.",
                    "tag": "2+3"
                },
                {
                    "index": "278-4",
                    "sentence": "We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-279",
            "text": [
                {
                    "index": "279-0",
                    "sentence": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences.",
                    "tag": "1+2"
                },
                {
                    "index": "279-1",
                    "sentence": "Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date.",
                    "tag": "1"
                },
                {
                    "index": "279-2",
                    "sentence": "In this paper, we show that these results can be improved by using an in-order linearization instead.",
                    "tag": "1"
                },
                {
                    "index": "279-3",
                    "sentence": "Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers.",
                    "tag": "3+4"
                },
                {
                    "index": "279-4",
                    "sentence": "Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-280",
            "text": [
                {
                    "index": "280-0",
                    "sentence": "A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar.",
                    "tag": "1+2"
                },
                {
                    "index": "280-1",
                    "sentence": "We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs).",
                    "tag": "2+3"
                },
                {
                    "index": "280-2",
                    "sentence": "The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules.",
                    "tag": "2+3"
                },
                {
                    "index": "280-3",
                    "sentence": "In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-281",
            "text": [
                {
                    "index": "281-0",
                    "sentence": "Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT.",
                    "tag": "1"
                },
                {
                    "index": "281-1",
                    "sentence": "Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like.",
                    "tag": "1"
                },
                {
                    "index": "281-2",
                    "sentence": "A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank.",
                    "tag": "1"
                },
                {
                    "index": "281-3",
                    "sentence": "We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias.",
                    "tag": "2"
                },
                {
                    "index": "281-4",
                    "sentence": "While known techniques for tackling these biases improve results, they still do not make the parser state of the art.",
                    "tag": "1"
                },
                {
                    "index": "281-5",
                    "sentence": "Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation.",
                    "tag": "4"
                },
                {
                    "index": "281-6",
                    "sentence": "The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-282",
            "text": [
                {
                    "index": "282-0",
                    "sentence": "Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser.",
                    "tag": "1+2"
                },
                {
                    "index": "282-1",
                    "sentence": "However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology.",
                    "tag": "1+2"
                },
                {
                    "index": "282-2",
                    "sentence": "In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech.",
                    "tag": "2"
                },
                {
                    "index": "282-3",
                    "sentence": "In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs).",
                    "tag": "2"
                },
                {
                    "index": "282-4",
                    "sentence": "We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech.",
                    "tag": "3"
                },
                {
                    "index": "282-5",
                    "sentence": "We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-283",
            "text": [
                {
                    "index": "283-0",
                    "sentence": "To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.",
                    "tag": "1"
                },
                {
                    "index": "283-1",
                    "sentence": "In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.",
                    "tag": "2+3"
                },
                {
                    "index": "283-2",
                    "sentence": "We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.",
                    "tag": "1+2"
                },
                {
                    "index": "283-3",
                    "sentence": "Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.",
                    "tag": "3"
                },
                {
                    "index": "283-4",
                    "sentence": "Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.",
                    "tag": "3"
                },
                {
                    "index": "283-5",
                    "sentence": "Our framework shows that this model is capable of generating a significant number of inconsistent explanations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-284",
            "text": [
                {
                    "index": "284-0",
                    "sentence": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).",
                    "tag": "1+2"
                },
                {
                    "index": "284-1",
                    "sentence": "The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.",
                    "tag": "2"
                },
                {
                    "index": "284-2",
                    "sentence": "However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.",
                    "tag": "3"
                },
                {
                    "index": "284-3",
                    "sentence": "Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).",
                    "tag": "2"
                },
                {
                    "index": "284-4",
                    "sentence": "Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.",
                    "tag": "3"
                },
                {
                    "index": "284-5",
                    "sentence": "Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.",
                    "tag": "4"
                },
                {
                    "index": "284-6",
                    "sentence": "We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-285",
            "text": [
                {
                    "index": "285-0",
                    "sentence": "Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence.",
                    "tag": "1+2"
                },
                {
                    "index": "285-1",
                    "sentence": "We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.",
                    "tag": "1"
                },
                {
                    "index": "285-2",
                    "sentence": "We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.",
                    "tag": "2+3"
                },
                {
                    "index": "285-3",
                    "sentence": "The Transformer outperforms the LSTM in all analyses.",
                    "tag": "1+2"
                },
                {
                    "index": "285-4",
                    "sentence": "Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.",
                    "tag": "4+5"
                },
                {
                    "index": "285-5",
                    "sentence": "However, we find traces of the latter aspect, too.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-286",
            "text": [
                {
                    "index": "286-0",
                    "sentence": "In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer.",
                    "tag": "1"
                },
                {
                    "index": "286-1",
                    "sentence": "Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.",
                    "tag": "1"
                },
                {
                    "index": "286-2",
                    "sentence": "This makes attention weights unreliable as explanations probes.",
                    "tag": "1"
                },
                {
                    "index": "286-3",
                    "sentence": "In this paper, we consider the problem of quantifying this flow of information through self-attention.",
                    "tag": "2"
                },
                {
                    "index": "286-4",
                    "sentence": "We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.",
                    "tag": "2+3"
                },
                {
                    "index": "286-5",
                    "sentence": "We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-287",
            "text": [
                {
                    "index": "287-0",
                    "sentence": "Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions.",
                    "tag": "1"
                },
                {
                    "index": "287-1",
                    "sentence": "Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction.",
                    "tag": "1"
                },
                {
                    "index": "287-2",
                    "sentence": "They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions.",
                    "tag": "1"
                },
                {
                    "index": "287-3",
                    "sentence": "In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions.",
                    "tag": "2"
                },
                {
                    "index": "287-4",
                    "sentence": "We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions.",
                    "tag": "4"
                },
                {
                    "index": "287-5",
                    "sentence": "Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.",
                    "tag": "4"
                },
                {
                    "index": "287-6",
                    "sentence": "To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse.",
                    "tag": "2"
                },
                {
                    "index": "287-7",
                    "sentence": "We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.",
                    "tag": "4"
                },
                {
                    "index": "287-8",
                    "sentence": "Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions.",
                    "tag": "4"
                },
                {
                    "index": "287-9",
                    "sentence": "Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-288",
            "text": [
                {
                    "index": "288-0",
                    "sentence": "Multi-task Learning methods have achieved great progress in text classification.",
                    "tag": "1"
                },
                {
                    "index": "288-1",
                    "sentence": "However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.",
                    "tag": "1+2"
                },
                {
                    "index": "288-2",
                    "sentence": "To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.",
                    "tag": "2"
                },
                {
                    "index": "288-3",
                    "sentence": "The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-289",
            "text": [
                {
                    "index": "289-0",
                    "sentence": "This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.",
                    "tag": "2+3"
                },
                {
                    "index": "289-1",
                    "sentence": "Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.",
                    "tag": "3"
                },
                {
                    "index": "289-2",
                    "sentence": "The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-290",
            "text": [
                {
                    "index": "290-0",
                    "sentence": "Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.",
                    "tag": "1"
                },
                {
                    "index": "290-1",
                    "sentence": "However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.",
                    "tag": "1"
                },
                {
                    "index": "290-2",
                    "sentence": "In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).",
                    "tag": "2"
                },
                {
                    "index": "290-3",
                    "sentence": "We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.",
                    "tag": "2+3"
                },
                {
                    "index": "290-4",
                    "sentence": "We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-291",
            "text": [
                {
                    "index": "291-0",
                    "sentence": "This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).",
                    "tag": "2"
                },
                {
                    "index": "291-1",
                    "sentence": "The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.",
                    "tag": "1"
                },
                {
                    "index": "291-2",
                    "sentence": "For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.",
                    "tag": "1"
                },
                {
                    "index": "291-3",
                    "sentence": "Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM.",
                    "tag": "3+4"
                },
                {
                    "index": "291-4",
                    "sentence": "The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "291-5",
                    "sentence": "Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-292",
            "text": [
                {
                    "index": "292-0",
                    "sentence": "With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.",
                    "tag": "1"
                },
                {
                    "index": "292-1",
                    "sentence": "Most existing methods usually learn the representations of users and news from news contents for recommendation.",
                    "tag": "1"
                },
                {
                    "index": "292-2",
                    "sentence": "However, they seldom consider high-order connectivity underlying the user-news interactions.",
                    "tag": "1"
                },
                {
                    "index": "292-3",
                    "sentence": "Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news.",
                    "tag": "1"
                },
                {
                    "index": "292-4",
                    "sentence": "In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.",
                    "tag": "2"
                },
                {
                    "index": "292-5",
                    "sentence": "Our model can encode high-order relationships into user and news representations by information propagation along the graph.",
                    "tag": "2+3"
                },
                {
                    "index": "292-6",
                    "sentence": "Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.",
                    "tag": "3"
                },
                {
                    "index": "292-7",
                    "sentence": "A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.",
                    "tag": "3+4"
                },
                {
                    "index": "292-8",
                    "sentence": "Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-293",
            "text": [
                {
                    "index": "293-0",
                    "sentence": "In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.",
                    "tag": "1"
                },
                {
                    "index": "293-1",
                    "sentence": "We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.",
                    "tag": "2"
                },
                {
                    "index": "293-2",
                    "sentence": "We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework.",
                    "tag": "2+3"
                },
                {
                    "index": "293-3",
                    "sentence": "Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-294",
            "text": [
                {
                    "index": "294-0",
                    "sentence": "The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.",
                    "tag": "1"
                },
                {
                    "index": "294-1",
                    "sentence": "Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.",
                    "tag": "1"
                },
                {
                    "index": "294-2",
                    "sentence": "While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.",
                    "tag": "2+3"
                },
                {
                    "index": "294-3",
                    "sentence": "The latter is, however, inextricably linked to abusive behaviour.",
                    "tag": "3"
                },
                {
                    "index": "294-4",
                    "sentence": "In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.",
                    "tag": "3"
                },
                {
                    "index": "294-5",
                    "sentence": "Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-295",
            "text": [
                {
                    "index": "295-0",
                    "sentence": "The key to effortless end-user programming is natural language.",
                    "tag": "1+2"
                },
                {
                    "index": "295-1",
                    "sentence": "We examine how to teach intelligent systems new functions, expressed in natural language.",
                    "tag": "1+2"
                },
                {
                    "index": "295-2",
                    "sentence": "As a first step, we collected 3168 samples of teaching efforts in plain English.",
                    "tag": "3"
                },
                {
                    "index": "295-3",
                    "sentence": "Then we built fuSE, a novel system that translates English function descriptions into code.",
                    "tag": "3+4"
                },
                {
                    "index": "295-4",
                    "sentence": "Our approach is three-tiered and each task is evaluated separately.",
                    "tag": "4"
                },
                {
                    "index": "295-5",
                    "sentence": "We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).",
                    "tag": "4"
                },
                {
                    "index": "295-6",
                    "sentence": "Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).",
                    "tag": "3+4"
                },
                {
                    "index": "295-7",
                    "sentence": "Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).",
                    "tag": "3+4"
                },
                {
                    "index": "295-8",
                    "sentence": "In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-296",
            "text": [
                {
                    "index": "296-0",
                    "sentence": "Moderation is crucial to promoting healthy online discussions.",
                    "tag": "1"
                },
                {
                    "index": "296-1",
                    "sentence": "Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.",
                    "tag": "1+2"
                },
                {
                    "index": "296-2",
                    "sentence": "We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?",
                    "tag": "3"
                },
                {
                    "index": "296-3",
                    "sentence": "We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.",
                    "tag": "3"
                },
                {
                    "index": "296-4",
                    "sentence": "We find that context can both amplify or mitigate the perceived toxicity of posts.",
                    "tag": "3"
                },
                {
                    "index": "296-5",
                    "sentence": "Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.",
                    "tag": "4"
                },
                {
                    "index": "296-6",
                    "sentence": "Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.",
                    "tag": "4"
                },
                {
                    "index": "296-7",
                    "sentence": "This points to the need for larger datasets of comments annotated in context.",
                    "tag": "5"
                },
                {
                    "index": "296-8",
                    "sentence": "We make our code and data publicly available.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-297",
            "text": [
                {
                    "index": "297-0",
                    "sentence": "Answering natural language questions over tables is usually seen as a semantic parsing task.",
                    "tag": "1"
                },
                {
                    "index": "297-1",
                    "sentence": "To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.",
                    "tag": "2"
                },
                {
                    "index": "297-2",
                    "sentence": "However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.",
                    "tag": "1"
                },
                {
                    "index": "297-3",
                    "sentence": "In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.",
                    "tag": "2+3"
                },
                {
                    "index": "297-4",
                    "sentence": "TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.",
                    "tag": "3"
                },
                {
                    "index": "297-5",
                    "sentence": "TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.",
                    "tag": "3+4"
                },
                {
                    "index": "297-6",
                    "sentence": "We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.",
                    "tag": "3"
                },
                {
                    "index": "297-7",
                    "sentence": "We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-298",
            "text": [
                {
                    "index": "298-0",
                    "sentence": "In argumentation, people state premises to reason towards a conclusion.",
                    "tag": "1"
                },
                {
                    "index": "298-1",
                    "sentence": "The conclusion conveys a stance towards some target, such as a concept or statement.",
                    "tag": "1"
                },
                {
                    "index": "298-2",
                    "sentence": "Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.",
                    "tag": "1"
                },
                {
                    "index": "298-3",
                    "sentence": "However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.",
                    "tag": "1+2"
                },
                {
                    "index": "298-4",
                    "sentence": "We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises.",
                    "tag": "2"
                },
                {
                    "index": "298-5",
                    "sentence": "In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets.",
                    "tag": "2+3"
                },
                {
                    "index": "298-6",
                    "sentence": "We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.",
                    "tag": "3"
                },
                {
                    "index": "298-7",
                    "sentence": "Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.",
                    "tag": "4"
                },
                {
                    "index": "298-8",
                    "sentence": "According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-299",
            "text": [
                {
                    "index": "299-0",
                    "sentence": "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.",
                    "tag": "1+2"
                },
                {
                    "index": "299-1",
                    "sentence": "Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.",
                    "tag": "1"
                },
                {
                    "index": "299-2",
                    "sentence": "Equally treating all modalities may encode too much useless information from less important modalities.",
                    "tag": "1"
                },
                {
                    "index": "299-3",
                    "sentence": "In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.",
                    "tag": "2"
                },
                {
                    "index": "299-4",
                    "sentence": "The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.",
                    "tag": "3"
                },
                {
                    "index": "299-5",
                    "sentence": "Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-300",
            "text": [
                {
                    "index": "300-0",
                    "sentence": "In this paper, we present a simple yet effective padding scheme that can be used as a drop-in module for existing convolutional neural networks.",
                    "tag": "2"
                },
                {
                    "index": "300-1",
                    "sentence": "We call it partial convolution based padding, with the intuition that the padded region can be treated as holes and the original input as non-holes.",
                    "tag": "2+3"
                },
                {
                    "index": "300-2",
                    "sentence": "Specifically, during the convolution operation, the convolution results are re-weighted near image borders based on the ratios between the padded area and the convolution sliding window area.",
                    "tag": "1"
                },
                {
                    "index": "300-3",
                    "sentence": "Extensive experiments with various deep network models on ImageNet classification and semantic segmentation demonstrate that the proposed padding scheme consistently outperforms standard zero padding with better accuracy.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-301",
            "text": [
                {
                    "index": "301-0",
                    "sentence": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions.",
                    "tag": "1"
                },
                {
                    "index": "301-1",
                    "sentence": "Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples.",
                    "tag": "1+2"
                },
                {
                    "index": "301-2",
                    "sentence": "Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output.",
                    "tag": "3"
                },
                {
                    "index": "301-3",
                    "sentence": "Among them, we show one instantiation of EBGAN framework as using an auto encoder architecture, with the energy being the reconstruction error, in place of the discriminator.",
                    "tag": "2"
                },
                {
                    "index": "301-4",
                    "sentence": "We show that this form of EBGAN exhibits more stable behavior than regular GANs during training.",
                    "tag": "3+4"
                },
                {
                    "index": "301-5",
                    "sentence": "We also show that a single-scale architecture can be trained to generate high-resolution images.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-302",
            "text": [
                {
                    "index": "302-0",
                    "sentence": "Training modern deep learning models requires large amounts of computation, often provided by GPUs.",
                    "tag": "1+2"
                },
                {
                    "index": "302-1",
                    "sentence": "Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications.",
                    "tag": "1+2"
                },
                {
                    "index": "302-2",
                    "sentence": "First, the training library must support inter-GPU communication.",
                    "tag": "3"
                },
                {
                    "index": "302-3",
                    "sentence": "Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead.",
                    "tag": "3"
                },
                {
                    "index": "302-4",
                    "sentence": "Second, the user must modify his or her training code to take advantage of inter-GPU communication.",
                    "tag": "3"
                },
                {
                    "index": "302-5",
                    "sentence": "Depending on the training library's API, the modification required may be either significant or minimal.",
                    "tag": "3"
                },
                {
                    "index": "302-6",
                    "sentence": "Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training.",
                    "tag": "3"
                },
                {
                    "index": "302-7",
                    "sentence": "In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow.",
                    "tag": "4"
                },
                {
                    "index": "302-8",
                    "sentence": "Horovod is available under the Apache 2.0 license at this https URL",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-303",
            "text": [
                {
                    "index": "303-0",
                    "sentence": "Conditional GANs are at the forefront of natural image synthesis.",
                    "tag": "1"
                },
                {
                    "index": "303-1",
                    "sentence": "The main drawback of such models is the necessity for labeled data.",
                    "tag": "1"
                },
                {
                    "index": "303-2",
                    "sentence": "In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs.",
                    "tag": "2"
                },
                {
                    "index": "303-3",
                    "sentence": "In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game.",
                    "tag": "2"
                },
                {
                    "index": "303-4",
                    "sentence": "The role of self supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training.",
                    "tag": "2"
                },
                {
                    "index": "303-5",
                    "sentence": "We test empirically both the quality of the learned image representations, and the quality of the synthesized images.",
                    "tag": "3"
                },
                {
                    "index": "303-6",
                    "sentence": "Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts.",
                    "tag": "4"
                },
                {
                    "index": "303-7",
                    "sentence": "Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-304",
            "text": [
                {
                    "index": "304-0",
                    "sentence": "One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural networks.",
                    "tag": "1"
                },
                {
                    "index": "304-1",
                    "sentence": "In this work, we introduce SqueezeNext, a new family of neural network architectures whose design was guided by considering previous architectures such as SqueezeNet, as well as by simulation results on a neural network accelerator.",
                    "tag": "2"
                },
                {
                    "index": "304-2",
                    "sentence": "This new network is able to match AlexNet's accuracy on the ImageNet benchmark with 112× fewer parameters, and one of its deeper variants is able to achieve VGG-19 accuracy with only 4.4 Million parameters, (31× smaller than VGG-19).",
                    "tag": "2"
                },
                {
                    "index": "304-3",
                    "sentence": "SqueezeNext also achieves better top-5 classification accuracy with 1.3× fewer parameters as compared to MobileNet, but avoids using depthwise-separable convolutions that are inefficient on some mobile processor platforms.",
                    "tag": "2+3"
                },
                {
                    "index": "304-4",
                    "sentence": "This wide range of accuracy gives the user the ability to make speed accuracy tradeoffs, depending on the available resources on the target hardware.",
                    "tag": "3"
                },
                {
                    "index": "304-5",
                    "sentence": "Using hardware simulation results for power and inference speed on an embedded system has guided us to design variations of the baseline model that are 2.59×/8.26× faster and 2.25×/7.5× more energy efficient as compared to SqueezeNet/AlexNet without any accuracy degradation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-305",
            "text": [
                {
                    "index": "305-0",
                    "sentence": "We consider the problem of anomaly detection in images, and present a new detection technique.",
                    "tag": "1"
                },
                {
                    "index": "305-1",
                    "sentence": "Given a sample of images, all known to belong to a \"normal\" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects).",
                    "tag": "2"
                },
                {
                    "index": "305-2",
                    "sentence": "The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images.",
                    "tag": "2"
                },
                {
                    "index": "305-3",
                    "sentence": "The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images.",
                    "tag": "2+3"
                },
                {
                    "index": "305-4",
                    "sentence": "We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-306",
            "text": [
                {
                    "index": "306-0",
                    "sentence": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.",
                    "tag": "1"
                },
                {
                    "index": "306-1",
                    "sentence": "In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.",
                    "tag": "2"
                },
                {
                    "index": "306-2",
                    "sentence": "Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.",
                    "tag": "2"
                },
                {
                    "index": "306-3",
                    "sentence": "We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.",
                    "tag": "2+3"
                },
                {
                    "index": "306-4",
                    "sentence": "To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.",
                    "tag": "3+4"
                },
                {
                    "index": "306-5",
                    "sentence": "In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.",
                    "tag": "4"
                },
                {
                    "index": "306-6",
                    "sentence": "Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",
                    "tag": "4"
                },
                {
                    "index": "306-7",
                    "sentence": "Source code is at this https URL.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-307",
            "text": [
                {
                    "index": "307-0",
                    "sentence": "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "307-1",
                    "sentence": "However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value.",
                    "tag": "1"
                },
                {
                    "index": "307-2",
                    "sentence": "The optimal objective for a metric is the metric itself.",
                    "tag": "2"
                },
                {
                    "index": "307-3",
                    "sentence": "In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss.",
                    "tag": "2+3"
                },
                {
                    "index": "307-4",
                    "sentence": "However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes.",
                    "tag": "3+4"
                },
                {
                    "index": "307-5",
                    "sentence": "In this paper, we address the weaknesses of IoU by introducing a generalized version as both a new loss and a new metric.",
                    "tag": "2"
                },
                {
                    "index": "307-6",
                    "sentence": "By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-308",
            "text": [
                {
                    "index": "308-0",
                    "sentence": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.",
                    "tag": "1"
                },
                {
                    "index": "308-1",
                    "sentence": "We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.",
                    "tag": "1+2"
                },
                {
                    "index": "308-2",
                    "sentence": "When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.",
                    "tag": "2+3"
                },
                {
                    "index": "308-3",
                    "sentence": "The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.",
                    "tag": "3+4"
                },
                {
                    "index": "308-4",
                    "sentence": "Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebTex",
                    "tag": "4"
                },
                {
                    "index": "308-5",
                    "sentence": "Samples from the model reflect these improvements and contain coherent paragraphs of text.",
                    "tag": "4"
                },
                {
                    "index": "308-6",
                    "sentence": "These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-309",
            "text": [
                {
                    "index": "309-0",
                    "sentence": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.",
                    "tag": "1"
                },
                {
                    "index": "309-1",
                    "sentence": "These representations are typically used as general purpose features for words across a range of NLP problems.",
                    "tag": "1"
                },
                {
                    "index": "309-2",
                    "sentence": "However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem.",
                    "tag": "1"
                },
                {
                    "index": "309-3",
                    "sentence": "Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations.",
                    "tag": "1+2"
                },
                {
                    "index": "309-4",
                    "sentence": "In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model.",
                    "tag": "2"
                },
                {
                    "index": "309-5",
                    "sentence": "We train this model on several data sources with multiple training objectives on over 100 million sentences.",
                    "tag": "3"
                },
                {
                    "index": "309-6",
                    "sentence": "Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods.",
                    "tag": "3+4"
                },
                {
                    "index": "309-7",
                    "sentence": "We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-310",
            "text": [
                {
                    "index": "310-0",
                    "sentence": "We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models.",
                    "tag": "1+2"
                },
                {
                    "index": "310-1",
                    "sentence": "By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees.",
                    "tag": "2"
                },
                {
                    "index": "310-2",
                    "sentence": "We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks.",
                    "tag": "2+3"
                },
                {
                    "index": "310-3",
                    "sentence": "We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-311",
            "text": [
                {
                    "index": "311-0",
                    "sentence": "Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase.",
                    "tag": "1"
                },
                {
                    "index": "311-1",
                    "sentence": "To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation --- which take into account all the information available in the raw audio signal, including the phase.",
                    "tag": "1+2"
                },
                {
                    "index": "311-2",
                    "sentence": "Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model.",
                    "tag": "2+3"
                },
                {
                    "index": "311-3",
                    "sentence": "Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-312",
            "text": [
                {
                    "index": "312-0",
                    "sentence": "We introduce a convolutional recurrent neural network (CRNN) for music tagging.",
                    "tag": "2"
                },
                {
                    "index": "312-1",
                    "sentence": "CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features.",
                    "tag": "3"
                },
                {
                    "index": "312-2",
                    "sentence": "We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample.",
                    "tag": "3"
                },
                {
                    "index": "312-3",
                    "sentence": "Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-313",
            "text": [
                {
                    "index": "313-0",
                    "sentence": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error.",
                    "tag": "1+2"
                },
                {
                    "index": "313-1",
                    "sentence": "Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator.",
                    "tag": "2+3"
                },
                {
                    "index": "313-2",
                    "sentence": "In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance.",
                    "tag": "3+4"
                },
                {
                    "index": "313-3",
                    "sentence": "We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-314",
            "text": [
                {
                    "index": "314-0",
                    "sentence": "Numerical studies on race car aerodynamics at wing in ground effect have been carried out using a steady 3d, double precision, pressure-based, and standard k-epsilon turbulence model.",
                    "tag": "1"
                },
                {
                    "index": "314-1",
                    "sentence": "Through various parametric analytical studies we have observed that at a particular speed and ground clearance of the wings a favorable negative lift was found high at a particular angle of attack for all the physical models considered in this paper.",
                    "tag": "2"
                },
                {
                    "index": "314-2",
                    "sentence": "The fact is that if the ground clearance height to chord length (h/c) is too small, the developing boundary layers from either side (the ground and the lower surface of the wing) can interact, leading to an altered variation of the aerodynamic characteristics at wing in ground effect.",
                    "tag": "4"
                },
                {
                    "index": "314-3",
                    "sentence": "Therefore a suitable ground clearance must be predicted throughout the racing for a better performance of the race car, which obviously depends upon the coupled effects of the topography, wing orientation with respect to the ground, the incoming flow features and/or the race car speed.",
                    "tag": "4"
                },
                {
                    "index": "314-4",
                    "sentence": "We have concluded that for the design of high performance and high speed race cars the adjustable wings capable to alter the ground clearance and the angles of attack is the best design option for any race car for racing safely with variable speeds.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-315",
            "text": [
                {
                    "index": "315-0",
                    "sentence": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "315-1",
                    "sentence": "Expressing these in a common form, we derive a novel algorithm, Retrace(λ), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies.",
                    "tag": "1+2"
                },
                {
                    "index": "315-2",
                    "sentence": "We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms.",
                    "tag": "3"
                },
                {
                    "index": "315-3",
                    "sentence": "We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration).",
                    "tag": "3"
                },
                {
                    "index": "315-4",
                    "sentence": "As a corollary, we prove the convergence of Watkins' Q(λ), which was an open problem since 1989.",
                    "tag": "4"
                },
                {
                    "index": "315-5",
                    "sentence": "We illustrate the benefits of Retrace(λ) on a standard suite of Atari 2600 games.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-316",
            "text": [
                {
                    "index": "316-0",
                    "sentence": "Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms.",
                    "tag": "1"
                },
                {
                    "index": "316-1",
                    "sentence": "There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map.",
                    "tag": "2+3"
                },
                {
                    "index": "316-2",
                    "sentence": "The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards.",
                    "tag": "3"
                },
                {
                    "index": "316-3",
                    "sentence": "The value function of a state can be computed as the inner product between the successor map and the reward weights.",
                    "tag": "3"
                },
                {
                    "index": "316-4",
                    "sentence": "In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework.",
                    "tag": "2+3"
                },
                {
                    "index": "316-5",
                    "sentence": "DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy.",
                    "tag": "4"
                },
                {
                    "index": "316-6",
                    "sentence": "We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-317",
            "text": [
                {
                    "index": "317-0",
                    "sentence": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations.",
                    "tag": "1+2"
                },
                {
                    "index": "317-1",
                    "sentence": "Specifically, we focus on the problem of exploration in non-tabular reinforcement learning.",
                    "tag": "3"
                },
                {
                    "index": "317-2",
                    "sentence": "Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model.",
                    "tag": "3"
                },
                {
                    "index": "317-3",
                    "sentence": "This technique enables us to generalize count-based exploration algorithms to the non-tabular case.",
                    "tag": "4"
                },
                {
                    "index": "317-4",
                    "sentence": "We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels.",
                    "tag": "3+4"
                },
                {
                    "index": "317-5",
                    "sentence": "We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-318",
            "text": [
                {
                    "index": "318-0",
                    "sentence": "Partially observed control problems are a challenging aspect of reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "318-1",
                    "sentence": "We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time.",
                    "tag": "2"
                },
                {
                    "index": "318-2",
                    "sentence": "We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements.",
                    "tag": "4"
                },
                {
                    "index": "318-3",
                    "sentence": "These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps.",
                    "tag": "3"
                },
                {
                    "index": "318-4",
                    "sentence": "We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task.",
                    "tag": "4"
                },
                {
                    "index": "318-5",
                    "sentence": "Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels.",
                    "tag": "4"
                },
                {
                    "index": "318-6",
                    "sentence": "We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-319",
            "text": [
                {
                    "index": "319-0",
                    "sentence": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world).",
                    "tag": "2"
                },
                {
                    "index": "319-1",
                    "sentence": "We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures.",
                    "tag": "3"
                },
                {
                    "index": "319-2",
                    "sentence": "These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks.",
                    "tag": "3"
                },
                {
                    "index": "319-3",
                    "sentence": "While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures.",
                    "tag": "3"
                },
                {
                    "index": "319-4",
                    "sentence": "Additionally, we evaluate the generalization performance of the architectures on environments not used during training.",
                    "tag": "3"
                },
                {
                    "index": "319-5",
                    "sentence": "The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-320",
            "text": [
                {
                    "index": "320-0",
                    "sentence": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain.",
                    "tag": "1"
                },
                {
                    "index": "320-1",
                    "sentence": "One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate.",
                    "tag": "1"
                },
                {
                    "index": "320-2",
                    "sentence": "It decides the granularity at which agents can control game play.",
                    "tag": "1"
                },
                {
                    "index": "320-3",
                    "sentence": "A frame skip value of k allows the agent to repeat a selected action k number of times.",
                    "tag": "1+2"
                },
                {
                    "index": "320-4",
                    "sentence": "The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state.",
                    "tag": "3"
                },
                {
                    "index": "320-5",
                    "sentence": "In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter.",
                    "tag": "2+3"
                },
                {
                    "index": "320-6",
                    "sentence": "This allows us to choose the number of times an action is to be repeated based on the current state.",
                    "tag": "3"
                },
                {
                    "index": "320-7",
                    "sentence": "We show empirically that such a setting improves the performance on relatively harder games like Seaquest.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-321",
            "text": [
                {
                    "index": "321-0",
                    "sentence": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states.",
                    "tag": "2"
                },
                {
                    "index": "321-1",
                    "sentence": "Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms.",
                    "tag": "3"
                },
                {
                    "index": "321-2",
                    "sentence": "These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch.",
                    "tag": "2+3"
                },
                {
                    "index": "321-3",
                    "sentence": "We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states.",
                    "tag": "3"
                },
                {
                    "index": "321-4",
                    "sentence": "The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure.",
                    "tag": "3"
                },
                {
                    "index": "321-5",
                    "sentence": "Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states.",
                    "tag": "4"
                },
                {
                    "index": "321-6",
                    "sentence": "The connectivity information from PCCA+ is used to generate these skills or options.",
                    "tag": "4"
                },
                {
                    "index": "321-7",
                    "sentence": "These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model.",
                    "tag": "4+5"
                },
                {
                    "index": "321-8",
                    "sentence": "This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate.",
                    "tag": "4+5"
                },
                {
                    "index": "321-9",
                    "sentence": "We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-322",
            "text": [
                {
                    "index": "322-0",
                    "sentence": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms.",
                    "tag": "1"
                },
                {
                    "index": "322-1",
                    "sentence": "The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions.",
                    "tag": "1"
                },
                {
                    "index": "322-2",
                    "sentence": "Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems.",
                    "tag": "1"
                },
                {
                    "index": "322-3",
                    "sentence": "Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment.",
                    "tag": "1"
                },
                {
                    "index": "322-4",
                    "sentence": "We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning.",
                    "tag": "2"
                },
                {
                    "index": "322-5",
                    "sentence": "A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals.",
                    "tag": "3"
                },
                {
                    "index": "322-6",
                    "sentence": "h-DQN allows for flexible goal specifications, such as functions over entities and relations.",
                    "tag": "3"
                },
                {
                    "index": "322-7",
                    "sentence": "This provides an efficient space for exploration in complicated environments.",
                    "tag": "3"
                },
                {
                    "index": "322-8",
                    "sentence": "We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-323",
            "text": [
                {
                    "index": "323-0",
                    "sentence": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images.",
                    "tag": "1"
                },
                {
                    "index": "323-1",
                    "sentence": "To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose.",
                    "tag": "2+3"
                },
                {
                    "index": "323-2",
                    "sentence": "This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination.",
                    "tag": "3"
                },
                {
                    "index": "323-3",
                    "sentence": "We then use this network to servo the gripper in real time to achieve successful grasps.",
                    "tag": "3"
                },
                {
                    "index": "323-4",
                    "sentence": "To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware.",
                    "tag": "3+4"
                },
                {
                    "index": "323-5",
                    "sentence": "Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-324",
            "text": [
                {
                    "index": "324-0",
                    "sentence": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions.",
                    "tag": "1"
                },
                {
                    "index": "324-1",
                    "sentence": "However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems.",
                    "tag": "1"
                },
                {
                    "index": "324-2",
                    "sentence": "In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks.",
                    "tag": "2"
                },
                {
                    "index": "324-3",
                    "sentence": "We propose two complementary techniques for improving the efficiency of such algorithms.",
                    "tag": "3"
                },
                {
                    "index": "324-4",
                    "sentence": "First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods.",
                    "tag": "3"
                },
                {
                    "index": "324-5",
                    "sentence": "NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks.",
                    "tag": "3"
                },
                {
                    "index": "324-6",
                    "sentence": "To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning.",
                    "tag": "3+4"
                },
                {
                    "index": "324-7",
                    "sentence": "We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-325",
            "text": [
                {
                    "index": "325-0",
                    "sentence": "Reinforcement learning can acquire complex behaviors from high-level specifications.",
                    "tag": "1"
                },
                {
                    "index": "325-1",
                    "sentence": "However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice.",
                    "tag": "1"
                },
                {
                    "index": "325-2",
                    "sentence": "We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems.",
                    "tag": "2"
                },
                {
                    "index": "325-3",
                    "sentence": "Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems.",
                    "tag": "2"
                },
                {
                    "index": "325-4",
                    "sentence": "To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering.",
                    "tag": "2+3"
                },
                {
                    "index": "325-5",
                    "sentence": "To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC.",
                    "tag": "3+4"
                },
                {
                    "index": "325-6",
                    "sentence": "We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-326",
            "text": [
                {
                    "index": "326-0",
                    "sentence": "Efficient exploration in complex environments remains a major challenge for reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "326-1",
                    "sentence": "We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions.",
                    "tag": "1+2"
                },
                {
                    "index": "326-2",
                    "sentence": "Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning.",
                    "tag": "1"
                },
                {
                    "index": "326-3",
                    "sentence": "We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment.",
                    "tag": "2+3"
                },
                {
                    "index": "326-4",
                    "sentence": "Bootstrapped DQN substantially improves learning times and performance across most Atari games.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-327",
            "text": [
                {
                    "index": "327-0",
                    "sentence": "We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within.",
                    "tag": "2"
                },
                {
                    "index": "327-1",
                    "sentence": "VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.",
                    "tag": "2"
                },
                {
                    "index": "327-2",
                    "sentence": "Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.",
                    "tag": "2"
                },
                {
                    "index": "327-3",
                    "sentence": "We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task.",
                    "tag": "3"
                },
                {
                    "index": "327-4",
                    "sentence": "We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-328",
            "text": [
                {
                    "index": "328-0",
                    "sentence": "We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks.",
                    "tag": "2"
                },
                {
                    "index": "328-1",
                    "sentence": "In these tasks, the agents are not given any pre-designed communication protocol.",
                    "tag": "3"
                },
                {
                    "index": "328-2",
                    "sentence": "Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol.",
                    "tag": "3"
                },
                {
                    "index": "328-3",
                    "sentence": "We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so.",
                    "tag": "4"
                },
                {
                    "index": "328-4",
                    "sentence": "To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols.",
                    "tag": "4"
                },
                {
                    "index": "328-5",
                    "sentence": "In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-329",
            "text": [
                {
                    "index": "329-0",
                    "sentence": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves.",
                    "tag": "1"
                },
                {
                    "index": "329-1",
                    "sentence": "Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves.",
                    "tag": "1+2"
                },
                {
                    "index": "329-2",
                    "sentence": "These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.",
                    "tag": "3"
                },
                {
                    "index": "329-3",
                    "sentence": "Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play.",
                    "tag": "3"
                },
                {
                    "index": "329-4",
                    "sentence": "We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks.",
                    "tag": "3"
                },
                {
                    "index": "329-5",
                    "sentence": "Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.",
                    "tag": "4+5"
                },
                {
                    "index": "329-6",
                    "sentence": "This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-330",
            "text": [
                {
                    "index": "330-0",
                    "sentence": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.",
                    "tag": "1"
                },
                {
                    "index": "330-1",
                    "sentence": "In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers.",
                    "tag": "1+2"
                },
                {
                    "index": "330-2",
                    "sentence": "Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora.",
                    "tag": "3+4"
                },
                {
                    "index": "330-3",
                    "sentence": "Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-331",
            "text": [
                {
                    "index": "331-0",
                    "sentence": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding.",
                    "tag": "2"
                },
                {
                    "index": "331-1",
                    "sentence": "We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language.",
                    "tag": "3"
                },
                {
                    "index": "331-2",
                    "sentence": "We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.",
                    "tag": "3"
                },
                {
                    "index": "331-3",
                    "sentence": "Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7.",
                    "tag": "4"
                },
                {
                    "index": "331-4",
                    "sentence": "We also release these models for the NLP and ML community to study and improve upon.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-332",
            "text": [
                {
                    "index": "332-0",
                    "sentence": "Teaching machines to read natural language documents remains an elusive challenge.",
                    "tag": "1"
                },
                {
                    "index": "332-1",
                    "sentence": "Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation.",
                    "tag": "1"
                },
                {
                    "index": "332-2",
                    "sentence": "In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data.",
                    "tag": "1+2"
                },
                {
                    "index": "332-3",
                    "sentence": "This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-333",
            "text": [
                {
                    "index": "333-0",
                    "sentence": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.",
                    "tag": "1"
                },
                {
                    "index": "333-1",
                    "sentence": "However, there has been little work exploring useful architectures for attention-based NMT.",
                    "tag": "1"
                },
                {
                    "index": "333-2",
                    "sentence": "This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.",
                    "tag": "1+2"
                },
                {
                    "index": "333-3",
                    "sentence": "We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions.",
                    "tag": "3"
                },
                {
                    "index": "333-4",
                    "sentence": "With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout.",
                    "tag": "3"
                },
                {
                    "index": "333-5",
                    "sentence": "Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-334",
            "text": [
                {
                    "index": "334-0",
                    "sentence": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding.",
                    "tag": "1"
                },
                {
                    "index": "334-1",
                    "sentence": "Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks.",
                    "tag": "1"
                },
                {
                    "index": "334-2",
                    "sentence": "One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects.",
                    "tag": "1"
                },
                {
                    "index": "334-3",
                    "sentence": "To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling.",
                    "tag": "2"
                },
                {
                    "index": "334-4",
                    "sentence": "To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.",
                    "tag": "3"
                },
                {
                    "index": "334-5",
                    "sentence": "This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs.",
                    "tag": "3"
                },
                {
                    "index": "334-6",
                    "sentence": "Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation.",
                    "tag": "3"
                },
                {
                    "index": "334-7",
                    "sentence": "We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-335",
            "text": [
                {
                    "index": "335-0",
                    "sentence": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes.",
                    "tag": "1+2"
                },
                {
                    "index": "335-1",
                    "sentence": "The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.",
                    "tag": "3+4"
                },
                {
                    "index": "335-2",
                    "sentence": "Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-336",
            "text": [
                {
                    "index": "336-0",
                    "sentence": "Neural machine translation is a recently proposed approach to machine translation.",
                    "tag": "1"
                },
                {
                    "index": "336-1",
                    "sentence": "Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.",
                    "tag": "1"
                },
                {
                    "index": "336-2",
                    "sentence": "The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.",
                    "tag": "1"
                },
                {
                    "index": "336-3",
                    "sentence": "In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.",
                    "tag": "2"
                },
                {
                    "index": "336-4",
                    "sentence": "With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation.",
                    "tag": "4"
                },
                {
                    "index": "336-5",
                    "sentence": "Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-337",
            "text": [
                {
                    "index": "337-0",
                    "sentence": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks.",
                    "tag": "1"
                },
                {
                    "index": "337-1",
                    "sentence": "Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.",
                    "tag": "1"
                },
                {
                    "index": "337-2",
                    "sentence": "In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.",
                    "tag": "2"
                },
                {
                    "index": "337-3",
                    "sentence": "Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.",
                    "tag": "3"
                },
                {
                    "index": "337-4",
                    "sentence": "Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words.",
                    "tag": "4"
                },
                {
                    "index": "337-5",
                    "sentence": "Additionally, the LSTM did not have difficulty on long sentences.",
                    "tag": "4"
                },
                {
                    "index": "337-6",
                    "sentence": "For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset.",
                    "tag": "4"
                },
                {
                    "index": "337-7",
                    "sentence": "When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task.",
                    "tag": "3+4"
                },
                {
                    "index": "337-8",
                    "sentence": "The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice.",
                    "tag": "4"
                },
                {
                    "index": "337-9",
                    "sentence": "Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-338",
            "text": [
                {
                    "index": "338-0",
                    "sentence": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN).",
                    "tag": "2"
                },
                {
                    "index": "338-1",
                    "sentence": "One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols.",
                    "tag": "3"
                },
                {
                    "index": "338-2",
                    "sentence": "The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence.",
                    "tag": "3"
                },
                {
                    "index": "338-3",
                    "sentence": "The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model.",
                    "tag": "3+4"
                },
                {
                    "index": "338-4",
                    "sentence": "Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-339",
            "text": [
                {
                    "index": "339-0",
                    "sentence": "The ability to accurately represent sentences is central to language understanding.",
                    "tag": "1"
                },
                {
                    "index": "339-1",
                    "sentence": "We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences.",
                    "tag": "2"
                },
                {
                    "index": "339-2",
                    "sentence": "The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences.",
                    "tag": "2+3"
                },
                {
                    "index": "339-3",
                    "sentence": "The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations.",
                    "tag": "2+3"
                },
                {
                    "index": "339-4",
                    "sentence": "The network does not rely on a parse tree and is easily applicable to any language.",
                    "tag": "2+3"
                },
                {
                    "index": "339-5",
                    "sentence": "We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision.",
                    "tag": "3"
                },
                {
                    "index": "339-6",
                    "sentence": "The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-340",
            "text": [
                {
                    "index": "340-0",
                    "sentence": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector.",
                    "tag": "1"
                },
                {
                    "index": "340-1",
                    "sentence": "When it comes to texts, one of the most common fixed-length features is bag-of-words.",
                    "tag": "1"
                },
                {
                    "index": "340-2",
                    "sentence": "Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words.",
                    "tag": "1"
                },
                {
                    "index": "340-3",
                    "sentence": "For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant.",
                    "tag": "1"
                },
                {
                    "index": "340-4",
                    "sentence": "In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.",
                    "tag": "2"
                },
                {
                    "index": "340-5",
                    "sentence": "Our algorithm represents each document by a dense vector which is trained to predict words in the document.",
                    "tag": "2+3"
                },
                {
                    "index": "340-6",
                    "sentence": "Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models.",
                    "tag": "3+4"
                },
                {
                    "index": "340-7",
                    "sentence": "Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.",
                    "tag": "4"
                },
                {
                    "index": "340-8",
                    "sentence": "Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-341",
            "text": [
                {
                    "index": "341-0",
                    "sentence": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.",
                    "tag": "1"
                },
                {
                    "index": "341-1",
                    "sentence": "In this paper we present several extensions that improve both the quality of the vectors and the training speed.",
                    "tag": "2"
                },
                {
                    "index": "341-2",
                    "sentence": "By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations.",
                    "tag": "3"
                },
                {
                    "index": "341-3",
                    "sentence": "We also describe a simple alternative to the hierarchical softmax called negative sampling.",
                    "tag": "3"
                },
                {
                    "index": "341-4",
                    "sentence": "An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.",
                    "tag": "4"
                },
                {
                    "index": "341-5",
                    "sentence": "For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".",
                    "tag": "4"
                },
                {
                    "index": "341-6",
                    "sentence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-342",
            "text": [
                {
                    "index": "342-0",
                    "sentence": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets.",
                    "tag": "2"
                },
                {
                    "index": "342-1",
                    "sentence": "The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.",
                    "tag": "2+3"
                },
                {
                    "index": "342-2",
                    "sentence": "We observe large improvements in accuracy at much lower computational cost, i.e.it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.",
                    "tag": "3"
                },
                {
                    "index": "342-3",
                    "sentence": "Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-343",
            "text": [
                {
                    "index": "343-0",
                    "sentence": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time.",
                    "tag": "2"
                },
                {
                    "index": "343-1",
                    "sentence": "The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued).",
                    "tag": "2+3"
                },
                {
                    "index": "343-2",
                    "sentence": "It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence.",
                    "tag": "3"
                },
                {
                    "index": "343-3",
                    "sentence": "The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-344",
            "text": [
                {
                    "index": "344-0",
                    "sentence": "Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs).",
                    "tag": "1"
                },
                {
                    "index": "344-1",
                    "sentence": "Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding.",
                    "tag": "1"
                },
                {
                    "index": "344-2",
                    "sentence": "We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level.",
                    "tag": "2"
                },
                {
                    "index": "344-3",
                    "sentence": "Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN.",
                    "tag": "3"
                },
                {
                    "index": "344-4",
                    "sentence": "For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames.",
                    "tag": "3"
                },
                {
                    "index": "344-5",
                    "sentence": "We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length.",
                    "tag": "3"
                },
                {
                    "index": "344-6",
                    "sentence": "Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-345",
            "text": [
                {
                    "index": "345-0",
                    "sentence": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages.",
                    "tag": "2"
                },
                {
                    "index": "345-1",
                    "sentence": "Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.",
                    "tag": "3"
                },
                {
                    "index": "345-2",
                    "sentence": "Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system.",
                    "tag": "2+3"
                },
                {
                    "index": "345-3",
                    "sentence": "Because of this efficiency, experiments that previously took weeks now run in days.",
                    "tag": "4"
                },
                {
                    "index": "345-4",
                    "sentence": "This enables us to iterate more quickly to identify superior architectures and algorithms.",
                    "tag": "4"
                },
                {
                    "index": "345-5",
                    "sentence": "As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets.",
                    "tag": "4+5"
                },
                {
                    "index": "345-6",
                    "sentence": "Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-346",
            "text": [
                {
                    "index": "346-0",
                    "sentence": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition.",
                    "tag": "1+2"
                },
                {
                    "index": "346-1",
                    "sentence": "We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output.",
                    "tag": "2"
                },
                {
                    "index": "346-2",
                    "sentence": "The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error.",
                    "tag": "2+3"
                },
                {
                    "index": "346-3",
                    "sentence": "We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance.",
                    "tag": "3"
                },
                {
                    "index": "346-4",
                    "sentence": "Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-347",
            "text": [
                {
                    "index": "347-0",
                    "sentence": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control.",
                    "tag": "1"
                },
                {
                    "index": "347-1",
                    "sentence": "In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately?",
                    "tag": "1+2"
                },
                {
                    "index": "347-2",
                    "sentence": "To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors.",
                    "tag": "2+3"
                },
                {
                    "index": "347-3",
                    "sentence": "The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method.",
                    "tag": "3+4"
                },
                {
                    "index": "347-4",
                    "sentence": "We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-348",
            "text": [
                {
                    "index": "348-0",
                    "sentence": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain.",
                    "tag": "2"
                },
                {
                    "index": "348-1",
                    "sentence": "We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.",
                    "tag": "2+3"
                },
                {
                    "index": "348-2",
                    "sentence": "Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving.",
                    "tag": "2+3"
                },
                {
                    "index": "348-3",
                    "sentence": "Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives.",
                    "tag": "4"
                },
                {
                    "index": "348-4",
                    "sentence": "We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-349",
            "text": [
                {
                    "index": "349-0",
                    "sentence": "We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects.",
                    "tag": "2"
                },
                {
                    "index": "349-1",
                    "sentence": "In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features.",
                    "tag": "3"
                },
                {
                    "index": "349-2",
                    "sentence": "This presents two main challenges.",
                    "tag": "2+3"
                },
                {
                    "index": "349-3",
                    "sentence": "First, we need to evaluate a huge number of candidate grasps.",
                    "tag": "3"
                },
                {
                    "index": "349-4",
                    "sentence": "In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second.",
                    "tag": "3"
                },
                {
                    "index": "349-5",
                    "sentence": "The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps.",
                    "tag": "3"
                },
                {
                    "index": "349-6",
                    "sentence": "The second, with more features, is slower but has to run only on the top few detections.",
                    "tag": "3"
                },
                {
                    "index": "349-7",
                    "sentence": "Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization.",
                    "tag": "3"
                },
                {
                    "index": "349-8",
                    "sentence": "We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-350",
            "text": [
                {
                    "index": "350-0",
                    "sentence": "We propose a deep learning method for single image super-resolution (SR).",
                    "tag": "2"
                },
                {
                    "index": "350-1",
                    "sentence": "Our method directly learns an end-to-end mapping between the low/high-resolution images.",
                    "tag": "2"
                },
                {
                    "index": "350-2",
                    "sentence": "The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one.",
                    "tag": "2+3"
                },
                {
                    "index": "350-3",
                    "sentence": "We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network.",
                    "tag": "2+3"
                },
                {
                    "index": "350-4",
                    "sentence": "But unlike traditional methods that handle each component separately, our method jointly optimizes all layers.",
                    "tag": "2+3"
                },
                {
                    "index": "350-5",
                    "sentence": "Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",
                    "tag": "3"
                },
                {
                    "index": "350-6",
                    "sentence": "We explore different network structures and parameter settings to achieve trade-offs between performance and speed.",
                    "tag": "3"
                },
                {
                    "index": "350-7",
                    "sentence": "Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-351",
            "text": [
                {
                    "index": "351-0",
                    "sentence": "In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image.",
                    "tag": "1"
                },
                {
                    "index": "351-1",
                    "sentence": "Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities.",
                    "tag": "1"
                },
                {
                    "index": "351-2",
                    "sentence": "However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.",
                    "tag": "1"
                },
                {
                    "index": "351-3",
                    "sentence": "Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality.",
                    "tag": "2"
                },
                {
                    "index": "351-4",
                    "sentence": "The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images.",
                    "tag": "2"
                },
                {
                    "index": "351-5",
                    "sentence": "Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-352",
            "text": [
                {
                    "index": "352-0",
                    "sentence": "We present a model that generates natural language descriptions of images and their regions.",
                    "tag": "2"
                },
                {
                    "index": "352-1",
                    "sentence": "Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data.",
                    "tag": "2"
                },
                {
                    "index": "352-2",
                    "sentence": "Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding.",
                    "tag": "2+3"
                },
                {
                    "index": "352-3",
                    "sentence": "We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions.",
                    "tag": "3+4"
                },
                {
                    "index": "352-4",
                    "sentence": "We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets.",
                    "tag": "3+4"
                },
                {
                    "index": "352-5",
                    "sentence": "We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-353",
            "text": [
                {
                    "index": "353-0",
                    "sentence": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images.",
                    "tag": "2"
                },
                {
                    "index": "353-1",
                    "sentence": "We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound.",
                    "tag": "2+3"
                },
                {
                    "index": "353-2",
                    "sentence": "We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence.",
                    "tag": "3+4"
                },
                {
                    "index": "353-3",
                    "sentence": "We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-354",
            "text": [
                {
                    "index": "354-0",
                    "sentence": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.",
                    "tag": "1"
                },
                {
                    "index": "354-1",
                    "sentence": "In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image.",
                    "tag": "2"
                },
                {
                    "index": "354-2",
                    "sentence": "The model is trained to maximize the likelihood of the target description sentence given the training image.",
                    "tag": "2+3"
                },
                {
                    "index": "354-3",
                    "sentence": "Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions.",
                    "tag": "3+4"
                },
                {
                    "index": "354-4",
                    "sentence": "Our model is often quite accurate, which we verify both qualitatively and quantitatively.",
                    "tag": "4"
                },
                {
                    "index": "354-5",
                    "sentence": "For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69.",
                    "tag": "3+4"
                },
                {
                    "index": "354-6",
                    "sentence": "We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28.",
                    "tag": "4"
                },
                {
                    "index": "354-7",
                    "sentence": "Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-355",
            "text": [
                {
                    "index": "355-0",
                    "sentence": "We consider the automated recognition of human actions in surveillance videos.",
                    "tag": "2"
                },
                {
                    "index": "355-1",
                    "sentence": "Most current methods build classifiers based on complex handcrafted features computed from the raw inputs.",
                    "tag": "1"
                },
                {
                    "index": "355-2",
                    "sentence": "Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs.",
                    "tag": "1"
                },
                {
                    "index": "355-3",
                    "sentence": "However, such models are currently limited to handling 2D inputs.",
                    "tag": "1"
                },
                {
                    "index": "355-4",
                    "sentence": "In this paper, we develop a novel 3D CNN model for action recognition.",
                    "tag": "2"
                },
                {
                    "index": "355-5",
                    "sentence": "This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames.",
                    "tag": "3"
                },
                {
                    "index": "355-6",
                    "sentence": "The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels.",
                    "tag": "3"
                },
                {
                    "index": "355-7",
                    "sentence": "To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models.",
                    "tag": "3"
                },
                {
                    "index": "355-8",
                    "sentence": "We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-356",
            "text": [
                {
                    "index": "356-0",
                    "sentence": "Convolutional networks are powerful visual models that yield hierarchies of features.",
                    "tag": "1"
                },
                {
                    "index": "356-1",
                    "sentence": "We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation.",
                    "tag": "1+2"
                },
                {
                    "index": "356-2",
                    "sentence": "Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.",
                    "tag": "2"
                },
                {
                    "index": "356-3",
                    "sentence": "We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models.",
                    "tag": "2+3"
                },
                {
                    "index": "356-4",
                    "sentence": "We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task.",
                    "tag": "3"
                },
                {
                    "index": "356-5",
                    "sentence": "We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.",
                    "tag": "3+4"
                },
                {
                    "index": "356-6",
                    "sentence": "Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-357",
            "text": [
                {
                    "index": "357-0",
                    "sentence": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.",
                    "tag": "1"
                },
                {
                    "index": "357-1",
                    "sentence": "Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck.",
                    "tag": "1"
                },
                {
                    "index": "357-2",
                    "sentence": "In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.",
                    "tag": "1+2"
                },
                {
                    "index": "357-3",
                    "sentence": "An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.",
                    "tag": "3"
                },
                {
                    "index": "357-4",
                    "sentence": "The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.",
                    "tag": "3"
                },
                {
                    "index": "357-5",
                    "sentence": "We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look.",
                    "tag": "3+4"
                },
                {
                    "index": "357-6",
                    "sentence": "For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image.",
                    "tag": "4"
                },
                {
                    "index": "357-7",
                    "sentence": "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks.",
                    "tag": "4"
                },
                {
                    "index": "357-8",
                    "sentence": "Code has been made publicly available.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-358",
            "text": [
                {
                    "index": "358-0",
                    "sentence": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.",
                    "tag": "1"
                },
                {
                    "index": "358-1",
                    "sentence": "The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context.",
                    "tag": "1"
                },
                {
                    "index": "358-2",
                    "sentence": "In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%.",
                    "tag": "2"
                },
                {
                    "index": "358-3",
                    "sentence": "Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.",
                    "tag": "2+3"
                },
                {
                    "index": "358-4",
                    "sentence": "Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.",
                    "tag": "3"
                },
                {
                    "index": "358-5",
                    "sentence": "We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture.",
                    "tag": "3"
                },
                {
                    "index": "358-6",
                    "sentence": "We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset.",
                    "tag": "3+4"
                },
                {
                    "index": "358-7",
                    "sentence": "Source code for the complete system is available at this http URL.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-359",
            "text": [
                {
                    "index": "359-0",
                    "sentence": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image.",
                    "tag": "1"
                },
                {
                    "index": "359-1",
                    "sentence": "This requirement is \"artificial\" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale.",
                    "tag": "1"
                },
                {
                    "index": "359-2",
                    "sentence": "In this work, we equip the networks with another pooling strategy, \"spatial pyramid pooling\", to eliminate the above requirement.",
                    "tag": "2"
                },
                {
                    "index": "359-3",
                    "sentence": "The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale.",
                    "tag": "2+3"
                },
                {
                    "index": "359-4",
                    "sentence": "Pyramid pooling is also robust to object deformations.",
                    "tag": "3"
                },
                {
                    "index": "359-5",
                    "sentence": "With these advantages, SPP-net should in general improve all CNN-based image classification methods.",
                    "tag": "3"
                },
                {
                    "index": "359-6",
                    "sentence": "On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs.",
                    "tag": "4"
                },
                {
                    "index": "359-7",
                    "sentence": "On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.",
                    "tag": "3+4"
                },
                {
                    "index": "359-8",
                    "sentence": "The power of SPP-net is also significant in object detection.",
                    "tag": "4"
                },
                {
                    "index": "359-9",
                    "sentence": "Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors.",
                    "tag": "4"
                },
                {
                    "index": "359-10",
                    "sentence": "This method avoids repeatedly computing the convolutional features.",
                    "tag": "1"
                },
                {
                    "index": "359-11",
                    "sentence": "In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.",
                    "tag": "1"
                },
                {
                    "index": "359-12",
                    "sentence": "In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams.",
                    "tag": "1"
                },
                {
                    "index": "359-13",
                    "sentence": "This manuscript also introduces the improvement made for this competition.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-360",
            "text": [
                {
                    "index": "360-0",
                    "sentence": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to.",
                    "tag": "1"
                },
                {
                    "index": "360-1",
                    "sentence": "We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel.",
                    "tag": "1+2"
                },
                {
                    "index": "360-2",
                    "sentence": "The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information.",
                    "tag": "3"
                },
                {
                    "index": "360-3",
                    "sentence": "We report results using multiple postprocessing methods to produce the final labeling.",
                    "tag": "3"
                },
                {
                    "index": "360-4",
                    "sentence": "Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations.",
                    "tag": "3"
                },
                {
                    "index": "360-5",
                    "sentence": "The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-361",
            "text": [
                {
                    "index": "361-0",
                    "sentence": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks.",
                    "tag": "1"
                },
                {
                    "index": "361-1",
                    "sentence": "Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks.",
                    "tag": "1"
                },
                {
                    "index": "361-2",
                    "sentence": "Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios.",
                    "tag": "1"
                },
                {
                    "index": "361-3",
                    "sentence": "Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization.",
                    "tag": "2+3"
                },
                {
                    "index": "361-4",
                    "sentence": "We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.",
                    "tag": "2+3"
                },
                {
                    "index": "361-5",
                    "sentence": "With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-362",
            "text": [
                {
                    "index": "362-0",
                    "sentence": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years.",
                    "tag": "1"
                },
                {
                    "index": "362-1",
                    "sentence": "One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost.",
                    "tag": "1"
                },
                {
                    "index": "362-2",
                    "sentence": "Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network.",
                    "tag": "1"
                },
                {
                    "index": "362-3",
                    "sentence": "This raises the question of whether there are any benefit in combining the Inception architecture with residual connections.",
                    "tag": "1+2"
                },
                {
                    "index": "362-4",
                    "sentence": "Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly.",
                    "tag": "2"
                },
                {
                    "index": "362-5",
                    "sentence": "There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin.",
                    "tag": "4"
                },
                {
                    "index": "362-6",
                    "sentence": "We also present several new streamlined architectures for both residual and non-residual Inception networks.",
                    "tag": "2"
                },
                {
                    "index": "362-7",
                    "sentence": "These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly.",
                    "tag": "4"
                },
                {
                    "index": "362-8",
                    "sentence": "We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks.",
                    "tag": "4"
                },
                {
                    "index": "362-9",
                    "sentence": "With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-363",
            "text": [
                {
                    "index": "363-0",
                    "sentence": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors.",
                    "tag": "1"
                },
                {
                    "index": "363-1",
                    "sentence": "In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation.",
                    "tag": "1+2"
                },
                {
                    "index": "363-2",
                    "sentence": "A series of ablation experiments support the importance of these identity mappings.",
                    "tag": "1"
                },
                {
                    "index": "363-3",
                    "sentence": "This motivates us to propose a new residual unit, which makes training easier and improves generalization.",
                    "tag": "1+2"
                },
                {
                    "index": "363-4",
                    "sentence": "We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",
                    "tag": "3+4"
                },
                {
                    "index": "363-5",
                    "sentence": "Code is available at: this https URL",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-364",
            "text": [
                {
                    "index": "364-0",
                    "sentence": "Deeper neural networks are more difficult to train.",
                    "tag": "1"
                },
                {
                    "index": "364-1",
                    "sentence": "We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.",
                    "tag": "1+2"
                },
                {
                    "index": "364-2",
                    "sentence": "We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
                    "tag": "2+3"
                },
                {
                    "index": "364-3",
                    "sentence": "We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.",
                    "tag": "2+3"
                },
                {
                    "index": "364-4",
                    "sentence": "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.",
                    "tag": "3"
                },
                {
                    "index": "364-5",
                    "sentence": "An ensemble of these residual nets achieves 3.57% error on the ImageNet test set.",
                    "tag": "3+4"
                },
                {
                    "index": "364-6",
                    "sentence": "This result won the 1st place on the ILSVRC 2015 classification task.",
                    "tag": "4"
                },
                {
                    "index": "364-7",
                    "sentence": "We also present analysis on CIFAR-10 with 100 and 1000 layers.",
                    "tag": "4"
                },
                {
                    "index": "364-8",
                    "sentence": "The depth of representations is of central importance for many visual recognition tasks.",
                    "tag": "4"
                },
                {
                    "index": "364-9",
                    "sentence": "Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.",
                    "tag": "4"
                },
                {
                    "index": "364-10",
                    "sentence": "Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-365",
            "text": [
                {
                    "index": "365-0",
                    "sentence": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.",
                    "tag": "1"
                },
                {
                    "index": "365-1",
                    "sentence": "In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.",
                    "tag": "2"
                },
                {
                    "index": "365-2",
                    "sentence": "This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.",
                    "tag": "3"
                },
                {
                    "index": "365-3",
                    "sentence": "We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-366",
            "text": [
                {
                    "index": "366-0",
                    "sentence": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014).",
                    "tag": "2+3"
                },
                {
                    "index": "366-1",
                    "sentence": "The main hallmark of this architecture is the improved utilization of the computing resources inside the network.",
                    "tag": "4"
                },
                {
                    "index": "366-2",
                    "sentence": "This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant.",
                    "tag": "4"
                },
                {
                    "index": "366-3",
                    "sentence": "To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.",
                    "tag": "3+4"
                },
                {
                    "index": "366-4",
                    "sentence": "One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-367",
            "text": [
                {
                    "index": "367-0",
                    "sentence": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting.",
                    "tag": "2"
                },
                {
                    "index": "367-1",
                    "sentence": "Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers.",
                    "tag": "3+4"
                },
                {
                    "index": "367-2",
                    "sentence": "These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.",
                    "tag": "4"
                },
                {
                    "index": "367-3",
                    "sentence": "We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results.",
                    "tag": "4"
                },
                {
                    "index": "367-4",
                    "sentence": "We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-368",
            "text": [
                {
                    "index": "368-0",
                    "sentence": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods.",
                    "tag": "1"
                },
                {
                    "index": "368-1",
                    "sentence": "Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector.",
                    "tag": "1"
                },
                {
                    "index": "368-2",
                    "sentence": "This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details.",
                    "tag": "2+3"
                },
                {
                    "index": "368-3",
                    "sentence": "We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance.",
                    "tag": "3"
                },
                {
                    "index": "368-4",
                    "sentence": "We also identify aspects of deep and shallow methods that can be successfully shared.",
                    "tag": "3"
                },
                {
                    "index": "368-5",
                    "sentence": "In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost.",
                    "tag": "5"
                },
                {
                    "index": "368-6",
                    "sentence": "Source code and models to reproduce the experiments in the paper is made publicly available.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-369",
            "text": [
                {
                    "index": "369-0",
                    "sentence": "We present an integrated framework for using Convolutional Networks for classification, localization and detection.",
                    "tag": "2"
                },
                {
                    "index": "369-1",
                    "sentence": "We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet.",
                    "tag": "2"
                },
                {
                    "index": "369-2",
                    "sentence": "We also introduce a novel deep learning approach to localization by learning to predict object boundaries.",
                    "tag": "2"
                },
                {
                    "index": "369-3",
                    "sentence": "Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence.",
                    "tag": "4"
                },
                {
                    "index": "369-4",
                    "sentence": "We show that different tasks can be learned simultaneously using a single shared network.",
                    "tag": "4"
                },
                {
                    "index": "369-5",
                    "sentence": "This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks.",
                    "tag": "4"
                },
                {
                    "index": "369-6",
                    "sentence": "In post-competition work, we establish a new state of the art for the detection task.",
                    "tag": "5"
                },
                {
                    "index": "369-7",
                    "sentence": "Finally, we release a feature extractor from our best model called OverFeat.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-370",
            "text": [
                {
                    "index": "370-0",
                    "sentence": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout.",
                    "tag": "2+3"
                },
                {
                    "index": "370-1",
                    "sentence": "We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique.",
                    "tag": "3"
                },
                {
                    "index": "370-2",
                    "sentence": "We empirically verify that the model successfully accomplishes both of these tasks.",
                    "tag": "3"
                },
                {
                    "index": "370-3",
                    "sentence": "We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-371",
            "text": [
                {
                    "index": "371-0",
                    "sentence": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field.",
                    "tag": "2"
                },
                {
                    "index": "371-1",
                    "sentence": "The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input.",
                    "tag": "3"
                },
                {
                    "index": "371-2",
                    "sentence": "Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field.",
                    "tag": "3"
                },
                {
                    "index": "371-3",
                    "sentence": "We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator.",
                    "tag": "3"
                },
                {
                    "index": "371-4",
                    "sentence": "The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer.",
                    "tag": "3"
                },
                {
                    "index": "371-5",
                    "sentence": "Deep NIN can be implemented by stacking mutiple of the above described structure.",
                    "tag": "3"
                },
                {
                    "index": "371-6",
                    "sentence": "With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers.",
                    "tag": "4"
                },
                {
                    "index": "371-7",
                    "sentence": "We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-372",
            "text": [
                {
                    "index": "372-0",
                    "sentence": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework.",
                    "tag": "2"
                },
                {
                    "index": "372-1",
                    "sentence": "We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic.",
                    "tag": "2"
                },
                {
                    "index": "372-2",
                    "sentence": "Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels.",
                    "tag": "2"
                },
                {
                    "index": "372-3",
                    "sentence": "Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN.",
                    "tag": "4"
                },
                {
                    "index": "372-4",
                    "sentence": "The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%.",
                    "tag": "4+5"
                },
                {
                    "index": "372-5",
                    "sentence": "We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-373",
            "text": [
                {
                    "index": "373-0",
                    "sentence": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications.",
                    "tag": "1"
                },
                {
                    "index": "373-1",
                    "sentence": "Comparatively, unsupervised learning with CNNs has received less attention.",
                    "tag": "1"
                },
                {
                    "index": "373-2",
                    "sentence": "In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning.",
                    "tag": "1"
                },
                {
                    "index": "373-3",
                    "sentence": "We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.",
                    "tag": "2+3"
                },
                {
                    "index": "373-4",
                    "sentence": "Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator.",
                    "tag": "3+4"
                },
                {
                    "index": "373-5",
                    "sentence": "Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-374",
            "text": [
                {
                    "index": "374-0",
                    "sentence": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation.",
                    "tag": "2"
                },
                {
                    "index": "374-1",
                    "sentence": "DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images.",
                    "tag": "3"
                },
                {
                    "index": "374-2",
                    "sentence": "The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-375",
            "text": [
                {
                    "index": "375-0",
                    "sentence": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. ",
                    "tag": "2+3"
                },
                {
                    "index": "375-1",
                    "sentence": "The training procedure for G is to maximize the probability of D making a mistake.",
                    "tag": "3"
                },
                {
                    "index": "375-2",
                    "sentence": "This framework corresponds to a minimax two-player game.",
                    "tag": "4"
                },
                {
                    "index": "375-3",
                    "sentence": "In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere.",
                    "tag": "4"
                },
                {
                    "index": "375-4",
                    "sentence": "In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation.",
                    "tag": "4"
                },
                {
                    "index": "375-5",
                    "sentence": "There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.",
                    "tag": "4"
                },
                {
                    "index": "375-6",
                    "sentence": "Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-376",
            "text": [
                {
                    "index": "376-0",
                    "sentence": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data.",
                    "tag": "1"
                },
                {
                    "index": "376-1",
                    "sentence": "For example, is it possible to learn a face detector using only unlabeled images?",
                    "tag": "1"
                },
                {
                    "index": "376-2",
                    "sentence": "To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet).",
                    "tag": "2+3"
                },
                {
                    "index": "376-3",
                    "sentence": "We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days.",
                    "tag": "2+3"
                },
                {
                    "index": "376-4",
                    "sentence": "Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not.",
                    "tag": "4"
                },
                {
                    "index": "376-5",
                    "sentence": "Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.",
                    "tag": "4"
                },
                {
                    "index": "376-6",
                    "sentence": "We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies.",
                    "tag": "4"
                },
                {
                    "index": "376-7",
                    "sentence": "Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-377",
            "text": [
                {
                    "index": "377-0",
                    "sentence": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success.",
                    "tag": "1"
                },
                {
                    "index": "377-1",
                    "sentence": "However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem.",
                    "tag": "1"
                },
                {
                    "index": "377-2",
                    "sentence": "Here we introduce a new architecture designed to overcome this.",
                    "tag": "1"
                },
                {
                    "index": "377-3",
                    "sentence": "Our so-called highway networks allow unimpeded information flow across many layers on information highways.",
                    "tag": "1"
                },
                {
                    "index": "377-4",
                    "sentence": "They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow.",
                    "tag": "2+3"
                },
                {
                    "index": "377-5",
                    "sentence": "Even with hundreds of layers, highway networks can be trained directly through simple gradient descent.",
                    "tag": "3"
                },
                {
                    "index": "377-6",
                    "sentence": "This enables the study of extremely deep and efficient architectures.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-378",
            "text": [
                {
                    "index": "378-0",
                    "sentence": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.",
                    "tag": "1"
                },
                {
                    "index": "378-1",
                    "sentence": "This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.",
                    "tag": "1"
                },
                {
                    "index": "378-2",
                    "sentence": "We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.",
                    "tag": "2"
                },
                {
                    "index": "378-3",
                    "sentence": "Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch.",
                    "tag": "3+4"
                },
                {
                    "index": "378-4",
                    "sentence": "Batch Normalization allows us to use much higher learning rates and be less careful about initialization.",
                    "tag": "3+4"
                },
                {
                    "index": "378-5",
                    "sentence": "It also acts as a regularizer, in some cases eliminating the need for Dropout.",
                    "tag": "4"
                },
                {
                    "index": "378-6",
                    "sentence": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.",
                    "tag": "4"
                },
                {
                    "index": "378-7",
                    "sentence": "Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-379",
            "text": [
                {
                    "index": "379-0",
                    "sentence": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.",
                    "tag": "1"
                },
                {
                    "index": "379-1",
                    "sentence": "In this work, we study rectifier neural networks for image classification from two aspects.",
                    "tag": "2"
                },
                {
                    "index": "379-2",
                    "sentence": "First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit.",
                    "tag": "2"
                },
                {
                    "index": "379-3",
                    "sentence": "PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk.",
                    "tag": "4"
                },
                {
                    "index": "379-4",
                    "sentence": "Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities.",
                    "tag": "4"
                },
                {
                    "index": "379-5",
                    "sentence": "This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures.",
                    "tag": "5"
                },
                {
                    "index": "379-6",
                    "sentence": "Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset.",
                    "tag": "4"
                },
                {
                    "index": "379-7",
                    "sentence": "This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%).",
                    "tag": "4"
                },
                {
                    "index": "379-8",
                    "sentence": "To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-380",
            "text": [
                {
                    "index": "380-0",
                    "sentence": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data.",
                    "tag": "1"
                },
                {
                    "index": "380-1",
                    "sentence": "This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case.",
                    "tag": "2+3"
                },
                {
                    "index": "380-2",
                    "sentence": "This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors.",
                    "tag": "4"
                },
                {
                    "index": "380-3",
                    "sentence": "Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate.",
                    "tag": "3+4"
                },
                {
                    "index": "380-4",
                    "sentence": "Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-381",
            "text": [
                {
                    "index": "381-0",
                    "sentence": "Training state-of-the-art, deep neural networks is computationally expensive.",
                    "tag": "1"
                },
                {
                    "index": "381-1",
                    "sentence": "One way to reduce the training time is to normalize the activities of the neurons.",
                    "tag": "1"
                },
                {
                    "index": "381-2",
                    "sentence": "A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.",
                    "tag": "1"
                },
                {
                    "index": "381-3",
                    "sentence": "This significantly reduces the training time in feed-forward neural networks.",
                    "tag": "1"
                },
                {
                    "index": "381-4",
                    "sentence": "However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks.",
                    "tag": "1"
                },
                {
                    "index": "381-5",
                    "sentence": "In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.",
                    "tag": "2+3"
                },
                {
                    "index": "381-6",
                    "sentence": "Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity.",
                    "tag": "2"
                },
                {
                    "index": "381-7",
                    "sentence": "Unlike batch normalization, layer normalization performs exactly the same computation at training and test times.",
                    "tag": "3"
                },
                {
                    "index": "381-8",
                    "sentence": "It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step.",
                    "tag": "3"
                },
                {
                    "index": "381-9",
                    "sentence": "Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.",
                    "tag": "5"
                },
                {
                    "index": "381-10",
                    "sentence": "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-382",
            "text": [
                {
                    "index": "382-0",
                    "sentence": "The move from hand-designed features to learned features in machine learning has been wildly successful.",
                    "tag": "1"
                },
                {
                    "index": "382-1",
                    "sentence": "In spite of this, optimization algorithms are still designed by hand.",
                    "tag": "1"
                },
                {
                    "index": "382-2",
                    "sentence": "In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way.",
                    "tag": "2+3"
                },
                {
                    "index": "382-3",
                    "sentence": "Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure.",
                    "tag": "3"
                },
                {
                    "index": "382-4",
                    "sentence": "We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-383",
            "text": [
                {
                    "index": "383-0",
                    "sentence": "Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result.",
                    "tag": "1"
                },
                {
                    "index": "383-1",
                    "sentence": "Unless the user has considerable artistic skill, it is easy to \"fall off\" the manifold of natural images while editing.",
                    "tag": "1"
                },
                {
                    "index": "383-2",
                    "sentence": "In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network.",
                    "tag": "2+3"
                },
                {
                    "index": "383-3",
                    "sentence": "We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times.",
                    "tag": "2"
                },
                {
                    "index": "383-4",
                    "sentence": "The model automatically adjusts the output keeping all edits as realistic as possible.",
                    "tag": "3"
                },
                {
                    "index": "383-5",
                    "sentence": "All our manipulations are expressed in terms of constrained optimization and are applied in near-real time.",
                    "tag": "4"
                },
                {
                    "index": "383-6",
                    "sentence": "We evaluate our algorithm on the task of realistic photo manipulation of shape and color.",
                    "tag": "3"
                },
                {
                    "index": "383-7",
                    "sentence": "The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-384",
            "text": [
                {
                    "index": "384-0",
                    "sentence": "Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example.",
                    "tag": "1"
                },
                {
                    "index": "384-1",
                    "sentence": "However, their methods requires a slow and memory-consuming optimization process.",
                    "tag": "1"
                },
                {
                    "index": "384-2",
                    "sentence": "We propose here an alternative approach that moves the computational burden to a learning stage.",
                    "tag": "2"
                },
                {
                    "index": "384-3",
                    "sentence": "Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image.",
                    "tag": "2+3"
                },
                {
                    "index": "384-4",
                    "sentence": "The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster.",
                    "tag": "4"
                },
                {
                    "index": "384-5",
                    "sentence": "More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-385",
            "text": [
                {
                    "index": "385-0",
                    "sentence": "Recent research on deep neural networks has focused primarily on improving accuracy.",
                    "tag": "1"
                },
                {
                    "index": "385-1",
                    "sentence": "For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level.",
                    "tag": "1"
                },
                {
                    "index": "385-2",
                    "sentence": "With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training.",
                    "tag": "1"
                },
                {
                    "index": "385-3",
                    "sentence": "(2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car.",
                    "tag": "1"
                },
                {
                    "index": "385-4",
                    "sentence": "(3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory.",
                    "tag": "1"
                },
                {
                    "index": "385-5",
                    "sentence": "To provide all of these advantages, we propose a small DNN architecture called SqueezeNet.",
                    "tag": "2"
                },
                {
                    "index": "385-6",
                    "sentence": "SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.",
                    "tag": "4"
                },
                {
                    "index": "385-7",
                    "sentence": "Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).",
                    "tag": "4"
                },
                {
                    "index": "385-8",
                    "sentence": "The SqueezeNet architecture is available for download here: this https URL",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-386",
            "text": [
                {
                    "index": "386-0",
                    "sentence": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets.",
                    "tag": "1"
                },
                {
                    "index": "386-1",
                    "sentence": "While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.",
                    "tag": "1"
                },
                {
                    "index": "386-2",
                    "sentence": "Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM.",
                    "tag": "1"
                },
                {
                    "index": "386-3",
                    "sentence": "This compression is achieved by pruning the redundant connections and having multiple connections share the same weight.",
                    "tag": "1"
                },
                {
                    "index": "386-4",
                    "sentence": "We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing.",
                    "tag": "2+3"
                },
                {
                    "index": "386-5",
                    "sentence": "Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.",
                    "tag": "4"
                },
                {
                    "index": "386-6",
                    "sentence": "Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression.",
                    "tag": "4"
                },
                {
                    "index": "386-7",
                    "sentence": "EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW.",
                    "tag": "4"
                },
                {
                    "index": "386-8",
                    "sentence": "It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively.",
                    "tag": "4"
                },
                {
                    "index": "386-9",
                    "sentence": "Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-387",
            "text": [
                {
                    "index": "387-0",
                    "sentence": "We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time.",
                    "tag": "2"
                },
                {
                    "index": "387-1",
                    "sentence": "At training-time the binary weights and activations are used for computing the parameters gradients.",
                    "tag": "3"
                },
                {
                    "index": "387-2",
                    "sentence": "During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency.",
                    "tag": "3"
                },
                {
                    "index": "387-3",
                    "sentence": "To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks.",
                    "tag": "3"
                },
                {
                    "index": "387-4",
                    "sentence": "On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets.",
                    "tag": "4"
                },
                {
                    "index": "387-5",
                    "sentence": "Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy.",
                    "tag": "4"
                },
                {
                    "index": "387-6",
                    "sentence": "The code for training and running our BNNs is available on-line.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-388",
            "text": [
                {
                    "index": "388-0",
                    "sentence": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering.",
                    "tag": "1"
                },
                {
                    "index": "388-1",
                    "sentence": "One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks.",
                    "tag": "1"
                },
                {
                    "index": "388-2",
                    "sentence": "However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images.",
                    "tag": "1"
                },
                {
                    "index": "388-3",
                    "sentence": "Based on an analysis of the DMN, we propose several improvements to its memory and input modules.",
                    "tag": "2"
                },
                {
                    "index": "388-4",
                    "sentence": "Together with these changes we introduce a novel input module for images in order to be able to answer visual questions.",
                    "tag": "1+2"
                },
                {
                    "index": "388-5",
                    "sentence": "Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-389",
            "text": [
                {
                    "index": "389-0",
                    "sentence": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.",
                    "tag": "2"
                },
                {
                    "index": "389-1",
                    "sentence": "SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer.",
                    "tag": "1"
                },
                {
                    "index": "389-2",
                    "sentence": "We argue that image question answering (QA) often requires multiple steps of reasoning.",
                    "tag": "2"
                },
                {
                    "index": "389-3",
                    "sentence": "Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively.",
                    "tag": "3"
                },
                {
                    "index": "389-4",
                    "sentence": "Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches.",
                    "tag": "4"
                },
                {
                    "index": "389-5",
                    "sentence": "The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-390",
            "text": [
                {
                    "index": "390-0",
                    "sentence": "Batch Normalization is quite effective at accelerating and improving the training of deep models.",
                    "tag": "1"
                },
                {
                    "index": "390-1",
                    "sentence": "However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples.",
                    "tag": "1"
                },
                {
                    "index": "390-2",
                    "sentence": "We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference.",
                    "tag": "1"
                },
                {
                    "index": "390-3",
                    "sentence": "We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch.",
                    "tag": "2+3"
                },
                {
                    "index": "390-4",
                    "sentence": "Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches.",
                    "tag": "3+4"
                },
                {
                    "index": "390-5",
                    "sentence": "At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-391",
            "text": [
                {
                    "index": "391-0",
                    "sentence": "Scale variation is one of the key challenges in object detection.",
                    "tag": "1"
                },
                {
                    "index": "391-1",
                    "sentence": "In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection.",
                    "tag": "2"
                },
                {
                    "index": "391-2",
                    "sentence": "Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power.",
                    "tag": "2"
                },
                {
                    "index": "391-3",
                    "sentence": "We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields.",
                    "tag": "3"
                },
                {
                    "index": "391-4",
                    "sentence": "Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training.",
                    "tag": "3"
                },
                {
                    "index": "391-5",
                    "sentence": "As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector.",
                    "tag": "4"
                },
                {
                    "index": "391-6",
                    "sentence": "On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP.",
                    "tag": "4"
                },
                {
                    "index": "391-7",
                    "sentence": "Codes are available at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-392",
            "text": [
                {
                    "index": "392-0",
                    "sentence": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.",
                    "tag": "1"
                },
                {
                    "index": "392-1",
                    "sentence": "Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.",
                    "tag": "1"
                },
                {
                    "index": "392-2",
                    "sentence": "However, we found that this loss function may lead to the vanishing gradients problem during the learning process.",
                    "tag": "1"
                },
                {
                    "index": "392-3",
                    "sentence": "To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.",
                    "tag": "2"
                },
                {
                    "index": "392-4",
                    "sentence": "We show that minimizing the objective function of LSGAN yields minimizing the Pearson χ2 divergence.",
                    "tag": "3+4"
                },
                {
                    "index": "392-5",
                    "sentence": "There are two benefits of LSGANs over regular GANs.",
                    "tag": "4"
                },
                {
                    "index": "392-6",
                    "sentence": "First, LSGANs are able to generate higher quality images than regular GANs.",
                    "tag": "4"
                },
                {
                    "index": "392-7",
                    "sentence": "Second, LSGANs perform more stable during the learning process.",
                    "tag": "4"
                },
                {
                    "index": "392-8",
                    "sentence": "We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.",
                    "tag": "3+4"
                },
                {
                    "index": "392-9",
                    "sentence": "We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-393",
            "text": [
                {
                    "index": "393-0",
                    "sentence": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation.",
                    "tag": "1"
                },
                {
                    "index": "393-1",
                    "sentence": "In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation?",
                    "tag": "1"
                },
                {
                    "index": "393-2",
                    "sentence": "To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.",
                    "tag": "2+3"
                },
                {
                    "index": "393-3",
                    "sentence": "Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs.",
                    "tag": "4"
                },
                {
                    "index": "393-4",
                    "sentence": "Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-394",
            "text": [
                {
                    "index": "394-0",
                    "sentence": "Object category localization is a challenging problem in computer vision.",
                    "tag": "1"
                },
                {
                    "index": "394-1",
                    "sentence": "Standard supervised training requires bounding box annotations of object instances.",
                    "tag": "1"
                },
                {
                    "index": "394-2",
                    "sentence": "This time-consuming annotation process is sidestepped in weakly supervised learning.",
                    "tag": "1"
                },
                {
                    "index": "394-3",
                    "sentence": "In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations.",
                    "tag": "1"
                },
                {
                    "index": "394-4",
                    "sentence": "We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images.",
                    "tag": "1"
                },
                {
                    "index": "394-5",
                    "sentence": "Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations.",
                    "tag": "2+3"
                },
                {
                    "index": "394-6",
                    "sentence": "This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features.",
                    "tag": "3"
                },
                {
                    "index": "394-7",
                    "sentence": "We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior.",
                    "tag": "3"
                },
                {
                    "index": "394-8",
                    "sentence": "We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-395",
            "text": [
                {
                    "index": "395-0",
                    "sentence": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output.",
                    "tag": "2+3"
                },
                {
                    "index": "395-1",
                    "sentence": "ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients.",
                    "tag": "3"
                },
                {
                    "index": "395-2",
                    "sentence": "Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers.",
                    "tag": "3"
                },
                {
                    "index": "395-3",
                    "sentence": "Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem.",
                    "tag": "3"
                },
                {
                    "index": "395-4",
                    "sentence": "We also present character-level language modelling results on the Hutter prize Wikipedia dataset.",
                    "tag": "3"
                },
                {
                    "index": "395-5",
                    "sentence": "In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences.",
                    "tag": "4+5"
                },
                {
                    "index": "395-6",
                    "sentence": "This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-396",
            "text": [
                {
                    "index": "396-0",
                    "sentence": "Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition.",
                    "tag": "1"
                },
                {
                    "index": "396-1",
                    "sentence": "The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context.",
                    "tag": "1"
                },
                {
                    "index": "396-2",
                    "sentence": "In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent.",
                    "tag": "2+3"
                },
                {
                    "index": "396-3",
                    "sentence": "Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly.",
                    "tag": "3+4"
                },
                {
                    "index": "396-4",
                    "sentence": "Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.",
                    "tag": "4"
                },
                {
                    "index": "396-5",
                    "sentence": "Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-397",
            "text": [
                {
                    "index": "397-0",
                    "sentence": "Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc.",
                    "tag": "1"
                },
                {
                    "index": "397-1",
                    "sentence": "Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance.",
                    "tag": "1"
                },
                {
                    "index": "397-2",
                    "sentence": "In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN.",
                    "tag": "2+3"
                },
                {
                    "index": "397-3",
                    "sentence": "It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances.",
                    "tag": "3"
                },
                {
                    "index": "397-4",
                    "sentence": "Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis.",
                    "tag": "4"
                },
                {
                    "index": "397-5",
                    "sentence": "Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets.",
                    "tag": "4"
                },
                {
                    "index": "397-6",
                    "sentence": "Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task.",
                    "tag": "5"
                },
                {
                    "index": "397-7",
                    "sentence": "Code and models are public available.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-398",
            "text": [
                {
                    "index": "398-0",
                    "sentence": "This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors.",
                    "tag": "2"
                },
                {
                    "index": "398-1",
                    "sentence": "Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task.",
                    "tag": "3"
                },
                {
                    "index": "398-2",
                    "sentence": "We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object.",
                    "tag": "2+3"
                },
                {
                    "index": "398-3",
                    "sentence": "Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category.",
                    "tag": "4"
                },
                {
                    "index": "398-4",
                    "sentence": "Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation.",
                    "tag": "4"
                },
                {
                    "index": "398-5",
                    "sentence": "The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at this http URL.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-399",
            "text": [
                {
                    "index": "399-0",
                    "sentence": "Multi-person pose estimation in the wild is challenging.",
                    "tag": "1"
                },
                {
                    "index": "399-1",
                    "sentence": "Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable.",
                    "tag": "1"
                },
                {
                    "index": "399-2",
                    "sentence": "These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results.",
                    "tag": "1"
                },
                {
                    "index": "399-3",
                    "sentence": "In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes.",
                    "tag": "2"
                },
                {
                    "index": "399-4",
                    "sentence": "Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG).",
                    "tag": "3"
                },
                {
                    "index": "399-5",
                    "sentence": "Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.",
                    "tag": "4"
                },
                {
                    "index": "399-6",
                    "sentence": "Our model and source codes are publicly available.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-400",
            "text": [
                {
                    "index": "400-0",
                    "sentence": "Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis.",
                    "tag": "1"
                },
                {
                    "index": "400-1",
                    "sentence": "The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression.",
                    "tag": "1"
                },
                {
                    "index": "400-2",
                    "sentence": "While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset.",
                    "tag": "1"
                },
                {
                    "index": "400-3",
                    "sentence": "To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression.",
                    "tag": "2+3"
                },
                {
                    "index": "400-4",
                    "sentence": "Our approach allows controlling the magnitude of activation of each AU and combine several of them.",
                    "tag": "4"
                },
                {
                    "index": "400-5",
                    "sentence": "Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-401",
            "text": [
                {
                    "index": "401-0",
                    "sentence": "Domain adaptation is critical for success in new, unseen environments.",
                    "tag": "1"
                },
                {
                    "index": "401-1",
                    "sentence": "Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.",
                    "tag": "1"
                },
                {
                    "index": "401-2",
                    "sentence": "Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs.",
                    "tag": "1"
                },
                {
                    "index": "401-3",
                    "sentence": "We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.",
                    "tag": "2"
                },
                {
                    "index": "401-4",
                    "sentence": "CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.",
                    "tag": "4"
                },
                {
                    "index": "401-5",
                    "sentence": "Our model can be applied in a variety of visual recognition and prediction settings.",
                    "tag": "4"
                },
                {
                    "index": "401-6",
                    "sentence": "We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-402",
            "text": [
                {
                    "index": "402-0",
                    "sentence": "On the one hand, deep neural networks are effective in learning large datasets.",
                    "tag": "1"
                },
                {
                    "index": "402-1",
                    "sentence": "On the other, they are inefficient with their data usage.",
                    "tag": "1"
                },
                {
                    "index": "402-2",
                    "sentence": "They often require copious amount of labeled-data to train their scads of parameters.",
                    "tag": "1"
                },
                {
                    "index": "402-3",
                    "sentence": "Training larger and deeper networks is hard without appropriate regularization, particularly while using a small dataset.",
                    "tag": "1"
                },
                {
                    "index": "402-4",
                    "sentence": "Laterally, collecting well-annotated data is expensive, time-consuming and often infeasible.",
                    "tag": "1"
                },
                {
                    "index": "402-5",
                    "sentence": "A popular way to regularize these networks is to simply train the network with more data from an alternate representative dataset.",
                    "tag": "1"
                },
                {
                    "index": "402-6",
                    "sentence": "This can lead to adverse effects if the statistics of the representative dataset are dissimilar to our target.",
                    "tag": "1"
                },
                {
                    "index": "402-7",
                    "sentence": "This predicament is due to the problem of domain shift.",
                    "tag": "1"
                },
                {
                    "index": "402-8",
                    "sentence": "Data from a shifted domain might not produce bespoke features when a feature extractor from the representative domain is used.",
                    "tag": "1"
                },
                {
                    "index": "402-9",
                    "sentence": "Several techniques of domain adaptation have been proposed in the past to solve this problem.",
                    "tag": "1"
                },
                {
                    "index": "402-10",
                    "sentence": "In this paper, we propose a new technique (d-SNE) of domain adaptation that cleverly uses stochastic neighborhood embedding techniques and a novel modified-Hausdorff distance.",
                    "tag": "2+3"
                },
                {
                    "index": "402-11",
                    "sentence": "The proposed technique is learnable end-to-end and is therefore, ideally suited to train neural networks.",
                    "tag": "4"
                },
                {
                    "index": "402-12",
                    "sentence": "Extensive experiments demonstrate that d-SNE outperforms the current states-of-the-art and is robust to the variances in different datasets, even in the one-shot and semi-supervised learning settings.",
                    "tag": "4+5"
                },
                {
                    "index": "402-13",
                    "sentence": "d-SNE also demonstrates the ability to generalize to multiple domains concurrently.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-403",
            "text": [
                {
                    "index": "403-0",
                    "sentence": "Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community.",
                    "tag": "1"
                },
                {
                    "index": "403-1",
                    "sentence": "A challenging benchmark named REDS is released in the NTIRE19 Challenge.",
                    "tag": "1"
                },
                {
                    "index": "403-2",
                    "sentence": "This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur.",
                    "tag": "2+3"
                },
                {
                    "index": "403-3",
                    "sentence": "In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges.",
                    "tag": "2"
                },
                {
                    "index": "403-4",
                    "sentence": "First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner.",
                    "tag": "3"
                },
                {
                    "index": "403-5",
                    "sentence": "Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration.",
                    "tag": "3"
                },
                {
                    "index": "403-6",
                    "sentence": "Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges.",
                    "tag": "4"
                },
                {
                    "index": "403-7",
                    "sentence": "EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring.",
                    "tag": "4"
                },
                {
                    "index": "403-8",
                    "sentence": "The code is available at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-404",
            "text": [
                {
                    "index": "404-0",
                    "sentence": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution.",
                    "tag": "1"
                },
                {
                    "index": "404-1",
                    "sentence": "In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction.",
                    "tag": "1"
                },
                {
                    "index": "404-2",
                    "sentence": "This means that the super-resolution (SR) operation is performed in HR space.",
                    "tag": "1"
                },
                {
                    "index": "404-3",
                    "sentence": "We demonstrate that this is sub-optimal and adds computational complexity.",
                    "tag": "1"
                },
                {
                    "index": "404-4",
                    "sentence": "In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU.",
                    "tag": "2"
                },
                {
                    "index": "404-5",
                    "sentence": "To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space.",
                    "tag": "4"
                },
                {
                    "index": "404-6",
                    "sentence": "In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output.",
                    "tag": "2+3"
                },
                {
                    "index": "404-7",
                    "sentence": "By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation.",
                    "tag": "4"
                },
                {
                    "index": "404-8",
                    "sentence": "We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-405",
            "text": [
                {
                    "index": "405-0",
                    "sentence": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors?",
                    "tag": "1"
                },
                {
                    "index": "405-1",
                    "sentence": "The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function.",
                    "tag": "1"
                },
                {
                    "index": "405-2",
                    "sentence": "Recent work has largely focused on minimizing the mean squared reconstruction error.",
                    "tag": "1"
                },
                {
                    "index": "405-3",
                    "sentence": "The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.",
                    "tag": "1"
                },
                {
                    "index": "405-4",
                    "sentence": "In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR).",
                    "tag": "2"
                },
                {
                    "index": "405-5",
                    "sentence": "To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors.",
                    "tag": "4"
                },
                {
                    "index": "405-6",
                    "sentence": "To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss.",
                    "tag": "3"
                },
                {
                    "index": "405-7",
                    "sentence": "The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images.",
                    "tag": "3+4"
                },
                {
                    "index": "405-8",
                    "sentence": "In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space.",
                    "tag": "3"
                },
                {
                    "index": "405-9",
                    "sentence": "Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "405-10",
                    "sentence": "An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN.",
                    "tag": "1"
                },
                {
                    "index": "405-11",
                    "sentence": "The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
                    "tag": "1"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-406",
            "text": [
                {
                    "index": "406-0",
                    "sentence": "Non-local self-similarity is well-known to be an effective prior for the image denoising problem.",
                    "tag": "1"
                },
                {
                    "index": "406-1",
                    "sentence": "However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information.",
                    "tag": "1"
                },
                {
                    "index": "406-2",
                    "sentence": "In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields.",
                    "tag": "2+3"
                },
                {
                    "index": "406-3",
                    "sentence": "The graph convolution operation generalizes the classic convolution to arbitrary graphs.",
                    "tag": "1"
                },
                {
                    "index": "406-4",
                    "sentence": "In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns.",
                    "tag": "2+3"
                },
                {
                    "index": "406-5",
                    "sentence": "We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution.",
                    "tag": "2"
                },
                {
                    "index": "406-6",
                    "sentence": "Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-407",
            "text": [
                {
                    "index": "407-0",
                    "sentence": "Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process.",
                    "tag": "1"
                },
                {
                    "index": "407-1",
                    "sentence": "Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments.",
                    "tag": "1"
                },
                {
                    "index": "407-2",
                    "sentence": "We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals.",
                    "tag": "2+3"
                },
                {
                    "index": "407-3",
                    "sentence": "Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink).",
                    "tag": "3"
                },
                {
                    "index": "407-4",
                    "sentence": "The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols.",
                    "tag": "4"
                },
                {
                    "index": "407-5",
                    "sentence": "In addition, the modular design enables various components to be easily usable independently in other projects.",
                    "tag": "4"
                },
                {
                    "index": "407-6",
                    "sentence": "We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-408",
            "text": [
                {
                    "index": "408-0",
                    "sentence": "Learning compressed representations of multivariate time series (MTS) facilitates data analysis in the presence of noise and redundant information, and for a large number of variates and time steps.",
                    "tag": "1"
                },
                {
                    "index": "408-1",
                    "sentence": "However, classical dimensionality reduction approaches are designed for vectorial data and cannot deal explicitly with missing values.",
                    "tag": "2"
                },
                {
                    "index": "408-2",
                    "sentence": "In this work, we propose a novel autoencoder architecture based on recurrent neural networks to generate compressed representations of MTS.",
                    "tag": "2"
                },
                {
                    "index": "408-3",
                    "sentence": "The proposed model can process inputs characterized by variable lengths and it is specifically designed to handle missing data.",
                    "tag": "3"
                },
                {
                    "index": "408-4",
                    "sentence": "Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned to a kernel function that operates in input space and that handles missing values.",
                    "tag": "3"
                },
                {
                    "index": "408-5",
                    "sentence": "This allows to learn good representations, even in the presence of a significant amount of missing data.",
                    "tag": "4"
                },
                {
                    "index": "408-6",
                    "sentence": "To show the effectiveness of the proposed approach, we evaluate the quality of the learned representations in several classification tasks, including those involving medical data, and we compare to other methods for dimensionality reduction.",
                    "tag": "2+3"
                },
                {
                    "index": "408-7",
                    "sentence": "Successively, we design two frameworks based on the proposed architecture: one for imputing missing data and another for one-class classification.",
                    "tag": "3"
                },
                {
                    "index": "408-8",
                    "sentence": "Finally, we analyze under what circumstances an autoencoder with recurrent layers can learn better compressed representations of MTS than feed-forward architectures.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-409",
            "text": [
                {
                    "index": "409-0",
                    "sentence": "Localization of salient facial landmark points, such as eye corners or the tip of the nose, is still considered a challenging computer vision problem despite recent efforts.",
                    "tag": "1"
                },
                {
                    "index": "409-1",
                    "sentence": "This is especially evident in unconstrained environments, i.e., in the presence of background clutter and large head pose variations.",
                    "tag": "1"
                },
                {
                    "index": "409-2",
                    "sentence": "Most methods that achieve state-of-the-art accuracy are slow, and, thus, have limited applications.",
                    "tag": "1"
                },
                {
                    "index": "409-3",
                    "sentence": "We describe a method that can accurately estimate the positions of relevant facial landmarks in real-time even on hardware with limited processing power, such as mobile devices.",
                    "tag": "2"
                },
                {
                    "index": "409-4",
                    "sentence": "This is achieved with a sequence of estimators based on ensembles of regression trees.",
                    "tag": "3"
                },
                {
                    "index": "409-5",
                    "sentence": "The trees use simple pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast.",
                    "tag": "3"
                },
                {
                    "index": "409-6",
                    "sentence": "We test the developed system on several publicly available datasets and analyse its processing speed on various devices.",
                    "tag": "3"
                },
                {
                    "index": "409-7",
                    "sentence": "Experimental results show that our method has practical value.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-410",
            "text": [
                {
                    "index": "410-0",
                    "sentence": "We describe efforts to adapt the Tesseract open source OCR engine for multiple scripts and languages.",
                    "tag": "2"
                },
                {
                    "index": "410-1",
                    "sentence": "Effort has been concentrated on enabling generic multi-lingual operation such that negligible customization is required for a new language beyond providing a corpus of text.",
                    "tag": "3"
                },
                {
                    "index": "410-2",
                    "sentence": "Although change was required to various modules, including physical layout analysis, and linguistic post-processing, no change was required to the character classifier beyond changing a few limits.",
                    "tag": "3"
                },
                {
                    "index": "410-3",
                    "sentence": "The Tesseract classifier has adapted easily to Simplified Chinese.",
                    "tag": "4"
                },
                {
                    "index": "410-4",
                    "sentence": "Test results on English, a mixture of European languages, and Russian, taken from a random sample of books, show a reasonably consistent word error rate between 3.72% and 5.78%, and Simplified Chinese has a character error rate of only 3.77%.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-411",
            "text": [
                {
                    "index": "411-0",
                    "sentence": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time.",
                    "tag": "1"
                },
                {
                    "index": "411-1",
                    "sentence": "In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies.",
                    "tag": "2"
                },
                {
                    "index": "411-2",
                    "sentence": "Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions.",
                    "tag": "3"
                },
                {
                    "index": "411-3",
                    "sentence": "This building block can be plugged into many computer vision architectures.",
                    "tag": "4"
                },
                {
                    "index": "411-4",
                    "sentence": "On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets.",
                    "tag": "4"
                },
                {
                    "index": "411-5",
                    "sentence": "In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks.",
                    "tag": "4"
                },
                {
                    "index": "411-6",
                    "sentence": "Code is available at this https URL .",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-412",
            "text": [
                {
                    "index": "412-0",
                    "sentence": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand.",
                    "tag": "2+3"
                },
                {
                    "index": "412-1",
                    "sentence": "We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand.",
                    "tag": "4"
                },
                {
                    "index": "412-2",
                    "sentence": "The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers.",
                    "tag": "4"
                },
                {
                    "index": "412-3",
                    "sentence": "Finally, the reprojected triangulations are used as new labeled training data to improve the detector.",
                    "tag": "4"
                },
                {
                    "index": "412-4",
                    "sentence": "We repeat this process, generating more labeled data in each iteration.",
                    "tag": "4"
                },
                {
                    "index": "412-5",
                    "sentence": "We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector.",
                    "tag": "4"
                },
                {
                    "index": "412-6",
                    "sentence": "The method is used to train a hand keypoint detector for single images.",
                    "tag": "4"
                },
                {
                    "index": "412-7",
                    "sentence": "The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors.",
                    "tag": "4"
                },
                {
                    "index": "412-8",
                    "sentence": "The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-413",
            "text": [
                {
                    "index": "413-0",
                    "sentence": "Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos.",
                    "tag": "1"
                },
                {
                    "index": "413-1",
                    "sentence": "In this work, we present a realtime approach to detect the 2D pose of multiple people in an image.",
                    "tag": "2"
                },
                {
                    "index": "413-2",
                    "sentence": "The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image.",
                    "tag": "2+3"
                },
                {
                    "index": "413-3",
                    "sentence": "This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image.",
                    "tag": "4"
                },
                {
                    "index": "413-4",
                    "sentence": "In previous work, PAFs and body part location estimation were refined simultaneously across training stages.",
                    "tag": "1"
                },
                {
                    "index": "413-5",
                    "sentence": "We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy.",
                    "tag": "4"
                },
                {
                    "index": "413-6",
                    "sentence": "We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released.",
                    "tag": "4"
                },
                {
                    "index": "413-7",
                    "sentence": "We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually.",
                    "tag": "4"
                },
                {
                    "index": "413-8",
                    "sentence": "This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-414",
            "text": [
                {
                    "index": "414-0",
                    "sentence": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.",
                    "tag": "1"
                },
                {
                    "index": "414-1",
                    "sentence": "In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.",
                    "tag": "2+3"
                },
                {
                    "index": "414-2",
                    "sentence": "The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.",
                    "tag": "3"
                },
                {
                    "index": "414-3",
                    "sentence": "We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.",
                    "tag": "3"
                },
                {
                    "index": "414-4",
                    "sentence": "Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.",
                    "tag": "3+4"
                },
                {
                    "index": "414-5",
                    "sentence": "We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-415",
            "text": [
                {
                    "index": "415-0",
                    "sentence": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.",
                    "tag": "2"
                },
                {
                    "index": "415-1",
                    "sentence": "After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings.",
                    "tag": "4"
                },
                {
                    "index": "415-2",
                    "sentence": "While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts.",
                    "tag": "4"
                },
                {
                    "index": "415-3",
                    "sentence": "Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy.",
                    "tag": "4"
                },
                {
                    "index": "415-4",
                    "sentence": "As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-416",
            "text": [
                {
                    "index": "416-0",
                    "sentence": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks.",
                    "tag": "2"
                },
                {
                    "index": "416-1",
                    "sentence": "Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps.",
                    "tag": "1"
                },
                {
                    "index": "416-2",
                    "sentence": "In SAGAN, details can be generated using cues from all feature locations.",
                    "tag": "4"
                },
                {
                    "index": "416-3",
                    "sentence": "Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.",
                    "tag": "4"
                },
                {
                    "index": "416-4",
                    "sentence": "Furthermore, recent work has shown that generator conditioning affects GAN performance.",
                    "tag": "1"
                },
                {
                    "index": "416-5",
                    "sentence": "Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics.",
                    "tag": "4"
                },
                {
                    "index": "416-6",
                    "sentence": "The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset.",
                    "tag": "4"
                },
                {
                    "index": "416-7",
                    "sentence": "Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-417",
            "text": [
                {
                    "index": "417-0",
                    "sentence": "Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures.",
                    "tag": "1"
                },
                {
                    "index": "417-1",
                    "sentence": "This paper tackles the problem of better utilizing GPUs for this task.",
                    "tag": "2"
                },
                {
                    "index": "417-2",
                    "sentence": "While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.",
                    "tag": "3"
                },
                {
                    "index": "417-3",
                    "sentence": "We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art.",
                    "tag": "4"
                },
                {
                    "index": "417-4",
                    "sentence": "We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization.",
                    "tag": "3"
                },
                {
                    "index": "417-5",
                    "sentence": "In all these setups, we outperform the state of the art by large margins.",
                    "tag": "4"
                },
                {
                    "index": "417-6",
                    "sentence": "Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs.",
                    "tag": "3+4"
                },
                {
                    "index": "417-7",
                    "sentence": "We have open-sourced our approach for the sake of comparison and reproducibility.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-418",
            "text": [
                {
                    "index": "418-0",
                    "sentence": "This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs.",
                    "tag": "2+3"
                },
                {
                    "index": "418-1",
                    "sentence": "The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions.",
                    "tag": "2+3"
                },
                {
                    "index": "418-2",
                    "sentence": "The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction.",
                    "tag": "3+4"
                },
                {
                    "index": "418-3",
                    "sentence": "The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning.",
                    "tag": "3+4"
                },
                {
                    "index": "418-4",
                    "sentence": "The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration.",
                    "tag": "4+5"
                },
                {
                    "index": "418-5",
                    "sentence": "The paper demonstrates the effectiveness of this non-hierarchical execution experimentally.",
                    "tag": "4"
                },
                {
                    "index": "418-6",
                    "sentence": "Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-419",
            "text": [
                {
                    "index": "419-0",
                    "sentence": "We present the first massively distributed architecture for deep reinforcement learning.",
                    "tag": "1+2"
                },
                {
                    "index": "419-1",
                    "sentence": "This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience.",
                    "tag": "3"
                },
                {
                    "index": "419-2",
                    "sentence": "We used our architecture to implement the Deep Q-Network algorithm (DQN).",
                    "tag": "3"
                },
                {
                    "index": "419-3",
                    "sentence": "Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters.",
                    "tag": "3"
                },
                {
                    "index": "419-4",
                    "sentence": "Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-420",
            "text": [
                {
                    "index": "420-0",
                    "sentence": "In this paper, we consider the task of learning control policies for text-based games.",
                    "tag": "2"
                },
                {
                    "index": "420-1",
                    "sentence": "In these games, all interactions in the virtual world are through text and the underlying state is not observed.",
                    "tag": "1"
                },
                {
                    "index": "420-2",
                    "sentence": "The resulting language barrier makes such environments challenging for automatic game players.",
                    "tag": "1"
                },
                {
                    "index": "420-3",
                    "sentence": "We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback.",
                    "tag": "3"
                },
                {
                    "index": "420-4",
                    "sentence": "This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.",
                    "tag": "3"
                },
                {
                    "index": "420-5",
                    "sentence": "We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations.",
                    "tag": "3"
                },
                {
                    "index": "420-6",
                    "sentence": "Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-421",
            "text": [
                {
                    "index": "421-0",
                    "sentence": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks.",
                    "tag": "1"
                },
                {
                    "index": "421-1",
                    "sentence": "However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point.",
                    "tag": "1+2"
                },
                {
                    "index": "421-2",
                    "sentence": "To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM.",
                    "tag": "2+3"
                },
                {
                    "index": "421-3",
                    "sentence": "The resulting Deep Recurrent Q-Network(DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens.",
                    "tag": "4+5"
                },
                {
                    "index": "421-4",
                    "sentence": "Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability.",
                    "tag": "5"
                },
                {
                    "index": "421-5",
                    "sentence": "Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's.",
                    "tag": "5"
                },
                {
                    "index": "421-6",
                    "sentence": "Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-422",
            "text": [
                {
                    "index": "422-0",
                    "sentence": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer.",
                    "tag": "2"
                },
                {
                    "index": "422-1",
                    "sentence": "Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition.",
                    "tag": "4"
                },
                {
                    "index": "422-2",
                    "sentence": "The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters.",
                    "tag": "3"
                },
                {
                    "index": "422-3",
                    "sentence": "Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-423",
            "text": [
                {
                    "index": "423-0",
                    "sentence": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only.",
                    "tag": "2"
                },
                {
                    "index": "423-1",
                    "sentence": "The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time.",
                    "tag": "1"
                },
                {
                    "index": "423-2",
                    "sentence": "We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation.",
                    "tag": "2+3"
                },
                {
                    "index": "423-3",
                    "sentence": "A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation.",
                    "tag": "2+3"
                },
                {
                    "index": "423-4",
                    "sentence": "Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-424",
            "text": [
                {
                    "index": "424-0",
                    "sentence": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past.",
                    "tag": "1"
                },
                {
                    "index": "424-1",
                    "sentence": "In prior work, experience transitions were uniformly sampled from a replay memory.",
                    "tag": "1"
                },
                {
                    "index": "424-2",
                    "sentence": "However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance.",
                    "tag": "1"
                },
                {
                    "index": "424-3",
                    "sentence": "In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently.",
                    "tag": "2+3"
                },
                {
                    "index": "424-4",
                    "sentence": "We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games.",
                    "tag": "3"
                },
                {
                    "index": "424-5",
                    "sentence": "DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-425",
            "text": [
                {
                    "index": "425-0",
                    "sentence": "In recent years there have been many successes of using deep representations in reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "425-1",
                    "sentence": "Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders.",
                    "tag": "1"
                },
                {
                    "index": "425-2",
                    "sentence": "In this paper, we present a new neural network architecture for model-free reinforcement learning.",
                    "tag": "2"
                },
                {
                    "index": "425-3",
                    "sentence": "Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function.",
                    "tag": "2"
                },
                {
                    "index": "425-4",
                    "sentence": "The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm.",
                    "tag": "5"
                },
                {
                    "index": "425-5",
                    "sentence": "Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions.",
                    "tag": "5"
                },
                {
                    "index": "425-6",
                    "sentence": "Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-426",
            "text": [
                {
                    "index": "426-0",
                    "sentence": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity.",
                    "tag": "1"
                },
                {
                    "index": "426-1",
                    "sentence": "Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN).",
                    "tag": "2+3"
                },
                {
                    "index": "426-2",
                    "sentence": "When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps.",
                    "tag": "4"
                },
                {
                    "index": "426-3",
                    "sentence": "When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments.",
                    "tag": "4"
                },
                {
                    "index": "426-4",
                    "sentence": "We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting.",
                    "tag": "4"
                },
                {
                    "index": "426-5",
                    "sentence": "We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-427",
            "text": [
                {
                    "index": "427-0",
                    "sentence": "A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels.",
                    "tag": "1"
                },
                {
                    "index": "427-1",
                    "sentence": "Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN).",
                    "tag": "1"
                },
                {
                    "index": "427-2",
                    "sentence": "We present an extension of DQN by \"soft\" and \"hard\" attention mechanisms.",
                    "tag": "2"
                },
                {
                    "index": "427-3",
                    "sentence": "Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN.",
                    "tag": "4"
                },
                {
                    "index": "427-4",
                    "sentence": "Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-428",
            "text": [
                {
                    "index": "428-0",
                    "sentence": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance.",
                    "tag": "1"
                },
                {
                    "index": "428-1",
                    "sentence": "In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient.",
                    "tag": "2+3"
                },
                {
                    "index": "428-2",
                    "sentence": "Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy.",
                    "tag": "3"
                },
                {
                    "index": "428-3",
                    "sentence": "We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-429",
            "text": [
                {
                    "index": "429-0",
                    "sentence": "In recent years there is a growing interest in using deep representations for reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "429-1",
                    "sentence": "In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter.",
                    "tag": "2"
                },
                {
                    "index": "429-2",
                    "sentence": "Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically.",
                    "tag": "4"
                },
                {
                    "index": "429-3",
                    "sentence": "The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work.",
                    "tag": "6"
                },
                {
                    "index": "429-4",
                    "sentence": "Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success.",
                    "tag": "4"
                },
                {
                    "index": "429-5",
                    "sentence": "Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-430",
            "text": [
                {
                    "index": "430-0",
                    "sentence": "Many real-world applications can be described as large-scale games of imperfect information.",
                    "tag": "1"
                },
                {
                    "index": "430-1",
                    "sentence": "To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain.",
                    "tag": "1"
                },
                {
                    "index": "430-2",
                    "sentence": "In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge.",
                    "tag": "2"
                },
                {
                    "index": "430-3",
                    "sentence": "Our method combines fictitious self-play with deep reinforcement learning.",
                    "tag": "3"
                },
                {
                    "index": "430-4",
                    "sentence": "When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged.",
                    "tag": "4"
                },
                {
                    "index": "430-5",
                    "sentence": "In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-431",
            "text": [
                {
                    "index": "431-0",
                    "sentence": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement.",
                    "tag": "1"
                },
                {
                    "index": "431-1",
                    "sentence": "By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO).",
                    "tag": "2+3"
                },
                {
                    "index": "431-2",
                    "sentence": "This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks.",
                    "tag": "4"
                },
                {
                    "index": "431-3",
                    "sentence": "Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input.",
                    "tag": "4"
                },
                {
                    "index": "431-4",
                    "sentence": "Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-432",
            "text": [
                {
                    "index": "432-0",
                    "sentence": "Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters.",
                    "tag": "1"
                },
                {
                    "index": "432-1",
                    "sentence": "However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments.",
                    "tag": "1"
                },
                {
                    "index": "432-2",
                    "sentence": "Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found.",
                    "tag": "1"
                },
                {
                    "index": "432-3",
                    "sentence": "We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment.",
                    "tag": "2+3"
                },
                {
                    "index": "432-4",
                    "sentence": "This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors.",
                    "tag": "3"
                },
                {
                    "index": "432-5",
                    "sentence": "After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC.",
                    "tag": "4"
                },
                {
                    "index": "432-6",
                    "sentence": "We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-433",
            "text": [
                {
                    "index": "433-0",
                    "sentence": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks.",
                    "tag": "1"
                },
                {
                    "index": "433-1",
                    "sentence": "The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data.",
                    "tag": "2"
                },
                {
                    "index": "433-2",
                    "sentence": "We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).",
                    "tag": "3"
                },
                {
                    "index": "433-3",
                    "sentence": "We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.",
                    "tag": "3"
                },
                {
                    "index": "433-4",
                    "sentence": "Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground.",
                    "tag": "4"
                },
                {
                    "index": "433-5",
                    "sentence": "In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques.",
                    "tag": "4"
                },
                {
                    "index": "433-6",
                    "sentence": "Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-434",
            "text": [
                {
                    "index": "434-0",
                    "sentence": "This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation.",
                    "tag": "1+2"
                },
                {
                    "index": "434-1",
                    "sentence": "The algorithm is based on two innovations.",
                    "tag": "3"
                },
                {
                    "index": "434-2",
                    "sentence": "Firstly, we present a temporal-difference based method for learning the gradient of the value-function.",
                    "tag": "3"
                },
                {
                    "index": "434-3",
                    "sentence": "Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively.",
                    "tag": "3"
                },
                {
                    "index": "434-4",
                    "sentence": "We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark.",
                    "tag": "3"
                },
                {
                    "index": "434-5",
                    "sentence": "GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-435",
            "text": [
                {
                    "index": "435-0",
                    "sentence": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces.",
                    "tag": "1"
                },
                {
                    "index": "435-1",
                    "sentence": "However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces.",
                    "tag": "1"
                },
                {
                    "index": "435-2",
                    "sentence": "To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables.",
                    "tag": "2+3"
                },
                {
                    "index": "435-3",
                    "sentence": "The best learned agent can score goals more reliably than the 2012 RoboCup champion agent.",
                    "tag": "4"
                },
                {
                    "index": "435-4",
                    "sentence": "As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-436",
            "text": [
                {
                    "index": "436-0",
                    "sentence": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "436-1",
                    "sentence": "While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space.",
                    "tag": "1"
                },
                {
                    "index": "436-2",
                    "sentence": "Hence, exploration in complex domains is often performed with simple epsilon-greedy methods.",
                    "tag": "1"
                },
                {
                    "index": "436-3",
                    "sentence": "In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards.",
                    "tag": "1"
                },
                {
                    "index": "436-4",
                    "sentence": "We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics.",
                    "tag": "2+3"
                },
                {
                    "index": "436-5",
                    "sentence": "By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces.",
                    "tag": "2"
                },
                {
                    "index": "436-6",
                    "sentence": "In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods.",
                    "tag": "4"
                },
                {
                    "index": "436-7",
                    "sentence": "In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-437",
            "text": [
                {
                    "index": "437-0",
                    "sentence": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames.",
                    "tag": "1"
                },
                {
                    "index": "437-1",
                    "sentence": "While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability.",
                    "tag": "1"
                },
                {
                    "index": "437-2",
                    "sentence": "We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks.",
                    "tag": "2+3"
                },
                {
                    "index": "437-3",
                    "sentence": "Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games.",
                    "tag": "4"
                },
                {
                    "index": "437-4",
                    "sentence": "To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-438",
            "text": [
                {
                    "index": "438-0",
                    "sentence": "Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems.",
                    "tag": "1"
                },
                {
                    "index": "438-1",
                    "sentence": "We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (\"torques\") from pixel information only.",
                    "tag": "1"
                },
                {
                    "index": "438-2",
                    "sentence": "We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information.",
                    "tag": "2"
                },
                {
                    "index": "438-3",
                    "sentence": "The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space.",
                    "tag": "3"
                },
                {
                    "index": "438-4",
                    "sentence": "Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control.",
                    "tag": "4"
                },
                {
                    "index": "438-5",
                    "sentence": "Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-439",
            "text": [
                {
                    "index": "439-0",
                    "sentence": "We present a unified framework for learning continuous control policies using backpropagation.",
                    "tag": "2+3"
                },
                {
                    "index": "439-1",
                    "sentence": "It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise.",
                    "tag": "3"
                },
                {
                    "index": "439-2",
                    "sentence": "The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions.",
                    "tag": "3"
                },
                {
                    "index": "439-3",
                    "sentence": "We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors.",
                    "tag": "3"
                },
                {
                    "index": "439-4",
                    "sentence": "We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation.",
                    "tag": "3"
                },
                {
                    "index": "439-5",
                    "sentence": "One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-440",
            "text": [
                {
                    "index": "440-0",
                    "sentence": "The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents.",
                    "tag": "1"
                },
                {
                    "index": "440-1",
                    "sentence": "In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\").",
                    "tag": "2+3"
                },
                {
                    "index": "440-2",
                    "sentence": "Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws.",
                    "tag": "3"
                },
                {
                    "index": "440-3",
                    "sentence": "The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before.",
                    "tag": "4"
                },
                {
                    "index": "440-4",
                    "sentence": "We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-441",
            "text": [
                {
                    "index": "441-0",
                    "sentence": "We present an active detection model for localizing objects in scenes.",
                    "tag": "1+2"
                },
                {
                    "index": "441-1",
                    "sentence": "The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object.",
                    "tag": "1"
                },
                {
                    "index": "441-2",
                    "sentence": "This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning.",
                    "tag": "3"
                },
                {
                    "index": "441-3",
                    "sentence": "The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset.",
                    "tag": "3"
                },
                {
                    "index": "441-4",
                    "sentence": "We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-442",
            "text": [
                {
                    "index": "442-0",
                    "sentence": "We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively.",
                    "tag": "2"
                },
                {
                    "index": "442-1",
                    "sentence": "The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts.",
                    "tag": "2+3"
                },
                {
                    "index": "442-2",
                    "sentence": "Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN.",
                    "tag": "3"
                },
                {
                    "index": "442-3",
                    "sentence": "Take rephrasing a natural sentence as an example.",
                    "tag": "3"
                },
                {
                    "index": "442-4",
                    "sentence": "This list can contain ranked potential words.",
                    "tag": "4"
                },
                {
                    "index": "442-5",
                    "sentence": "Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence.",
                    "tag": "3"
                },
                {
                    "index": "442-6",
                    "sentence": "The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration.",
                    "tag": "3"
                },
                {
                    "index": "442-7",
                    "sentence": "In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded.",
                    "tag": "3"
                },
                {
                    "index": "442-8",
                    "sentence": "For evaluation, the proposed strategy was trained to decode ten thousands natural sentences.",
                    "tag": "3"
                },
                {
                    "index": "442-9",
                    "sentence": "Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-443",
            "text": [
                {
                    "index": "443-0",
                    "sentence": "We present a novel definition of the reinforcement learning state, actions and reward function that allows a deep Q-network (DQN) to learn to control an optimization hyperparameter.",
                    "tag": "2+3"
                },
                {
                    "index": "443-1",
                    "sentence": "Using Q-learning with experience replay, we train two DQNs to accept a state representation of an objective function as input and output the expected discounted return of rewards, or q-values, connected to the actions of either adjusting the learning rate or leaving it unchanged.",
                    "tag": "3"
                },
                {
                    "index": "443-2",
                    "sentence": "The two DQNs learn a policy similar to a line search, but differ in the number of allowed actions.",
                    "tag": "4"
                },
                {
                    "index": "443-3",
                    "sentence": "The trained DQNs in combination with a gradient-based update routine form the basis of the Q-gradient descent algorithms.",
                    "tag": "4"
                },
                {
                    "index": "443-4",
                    "sentence": "To demonstrate the viability of this framework, we show that the DQN's q-values associated with optimal action converge and that the Q-gradient descent algorithms outperform gradient descent with an Armijo or nonmonotone line search.",
                    "tag": "4"
                },
                {
                    "index": "443-5",
                    "sentence": "Unlike traditional optimization methods, Q-gradient descent can incorporate any objective statistic and by varying the actions we gain insight into the type of learning rate adjustment strategies that are successful for neural network optimization.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-444",
            "text": [
                {
                    "index": "444-0",
                    "sentence": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents.",
                    "tag": "1"
                },
                {
                    "index": "444-1",
                    "sentence": "Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications.",
                    "tag": "1"
                },
                {
                    "index": "444-2",
                    "sentence": "This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning.",
                    "tag": "2"
                },
                {
                    "index": "444-3",
                    "sentence": "We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment.",
                    "tag": "2+3"
                },
                {
                    "index": "444-4",
                    "sentence": "Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-445",
            "text": [
                {
                    "index": "445-0",
                    "sentence": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning.",
                    "tag": "1"
                },
                {
                    "index": "445-1",
                    "sentence": "Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI.",
                    "tag": "1"
                },
                {
                    "index": "445-2",
                    "sentence": "It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general.",
                    "tag": "1"
                },
                {
                    "index": "445-3",
                    "sentence": "This paper attempts to understand the principles that underlie DQN's impressive performance and to better contextualize its success.",
                    "tag": "2"
                },
                {
                    "index": "445-4",
                    "sentence": "We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts.",
                    "tag": "2+3"
                },
                {
                    "index": "445-5",
                    "sentence": "Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE.",
                    "tag": "3+4"
                },
                {
                    "index": "445-6",
                    "sentence": "Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game.",
                    "tag": "3+4"
                },
                {
                    "index": "445-7",
                    "sentence": "Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-446",
            "text": [
                {
                    "index": "446-0",
                    "sentence": "With the demand for machine learning increasing, so does the demand for tools which make it easier to use.",
                    "tag": "1"
                },
                {
                    "index": "446-1",
                    "sentence": "Automated machine learning (AutoML) tools have been developed to address this need, such as the Tree-Based Pipeline Optimization Tool (TPOT) which uses genetic programming to build optimal pipelines.",
                    "tag": "1"
                },
                {
                    "index": "446-2",
                    "sentence": "We introduce Layered TPOT, a modification to TPOT which aims to create pipelines equally good as the original, but in significantly less time.",
                    "tag": "2"
                },
                {
                    "index": "446-3",
                    "sentence": "This approach evaluates candidate pipelines on increasingly large subsets of the data according to their fitness, using a modified evolutionary algorithm to allow for separate competition between pipelines trained on different sample sizes.",
                    "tag": "2+3"
                },
                {
                    "index": "446-4",
                    "sentence": "Empirical evaluation shows that, on sufficiently large datasets, Layered TPOT indeed finds better models faster.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-447",
            "text": [
                {
                    "index": "447-0",
                    "sentence": "Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost.",
                    "tag": "1"
                },
                {
                    "index": "447-1",
                    "sentence": "Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search.",
                    "tag": "1"
                },
                {
                    "index": "447-2",
                    "sentence": "In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search.",
                    "tag": "2"
                },
                {
                    "index": "447-3",
                    "sentence": "The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space.",
                    "tag": "4"
                },
                {
                    "index": "447-4",
                    "sentence": "Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods.",
                    "tag": "3+4"
                },
                {
                    "index": "447-5",
                    "sentence": "Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras.",
                    "tag": "2+3"
                },
                {
                    "index": "447-6",
                    "sentence": "The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-448",
            "text": [
                {
                    "index": "448-0",
                    "sentence": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability.",
                    "tag": "1"
                },
                {
                    "index": "448-1",
                    "sentence": "The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge.",
                    "tag": "1"
                },
                {
                    "index": "448-2",
                    "sentence": "We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior.",
                    "tag": "1+2"
                },
                {
                    "index": "448-3",
                    "sentence": "We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.",
                    "tag": "2+3"
                },
                {
                    "index": "448-4",
                    "sentence": "Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.",
                    "tag": "4"
                },
                {
                    "index": "448-5",
                    "sentence": "We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-449",
            "text": [
                {
                    "index": "449-0",
                    "sentence": "We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels.",
                    "tag": "2+3"
                },
                {
                    "index": "449-1",
                    "sentence": "We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes.",
                    "tag": "3"
                },
                {
                    "index": "449-2",
                    "sentence": "At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G.",
                    "tag": "3"
                },
                {
                    "index": "449-3",
                    "sentence": "We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-450",
            "text": [
                {
                    "index": "450-0",
                    "sentence": "Machine learning models are powerful but fallible.",
                    "tag": "1"
                },
                {
                    "index": "450-1",
                    "sentence": "Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities.",
                    "tag": "1"
                },
                {
                    "index": "450-2",
                    "sentence": "Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks.",
                    "tag": "1"
                },
                {
                    "index": "450-3",
                    "sentence": "In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers.",
                    "tag": "2"
                },
                {
                    "index": "450-4",
                    "sentence": "We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-451",
            "text": [
                {
                    "index": "451-0",
                    "sentence": "Convolutional Neural Networks have achieved significant success across multiple computer vision tasks.",
                    "tag": "1"
                },
                {
                    "index": "451-1",
                    "sentence": "However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems.",
                    "tag": "1"
                },
                {
                    "index": "451-2",
                    "sentence": "This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations.",
                    "tag": "2"
                },
                {
                    "index": "451-3",
                    "sentence": "We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes.",
                    "tag": "4"
                },
                {
                    "index": "451-4",
                    "sentence": "A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images.",
                    "tag": "4"
                },
                {
                    "index": "451-5",
                    "sentence": "Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images.",
                    "tag": "4"
                },
                {
                    "index": "451-6",
                    "sentence": "The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings.",
                    "tag": "3"
                },
                {
                    "index": "451-7",
                    "sentence": "The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms.",
                    "tag": "4+5"
                },
                {
                    "index": "451-8",
                    "sentence": "Our codes are publicly available at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-452",
            "text": [
                {
                    "index": "452-0",
                    "sentence": "Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it.",
                    "tag": "1"
                },
                {
                    "index": "452-1",
                    "sentence": "With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety.",
                    "tag": "1"
                },
                {
                    "index": "452-2",
                    "sentence": "We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT).",
                    "tag": "1+2"
                },
                {
                    "index": "452-3",
                    "sentence": "We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image.",
                    "tag": "3"
                },
                {
                    "index": "452-4",
                    "sentence": "We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer.",
                    "tag": "3"
                },
                {
                    "index": "452-5",
                    "sentence": "Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations.",
                    "tag": "3"
                },
                {
                    "index": "452-6",
                    "sentence": "If found, adversarial examples can be shown to human testers and/or used to fine-tune the network.",
                    "tag": "3"
                },
                {
                    "index": "452-7",
                    "sentence": "We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks.",
                    "tag": "3"
                },
                {
                    "index": "452-8",
                    "sentence": "We also compare against existing techniques to search for adversarial examples and estimate network robustness.",
                    "tag": "3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-453",
            "text": [
                {
                    "index": "453-0",
                    "sentence": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research.",
                    "tag": "2"
                },
                {
                    "index": "453-1",
                    "sentence": "Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense).",
                    "tag": "3"
                },
                {
                    "index": "453-2",
                    "sentence": "Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a Macbook Pro notebook.",
                    "tag": "4"
                },
                {
                    "index": "453-3",
                    "sentence": "When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU.",
                    "tag": "4"
                },
                {
                    "index": "453-4",
                    "sentence": "In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like Arcade Learning Environment.",
                    "tag": "4"
                },
                {
                    "index": "453-5",
                    "sentence": "Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS.",
                    "tag": "3+4"
                },
                {
                    "index": "453-6",
                    "sentence": "Strong performance is also achieved on the other two games.",
                    "tag": "4"
                },
                {
                    "index": "453-7",
                    "sentence": "In game replays, we show our agents learn interesting strategies.",
                    "tag": "4"
                },
                {
                    "index": "453-8",
                    "sentence": "ELF, along with its RL platform, is open-sourced at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-454",
            "text": [
                {
                    "index": "454-0",
                    "sentence": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles.",
                    "tag": "1"
                },
                {
                    "index": "454-1",
                    "sentence": "There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems.",
                    "tag": "1"
                },
                {
                    "index": "454-2",
                    "sentence": "To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients.",
                    "tag": "2"
                },
                {
                    "index": "454-3",
                    "sentence": "COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies.",
                    "tag": "3"
                },
                {
                    "index": "454-4",
                    "sentence": "In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed.",
                    "tag": "3"
                },
                {
                    "index": "454-5",
                    "sentence": "COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.",
                    "tag": "3"
                },
                {
                    "index": "454-6",
                    "sentence": "We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability.",
                    "tag": "3"
                },
                {
                    "index": "454-7",
                    "sentence": "COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-455",
            "text": [
                {
                    "index": "455-0",
                    "sentence": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science.",
                    "tag": "1"
                },
                {
                    "index": "455-1",
                    "sentence": "Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature.",
                    "tag": "1"
                },
                {
                    "index": "455-2",
                    "sentence": "These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph.",
                    "tag": "1"
                },
                {
                    "index": "455-3",
                    "sentence": "At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach.",
                    "tag": "1"
                },
                {
                    "index": "455-4",
                    "sentence": "In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.",
                    "tag": "2+3"
                },
                {
                    "index": "455-5",
                    "sentence": "Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-456",
            "text": [
                {
                    "index": "456-0",
                    "sentence": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery.",
                    "tag": "1"
                },
                {
                    "index": "456-1",
                    "sentence": "This article aims to give a general overview of MTL, particularly in deep neural networks.",
                    "tag": "2"
                },
                {
                    "index": "456-2",
                    "sentence": "It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances.",
                    "tag": "3"
                },
                {
                    "index": "456-3",
                    "sentence": "In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
                    "tag": "2"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-457",
            "text": [
                {
                    "index": "457-0",
                    "sentence": "Docking is an important tool in computational drug discovery that aims to predict the binding pose of a ligand to a target protein through a combination of pose scoring and optimization.",
                    "tag": "2"
                },
                {
                    "index": "457-1",
                    "sentence": "A scoring function that is differentiable with respect to atom positions can be used for both scoring and gradient-based optimization of poses for docking.",
                    "tag": "3"
                },
                {
                    "index": "457-2",
                    "sentence": "In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target.",
                    "tag": "2+3"
                },
                {
                    "index": "457-3",
                    "sentence": "Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules.",
                    "tag": "4"
                },
                {
                    "index": "457-4",
                    "sentence": "When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-458",
            "text": [
                {
                    "index": "458-0",
                    "sentence": "Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training.",
                    "tag": "1"
                },
                {
                    "index": "458-1",
                    "sentence": "Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks.",
                    "tag": "1"
                },
                {
                    "index": "458-2",
                    "sentence": "We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems.",
                    "tag": "2+3"
                },
                {
                    "index": "458-3",
                    "sentence": "Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths.",
                    "tag": "3"
                },
                {
                    "index": "458-4",
                    "sentence": "This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network.",
                    "tag": "3"
                },
                {
                    "index": "458-5",
                    "sentence": "Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation.",
                    "tag": "3"
                },
                {
                    "index": "458-6",
                    "sentence": "We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images.",
                    "tag": "3"
                },
                {
                    "index": "458-7",
                    "sentence": "HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks.",
                    "tag": "4"
                },
                {
                    "index": "458-8",
                    "sentence": "We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning.",
                    "tag": "4+5"
                },
                {
                    "index": "458-9",
                    "sentence": "Our code is publicly available at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-459",
            "text": [
                {
                    "index": "459-0",
                    "sentence": "Spatial studies of transcriptome provide biologists with gene expression maps of heterogeneous and complex tissues.",
                    "tag": "1"
                },
                {
                    "index": "459-1",
                    "sentence": "However, most experimental protocols for spatial transcriptomics suffer from the need to select beforehand a small fraction of genes to be quantified over the entire transcriptome.",
                    "tag": "1"
                },
                {
                    "index": "459-2",
                    "sentence": "Standard single-cell RNA sequencing (scRNA-seq) is more prevalent, easier to implement and can in principle capture any gene but cannot recover the spatial location of the cells.",
                    "tag": "1"
                },
                {
                    "index": "459-3",
                    "sentence": "In this manuscript, we focus on the problem of imputation of missing genes in spatial transcriptomic data based on (unpaired) standard scRNA-seq data from the same biological tissue.",
                    "tag": "2+3"
                },
                {
                    "index": "459-4",
                    "sentence": "Building upon domain adaptation work, we propose gimVI, a deep generative model for the integration of spatial transcriptomic data and scRNA-seq data that can be used to impute missing genes.",
                    "tag": "4"
                },
                {
                    "index": "459-5",
                    "sentence": "After describing our generative model and an inference procedure for it, we compare gimVI to alternative methods from computational biology or domain adaptation on real datasets and outperform Seurat Anchors, Liger and CORAL to impute held-out genes.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-460",
            "text": [
                {
                    "index": "460-0",
                    "sentence": "Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions.",
                    "tag": "1"
                },
                {
                    "index": "460-1",
                    "sentence": "In this paper, two neural network architectures are trained on spectrogram and raw waveform data for audio classification tasks on a newly created audio dataset and layer-wise relevance propagation (LRP), a previously proposed interpretability method, is applied to investigate the models' feature selection and decision making.",
                    "tag": "2+3"
                },
                {
                    "index": "460-2",
                    "sentence": "It is demonstrated that the networks are highly reliant on feature marked as relevant by LRP through systematic manipulation of the input data.",
                    "tag": "4"
                },
                {
                    "index": "460-3",
                    "sentence": "Our results show that by making deep audio classifiers interpretable, one can analyze and compare the properties and strategies of different models beyond classification accuracy, which potentially opens up new ways for model improvements.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-461",
            "text": [
                {
                    "index": "461-0",
                    "sentence": "Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales.",
                    "tag": "2+3"
                },
                {
                    "index": "461-1",
                    "sentence": "Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation.",
                    "tag": "4"
                },
                {
                    "index": "461-2",
                    "sentence": "In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio.",
                    "tag": "2"
                },
                {
                    "index": "461-3",
                    "sentence": "WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation.",
                    "tag": "4"
                },
                {
                    "index": "461-4",
                    "sentence": "Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano.",
                    "tag": "4"
                },
                {
                    "index": "461-5",
                    "sentence": "We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-462",
            "text": [
                {
                    "index": "462-0",
                    "sentence": "What is a good visual representation for autonomous agents?",
                    "tag": "1"
                },
                {
                    "index": "462-1",
                    "sentence": "We address this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a complex environment to a target object, e.g. go to the refrigerator.",
                    "tag": "1+2"
                },
                {
                    "index": "462-2",
                    "sentence": "Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues.",
                    "tag": "3"
                },
                {
                    "index": "462-3",
                    "sentence": "We propose to using high level semantic and contextual features including segmentation and detection masks obtained by off-the-shelf state-of-the-art vision as observations and use deep network to learn the navigation policy.",
                    "tag": "4"
                },
                {
                    "index": "462-4",
                    "sentence": "This choice allows using additional data, from orthogonal sources, to better train different parts of the model the representation extraction is trained on large standard vision datasets while the navigation component leverages large synthetic environments for training.",
                    "tag": "4"
                },
                {
                    "index": "462-5",
                    "sentence": "This combination of real and synthetic is possible because equitable feature representations are available in both (e.g., segmentation and detection masks), which alleviates the need for domain adaptation.",
                    "tag": "3"
                },
                {
                    "index": "462-6",
                    "sentence": "Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1].",
                    "tag": "4"
                },
                {
                    "index": "462-7",
                    "sentence": "Our approach gets successfully to the target in 54% of the cases in unexplored environments, compared to 46% for non-learning based approach, and 28% for the learning-based baseline.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-463",
            "text": [
                {
                    "index": "463-0",
                    "sentence": "Audio tagging aims to predict one or several labels in an audio clip.",
                    "tag": "2"
                },
                {
                    "index": "463-1",
                    "sentence": "Many previous works use weakly labelled data (WLD) for audio tagging, where only presence or absence of sound events is known, but the order of sound events is unknown.",
                    "tag": "1"
                },
                {
                    "index": "463-2",
                    "sentence": "To use the order information of sound events, we propose sequential labelled data (SLD), where both the presence or absence and the order information of sound events are known.",
                    "tag": "4"
                },
                {
                    "index": "463-3",
                    "sentence": "To utilize SLD in audio tagging, we propose a Convolutional Recurrent Neural Network followed by a Connectionist Temporal Classification (CRNN-CTC) objective function to map from an audio clip spectrogram to SLD.",
                    "tag": "4"
                },
                {
                    "index": "463-4",
                    "sentence": "Experiments show that CRNN-CTC obtains an Area Under Curve (AUC) score of 0.986 in audio tagging, outperforming the baseline CRNN of 0.908 and 0.815 with Max Pooling and Average Pooling, respectively.",
                    "tag": "4"
                },
                {
                    "index": "463-5",
                    "sentence": "In addition, we show CRNN-CTC has the ability to predict the order of sound events in an audio clip.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-464",
            "text": [
                {
                    "index": "464-0",
                    "sentence": "Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene.",
                    "tag": "2"
                },
                {
                    "index": "464-1",
                    "sentence": "In this paper we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning.",
                    "tag": "3"
                },
                {
                    "index": "464-2",
                    "sentence": "We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multi-label classification task.",
                    "tag": "4"
                },
                {
                    "index": "464-3",
                    "sentence": "For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multi-label classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available.",
                    "tag": "4"
                },
                {
                    "index": "464-4",
                    "sentence": "Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs.",
                    "tag": "3"
                },
                {
                    "index": "464-5",
                    "sentence": "For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to generate new data-driven features from the Mel-Filter Banks (MFBs) features.",
                    "tag": "4"
                },
                {
                    "index": "464-6",
                    "sentence": "The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline.",
                    "tag": "4"
                },
                {
                    "index": "464-7",
                    "sentence": "Compared with the standard Gaussian Mixture Model (GMM) baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set.",
                    "tag": "4"
                },
                {
                    "index": "464-8",
                    "sentence": "The proposed aDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set.",
                    "tag": "4"
                },
                {
                    "index": "464-9",
                    "sentence": "Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-465",
            "text": [
                {
                    "index": "465-0",
                    "sentence": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel.",
                    "tag": "1"
                },
                {
                    "index": "465-1",
                    "sentence": "Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds.",
                    "tag": "1"
                },
                {
                    "index": "465-2",
                    "sentence": "We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos.",
                    "tag": "2"
                },
                {
                    "index": "465-3",
                    "sentence": "Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair.",
                    "tag": "2"
                },
                {
                    "index": "465-4",
                    "sentence": "Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training.",
                    "tag": "3"
                },
                {
                    "index": "465-5",
                    "sentence": "We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-466",
            "text": [
                {
                    "index": "466-0",
                    "sentence": "Stereo matching algorithms usually consist of four steps, including matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement.",
                    "tag": "1"
                },
                {
                    "index": "466-1",
                    "sentence": "Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution.",
                    "tag": "1"
                },
                {
                    "index": "466-2",
                    "sentence": "In this paper, we propose a network architecture to incorporate all steps of stereo matching.",
                    "tag": "2"
                },
                {
                    "index": "466-3",
                    "sentence": "The network consists of three parts.",
                    "tag": "3"
                },
                {
                    "index": "466-4",
                    "sentence": "The first part calculates the multi-scale shared features.",
                    "tag": "3"
                },
                {
                    "index": "466-5",
                    "sentence": "The second part performs matching cost calculation, matching cost aggregation and disparity calculation to estimate the initial disparity using shared features.",
                    "tag": "3"
                },
                {
                    "index": "466-6",
                    "sentence": "The initial disparity and the shared features are used to calculate the feature constancy that measures correctness of the correspondence between two input images.",
                    "tag": "3"
                },
                {
                    "index": "466-7",
                    "sentence": "The initial disparity and the feature constancy are then fed to a sub-network to refine the initial disparity.",
                    "tag": "3"
                },
                {
                    "index": "466-8",
                    "sentence": "The proposed method has been evaluated on the Scene Flow and KITTI datasets.",
                    "tag": "3"
                },
                {
                    "index": "466-9",
                    "sentence": "It achieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015 benchmarks while maintaining a very fast running time.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-467",
            "text": [
                {
                    "index": "467-0",
                    "sentence": "Depth prediction is one of the fundamental problems in computer vision.",
                    "tag": "1"
                },
                {
                    "index": "467-1",
                    "sentence": "In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks.",
                    "tag": "2"
                },
                {
                    "index": "467-2",
                    "sentence": "Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN).",
                    "tag": "4"
                },
                {
                    "index": "467-3",
                    "sentence": "We can append this module to any output from a state-of-the-art (SOTA) depth estimation networks to improve their performances.",
                    "tag": "4"
                },
                {
                    "index": "467-4",
                    "sentence": "In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume.",
                    "tag": "3+4"
                },
                {
                    "index": "467-5",
                    "sentence": "For the tasks of sparse to dense, a.k.a depth completion.",
                    "tag": "4"
                },
                {
                    "index": "467-6",
                    "sentence": "We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network.",
                    "tag": "4"
                },
                {
                    "index": "467-7",
                    "sentence": "We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module.",
                    "tag": "4"
                },
                {
                    "index": "467-8",
                    "sentence": "The code of CSPN proposed in this work will be released at this https URL.",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-468",
            "text": [
                {
                    "index": "468-0",
                    "sentence": "This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60 fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps.",
                    "tag": "2+3"
                },
                {
                    "index": "468-1",
                    "sentence": "A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches.",
                    "tag": "3"
                },
                {
                    "index": "468-2",
                    "sentence": "This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision.",
                    "tag": "3+4"
                },
                {
                    "index": "468-3",
                    "sentence": "Spatial precision is achieved by employing a learned edge-aware upsampling function.",
                    "tag": "3"
                },
                {
                    "index": "468-4",
                    "sentence": "Our model uses a Siamese network to extract features from the left and right image.",
                    "tag": "3"
                },
                {
                    "index": "468-5",
                    "sentence": "A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks.",
                    "tag": "3"
                },
                {
                    "index": "468-6",
                    "sentence": "Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output.",
                    "tag": "4"
                },
                {
                    "index": "468-7",
                    "sentence": "We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.",
                    "tag": "5"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-469",
            "text": [
                {
                    "index": "469-0",
                    "sentence": "Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures.",
                    "tag": "1"
                },
                {
                    "index": "469-1",
                    "sentence": "Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models.",
                    "tag": "1"
                },
                {
                    "index": "469-2",
                    "sentence": "We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances.",
                    "tag": "2+3"
                },
                {
                    "index": "469-3",
                    "sentence": "StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables.",
                    "tag": "2"
                },
                {
                    "index": "469-4",
                    "sentence": "Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",
                    "tag": "4"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-470",
            "text": [
                {
                    "index": "470-0",
                    "sentence": "We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate.",
                    "tag": "1"
                },
                {
                    "index": "470-1",
                    "sentence": "MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks.",
                    "tag": "1"
                },
                {
                    "index": "470-2",
                    "sentence": "We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer.",
                    "tag": "1"
                },
                {
                    "index": "470-3",
                    "sentence": "To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training.",
                    "tag": "2+3"
                },
                {
                    "index": "470-4",
                    "sentence": "MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards.",
                    "tag": "4"
                },
                {
                    "index": "470-5",
                    "sentence": "We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing).",
                    "tag": "4"
                },
                {
                    "index": "470-6",
                    "sentence": "On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%.",
                    "tag": "4"
                },
                {
                    "index": "470-7",
                    "sentence": "On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision.",
                    "tag": "4"
                },
                {
                    "index": "470-8",
                    "sentence": "Our source code is available at this https URL",
                    "tag": "6"
                }
            ]
        },
        {
            "abstractID": "SPA_abs-471",
            "text": [
                {
                    "index": "471-0",
                    "sentence": "Cross-correlator plays a significant role in many visual perception tasks, such as object detection and tracking.",
                    "tag": "1"
                },
                {
                    "index": "471-1",
                    "sentence": "Beyond the linear cross-correlator, this paper proposes a kernel cross-correlator (KCC) that breaks traditional limitations.",
                    "tag": "2"
                },
                {
                    "index": "471-2",
                    "sentence": "First, by introducing the kernel trick, the KCC extends the linear cross-correlation to non-linear space, which is more robust to signal noises and distortions.",
                    "tag": "3+4"
                },
                {
                    "index": "471-3",
                    "sentence": "Second, the connection to the existing works shows that KCC provides a unified solution for correlation filters.",
                    "tag": "3+4"
                },
                {
                    "index": "471-4",
                    "sentence": "Third, KCC is applicable to any kernel function and is not limited to circulant structure on training data, thus it is able to predict affine transformations with customized properties.",
                    "tag": "3+4"
                },
                {
                    "index": "471-5",
                    "sentence": "Last, by leveraging the fast Fourier transform (FFT), KCC eliminates direct calculation of kernel vectors, thus achieves better performance yet still with a reasonable computational cost.",
                    "tag": "3+4"
                },
                {
                    "index": "471-6",
                    "sentence": "Comprehensive experiments on visual tracking and human activity recognition using wearable devices demonstrate its robustness, flexibility, and efficiency.",
                    "tag": "3+4"
                },
                {
                    "index": "471-7",
                    "sentence": "The source codes of both experiments are released at this https URL",
                    "tag": "6"
                }
            ]
        }
    ]
}
