{
    "name": "PA2021_Item1",
    "create_date": "20211108",
    "journal": "EMNLP2020_SPA2020",
    "total_abstract": 1070,
    "documents": [
        {
            "absNo": "1",
            "abstractID": "EMNLP_abs-1",
            "text": [
                {
                    "index": "1-0",
                    "sentence": "Finding attackable sentences in an argument is the first step toward successful refutation in argumentation.",
                    "sentence_kor": "논쟁에서 공격 가능한 문장을 찾는 것은 논쟁에서 성공적인 반박을 향한 첫 번째 단계이다.",
                    "tag": "1"
                },
                {
                    "index": "1-1",
                    "sentence": "We present a first large-scale analysis of sentence attackability in online arguments.",
                    "sentence_kor": "온라인 논쟁에서 문장 공격성에 대한 최초의 대규모 분석을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "1-2",
                    "sentence": "We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences.",
                    "sentence_kor": "우리는 논쟁에서 공격의 추진 이유를 분석하고 문장의 관련 특성을 파악한다.",
                    "tag": "2"
                },
                {
                    "index": "1-3",
                    "sentence": "We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability.",
                    "sentence_kor": "우리는 문장의 공격성이 문장의 내용, 제안 유형 및 어조에 관한 이러한 특성 중 많은 것과 관련이 있으며 외부 지식 소스가 공격성에 대한 유용한 정보를 제공할 수 있음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "1-4",
                    "sentence": "Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.",
                    "sentence_kor": "이러한 발견을 바탕으로, 우리는 기계 학습 모델이 여러 기준보다 훨씬 우수하고 비전문가에 비해 공격 가능한 문장을 자동으로 탐지할 수 있음을 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "2",
            "abstractID": "EMNLP_abs-2",
            "text": [
                {
                    "index": "2-0",
                    "sentence": "Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives.",
                    "sentence_kor": "논쟁은 질문, 보고된 연설, 명령과 같은 다양한 수사적 장치를 수용한다.",
                    "tag": "1"
                },
                {
                    "index": "2-1",
                    "sentence": "These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly.",
                    "sentence_kor": "이러한 수사학적 도구들은 대개 다소 암시적으로 주장하기 때문에, 그것들의 진정한 의미를 이해하는 것이 특정 주장을 적절하게 이해하는 데 핵심이다.",
                    "tag": "1"
                },
                {
                    "index": "2-2",
                    "sentence": "However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation.",
                    "sentence_kor": "그러나, 대부분의 주장 마이닝 시스템과 컴퓨터 언어학 연구는 주장에서 암시적으로 주장되는 제안들에 거의 주의를 기울이지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "2-3",
                    "sentence": "In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation.",
                    "sentence_kor": "본 논문에서, 우리는 질문, 보고된 음성 및 논쟁의 필수 사항에서 암시적으로 주장되는 제안을 추출하기 위한 광범위한 계산 방법을 검토한다.",
                    "tag": "2+3"
                },
                {
                    "index": "2-4",
                    "sentence": "By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models.",
                    "sentence_kor": "2016년 미국 대통령 토론 및 온라인 논평 말뭉치에 대한 모델을 평가하여 계산 모델의 효과와 한계를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "2-5",
                    "sentence": "Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.",
                    "sentence_kor": "우리의 연구는 논쟁에서 주장 마이닝과 이러한 수사적 장치의 의미론에 대한 향후 연구에 정보를 제공할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "3",
            "abstractID": "EMNLP_abs-3",
            "text": [
                {
                    "index": "3-0",
                    "sentence": "When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence.",
                    "sentence_kor": "어떤 주제에 대한 견해, 주장 또는 의견 모음을 요약할 때, 가장 두드러진 점을 추출할 뿐만 아니라 그 보급률을 계량화하는 것이 바람직하다.",
                    "tag": "1"
                },
                {
                    "index": "3-1",
                    "sentence": "Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect.",
                    "sentence_kor": "다중 문서 요약 작업은 전통적으로 텍스트 요약을 만드는 데 초점을 맞췄는데, 이러한 양적인 측면은 결여되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "3-2",
                    "sentence": "Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments.",
                    "sentence_kor": "최근 연구에서는 각 키 포인트의 선호도가 일치하는 인수의 수와 일치하는 전문가 생성 키 포인트의 작은 집합에 매핑하여 인수를 요약할 것을 제안했다.",
                    "tag": "1"
                },
                {
                    "index": "3-3",
                    "sentence": "The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert.",
                    "sentence_kor": "현재 연구는 두 가지 중요한 측면에서 핵심점 분석을 진전시킨다. 첫째, 우리는 완전 자동 분석이 가능한 핵심점 자동 추출 방법을 개발하고 인간 전문가에 버금가는 성능을 달성하는 것으로 나타났다.",
                    "tag": "2"
                },
                {
                    "index": "3-4",
                    "sentence": "Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data.",
                    "sentence_kor": "둘째, 핵심 분석 적용 가능성은 논쟁 데이터를 훨씬 뛰어넘는다는 것을 입증한다.",
                    "tag": "2"
                },
                {
                    "index": "3-5",
                    "sentence": "Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews.",
                    "sentence_kor": "공개적으로 사용 가능한 주장 데이터 세트에 대해 훈련된 모델을 사용하여, 우리는 두 가지 추가 영역, 즉 시 조사와 사용자 검토에서 유망한 결과를 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "3-6",
                    "sentence": "An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.",
                    "sentence_kor": "추가 기여는 인수 대 키 포인트 매칭 모델에 대한 심층 평가로, 이전 결과를 크게 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "4",
            "abstractID": "EMNLP_abs-4",
            "text": [
                {
                    "index": "4-0",
                    "sentence": "Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions.",
                    "sentence_kor": "소셜 미디어 플랫폼은 사용자들이 논쟁을 하고, 토론하고, 의견을 형성하는 온라인 숙고의 필수적인 장소가 되었다.",
                    "tag": "1"
                },
                {
                    "index": "4-1",
                    "sentence": "In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic.",
                    "sentence_kor": "본 논문에서, 우리는 주제에 관한 논쟁적 주장의 입장을 감지하기 위한 감독되지 않은 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "4-2",
                    "sentence": "Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic.",
                    "sentence_kor": "대부분의 관련 작업은 모든 긴급 토론 주제에 대해 훈련되어야 하는 주제별 감독 모델에 초점을 맞춘다.",
                    "tag": "3"
                },
                {
                    "index": "4-3",
                    "sentence": "To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences.",
                    "sentence_kor": "이러한 한계를 해결하기 위해, 우리는 자주 접하는 주장, 특히 결과로부터의 주장에 초점을 맞춘 주제 독립적 접근법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "4-4",
                    "sentence": "We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence.",
                    "sentence_kor": "우리는 클레임이 언급하는 효과를 추출하고 그 효과가 좋은 결과인지 나쁜 결과인지를 추론하기 위한 수단을 제안함으로써 이것을 한다.",
                    "tag": "3"
                },
                {
                    "index": "4-5",
                    "sentence": "Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT.",
                    "sentence_kor": "우리의 실험은 BERT와 비교할 수 있는 유망한 결과를 제공하며, 특히 BERT를 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "4-6",
                    "sentence": "Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.",
                    "sentence_kor": "또한 Amazon Mechanical Turk로 주석을 단 결과와 관련된 새로운 주장 데이터 세트를 발행한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "5",
            "abstractID": "EMNLP_abs-5",
            "text": [
                {
                    "index": "5-0",
                    "sentence": "The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems.",
                    "sentence_kor": "특히 고품질 시스템의 경우 기계 번역에 대한 자동 메트릭의 품질에 대해 점점 더 많은 문제가 제기되고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "5-1",
                    "sentence": "This paper demonstrates that, while choice of metric is important, the nature of the references is also critical.",
                    "sentence_kor": "이 논문은 메트릭의 선택도 중요하지만, 참조의 특성도 중요하다는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "5-2",
                    "sentence": "We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics.",
                    "sentence_kor": "다양한 시스템 및 메트릭에 대한 인간 평가와의 상관관계를 보고하여 참조 자료를 수집하고 자동 평가에서 그 가치를 비교하는 다양한 방법을 연구한다.",
                    "tag": "3"
                },
                {
                    "index": "5-3",
                    "sentence": "Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias.",
                    "sentence_kor": "일반적인 참고문헌이 번역어 주변에 집중하여 낮은 다양성을 보인다는 사실에 자극을 받아, 우리는 언어학자들이 기존의 참고문헌 번역에 대해 수행할 수 있는 의역 과제를 개발하여 이러한 편견을 상쇄한다.",
                    "tag": "3"
                },
                {
                    "index": "5-4",
                    "sentence": "Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references.",
                    "sentence_kor": "우리의 방법은 WMT 2019 영어를 독일어로 제출했을 뿐만 아니라 역번역 및 APE 증강 MT 출력에 대해서도 인간의 판단과 더 높은 상관관계를 산출한다. 이 출력은 표준 참조를 사용하는 자동 메트릭과 낮은 상관관계를 갖는 것으로 나타났다.",
                    "tag": "3+4"
                },
                {
                    "index": "5-5",
                    "sentence": "We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.",
                    "sentence_kor": "우리는 우리의 방법론이 임베딩 기반 방법을 포함하여 우리가 보는 모든 최신 평가 지표와의 상관 관계를 개선한다는 것을 입증한다.이 그림을 완성하기 위해 다중 기준 BLEU가 고품질 출력에 대한 상관 관계를 개선하지 않는다는 것을 밝히고 보다 효과적인 대체 다중 기준 공식을 제시한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "6",
            "abstractID": "EMNLP_abs-6",
            "text": [
                {
                    "index": "6-0",
                    "sentence": "The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation.",
                    "sentence_kor": "번역어라는 용어는 번역된 텍스트의 특징을 설명하는 데 사용되어 왔으며, 본 논문에서 우리는 번역어가 기계 번역 평가에 미치는 잠재적인 역효과에 대한 상세한 분석을 제공한다.",
                    "tag": "1+2"
                },
                {
                    "index": "6-1",
                    "sentence": "Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language.",
                    "sentence_kor": "우리의 분석은 원래 해당 언어로 구성된 텍스트로만 시험한 실험과 비교하여 시험 데이터에 번역어를 포함하는 평가에서 도출한 결론의 차이를 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "6-2",
                    "sentence": "For this reason we recommend that reverse-created test data be omitted from future machine translation test sets.",
                    "sentence_kor": "이러한 이유로 우리는 역생성 테스트 데이터를 향후 기계 번역 테스트 세트에서 생략하는 것이 좋습니다.",
                    "tag": "3"
                },
                {
                    "index": "6-3",
                    "sentence": "In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT.",
                    "sentence_kor": "또한 MT의 인간 패리티를 주장하는 과거 기계 번역 평가에 대한 재평가를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "6-4",
                    "sentence": "One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation.",
                    "sentence_kor": "한 가지 중요한 문제는 인간과 기계 번역의 비교에 적용되는 유의성 시험의 통계적 검정력이다.",
                    "tag": "4"
                },
                {
                    "index": "6-5",
                    "sentence": "Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test.",
                    "sentence_kor": "과거 평가의 바로 그 목적이 인간과 MT 시스템 간의 연관성을 조사하는 것이었기 때문에 전력 분석은 특히 중요하다. 예를 들어 저전력 시험의 적용으로 인한 제2형 오류에 해당하는 인간 동등성의 주장을 피하는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "6-6",
                    "sentence": "We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.",
                    "sentence_kor": "우리는 향후 연구에 적합한 최소 표본 크기를 표시하기 위해 그러한 평가에 사용된 시험의 상세한 분석을 제공한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "7",
            "abstractID": "EMNLP_abs-7",
            "text": [
                {
                    "index": "7-0",
                    "sentence": "Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings.",
                    "sentence_kor": "주어진 문장에 대해 많은 유효한 번역이 존재하지만 기계 번역(MT)은 단일 참조 번역으로 훈련되어 저자원 설정에서 데이터 희소성을 악화시킨다.",
                    "tag": "1"
                },
                {
                    "index": "7-1",
                    "sentence": "We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser’s distribution over possible tokens.",
                    "sentence_kor": "패러프라서에서 참조 문장의 의역본을 샘플링하고 가능한 토큰에 대한 패러프라서의 분포를 예측하도록 MT 모델을 훈련하여 가능한 번역의 전체 공간을 근사화하는 새로운 MT 훈련 방법인 시뮬레이션 다중 참조 훈련(SMRT)을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "7-2",
                    "sentence": "We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU.",
                    "sentence_kor": "우리는 영어로 번역할 때 1.2에서 7.0 BLEU까지 향상된 저자원 환경에서 SMRT의 효과를 입증한다.",
                    "tag": "2"
                },
                {
                    "index": "7-3",
                    "sentence": "We also find SMRT is complementary to back-translation.",
                    "sentence_kor": "우리는 또한 SMRT가 역번역을 보완한다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "8",
            "abstractID": "EMNLP_abs-8",
            "text": [
                {
                    "index": "8-0",
                    "sentence": "We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference.",
                    "sentence_kor": "우리는 기계 번역 평가 작업을 인간 참조에 따라 시퀀스 대 시퀀스 패러프라서를 사용하여 기계 번역 출력을 채점하는 작업 중 하나로 프레임화한다.",
                    "tag": "1"
                },
                {
                    "index": "8-1",
                    "sentence": "We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech).",
                    "sentence_kor": "패러프레이저를 다국어 NMT 시스템으로 훈련시켜 패러프레이싱을 제로샷 변환 작업(예: 체코어에서 체코어로)으로 처리할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "8-2",
                    "sentence": "This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference.",
                    "sentence_kor": "따라서 패러프라서의 출력 모드는 입력 시퀀스의 복사본을 중심으로 배치되며, 이는 MT 시스템 출력이 인간 참조와 일치하는 최상의 경우를 나타낸다.",
                    "tag": "3"
                },
                {
                    "index": "8-3",
                    "sentence": "Our method is simple and intuitive, and does not require human judgements for training.",
                    "sentence_kor": "우리의 방법은 간단하고 직관적이며 훈련을 위해 인간의 판단이 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "8-4",
                    "sentence": "Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data).",
                    "sentence_kor": "단일 모델(39개 언어로 교육됨)은 모든 언어에서 WMT 2019 세그먼트 수준 공유 메트릭 작업에서 모든 이전 메트릭스를 능가하거나 통계적으로 연결된다(모델에 교육 데이터가 없는 구자라티 제외).",
                    "tag": "4"
                },
                {
                    "index": "8-5",
                    "sentence": "We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.",
                    "sentence_kor": "또한 모델을 지표로 품질 추정 작업에 사용하는 방법을 탐구하여 참조 대신 소스에 대한 조건화함으로써 모든 언어 쌍에서 품질 추정에 대한 WMT 2019 공유 작업에 대한 모든 제출을 크게 능가한다는 것을 발견했다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "9",
            "abstractID": "EMNLP_abs-9",
            "text": [
                {
                    "index": "9-0",
                    "sentence": "Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language.",
                    "sentence_kor": "클라크 외 연구진(2020)의 최근 연구는 변압기가 자연어로 명시적으로 제공된 지식에 대한 질문에 답함으로써 \"소프트 정리 추진자\" 역할을 할 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "9-1",
                    "sentence": "In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs.",
                    "sentence_kor": "우리의 연구에서, 우리는 규칙 기반에 대한 이진 질문에 공동으로 답하고 해당 증명을 생성하는 해석 가능한 변압기 기반 모델인 PRover를 제안함으로써 형식 정리 지지자를 모방하는 데 한 걸음 더 다가섰다.",
                    "tag": "1+2"
                },
                {
                    "index": "9-2",
                    "sentence": "Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm.",
                    "sentence_kor": "우리 모델은 효율적인 제한 훈련 패러다임에서 증명 그래프에 해당하는 노드 및 에지를 예측하는 방법을 배운다.",
                    "tag": "3"
                },
                {
                    "index": "9-3",
                    "sentence": "During inference, a valid proof, satisfying a set of global constraints is generated.",
                    "sentence_kor": "추론 중에 전역 제약 조건을 만족하는 유효한 증거가 생성된다.",
                    "tag": "3"
                },
                {
                    "index": "9-4",
                    "sentence": "We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance.",
                    "sentence_kor": "우리는 강력한 일반화 성능으로 QA 및 증명 생성에 대한 유망한 결과를 보여주기 위해 합성, 수작업 및 인간이 해석한 규칙 기반에 대한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "9-5",
                    "sentence": "First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation).",
                    "sentence_kor": "첫째, PRover는 RuleCaster(제로샷 평가 시 최대 6% 향상)와 비교하여 QA 작업의 성능을 유지하거나 개선하면서 87%의 정확도로 증거를 생성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "9-6",
                    "sentence": "Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement).",
                    "sentence_kor": "둘째, 더 낮은 깊이의 추론을 요구하는 질문에 대해 훈련했을 때, 더 높은 깊이로 훨씬 더 잘 일반화된다(최대 15% 개선).",
                    "tag": "3+4"
                },
                {
                    "index": "9-7",
                    "sentence": "Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data.",
                    "sentence_kor": "셋째, PRover는 교육 데이터의 40%만 사용하여 98%에 가까운 완벽한 QA 정확도를 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "9-8",
                    "sentence": "However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for “depth 5”, indicating significant scope for future work.",
                    "sentence_kor": "그러나 더 깊은 추론을 요구하는 질문에 대한 증명을 생성하는 것은 어려워지고 정확도는 \"깊이 5\"의 경우 65%로 떨어져 향후 작업의 상당한 범위를 나타낸다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "10",
            "abstractID": "EMNLP_abs-10",
            "text": [
                {
                    "index": "10-0",
                    "sentence": "The aim of all Question Answering (QA) systems is to generalize to unseen questions.",
                    "sentence_kor": "모든 QA(질문응답) 시스템의 목적은 보이지 않는 질문으로 일반화하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "10-1",
                    "sentence": "Current supervised methods are reliant on expensive data annotation.",
                    "sentence_kor": "현재 감독되는 방법은 값비싼 데이터 주석에 의존합니다.",
                    "tag": "1"
                },
                {
                    "index": "10-2",
                    "sentence": "Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task.",
                    "sentence_kor": "또한 이러한 주석은 의도하지 않은 주석자 편향을 유발하여 시스템이 실제 작업보다 편향에 더 초점을 맞출 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "10-3",
                    "sentence": "This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs.",
                    "sentence_kor": "이 연구는 지식 그래프에 대한 자체 감독 작업인 지식 삼중 학습(KTL)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "10-4",
                    "sentence": "We propose heuristics to create synthetic graphs for commonsense and scientific knowledge.",
                    "sentence_kor": "우리는 상식과 과학적 지식을 위한 합성 그래프를 만들기 위한 휴리스틱을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "10-5",
                    "sentence": "We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.",
                    "sentence_kor": "우리는 KTL을 사용하여 제로샷 질문 답변을 수행할 것을 제안하며, 우리의 실험은 사전 훈련된 대형 변압기 언어 모델에 비해 상당히 개선된 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "11",
            "abstractID": "EMNLP_abs-11",
            "text": [
                {
                    "index": "11-0",
                    "sentence": "Deep learning models for linguistic tasks require large training datasets, which are expensive to create.",
                    "sentence_kor": "언어 작업을 위한 딥러닝 모델에는 대규모 교육 데이터 세트가 필요하며, 이 데이터 세트를 만드는 데는 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "11-1",
                    "sentence": "As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well.",
                    "sentence_kor": "하나의 인스턴스를 생성하는 프로세스를 반복하여 새로운 인스턴스를 생성하는 전통적인 접근 방식의 대안으로, 먼저 일련의 시드 예제를 수집한 다음 (규칙 기반 기계 섭동이 아닌) 인간 중심의 자연 섭동을 적용하여 골드 레이블도 변경할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "11-2",
                    "sentence": "Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples.",
                    "sentence_kor": "그러한 섭동은 완전히 새로운 예를 작성하는 것보다 비교적 쉽게(따라서 더 저렴하게) 생성할 수 있다는 장점이 있다.",
                    "tag": "1"
                },
                {
                    "index": "11-3",
                    "sentence": "Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input.",
                    "sentence_kor": "또한 NLP 데이터 세트에서 인간 수준 점수를 달성하는 모델도 입력의 작은 변화에 상당히 민감한 것으로 알려져 있는 문제를 해결하는 데 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "11-4",
                    "sentence": "To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch.",
                    "sentence_kor": "아이디어를 평가하기 위해 최근의 질의 응답 데이터 세트(BOOLQ)를 고려하고 기존 질문을 교란시키는 상대적 비용 대 처음부터 새로운 질문을 생성하는 데 따른 섭동 비용 비율의 함수로서 우리의 접근 방식을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "11-5",
                    "sentence": "We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.",
                    "sentence_kor": "우리는 자연 섭동이 생성 비용이 다소 저렴할 때(비용 비율 60% 미만) BOOLQ 모델 교육에 사용하는 것이 더 효과적이라는 것을 발견했다. 이러한 모델은 원래 BOOLQ 데이터 세트에서 성능을 유지하면서 9% 더 높은 견고성과 4.5% 더 강력한 일반화를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "12",
            "abstractID": "EMNLP_abs-12",
            "text": [
                {
                    "index": "12-0",
                    "sentence": "Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks.",
                    "sentence_kor": "BERT와 같은 심층 사전 훈련된 상황별 인코더는 다양한 다운스트림 작업에서 뛰어난 성능을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "12-1",
                    "sentence": "A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training.",
                    "sentence_kor": "탐사의 최근 연구는 사전 훈련 중에 이러한 모델에 의해 암묵적으로 학습된 언어 지식을 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "12-2",
                    "sentence": "While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms.",
                    "sentence_kor": "탐색 작업은 대부분 과제 수준에서 작동하지만 언어 과제는 거의 균일하지 않으며 다양한 형식으로 표현될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "12-3",
                    "sentence": "Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data.",
                    "sentence_kor": "따라서 모든 언어학 기반 조사 연구는 필연적으로 기본 데이터에 주석을 다는 데 사용되는 형식주의를 따른다.",
                    "tag": "1"
                },
                {
                    "index": "12-4",
                    "sentence": "Can the choice of formalism affect probing results?",
                    "sentence_kor": "형식주의의 선택이 조사 결과에 영향을 미칠 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "12-5",
                    "sentence": "To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics.",
                    "sentence_kor": "조사하기 위해 역할 의미론에서 심층 교차 형식주의 계층 탐색 연구를 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "12-6",
                    "sentence": "We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism.",
                    "sentence_kor": "우리는 형식주의에 따라 BERT에 의한 의미적 역할 및 프로토 역할 정보의 인코딩에서 언어적으로 의미 있는 차이를 발견하고 계층 조사가 동일한 언어적 형식주의의 구현 사이에 미묘한 차이를 감지할 수 있음을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "12-7",
                    "sentence": "Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.",
                    "sentence_kor": "우리의 결과는 언어 형식주의가 일반적으로 사용되는 교차 작업 및 교차 언어 실험 설정과 함께 조사 연구에서 중요한 차원이라는 것을 암시한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "13",
            "abstractID": "EMNLP_abs-13",
            "text": [
                {
                    "index": "13-0",
                    "sentence": "To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations.",
                    "sentence_kor": "사전 훈련된 표현이 일부 언어 속성을 얼마나 잘 인코딩하는지 측정하기 위해, 탐색의 정확도, 즉 표현에서 속성을 예측하도록 훈련된 분류기를 사용하는 것이 일반적이다.",
                    "tag": "1"
                },
                {
                    "index": "13-1",
                    "sentence": "Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations.",
                    "sentence_kor": "광범위한 조사기 채택에도 불구하고 정확도 차이는 표현의 차이를 적절히 반영하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "13-2",
                    "sentence": "For example, they do not substantially favour pretrained representations over randomly initialized ones.",
                    "sentence_kor": "예를 들어, 랜덤하게 초기화된 표현보다 사전 훈련된 표현을 선호하지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "13-3",
                    "sentence": "Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks.",
                    "sentence_kor": "이와 유사하게, 이들의 정확도는 진정한 언어 레이블을 탐색하고 무작위 합성 작업을 탐색할 때 유사할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "13-4",
                    "sentence": "To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size.",
                    "sentence_kor": "이러한 무작위 기준선에 대한 정확도의 합리적인 차이를 확인하기 위해 이전 연구는 프로브 훈련 데이터의 양 또는 모델 크기를 제한해야 했다.",
                    "tag": "1"
                },
                {
                    "index": "13-5",
                    "sentence": "Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL).",
                    "sentence_kor": "대신 표준 프로브에 대한 대안인 최소 설명 길이(MDL)를 가진 정보 이론적 프로빙을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "13-6",
                    "sentence": "With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data.",
                    "sentence_kor": "MDL 프로빙을 통해 라벨 예측 프로브를 훈련하면 데이터를 효과적으로 전송하도록 가르칠 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "13-7",
                    "sentence": "Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations.",
                    "sentence_kor": "따라서 관심 측정값이 프로브 정확도에서 지정된 레이블의 설명 길이로 변경됩니다.",
                    "tag": "3"
                },
                {
                    "index": "13-8",
                    "sentence": "In addition to probe quality, the description length evaluates “the amount of effort” needed to achieve the quality.",
                    "sentence_kor": "설명 길이는 프로브 품질 외에도 품질 달성에 필요한 \"노력 양\"을 평가합니다.",
                    "tag": "3"
                },
                {
                    "index": "13-9",
                    "sentence": "This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality.",
                    "sentence_kor": "이러한 노력의 양은 (i) 탐색 모델의 크기 또는 (ii) 고품질 달성에 필요한 데이터 양을 나타냅니다.",
                    "tag": "4"
                },
                {
                    "index": "13-10",
                    "sentence": "We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding.",
                    "sentence_kor": "표준 프로빙 파이프라인 위에 쉽게 구현할 수 있는 MDL을 추정하기 위한 두 가지 방법인 가변 코딩과 온라인 코딩 방법을 고려한다.",
                    "tag": "4"
                },
                {
                    "index": "13-11",
                    "sentence": "We show that these methods agree in results and are more informative and stable than the standard probes.",
                    "sentence_kor": "우리는 이러한 방법이 결과에 일치하고 표준 조사보다 더 유익하고 안정적이라는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "14",
            "abstractID": "EMNLP_abs-14",
            "text": [
                {
                    "index": "14-0",
                    "sentence": "Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks.",
                    "sentence_kor": "대부분의 최신 NLP 시스템은 다양한 작업에서 놀라울 정도로 높은 성능을 달성하는 사전 훈련된 상황 표현을 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "14-1",
                    "sentence": "Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it.",
                    "sentence_kor": "이러한 표현에 어떤 형태의 언어 구조가 반영되지 않는 한 그러한 높은 성과는 불가능해야 하며, 풍부한 연구가 그것을 탐구하기 위해 생겨났다.",
                    "tag": "1"
                },
                {
                    "index": "14-2",
                    "sentence": "In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted.",
                    "sentence_kor": "본 논문에서, 우리는 언어 정보가 표현 내에서 어떻게 구성되는지를 조사하는 내재적 조사와 이전 연구에서 인기 있는 외적 조사 사이의 차이를 그린다. 이 조사는 그것이 성공적으로 추출될 수 있다는 것을 보여줌으로써 그러한 정보의 존재를 주장할 뿐이다.",
                    "tag": "2+3"
                },
                {
                    "index": "14-3",
                    "sentence": "To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal.",
                    "sentence_kor": "내재적 조사를 가능하게 하기 위해, 우리는 단어 임베딩의 언어 정보가 분산되어 있는지 또는 집중되어 있는지를 결정할 수 있는 분해 가능한 다변량 가우스 탐사에 기초한 새로운 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "14-4",
                    "sentence": "We then probe fastText and BERT for various morphosyntactic attributes across 36 languages.",
                    "sentence_kor": "그런 다음 fastText와 BERT에서 36개 언어에 걸쳐 다양한 형태 동사적 속성을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "14-5",
                    "sentence": "We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.",
                    "sentence_kor": "우리는 대부분의 속성이 소수의 뉴런에 의해 신뢰성 있게 인코딩되며 fastText는 BERT보다 언어 구조에 더 집중되어 있다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "15",
            "abstractID": "EMNLP_abs-15",
            "text": [
                {
                    "index": "15-0",
                    "sentence": "One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding.",
                    "sentence_kor": "자기 지도 언어 과제에 대한 사전 훈련이 효과적인 한 가지 이유는 그것이 언어 이해에 도움이 되는 모델 특징을 가르치기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "15-1",
                    "sentence": "However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning.",
                    "sentence_kor": "그러나 사전 교육을 받은 모델이 언어적 특징을 나타낼 뿐만 아니라 미세 회전 중에 해당 특징을 우선적으로 사용하는 방법을 배우기를 원한다.",
                    "tag": "1"
                },
                {
                    "index": "15-2",
                    "sentence": "With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning.",
                    "sentence_kor": "이 목표를 염두에 두고 MSGS(Mixed Signals Generalization Set)라는 새로운 영어 진단 세트를 소개한다. MSGS는 사전 훈련된 모델이 미세 조정 중에 언어 또는 표면 일반화를 선호하는지 테스트하는 데 사용하는 20개의 애매한 이진 분류 작업으로 구성된다.",
                    "tag": "2+3"
                },
                {
                    "index": "15-3",
                    "sentence": "We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE.",
                    "sentence_kor": "우리는 RoBERTa를 1M~1B 단어 범위의 데이터 양에서 처음부터 사전 교육하고 MSGS의 성능을 공개적으로 사용 가능한 RoBERTA_BASE와 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "15-4",
                    "sentence": "We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones.",
                    "sentence_kor": "우리는 모델이 사전 훈련 데이터가 거의 없는 언어적 특징을 표현하는 것을 배울 수 있지만 표면적인 것보다 언어적 일반화를 선호하는 것을 배우는 데 훨씬 더 많은 데이터가 필요하다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "15-5",
                    "sentence": "Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity.",
                    "sentence_kor": "결국 사전 훈련 데이터의 약 30B 단어를 사용하여 RoBERTA_BASE는 일정한 규칙성을 가진 언어적 편향을 일관되게 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "15-6",
                    "sentence": "We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.",
                    "sentence_kor": "자가 지도 사전 훈련이 유용한 유도 편견을 학습하는 효과적인 방법이지만 모델이 어떤 특징을 학습하는 속도를 개선할 여지가 있을 수 있다는 결론을 내린다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "16",
            "abstractID": "EMNLP_abs-16",
            "text": [
                {
                    "index": "16-0",
                    "sentence": "The neural attention mechanism plays an important role in many natural language processing applications.",
                    "sentence_kor": "신경 주의 메커니즘은 많은 자연어 처리 애플리케이션에서 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "16-1",
                    "sentence": "In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives.",
                    "sentence_kor": "특히, 다중 헤드 어텐션은 모델이 다른 관점에서 정보에 공동으로 참여할 수 있게 함으로써 단일 헤드 어텐션을 확장한다.",
                    "tag": "2"
                },
                {
                    "index": "16-2",
                    "sentence": "However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model’s representation power.",
                    "sentence_kor": "그러나 명시적 제약 없이 다중 헤드 주의는 다른 헤드가 유사한 주의적 특징을 추출하도록 하여 모델의 표현력을 제한하는 문제인 주의력 붕괴를 겪을 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "16-3",
                    "sentence": "In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective.",
                    "sentence_kor": "본 논문에서 우리는 처음으로 베이지안 관점에서 다중 헤드 주의에 대한 새로운 이해를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "16-4",
                    "sentence": "Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model’s expressiveness.",
                    "sentence_kor": "최근에 개발된 입자 최적화 샘플링 기법을 기반으로 다중 헤드 주의에서 거부감을 명시적으로 개선하고 결과적으로 모델의 표현성을 강화하는 비모수 접근법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "16-5",
                    "sentence": "Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention.",
                    "sentence_kor": "놀랍게도, 우리의 베이지안 해석은 잘 이해되지 않는 질문들, 즉 왜 그리고 어떻게 멀티헤드 어텐션을 사용하는지에 대한 이론적 영감을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "16-6",
                    "sentence": "Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.",
                    "sentence_kor": "다양한 주의 모델과 애플리케이션에 대한 광범위한 실험은 제안된 거부적 주의가 학습된 특징 다양성을 개선하여 여러 작업에서 일관된 성능 개선으로 더 많은 정보를 제공할 수 있다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "17",
            "abstractID": "EMNLP_abs-17",
            "text": [
                {
                    "index": "17-0",
                    "sentence": "Syntactic parsers have dominated natural language understanding for decades.",
                    "sentence_kor": "구문 분석기는 수십 년 동안 자연어 이해를 지배해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "17-1",
                    "sentence": "Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners.",
                    "sentence_kor": "그러나 대규모 텍스트 표현 학습자의 성공으로 인해 이들의 구문 해석은 다운스트림 작업에서 중심을 잃고 있다.",
                    "tag": "1"
                },
                {
                    "index": "17-2",
                    "sentence": "In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference.",
                    "sentence_kor": "본 논문에서 우리는 KERMIT(해석 가능한 트리를 위한 재귀 메커니즘이 있는 커널에서 영감을 받은 인코더)를 인공 신경망에 심볼 구문 분석 트리를 내장하고 구문이 추론에서 사용되는 방법을 시각화할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "17-3",
                    "sentence": "We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks",
                    "sentence_kor": "우리는 두 개의 최첨단 변압기 기반 범용 문장 인코더(BERT 및 XLNet)와 짝을 이룬 KERMIT로 실험했고 KERMIT가 인간 코드 범용 구문 표현을 신경망에 효과적으로 내장함으로써 성능을 향상시킬 수 있다는 것을 보여주었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "18",
            "abstractID": "EMNLP_abs-18",
            "text": [
                {
                    "index": "18-0",
                    "sentence": "Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks.",
                    "sentence_kor": "트랜스포머 모델은 많은 자연어 처리(NLP) 작업에서 최첨단 기술을 발전시켰다.",
                    "tag": "1"
                },
                {
                    "index": "18-1",
                    "sentence": "In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.",
                    "sentence_kor": "본 논문에서 우리는 표준 트랜스포머 아키텍처의 두 가지 주요 과제인 입력 길이 확장과 구조화된 입력 인코딩을 다루는 새로운 트랜스포머 아키텍처인 \"확장 트랜스포머 구성\"을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "18-2",
                    "sentence": "To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens.",
                    "sentence_kor": "더 긴 입력에 대한 주의를 확장하기 위해 글로벌 토큰과 일반 입력 토큰 사이의 새로운 글로벌 로컬 주의 메커니즘을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "18-3",
                    "sentence": "We also show that combining global-local attention with relative position encodings and a “Contrastive Predictive Coding” (CPC) pre-training objective allows ETC to encode structured inputs.",
                    "sentence_kor": "또한 글로벌 로컬 주의와 상대적 위치 인코딩 및 \"대비 예측 코딩\"(CPC) 사전 훈련 목표를 결합하면 ETC가 구조화된 입력을 인코딩할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "18-4",
                    "sentence": "We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.",
                    "sentence_kor": "우리는 장기 및/또는 구조화된 입력이 필요한 4개의 자연어 데이터 세트에서 최첨단 결과를 얻는다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "19",
            "abstractID": "EMNLP_abs-19",
            "text": [
                {
                    "index": "19-0",
                    "sentence": "We introduce Electric, an energy-based cloze model for representation learning over text.",
                    "sentence_kor": "텍스트에 대한 표현 학습을 위한 에너지 기반 클로즈 모델인 Electric을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "19-1",
                    "sentence": "Like BERT, it is a conditional generative model of tokens given their contexts.",
                    "sentence_kor": "BERT와 마찬가지로, 컨텍스트가 주어진 토큰의 조건부 생성 모델이다.",
                    "tag": "1"
                },
                {
                    "index": "19-2",
                    "sentence": "However, Electric does not use masking or output a full distribution over tokens that could occur in a context.",
                    "sentence_kor": "그러나 Electric은 컨텍스트에서 발생할 수 있는 토큰에 대한 전체 분포를 마스킹하거나 출력하지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "19-3",
                    "sentence": "Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.",
                    "sentence_kor": "대신, 컨텍스트가 주어진 가능성을 나타내는 스칼라 에너지 점수를 각 입력 토큰에 할당합니다.",
                    "tag": "1"
                },
                {
                    "index": "19-4",
                    "sentence": "We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method.",
                    "sentence_kor": "소음 대비 추정에 기초한 알고리즘을 사용하여 전기를 교육하고 이 학습 목표가 최근 제안된 ELCTRA 사전 훈련 방법과 어떻게 밀접한 관련이 있는지 설명한다.",
                    "tag": "2+3"
                },
                {
                    "index": "19-5",
                    "sentence": "Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models.",
                    "sentence_kor": "전기는 다운스트림 작업으로 전송될 때 성능이 우수하며 텍스트에 대한 가능성 점수를 생성하는 데 특히 효과적이다. 음성 인식 n-best 목록을 언어 모델보다 더 잘, 마스킹 언어 모델보다 훨씬 더 빠르게 재생한다.",
                    "tag": "3+4"
                },
                {
                    "index": "19-6",
                    "sentence": "Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.",
                    "sentence_kor": "또한 사전 훈련 중에 ELCTRA가 학습하는 내용에 대한 보다 명확하고 원칙적인 관점을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "20",
            "abstractID": "EMNLP_abs-20",
            "text": [
                {
                    "index": "20-0",
                    "sentence": "Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated.",
                    "sentence_kor": "사전 훈련된 트랜스포머는 현재 자연어 처리에서 어디에나 있지만, 높은 최종 작업 성능에도 불구하고 그것들이 보정되었는지에 대해서는 경험적으로 거의 알려져 있지 않다.",
                    "tag": "1"
                },
                {
                    "index": "20-1",
                    "sentence": "Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example?",
                    "sentence_kor": "특히, 이러한 모델의 사후 확률은 주어진 예에서 모델이 정확할 가능성에 대한 정확한 경험적 측정을 제공하는가?",
                    "tag": "1"
                },
                {
                    "index": "20-2",
                    "sentence": "We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning.",
                    "sentence_kor": "우리는 이 연구에서 BERT와 RoBERTa에 초점을 맞추고 자연어 추론, 의역 감지 및 상식적 추론의 세 가지 작업에 걸쳐 보정을 분석한다.",
                    "tag": "2"
                },
                {
                    "index": "20-3",
                    "sentence": "For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about.",
                    "sentence_kor": "각 작업에 대해, 우리는 모델이 불확실해야 하는 더 많은 예에 직면하는 도전적인 영역 밖 설정뿐만 아니라 도메인 내 설정을 고려한다.",
                    "tag": "3"
                },
                {
                    "index": "20-4",
                    "sentence": "We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.",
                    "sentence_kor": "우리는 (1) 사전 훈련된 모델을 즉시 사용할 경우 도메인 내에서 보정되며, 기준선에 비해 교정 오류가 3.5배 더 낮을 수 있으며, (2) 온도 척도는 도메인 내 보정 오류를 추가로 줄이는 데 효과적이며, 레이블 평활을 사용하여 경험적 불확실성을 고의적으로 증가시키는 데 도움이 된다는 것을 보여준다.알리바이를 후발자에게 외부로 알리다",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "21",
            "abstractID": "EMNLP_abs-21",
            "text": [
                {
                    "index": "21-0",
                    "sentence": "Linguistic steganography studies how to hide secret messages in natural language cover texts.",
                    "sentence_kor": "언어학적인 스테가노그래피는 자연어 표지 텍스트에서 비밀 메시지를 숨기는 방법을 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "21-1",
                    "sentence": "Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification.",
                    "sentence_kor": "전통적인 방법은 어휘 대체 또는 구문 수정을 통해 비밀 메시지를 순수한 텍스트로 변환하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "21-2",
                    "sentence": "Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message.",
                    "sentence_kor": "최근 신경 언어 모델(LM)의 발전으로 비밀 메시지에 조건화된 커버 텍스트를 직접 생성할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "21-3",
                    "sentence": "In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model.",
                    "sentence_kor": "본 연구에서는 신경 언어 모델에 기초한 자가 조정 산술 코딩을 사용하여 비밀 메시지를 인코딩하는 새로운 언어 스테가노그래피 방법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "21-4",
                    "sentence": "We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively.",
                    "sentence_kor": "이 방법의 통계적 무감각성을 공식적으로 분석하고 비트/워드 및 KL 메트릭 측면에서 각각 15.3%, 38.9%의 이전 최신 방법을 능가한다는 것을 경험적으로 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "21-5",
                    "sentence": "Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.",
                    "sentence_kor": "마지막으로, 인간 평가에 따르면 생성된 표지 텍스트의 51%가 실제로 도청자를 속일 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "22",
            "abstractID": "EMNLP_abs-22",
            "text": [
                {
                    "index": "22-0",
                    "sentence": "Machine learning models are trained to find patterns in data.",
                    "sentence_kor": "기계 학습 모델은 데이터에서 패턴을 찾기 위해 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "22-1",
                    "sentence": "NLP models can inadvertently learn socially undesirable patterns when training on gender biased text.",
                    "sentence_kor": "NLP 모델은 성별 편향 텍스트에 대한 훈련 시 무심코 사회적으로 바람직하지 않은 패턴을 학습할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "22-2",
                    "sentence": "In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker.",
                    "sentence_kor": "본 연구에서 우리는 몇 가지 실용적이고 의미적인 차원을 따라 텍스트의 성 편견을 분해하는 새롭고 일반적인 프레임워크를 제안한다. 즉, 말하는 사람의 성별로부터의 편견, 말하는 사람의 성별로부터의 편견, 그리고 말하는 사람의 성별로부터의 편견이다.",
                    "tag": "2+3"
                },
                {
                    "index": "22-3",
                    "sentence": "Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information.",
                    "sentence_kor": "이 세분화된 프레임워크를 사용하여 성별 정보가 포함된 8개의 대규모 데이터 세트에 자동으로 주석을 달았다.",
                    "tag": "3"
                },
                {
                    "index": "22-4",
                    "sentence": "In addition, we collect a new, crowdsourced evaluation benchmark.",
                    "sentence_kor": "또한 우리는 크라우드소싱된 새로운 평가 벤치마크를 수집한다.",
                    "tag": "3"
                },
                {
                    "index": "22-5",
                    "sentence": "Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers.",
                    "sentence_kor": "여러 차원을 따라 성별 편향을 구별하면 성별 편향 분류자를 더 잘 훈련시킬 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "22-6",
                    "sentence": "We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.",
                    "sentence_kor": "우리는 우리의 분류기가 생성 모델의 성별 편향 제어, 임의의 텍스트의 성별 편향 감지, 텍스트의 성별 편향성 기반 공격적 분류 등 다양한 용도에 유용하다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "23",
            "abstractID": "EMNLP_abs-23",
            "text": [
                {
                    "index": "23-0",
                    "sentence": "Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets.",
                    "sentence_kor": "완벽한 교육 데이터 세트(즉, 상당히 크고 편견이 없으며 보이지 않는 사례를 잘 나타내는 데이터 세트)를 얻는 것은 불가능하기 때문에, 많은 실제 텍스트 분류기는 사용 가능하지만 불완전한 데이터 세트에 대해 교육을 받는다.",
                    "tag": "1"
                },
                {
                    "index": "23-1",
                    "sentence": "These classifiers are thus likely to have undesirable properties.",
                    "sentence_kor": "따라서 이러한 분류자는 바람직하지 않은 속성을 가질 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "23-2",
                    "sentence": "For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting.",
                    "sentence_kor": "예를 들어 일부 하위 모집단에 대한 편견이 있거나 과적합으로 인해 야생에서 효과적으로 작동하지 않을 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "23-3",
                    "sentence": "In this paper, we propose FIND – a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features.",
                    "sentence_kor": "본 논문에서 우리는 인간이 관련 없는 숨겨진 기능을 비활성화하여 딥 러닝 텍스트 분류기를 디버깅할 수 있는 프레임워크인 FIND를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "23-4",
                    "sentence": "Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).",
                    "sentence_kor": "실험에 따르면 인간은 FIND를 사용하여 서로 다른 유형의 불완전한 데이터 세트(편향이 있는 데이터 세트 및 서로 다른 열차 테스트 분포를 가진 데이터 세트 포함)에서 훈련된 CNN 텍스트 분류기를 개선할 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "24",
            "abstractID": "EMNLP_abs-24",
            "text": [
                {
                    "index": "24-0",
                    "sentence": "A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs.",
                    "sentence_kor": "고객 관리 대화에서 빈번한 패턴은 에이전트가 사용자의 요구를 해결하는 적절한 웹 페이지 URL로 응답하는 것입니다.",
                    "tag": "1"
                },
                {
                    "index": "24-1",
                    "sentence": "We study the task of predicting the documents that customer care agents can use to facilitate users’ needs.",
                    "sentence_kor": "우리는 고객 관리 에이전트가 사용자의 요구를 촉진하기 위해 사용할 수 있는 문서를 예측하는 작업을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "24-2",
                    "sentence": "We also introduce a new public dataset which supports the aforementioned problem.",
                    "sentence_kor": "또한 앞에서 언급한 문제를 지원하는 새로운 공개 데이터 세트를 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "24-3",
                    "sentence": "Using this dataset and two others, we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task.",
                    "sentence_kor": "이 데이터 세트와 기타 두 가지 데이터를 사용하여 작업에 대한 최첨단 딥 러닝(DL) 및 정보 검색(IR) 모델을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "24-4",
                    "sentence": "Additionally, we analyze the practicality of such systems in terms of inference time complexity.",
                    "sentence_kor": "또한 추론 시간 복잡성 측면에서 이러한 시스템의 실용성을 분석한다.",
                    "tag": "3"
                },
                {
                    "index": "24-5",
                    "sentence": "Our show that an hybrid IR+DL approach provides the best of both worlds.",
                    "sentence_kor": "하이브리드 IR+DL 접근 방식이 두 가지 장점을 모두 제공한다는 것을 보여줍니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "25",
            "abstractID": "EMNLP_abs-25",
            "text": [
                {
                    "index": "25-0",
                    "sentence": "While humans process language incrementally, the best language encoders currently used in NLP do not.",
                    "sentence_kor": "인간은 언어를 점진적으로 처리하는 반면, 현재 NLP에서 사용되는 최고의 언어 인코더는 그렇지 않다.",
                    "tag": "1"
                },
                {
                    "index": "25-1",
                    "sentence": "Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers).",
                    "sentence_kor": "양방향 LSTM과 트랜스포머 모두 인코딩할 시퀀스를 완전히 사용할 수 있으며, 전진 및 후진(BiLSTM) 또는 전체(트랜스포머)로 처리할 수 있다고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "25-2",
                    "sentence": "We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems.",
                    "sentence_kor": "대화형 시스템에서 발생할 수 있는 특정 시간 단계까지의 부분 입력을 기반으로 부분 출력을 제공해야 하는 경우 증분 인터페이스 하에서 이러한 출력이 어떻게 작동하는지 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "25-3",
                    "sentence": "We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics.",
                    "sentence_kor": "다양한 NLU 데이터 세트에서 5개 모델을 테스트하고 세 가지 증분 평가 메트릭을 사용하여 성능을 비교한다.",
                    "tag": "1"
                },
                {
                    "index": "25-4",
                    "sentence": "The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality.",
                    "sentence_kor": "그 결과는 대부분의 비증가 품질을 유지하면서 증분 모드에서 양방향 인코더를 사용할 수 있는 가능성을 지원한다.",
                    "tag": "3"
                },
                {
                    "index": "25-5",
                    "sentence": "The “omni-directional” BERT model, which achieves better non-incremental performance, is impacted more by the incremental access.",
                    "sentence_kor": "더 나은 비증분적 성능을 달성하는 \"옴니 방향\" BERT 모델은 증분 액세스의 영향을 더 많이 받습니다.",
                    "tag": "4"
                },
                {
                    "index": "25-6",
                    "sentence": "This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.",
                    "sentence_kor": "이는 일부 올바른 컨텍스트를 사용할 수 있을 때까지 출력을 지연시키거나 GPT-2와 같은 언어 모델에 의해 생성된 가상적인 우측 컨텍스트를 통합하여 훈련 체계(튜닝된 훈련) 또는 시험 절차를 적용함으로써 완화될 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "26",
            "abstractID": "EMNLP_abs-26",
            "text": [
                {
                    "index": "26-0",
                    "sentence": "We propose a generative framework for joint sequence labeling and sentence-level classification.",
                    "sentence_kor": "우리는 공동 시퀀스 라벨링 및 문장 수준 분류를 위한 생성 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "26-1",
                    "sentence": "Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space.",
                    "sentence_kor": "우리 모델은 단일 공유 자연어 출력 공간을 사용하여 여러 시퀀스 레이블링 작업을 동시에 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "26-2",
                    "sentence": "Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks.",
                    "sentence_kor": "이전의 차별적 방법과 달리, 우리의 모델은 자연스럽게 라벨 의미론을 통합하고 작업 전반에 걸쳐 지식을 공유한다.",
                    "tag": "3"
                },
                {
                    "index": "26-3",
                    "sentence": "Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks.",
                    "sentence_kor": "퓨샷 학습, 낮은 리소스 및 높은 리소스 작업에서 우수한 성능을 제공하는 프레임워크 일반 목적.",
                    "tag": "4"
                },
                {
                    "index": "26-4",
                    "sentence": "We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks.",
                    "sentence_kor": "우리는 널리 알려진 명명된 개체 인식, 슬롯 라벨링 및 의도 분류 벤치마크에서 이러한 장점을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "26-5",
                    "sentence": "We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results.",
                    "sentence_kor": "우리는 이전의 5발(75.0%에서 90.9%로) 및 1발(70.4%에서 81.0%) 결과를 크게 개선하여 퓨샷 슬롯 라벨링에 대한 새로운 최첨단 기술을 설정했다.",
                    "tag": "4"
                },
                {
                    "index": "26-6",
                    "sentence": "Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics.",
                    "sentence_kor": "또한, 우리 모델은 레이블 의미론을 통합하여 BERT 기준선에 비해 낮은 리소스 슬롯 라벨링에서 큰 개선(46.27% ~ 63.83%)을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "26-7",
                    "sentence": "We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.",
                    "sentence_kor": "또한 모든 작업에 대한 최첨단 기술의 두 지점 내에서 수행하고 SNIPS 데이터 세트에 새로운 최첨단 기술을 설정하는 등 높은 리소스 작업에 대한 경쟁력 있는 결과를 유지한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "27",
            "abstractID": "EMNLP_abs-27",
            "text": [
                {
                    "index": "27-0",
                    "sentence": "Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses.",
                    "sentence_kor": "기존의 개방형 도메인 대화 상자 모델은 일반적으로 대상 인간 응답의 혼란을 최소화하도록 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "27-1",
                    "sentence": "However, some human replies are more engaging than others, spawning more followup interactions.",
                    "sentence_kor": "그러나 일부 인간의 반응은 다른 사람보다 더 매력적이어서 더 많은 후속 상호작용을 일으킨다.",
                    "tag": "1"
                },
                {
                    "index": "27-2",
                    "sentence": "Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging.",
                    "sentence_kor": "현재의 대화 모델은 문맥과 관련된 턴을 생성할 수 있지만, 강력한 에이전트를 생산하기 위해서는 이러한 모델이 진정으로 매력적인 턴을 예측하고 최적화할 수 있어야 합니다.",
                    "tag": "2"
                },
                {
                    "index": "27-3",
                    "sentence": "We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction.",
                    "sentence_kor": "소셜 미디어 피드백 데이터(응답 수 및 상향 투표 수)를 활용하여 피드백 예측을 위한 대규모 교육 데이터 세트를 구축한다.",
                    "tag": "3"
                },
                {
                    "index": "27-4",
                    "sentence": "To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors.",
                    "sentence_kor": "피드백과 참여성 사이의 가능한 왜곡을 완화하기 위해 순위 문제를 교란 요인이 거의 없는 응답 쌍의 비교로 변환한다.",
                    "tag": "4"
                },
                {
                    "index": "27-5",
                    "sentence": "We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines.",
                    "sentence_kor": "우리는 133M 쌍의 인간 피드백 데이터를 기반으로 GPT-2 기반 모델 세트인 DialogRPT를 훈련했고 그 결과 랭커는 여러 기준선을 능가했다.",
                    "tag": "4"
                },
                {
                    "index": "27-6",
                    "sentence": "Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback.",
                    "sentence_kor": "특히, 당사의 랭커는 Reddit 피드백 예측에서 큰 마진을 보이며 기존의 대화 상자 난해성 기준선을 능가한다.",
                    "tag": "3"
                },
                {
                    "index": "27-7",
                    "sentence": "We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses.",
                    "sentence_kor": "마지막으로 피드백 예측 모델과 인간과 유사한 점수 모델을 결합하여 기계 생성 대화 상자 응답의 순위를 매긴다.",
                    "tag": "3"
                },
                {
                    "index": "27-8",
                    "sentence": "Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.",
                    "sentence_kor": "크라우드 소싱 인간 평가는 우리의 순위 방법이 기준 모델보다 실제 인간 선호도와 더 잘 상관된다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "28",
            "abstractID": "EMNLP_abs-28",
            "text": [
                {
                    "index": "28-0",
                    "sentence": "We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models.",
                    "sentence_kor": "Text-to-SQL 모델에 대한 대략적인 의미 정확도를 위한 테스트 세트 정확도를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "28-1",
                    "sentence": "Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases.",
                    "sentence_kor": "우리의 방법은 무작위로 생성된 다수의 데이터베이스에서 골드 쿼리에 대한 높은 코드 적용 범위를 달성하는 소규모 데이터베이스 테스트 세트를 증류한다.",
                    "tag": "3"
                },
                {
                    "index": "28-2",
                    "sentence": "At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently.",
                    "sentence_kor": "평가 시 증류된 테스트 제품군에 대한 예측 쿼리의 표시 정확도를 계산하므로 의미 정확도에 대한 엄격한 상한을 효율적으로 계산한다.",
                    "tag": "3"
                },
                {
                    "index": "28-3",
                    "sentence": "We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples.",
                    "sentence_kor": "Spider Leader 보드에 제출된 21개 모델을 평가하고 100개의 예에서 항상 올바른 방법을 수동으로 검증하기 위해 제안된 방법을 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "28-4",
                    "sentence": "In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed.",
                    "sentence_kor": "이와는 대조적으로 현재 스파이더 메트릭은 평균 2.5%, 최악의 경우 8.1%의 잘못된 음성률을 나타내며, 이는 테스트 세트 정확도가 필요하다는 것을 나타낸다.",
                    "tag": "4"
                },
                {
                    "index": "28-5",
                    "sentence": "Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.",
                    "sentence_kor": "11개의 Text-to-SQL 데이터 세트에 대한 증류식 테스트 제품군과 함께 구현이 공개적으로 제공된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "29",
            "abstractID": "EMNLP_abs-29",
            "text": [
                {
                    "index": "29-0",
                    "sentence": "In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering.",
                    "sentence_kor": "본 논문에서 우리는 질문 답변과 같은 대규모 NLP 작업을 위해 재사용 가능한 시퀀스 임베딩을 구축하는 데 중요한 사전 훈련 시퀀스 인코더의 새로운 접근 방식인 교차 사고를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "29-1",
                    "sentence": "Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words.",
                    "sentence_kor": "전체 문장의 원래 신호를 사용하는 대신 모델이 마스킹된 단어를 예측하는 데 가장 유용한 정보를 자동으로 선택할 수 있는 많은 짧은 시퀀스에 대해 트랜스포머 기반 시퀀스 인코더를 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "29-2",
                    "sentence": "Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines.",
                    "sentence_kor": "질문 답변 및 텍스트 수반 작업에 대한 실험은 사전 훈련된 인코더가 연속적인 문장 신호로 훈련된 최첨단 인코더와 기존의 마스킹 언어 모델링 기준을 능가할 수 있음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "29-3",
                    "sentence": "Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.",
                    "sentence_kor": "또한 우리가 제안한 접근 방식은 중간 정보 검색 성능을 향상시켜 핫팟QA(풀위키 설정)에 대한 새로운 최첨단 기술을 달성한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "30",
            "abstractID": "EMNLP_abs-30",
            "text": [
                {
                    "index": "30-0",
                    "sentence": "We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort.",
                    "sentence_kor": "우리는 데이터베이스의 질문에 수동 노력 없이 대답하는 의미론적 파서를 생성하는 방법론 및 툴킷인 AutoQA를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "30-1",
                    "sentence": "Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations.",
                    "sentence_kor": "데이터베이스 스키마와 해당 데이터가 주어지면 AutoQA는 다양한 데이터베이스 작업을 다루는 교육을 위해 대량의 고품질 질문을 자동으로 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "30-2",
                    "sentence": "It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech.",
                    "sentence_kor": "템플릿 기반 구문 분석과 결합된 자동 패러프레이싱을 사용하여 음성의 다른 부분에서 속성의 대체 표현식을 찾는다.",
                    "tag": "3"
                },
                {
                    "index": "30-3",
                    "sentence": "It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.",
                    "sentence_kor": "또한 새로운 필터링 자동 패러프라서를 사용하여 전체 문장의 올바른 패러프레이즈를 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "30-4",
                    "sentence": "We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers.",
                    "sentence_kor": "스키마2에 AutoQA를 적용합니다.QA 데이터 세트를 분석하고 자연적 질문에 대해 테스트했을 때 평균 62.9%의 논리적 형식 정확도를 얻는데, 이는 전문가 자연어 주석과 군중 작업자로부터 수집된 패러프레이즈 데이터로 훈련된 모델보다 6.4%밖에 낮지 않다.",
                    "tag": "4+5"
                },
                {
                    "index": "30-5",
                    "sentence": "To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset.",
                    "sentence_kor": "AutoQA의 일반성을 입증하기 위해 Overnight 데이터 세트에도 적용합니다.",
                    "tag": "4"
                },
                {
                    "index": "30-6",
                    "sentence": "AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.",
                    "sentence_kor": "AutoQA는 69.8%의 응답 정확도를 달성하여 최첨단 제로샷 모델보다 16.4% 높고 인간 데이터로 훈련된 동일한 모델보다 5.2%밖에 낮지 않다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "31",
            "abstractID": "EMNLP_abs-31",
            "text": [
                {
                    "index": "31-0",
                    "sentence": "Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents.",
                    "sentence_kor": "다중 문서 요약(MDS)은 여러 관련 문서에 대한 양질의 요약을 작성하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "31-1",
                    "sentence": "In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact.",
                    "sentence_kor": "본 논문에서 우리는 요약 후보자의 우수성이 소위 스펙트럼 영향과 밀접하게 연관되어 있다는 스펙트럼 기반 가설을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "31-2",
                    "sentence": "Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster.",
                    "sentence_kor": "여기서 스펙트럼 영향은 문서 클러스터에서 요약 후보를 떨어뜨릴 때 선호도 매트릭스의 지배적인 고유값에 대한 동요를 고려한다.",
                    "tag": "3"
                },
                {
                    "index": "31-3",
                    "sentence": "The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation.",
                    "sentence_kor": "이 가설은 의미론적 확장, 전파 역학 및 매트릭스 섭동의 세 가지 이론적 관점에서 검증된다.",
                    "tag": "3"
                },
                {
                    "index": "31-4",
                    "sentence": "According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact.",
                    "sentence_kor": "이 가설에 따르면, 우리는 MDS 과제를 스펙트럼 영향의 조합 최적화로 공식화하고 스펙트럼 영향의 대리에 기초한 가속 그리디 솔루션을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "31-5",
                    "sentence": "The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.",
                    "sentence_kor": "다양한 데이터 세트에 대한 평가 결과는 (1) 요약 후보자의 성능은 우리의 가설과 일치하는 스펙트럼 영향과 긍정적으로 상관된다. (2) 우리의 스펙트럼 기반 방법은 최첨단 MDS 시스템과 비교하여 경쟁력 있는 결과를 가지고 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "32",
            "abstractID": "EMNLP_abs-32",
            "text": [
                {
                    "index": "32-0",
                    "sentence": "Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years.",
                    "sentence_kor": "딥 러닝은 여러 해에 걸쳐 조사된 다양한 방법과 개선된 ROUGE 점수로 텍스트 요약에서 상당한 개선을 이끌어냈다.",
                    "tag": "1"
                },
                {
                    "index": "32-1",
                    "sentence": "However, gaps still exist between summaries produced by automatic summarizers and human professionals.",
                    "sentence_kor": "그러나 자동 요약자와 전문 인력 사이에 여전히 격차가 존재한다.",
                    "tag": "1"
                },
                {
                    "index": "32-2",
                    "sentence": "Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually.",
                    "sentence_kor": "세분화된 통사적 및 의미적 수준에 대한 강점과 한계와 관련하여 요약 시스템을 더 잘 이해하기 위해, 우리는 다차원 품질 메트릭(MQM)을 참조하고 10개의 대표적인 요약 모델에서 오류의 8가지 주요 소스를 수동으로 정량화한다.",
                    "tag": "1+2"
                },
                {
                    "index": "32-3",
                    "sentence": "Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.",
                    "sentence_kor": "주로 1) 유사한 환경에서 추출 요약자는 충실성과 사실 일관성에 대한 강점으로 인해 추상적 요약자보다 일반적으로 더 낫다는 것을 발견한다. 2) 복사, 커버리지 및 혼합 추출/추상 방법과 같은 이정표 기법은 특정 개선을 가져오지만 한계를 입증하기도 한다. 3) p재교육 기법, 특히 시퀀스 투 시퀀스 사전 교육은 BART가 최상의 결과를 제공하므로 텍스트 요약을 개선하는 데 매우 효과적이다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "33",
            "abstractID": "EMNLP_abs-33",
            "text": [
                {
                    "index": "33-0",
                    "sentence": "Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.",
                    "sentence_kor": "비지도 방법은 병렬 말뭉치가 필요하지 않다는 점에서 추상적 텍스트 요약에 유망하다.",
                    "tag": "1"
                },
                {
                    "index": "33-1",
                    "sentence": "However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.",
                    "sentence_kor": "하지만, 그들의 성과는 아직 만족스럽지 못하기 때문에, 유망한 해결책에 대한 연구가 진행 중이다.",
                    "tag": "1"
                },
                {
                    "index": "33-2",
                    "sentence": "In this paper, we propose a new approach based on Q-learning with an edit-based summarization.",
                    "sentence_kor": "본 논문에서, 우리는 편집 기반 요약을 포함한 Q-러닝을 기반으로 하는 새로운 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "33-3",
                    "sentence": "The method combines two key modules to form an Editorial Agent and Language Model converter (EALM).",
                    "sentence_kor": "이 방법은 두 개의 주요 모듈을 결합하여 편집 에이전트와 언어 모델 변환기(EALM)를 형성한다.",
                    "tag": "3"
                },
                {
                    "index": "33-4",
                    "sentence": "The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals.",
                    "sentence_kor": "에이전트는 편집 작업(예: 삭제, 유지 및 바꾸기)을 예측한 다음 LM 컨버터는 결정적으로 작업 신호를 기반으로 요약을 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "33-5",
                    "sentence": "Q-learning is leveraged to train the agent to produce proper edit actions.",
                    "sentence_kor": "Q-러닝을 활용하여 에이전트가 적절한 편집 작업을 수행하도록 교육합니다.",
                    "tag": "3"
                },
                {
                    "index": "33-6",
                    "sentence": "Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set).",
                    "sentence_kor": "실험 결과에 따르면 EALM은 실제로 페어링된 데이터가 0개라도 이전의 인코더-디코더 기반 방법과 비교하여 경쟁력 있는 성능을 제공했다(즉, 검증 세트 없음).",
                    "tag": "4"
                },
                {
                    "index": "33-7",
                    "sentence": "Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization.",
                    "sentence_kor": "과제를 Q-러닝으로 정의하면 경쟁력 있는 방법을 개발할 수 있을 뿐만 아니라 감독되지 않은 요약에 사용할 수 있는 강화 학습의 최신 기술을 개발할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "33-8",
                    "sentence": "We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.",
                    "sentence_kor": "우리는 또한 감독되지 않은 요약자에 대한 향후 연구에 대한 통찰력을 제공하는 정성 분석을 수행한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "34",
            "abstractID": "EMNLP_abs-34",
            "text": [
                {
                    "index": "34-0",
                    "sentence": "Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance.",
                    "sentence_kor": "추상적인 문서 요약은 트랜스포머 기반 모델이 최첨단 성능을 달성한 문서 이해 및 요약 생성을 포함한 포괄적인 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "34-1",
                    "sentence": "Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance.",
                    "sentence_kor": "트랜스포머와 비교하여 주제 모델은 명시적 문서 의미론을 더 잘 학습하므로 트랜스포머에 통합되어 성능을 더욱 향상시킬 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "34-2",
                    "sentence": "To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules.",
                    "sentence_kor": "이를 위해 주제 모델에서 학습한 의미론을 재정렬하고 탐구한 다음 세 개의 모듈을 포함한 주제 보조자(TA)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "34-3",
                    "sentence": "TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters.",
                    "sentence_kor": "i) TA는 기존 트랜스포머 네트워크의 어떤 구조도 깨지지 않는 플러그 앤 플레이 모델이기 때문에 사용자가 트랜스포머+를 쉽게 미세 조정할 수 있습니다.잘 훈련된 모델을 기반으로 한 TA. ii) TA는 소수의 추가 매개 변수만 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "34-4",
                    "sentence": "Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.",
                    "sentence_kor": "세 개의 데이터 세트에 대한 실험 결과는 TA가 여러 Transformer 기반 모델의 성능을 향상시킬 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "35",
            "abstractID": "EMNLP_abs-35",
            "text": [
                {
                    "index": "35-0",
                    "sentence": "Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one.",
                    "sentence_kor": "기존 언어 모델 압축 방법은 대부분 간단한 L_2 손실을 사용하여 큰 BERT 모델의 중간 표현에 대한 지식을 작은 모델로 증류한다.",
                    "tag": "1"
                },
                {
                    "index": "35-1",
                    "sentence": "Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network.",
                    "sentence_kor": "널리 사용되지만, 설계에 의한 이 목표는 은닉 표현의 모든 차원이 독립적이어서 교사 네트워크의 중간 계층에서 중요한 구조적 지식을 포착하지 못한다고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "35-2",
                    "sentence": "To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective.",
                    "sentence_kor": "더 나은 증류 효과를 달성하기 위해 학생이 대조적 목표를 통해 교사의 중간 레이어를 통해 지식을 증류하도록 훈련받는 원칙적인 지식 증류 프레임워크인 CoDIR(Constrastive Dreuation on Intermediate Resentation)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "35-3",
                    "sentence": "By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers.",
                    "sentence_kor": "CoDIR은 양성 샘플을 대규모 음성 샘플 세트와 구별하는 방법을 배움으로써 교사의 숨겨진 계층에서 풍부한 정보를 학생이 활용할 수 있도록 돕는다.",
                    "tag": "3"
                },
                {
                    "index": "35-4",
                    "sentence": "CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.",
                    "sentence_kor": "CoDIR은 사전 훈련 및 미세 조정 단계에서 대규모 언어 모델을 압축하는 데 쉽게 적용할 수 있으며 GLUE 벤치마크에서 탁월한 성능을 달성하여 최첨단 압축 방법을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "36",
            "abstractID": "EMNLP_abs-36",
            "text": [
                {
                    "index": "36-0",
                    "sentence": "Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks.",
                    "sentence_kor": "BERT와 같은 변압기 기반 사전 훈련 모델은 많은 자연어 처리 작업에서 놀라운 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "36-1",
                    "sentence": "However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices.",
                    "sentence_kor": "그러나 이러한 모델은 계산과 메모리 비용이 모두 많이 들어 자원이 제한된 장치에 대한 배치를 방해한다.",
                    "tag": "1"
                },
                {
                    "index": "36-2",
                    "sentence": "In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model.",
                    "sentence_kor": "본 연구에서는 미세 조정된 BERT 모델에서 가중치를 3분할 수 있는 TernaryBERT를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "36-3",
                    "sentence": "Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT.",
                    "sentence_kor": "특히, 근사 기반 및 손실 인식 3항화 방법을 모두 사용하고 BERT의 다른 부분의 3항화 세분성을 경험적으로 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "36-4",
                    "sentence": "Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process.",
                    "sentence_kor": "또한 낮은 비트의 낮은 용량으로 인한 정확도 저하를 줄이기 위해 교육 과정에서 지식 증류 기술을 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "36-5",
                    "sentence": "Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.",
                    "sentence_kor": "GLUE 벤치마크와 SQuAD에 대한 실험에 따르면, 우리가 제안한 TernaryBERT는 다른 BERT 양자화 방법을 능가하며, 14.9배 작으면서도 완전 정밀 모델과 비슷한 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "37",
            "abstractID": "EMNLP_abs-37",
            "text": [
                {
                    "index": "37-0",
                    "sentence": "Self-supervised pre-training of transformer models has revolutionized NLP applications.",
                    "sentence_kor": "변압기 모델의 자체 감독 사전 교육은 NLP 애플리케이션에 혁신을 가져왔다.",
                    "tag": "1"
                },
                {
                    "index": "37-1",
                    "sentence": "Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning.",
                    "sentence_kor": "언어 모델링 목표를 사용한 이러한 사전 교육은 미세 조정으로 새로운 작업에 잘 일반화하는 매개변수에 유용한 초기 지점을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "37-2",
                    "sentence": "However, fine-tuning is still data inefficient — when there are few labeled examples, accuracy can be low.",
                    "sentence_kor": "그러나 미세 조정은 여전히 데이터 효율성이 떨어집니다. 라벨이 붙은 예가 거의 없을 경우 정확도가 낮을 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "37-3",
                    "sentence": "Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem.",
                    "sentence_kor": "데이터 효율성은 향후 미세 조정을 위해 사전 교육을 직접 최적화하여 개선할 수 있으며, 이는 메타 학습 문제로 취급될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "37-4",
                    "sentence": "However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult.",
                    "sentence_kor": "그러나 표준 메타 학습 기법은 일반화를 위해 많은 훈련 과제가 필요하다. 불행히도 이러한 감독 과제를 다양하게 찾는 것은 어렵다.",
                    "tag": "2"
                },
                {
                    "index": "37-5",
                    "sentence": "This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text.",
                    "sentence_kor": "본 논문은 라벨이 부착되지 않은 텍스트에서 크고 풍부한 메타 학습 작업 분포를 생성하기 위한 자체 감독 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "37-6",
                    "sentence": "This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms.",
                    "sentence_kor": "이는 클로즈 스타일의 목표를 사용하여 수행되지만 소수의 어휘 용어 중에서 비워질 토큰을 수집하여 별도의 다중 클래스 분류 작업을 만든다.",
                    "tag": "3"
                },
                {
                    "index": "37-7",
                    "sentence": "This yields as many unique meta-training tasks as the number of subsets of vocabulary terms.",
                    "sentence_kor": "이는 어휘 용어의 하위 집합 수만큼 고유한 메타 훈련 작업을 산출한다.",
                    "tag": "3"
                },
                {
                    "index": "37-8",
                    "sentence": "We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework.",
                    "sentence_kor": "우리는 최근의 메타 학습 프레임워크를 사용하여 이러한 작업 분포에 대한 변압기 모델을 메타 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "37-9",
                    "sentence": "On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning.",
                    "sentence_kor": "17개의 NLP 작업에서, 우리는 이 메타 훈련이 언어 모델 사전 훈련과 미세 조정보다 더 나은 퓨샷 일반화로 이어진다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "37-10",
                    "sentence": "Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.",
                    "sentence_kor": "또한, 우리는 어떻게 자기 지도 과제를 메타 학습을 위한 감독 작업과 결합할 수 있는지 보여줌으로써 이전의 감독 메타 학습에 비해 상당한 정확성을 얻을 수 있는지 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "38",
            "abstractID": "EMNLP_abs-38",
            "text": [
                {
                    "index": "38-0",
                    "sentence": "Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning.",
                    "sentence_kor": "현재의 자연어 처리 모델은 단일 작업에 잘 작동하지만 평생 학습으로 알려진 과제인 이전 작업을 잊지 않고 지속적으로 학습하지 못하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "38-1",
                    "sentence": "State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time.",
                    "sentence_kor": "최첨단 평생 언어 학습 방법은 과거의 예를 일시적 기억에 저장하고 훈련 및 추론 시간에 모두 재생한다.",
                    "tag": "1"
                },
                {
                    "index": "38-2",
                    "sentence": "However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed.",
                    "sentence_kor": "그러나 나중에 실험에서 보여주듯이, (1) 양호한 성능을 달성하기 위해 비현실적으로 큰 메모리 모듈 필요, (2) 음성 전송으로 인한 고통, (3) 추론 속도를 현저히 늦추는 각 테스트 예제에 대해 다중 로컬 적응 단계 필요 등 세 가지 중요한 장애물이 있다.",
                    "tag": "1"
                },
                {
                    "index": "38-3",
                    "sentence": "In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion.",
                    "sentence_kor": "본 논문에서는 평생 학습 방법의 세 가지 공통 원칙을 식별하고 이를 시너지 방식으로 결합하는 효율적인 메타 수명 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "38-4",
                    "sentence": "To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation.",
                    "sentence_kor": "샘플 효율성을 달성하기 위해, 우리의 방법은 로컬 적응을 위해 더 나은 초기화를 학습하는 방식으로 모델을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "38-5",
                    "sentence": "Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning.",
                    "sentence_kor": "텍스트 분류 및 질문 답변 벤치마크에 대한 광범위한 실험은 1%의 메모리 크기만 사용하여 최첨단 성능을 달성하고 다중 작업 학습과의 격차를 줄임으로써 프레임워크의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "38-6",
                    "sentence": "We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.",
                    "sentence_kor": "우리는 또한 우리의 방법이 치명적인 망각과 음성 전송을 동시에 완화한다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "39",
            "abstractID": "EMNLP_abs-39",
            "text": [
                {
                    "index": "39-0",
                    "sentence": "Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language.",
                    "sentence_kor": "다국어 상황 임베딩은 다국어 BERT가 하나의 소스 언어로 미세 조정되고 다른 대상 언어로 평가되는 제로샷 교차 언어 전송 학습에서 최첨단 성능을 입증했다.",
                    "tag": "1"
                },
                {
                    "index": "39-1",
                    "sentence": "However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers.",
                    "sentence_kor": "그러나 mBERT 제로샷 정확도에 대해 발표된 결과는 4개 논문에 걸쳐 MLDoc 분류 작업에서 17점이나 차이가 난다.",
                    "tag": "1"
                },
                {
                    "index": "39-2",
                    "sentence": "We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks.",
                    "sentence_kor": "우리는 제로샷 설정에서 모델 선택에 영어 편차 정확도를 사용하는 표준 관행이 MLDoc 및 XNLI 작업에서 재현 가능한 결과를 얻기 어렵게 한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "39-3",
                    "sentence": "English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs.",
                    "sentence_kor": "영어 개발 정확도는 목표 언어 정확도와 상관관계가 없는 경우가 많으며 제로샷 성능은 동일한 미세 조정 실행 시 및 다른 미세 조정 실행 간에 서로 크게 다르다.",
                    "tag": "1"
                },
                {
                    "index": "39-4",
                    "sentence": "These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R).",
                    "sentence_kor": "이러한 재현성 문제는 사전 훈련된 임베딩이 다른 다른 작업(예: XLM-R이 있는 MLQA)에도 존재한다.",
                    "tag": "1"
                },
                {
                    "index": "39-5",
                    "sentence": "We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set.",
                    "sentence_kor": "제로샷 결과와 함께 오라클 점수를 제공하는 것이 좋습니다. 영어 데이터를 사용하여 미세 조정하되 대상 개발 세트가 있는 체크포인트를 선택하십시오.",
                    "tag": "1"
                },
                {
                    "index": "39-6",
                    "sentence": "Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.",
                    "sentence_kor": "이 상한을 보고하면 임의로 잘못된 체크포인트를 방지하여 결과를 보다 일관되게 유지할 수 있습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "40",
            "abstractID": "EMNLP_abs-40",
            "text": [
                {
                    "index": "40-0",
                    "sentence": "We present a novel supervised word alignment method based on cross-language span prediction.",
                    "sentence_kor": "우리는 교차 언어 범위 예측을 기반으로 한 새로운 감독 단어 정렬 방법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "40-1",
                    "sentence": "We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence.",
                    "sentence_kor": "먼저 단어 정렬 문제를 소스 문장의 토큰에서 대상 문장의 범위까지 독립적인 예측의 모음으로 공식화한다.",
                    "tag": "3"
                },
                {
                    "index": "40-2",
                    "sentence": "Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data.",
                    "sentence_kor": "이 단계는 SQuAD v2.0 스타일 질문 응답 작업과 같으므로 수동으로 생성된 금색 단어 정렬 데이터에 미세 조정되는 다국어 BERT를 사용하여 해결한다.",
                    "tag": "3"
                },
                {
                    "index": "40-3",
                    "sentence": "It is nontrivial to obtain accurate alignment from a set of independently predicted spans.",
                    "sentence_kor": "독립적으로 예측된 범위 집합에서 정확한 정렬을 얻는 것은 중요하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "40-4",
                    "sentence": "We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions.",
                    "sentence_kor": "소스 토큰의 컨텍스트를 추가하고 두 방향 예측을 대칭화하여 단어 정렬 정확도를 크게 향상시켰다.",
                    "tag": "4"
                },
                {
                    "index": "40-5",
                    "sentence": "In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining.",
                    "sentence_kor": "중국어, 일본어, 독일어, 루마니아어, 프랑스어 및 영어의 5개 단어 정렬 데이터 세트를 사용한 실험에서, 제안된 방법이 사전 훈련을 위한 물리침 없이 이전의 감독 및 감독되지 않은 단어 정렬 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "40-6",
                    "sentence": "For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.",
                    "sentence_kor": "예를 들어, 우리는 중국어-영어 데이터에 대해 86.7 F1 점수를 획득했는데, 이는 이전의 최첨단 감독 방법보다 13.3점 높은 점수이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "41",
            "abstractID": "EMNLP_abs-41",
            "text": [
                {
                    "index": "41-0",
                    "sentence": "Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism.",
                    "sentence_kor": "정렬 및 변환 방법을 공동으로 배우려는 원래 목표에도 불구하고, 이전 연구에 따르면 Transformer는 주의 메커니즘을 통해 열악한 단어 정렬을 포착하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "41-1",
                    "sentence": "In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET.",
                    "sentence_kor": "본 논문에서 우리는 주의 가중치가 정확한 단어 정렬을 포착한다는 것을 보여주고 Shift-At 및 Shift-AET 두 가지 새로운 단어 정렬 유도 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "41-2",
                    "sentence": "The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work.",
                    "sentence_kor": "주요 아이디어는 이전 작업과 같이 정렬 대상 토큰이 디코더 출력이 아닌 디코더 입력인 단계에서 정렬을 유도하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "41-3",
                    "sentence": "Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change.",
                    "sentence_kor": "Shift-At는 트랜스포머의 주의 가중치에서 정렬을 유도하는 해석 방법이며 매개변수 업데이트 또는 아키텍처 변경이 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "41-4",
                    "sentence": "Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments.",
                    "sentence_kor": "Shift-AET는 Transformer에 긴밀히 통합되고 대칭화된 Shift-At 정렬에서 관리자와 별도로 교육되는 추가 정렬 모듈에서 정렬을 추출합니다.",
                    "tag": "3"
                },
                {
                    "index": "41-5",
                    "sentence": "Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.",
                    "sentence_kor": "공개적으로 사용 가능한 세 개의 데이터 세트에 대한 실험을 통해 두 방법 모두 해당 신경 기준선보다 성능이 우수하며 Shift-AET는 GIZA++를 1.4-4.8 AER 포인트 차이로 크게 능가한다는 것을 입증했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "42",
            "abstractID": "EMNLP_abs-42",
            "text": [
                {
                    "index": "42-0",
                    "sentence": "Cherokee is a highly endangered Native American language spoken by the Cherokee people.",
                    "sentence_kor": "체로키어는 체로키족이 사용하는 멸종위기에 처한 아메리카 원주민 언어이다.",
                    "tag": "1"
                },
                {
                    "index": "42-1",
                    "sentence": "The Cherokee culture is deeply embedded in its language.",
                    "sentence_kor": "체로키 문화는 그 언어에 깊이 뿌리박고 있다.",
                    "tag": "1"
                },
                {
                    "index": "42-2",
                    "sentence": "However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year.",
                    "sentence_kor": "하지만, 세계에 약 2,000명의 유창한 제1언어 체로키족 화자들이 남아있고 그 수는 매년 감소하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "42-3",
                    "sentence": "To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English.",
                    "sentence_kor": "이러한 위험에 처한 언어를 구하기 위해 체로키와 영어 사이의 기계 번역 연구를 용이하게 하기 위해 체로키와 영어의 병렬 데이터 세트인 ChrEn을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "42-4",
                    "sentence": "Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total.",
                    "sentence_kor": "일부 인기 있는 기계 번역 언어 쌍과 비교하여 ChrEn은 총 14k개의 문장 쌍만 포함하는 매우 낮은 자원이다.",
                    "tag": "3"
                },
                {
                    "index": "42-5",
                    "sentence": "We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation.",
                    "sentence_kor": "우리는 도메인 내 및 도메인 외 평가를 모두 용이하게 하는 방법으로 병렬 데이터를 분할했다.",
                    "tag": "3"
                },
                {
                    "index": "42-6",
                    "sentence": "We also collect 5k Cherokee monolingual data to enable semi-supervised learning.",
                    "sentence_kor": "우리는 또한 준지도 학습을 가능하게 하기 위해 5k 체로키 단일 언어 데이터를 수집한다.",
                    "tag": "3"
                },
                {
                    "index": "42-7",
                    "sentence": "Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems.",
                    "sentence_kor": "이러한 데이터 세트 외에도 여러 체로키-영어 및 영어-체로키 기계 번역 시스템을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "42-8",
                    "sentence": "We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages.",
                    "sentence_kor": "시만텍(프레이즈 기반)과 NMT(RNN 기반 및 트랜스포머 기반) 시스템, 감독 대 준감독(언어 모델, 역번역 및 BERT/다언어-BERT) 방법 및 4개 다른 언어와 학습 대 다국어 공동 훈련을 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "42-9",
                    "sentence": "Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.",
                    "sentence_kor": "우리의 최고 결과는 각각 도메인 내 15.8/12.7 BLEU와 도메인 외부 Chr-En/EnChr 번역 6.5/5.0 BLEU이며, 우리의 데이터 세트와 시스템이 체로키 언어 활성화를 위한 커뮤니티의 향후 작업을 장려하기를 바란다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "43",
            "abstractID": "EMNLP_abs-43",
            "text": [
                {
                    "index": "43-0",
                    "sentence": "Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable.",
                    "sentence_kor": "사회에 널리 퍼졌음에도 불구하고, 사회적 편견은 식별하기 어려운데, 이는 주로 이 영역에서 인간의 판단이 신뢰할 수 없기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "43-1",
                    "sentence": "We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias.",
                    "sentence_kor": "우리는 논평 수준에서 여성에 대한 성 편견을 식별하기 위해 감독되지 않은 접근법을 취하고 편견을 포함할 가능성이 있는 텍스트를 표면화할 수 있는 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "43-2",
                    "sentence": "Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data.",
                    "sentence_kor": "우리의 주요 과제는 모델이 데이터의 다른 아티팩트가 아닌 암시적 편향의 징후에 초점을 맞추도록 하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "43-3",
                    "sentence": "Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning.",
                    "sentence_kor": "따라서 우리의 방법론은 성향 일치와 적대적 학습을 통해 조합의 영향을 줄이는 것을 포함한다.",
                    "tag": "3"
                },
                {
                    "index": "43-4",
                    "sentence": "Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization.",
                    "sentence_kor": "우리의 분석은 여성 정치인에 대한 편향된 논평이 얼마나 엇갈린 비판을 담고 있는지 보여주는 반면, 다른 여성 공인에 대한 논평은 외모와 성화에 초점을 맞추고 있다.",
                    "tag": "3"
                },
                {
                    "index": "43-5",
                    "sentence": "Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.",
                    "sentence_kor": "궁극적으로, 우리의 연구는 주관적인 인간의 판단에 의존하지 않고 다양한 영역에서 미묘한 편견을 포착하는 방법을 제공한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "44",
            "abstractID": "EMNLP_abs-44",
            "text": [
                {
                    "index": "44-0",
                    "sentence": "Offering condolence is a natural reaction to hearing someone’s distress.",
                    "sentence_kor": "조의를 표하는 것은 누군가의 고통을 듣는 것에 대한 자연스러운 반응이다.",
                    "tag": "1"
                },
                {
                    "index": "44-1",
                    "sentence": "Individuals frequently express distress in social media, where some communities can provide support.",
                    "sentence_kor": "개인들은 종종 일부 커뮤니티가 지원을 제공할 수 있는 소셜 미디어에서 고통을 표현한다.",
                    "tag": "2"
                },
                {
                    "index": "44-2",
                    "sentence": "However, not all condolence is equal—trite responses offer little actual support despite their good intentions.",
                    "sentence_kor": "그러나 모든 조문이 다 같은 것은 아닙니다. 진부한 답변은 좋은 의도에도 불구하고 실제 지원을 거의 제공하지 않습니다.",
                    "tag": "3"
                },
                {
                    "index": "44-3",
                    "sentence": "Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online.",
                    "sentence_kor": "여기서, 우리는 11개의 거대한 데이터 세트를 만들 수 있는 계산 도구를 개발합니다.4M의 고통 표현과 2.온라인 조문 동향을 조사하기 위해 8M의 상응하는 조문 제공.",
                    "tag": "3"
                },
                {
                    "index": "44-4",
                    "sentence": "Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement.",
                    "sentence_kor": "우리의 연구는 어떤 종류의 고통이 단지 관여가 아닌 보조적인 조의를 받는지에 대한 광범위한 차이를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "44-5",
                    "sentence": "Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory.",
                    "sentence_kor": "사회심리학 연구를 바탕으로, 우리는 조문 언어를 분석하고 감정 이론을 사용하여 조문에서의 공감을 수량화하기 위한 새로운 데이터 세트를 개발한다.",
                    "tag": "4"
                },
                {
                    "index": "44-6",
                    "sentence": "Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.",
                    "sentence_kor": "마지막으로, 우리는 개인들이 온라인에서 가장 도움이 된다고 생각하는 조문의 특징들이 대인관계 환경에서 볼 수 있는 특징들과 상당히 다르다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "45",
            "abstractID": "EMNLP_abs-45",
            "text": [
                {
                    "index": "45-0",
                    "sentence": "Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes.",
                    "sentence_kor": "입법자 선호도는 일반적으로 입법에 대한 점호 투표로부터 추정된 일반적인 이데올로기의 척도로 표현되며, 입법자의 정치적 태도에 중요한 뉘앙스를 가릴 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "45-1",
                    "sentence": "In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting.",
                    "sentence_kor": "본 논문에서 우리는 대체 선호 표현인 트윗을 사용하여 보다 구체적인 입법자 태도를 측정하는 방법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "45-2",
                    "sentence": "Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets.",
                    "sentence_kor": "특히, 우리는 국회의원 트윗의 빈도와 감정을 예측하기 위한 임베딩 기반 모델을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "45-3",
                    "sentence": "To illustrate our method, we model legislators’ attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets.",
                    "sentence_kor": "우리의 방법을 설명하기 위해, 우리는 도널드 트럼프 대통령에 대한 국회의원의 태도를 트럼프가 매일 트윗의 본문에서 신경망을 사용하여 구성한 자신을 위한 임베딩과 상호 작용하는 벡터 임베딩으로 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "45-4",
                    "sentence": "We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018.",
                    "sentence_kor": "2016년 11월부터 2018년 2월까지 미국 상하원 의원들이 작성한 트윗에서 모델의 예측 성과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "45-5",
                    "sentence": "We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.",
                    "sentence_kor": "우리는 입법자 선호도의 전통적인 척도와 비교하여 입법자에 대해 학습된 표현의 질을 추가로 평가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "46",
            "abstractID": "EMNLP_abs-46",
            "text": [
                {
                    "index": "46-0",
                    "sentence": "We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text.",
                    "sentence_kor": "우리는 문자 A에서 문자 B에서 문자 C로 전달되는 정보의 일부를 식별하기 위해 문헌으로 정보 전파를 모델링하는 과제를 제시하며, 텍스트로 활동에 대한 설명만 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "46-1",
                    "sentence": "We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied.",
                    "sentence_kor": "우리는 이 영역의 정보 전파를 측정하기 위한 새로운 파이프라인을 설명하고 스피커 속성을 위한 새로운 데이터 세트를 발표하여 이전에 연구한 것보다 광범위한 문헌 텍스트에서 이 파이프라인의 중요한 구성 요소를 평가할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "46-2",
                    "sentence": "Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.",
                    "sentence_kor": "이 파이프라인을 사용하여, 우리는 5,000개가 넘는 소설 작품에서 정보 전파의 역학을 분석합니다. 다양한 커뮤니티를 연결하는 구조적 구멍을 채우는 캐릭터를 통해 정보가 흐르고, 여성 캐릭터가 남성 캐릭터보다 훨씬 더 자주 이 역할을 채우는 것으로 묘사된다는 것을 알아냈습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "47",
            "abstractID": "EMNLP_abs-47",
            "text": [
                {
                    "index": "47-0",
                    "sentence": "Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives.",
                    "sentence_kor": "사회적 규범, 즉 받아들일 수 있는 사회적 행동에 대한 무언의 상식적인 규칙은 담화에서 사람들의 행동의 근본적인 원인과 의도를 이해하는 데 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "47-1",
                    "sentence": "For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct, such as “It is expected that you report crimes.” We present SOCIAL CHEMISTRY, a new conceptual formalism to study people’s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language.",
                    "sentence_kor": "예를 들어, \"이웃에게 경찰을 부르고 싶다\"와 같은 행동에는 \"당신이 범죄를 신고할 것으로 예상된다\"와 같은 우리의 행동을 알려주는 사회적 규범이 있다. 우리는 자연어로 묘사된 풍부한 범위의 실생활 상황에 대한 사람들의 일상적인 사회 규범과 도덕적 판단을 연구하기 위한 새로운 개념적 형식주의인 사회 화학을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "47-2",
                    "sentence": "We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units.",
                    "sentence_kor": "\"오전 5시에 블렌더를 가동하는 것은 무례한 행동\"과 같은 292k개의 덤블 규칙을 기본 개념 단위로 분류하는 대규모 말뭉치인 SOCHIC-101을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "47-3",
                    "sentence": "Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.",
                    "sentence_kor": "각각의 규칙들은 선악에 대한 사회적 판단, 도덕적 기초, 예상되는 문화적 압력 및 가정된 합법성을 포함한 12가지 차원의 사람들의 판단으로 더욱 세분화된다. 이 판단은 범주형 라벨과 자유 텍스트 설명에 대한 450만 개 이상의 주석에 달한다.",
                    "tag": "3"
                },
                {
                    "index": "47-4",
                    "sentence": "Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction.",
                    "sentence_kor": "최첨단 신경 모델에 기초한 포괄적인 경험적 결과는 사회 규범의 계산 모델링이 유망한 연구 방향임을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "47-5",
                    "sentence": "Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.",
                    "sentence_kor": "우리의 모델 프레임워크인 신경 규범 트랜스포머는 SOCial-CEM-101을 학습하고 일반화하여 이전에 보지 못했던 상황에 대해 성공적으로 추론하여 관련(그리고 잠재적으로 새로운) 속성 인식 사회 규칙을 생성한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "48",
            "abstractID": "EMNLP_abs-48",
            "text": [
                {
                    "index": "48-0",
                    "sentence": "The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments.",
                    "sentence_kor": "이벤트 추출 문제를 해결하려면 이벤트 트리거를 탐지하고 해당 인수를 추출해야 합니다.",
                    "tag": "1"
                },
                {
                    "index": "48-1",
                    "sentence": "Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation.",
                    "sentence_kor": "사건 인수 추출의 기존 작업은 일반적으로 사전 처리/동시 단계로서 개체 인식에 크게 의존하여 잘 알려진 오류 전파 문제를 야기한다.",
                    "tag": "1"
                },
                {
                    "index": "48-2",
                    "sentence": "To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner.",
                    "sentence_kor": "이 문제를 피하기 위해, 우리는 이벤트 인수를 엔드 투 엔드 방식으로 추출하는 질문 응답(QA) 작업으로 공식화하여 이벤트 추출을 위한 새로운 패러다임을 도입한다.",
                    "tag": "1+2"
                },
                {
                    "index": "48-3",
                    "sentence": "Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).",
                    "sentence_kor": "경험적 결과는 우리의 프레임워크가 이전 방법을 상당히 능가한다는 것을 보여준다. 또한, 훈련 시간에 볼 수 없는 역할(즉, 제로샷 학습 환경에서)에 대한 이벤트 인수를 추출할 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "49",
            "abstractID": "EMNLP_abs-49",
            "text": [
                {
                    "index": "49-0",
                    "sentence": "Event schemas can guide our understanding and ability to make predictions with respect to what might happen next.",
                    "sentence_kor": "이벤트 스키마는 다음에 발생할 수 있는 상황에 대한 우리의 이해와 예측 능력을 안내할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "49-1",
                    "sentence": "We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story.",
                    "sentence_kor": "우리는 일관된 스토리에서 중요한 역할을 채우는 실체를 포함하는 여러 경로를 통해 두 가지 이벤트 유형이 연결되는 새로운 이벤트 그래프 스키마를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "49-2",
                    "sentence": "We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas.",
                    "sentence_kor": "그런 다음 이벤트 경로에 대해 훈련된 자동 회귀 언어 모델인 경로 언어 모델을 소개하고 이러한 그래프 스키마를 확률적으로 구성하기 위해 두드러지고 일관된 경로를 선택한다.",
                    "tag": "3"
                },
                {
                    "index": "49-3",
                    "sentence": "We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph.",
                    "sentence_kor": "일관성 있는 이벤트 인스턴스가 스키마 그래프로 적용되는 시기를 확인하여 그래프 스키마 유도의 품질을 평가하기 위해 인스턴스 적용 범위와 인스턴스 일관성이라는 두 가지 평가 지표를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "49-4",
                    "sentence": "Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas.",
                    "sentence_kor": "내재적 평가에 따르면 우리의 접근 방식이 두드러지고 일관성 있는 스키마를 유도하는 데 매우 효과적이다.",
                    "tag": "4"
                },
                {
                    "index": "49-5",
                    "sentence": "Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.",
                    "sentence_kor": "외부 평가를 통해 유도 스키마 저장소가 인스턴스 그래프를 펼치기 위한 추가 전역 기능으로 사용될 때 최첨단 공동 신경 추출 모델에 비해 다운스트림 엔드 투 엔드 정보 추출에 상당한 개선을 제공한다는 것을 알 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "50",
            "abstractID": "EMNLP_abs-50",
            "text": [
                {
                    "index": "50-0",
                    "sentence": "Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.",
                    "sentence_kor": "자연어를 이해하는 것은 여러 사건이 구조적으로 그리고 일시적으로 서로 상호작용하는 방법을 인식하는 것을 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "50-1",
                    "sentence": "In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.",
                    "sentence_kor": "이 과정에서 시간적 순서와 멤버십 관계가 맞물려 다자 이벤트를 구성하는 이벤트 콤플렉스를 유도할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "50-2",
                    "sentence": "Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.",
                    "sentence_kor": "이러한 관계 현상에 대한 공동 레이블링 데이터의 부족과 그것이 표현하는 구조에 대한 제한으로 인해, 우리는 이벤트-이벤트 관계를 모델링하기 위한 공동 제한 학습 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "50-3",
                    "sentence": "Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives.",
                    "sentence_kor": "특히, 프레임워크는 이러한 제약조건을 차별화 가능한 학습 목표로 전환하여 이벤트의 여러 시간적 및 하위 사건 관계 내에서 논리적 제약조건을 시행한다.",
                    "tag": "3"
                },
                {
                    "index": "50-4",
                    "sentence": "We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.",
                    "sentence_kor": "우리는 공동 제약 학습 접근 방식이 공동 레이블링 데이터의 부족을 효과적으로 보완하고, 흔히 사용되지만 비용이 더 많이 드는 전역 추론 프로세스를 대체하면서 시간 관계 추출 및 이벤트 계층 구조 모두에 대한 벤치마크에서 SOTA 방법을 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "50-5",
                    "sentence": "We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.",
                    "sentence_kor": "또한 외부 말뭉치에 이벤트 복합체를 유도하는 접근법의 효과를 보여주는 유망한 사례 연구를 제시한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "51",
            "abstractID": "EMNLP_abs-51",
            "text": [
                {
                    "index": "51-0",
                    "sentence": "Conventional approaches to event detection usually require a fixed set of pre-defined event types.",
                    "sentence_kor": "이벤트 탐지에 대한 기존의 접근 방식에는 일반적으로 미리 정의된 이벤트 유형의 고정 세트가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "51-1",
                    "sentence": "Such a requirement is often challenged in real-world applications, as new events continually occur.",
                    "sentence_kor": "이러한 요구사항은 새로운 이벤트가 지속적으로 발생함에 따라 실제 애플리케이션에서는 문제가 되는 경우가 많습니다.",
                    "tag": "1"
                },
                {
                    "index": "51-2",
                    "sentence": "Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive.",
                    "sentence_kor": "막대한 계산 비용과 스토리지 전환으로 인해 새로운 이벤트가 발생할 때마다 이전 데이터를 모두 저장하고 이전 데이터와 새로운 데이터를 사용하여 모델을 재교육하는 것은 실현 불가능한 일입니다.",
                    "tag": "1"
                },
                {
                    "index": "51-3",
                    "sentence": "We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes.",
                    "sentence_kor": "우리는 증분 이벤트 감지와 같은 도전적인 시나리오를 공식화하는데, 이는 이전 클래스에 대한 성능 저하 없이 새로운 클래스를 점진적으로 학습하는 모델이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "51-4",
                    "sentence": "However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection.",
                    "sentence_kor": "그러나 기존의 증분 학습 방법은 증분 이벤트 감지 작업에서 오래된 클래스와 새로운 클래스 사이의 의미적 모호성과 데이터 불균형 문제를 처리할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "51-5",
                    "sentence": "In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues.",
                    "sentence_kor": "본 논문에서는 위의 문제를 해결하기 위한 지식 통합 네트워크(KCN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "51-6",
                    "sentence": "Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively.",
                    "sentence_kor": "특히, 우리는 각각 의미적 모호성과 등급 불균형의 역효과를 완화하기 위해 프로토타입 강화 회고와 계층적 증류라는 두 가지 구성 요소를 고안한다.",
                    "tag": "3"
                },
                {
                    "index": "51-7",
                    "sentence": "Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.",
                    "sentence_kor": "실험 결과는 ACE 벤치마크와 TAC KBP 벤치마크에서 각각 최신 F1 점수의 19%와 13.4%를 능가하는 제안된 방법의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "52",
            "abstractID": "EMNLP_abs-52",
            "text": [
                {
                    "index": "52-0",
                    "sentence": "Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive.",
                    "sentence_kor": "대부분의 이전 이벤트 추출 연구는 일련의 대상 이벤트 유형 및 해당 이벤트 주석이 제공된다고 가정하며, 이는 매우 비용이 많이 들 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "52-1",
                    "sentence": "In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types.",
                    "sentence_kor": "본 논문에서, 우리는 몇 가지 보이는 유형에 사용할 수 있는 주석을 활용하여 주어진 말뭉치에서 보이지 않는 유형 집합을 자동으로 발견하는 것을 목표로 하는 준감독 이벤트 유형 유도라는 새로운 과제를 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "52-2",
                    "sentence": "We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations.",
                    "sentence_kor": "우리는 보이는 유형과 보이지 않는 유형 각각에 대한 이산 잠재 유형 표현을 자동으로 학습하고 보이는 유형 이벤트 주석을 사용하여 최적화하기 위해 준감독 벡터 양자화 변이 자동 인코더 프레임워크를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "52-3",
                    "sentence": "A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution.",
                    "sentence_kor": "잠재적 유형 분포에 따라 언급된 각 이벤트의 재구성을 강제하기 위해 변형 자동 인코더가 추가로 도입된다.",
                    "tag": "3"
                },
                {
                    "index": "52-4",
                    "sentence": "Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.",
                    "sentence_kor": "실험에 따르면 우리의 접근 방식은 감독되는 이벤트 감지에서 최첨단 성능을 달성할 수 있을 뿐만 아니라 고품질의 새로운 이벤트 유형을 발견할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "53",
            "abstractID": "EMNLP_abs-53",
            "text": [
                {
                    "index": "53-0",
                    "sentence": "Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation.",
                    "sentence_kor": "일련의 텍스트 생성 작업에 대한 생성 사전 교육 언어 모델의 성공에도 불구하고, 생성 중에 기본 상식 지식에 대한 추론이 필요한 경우에는 여전히 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "53-1",
                    "sentence": "Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph.",
                    "sentence_kor": "상식적인 지식을 생성 사전 훈련된 언어 모델에 통합하는 기존 접근법은 지식 그래프 내의 풍부한 연결을 무시한 채 개별 지식 3배에 대한 사후 훈련을 통해 관계적 지식을 전달하기만 한다.",
                    "tag": "1"
                },
                {
                    "index": "53-2",
                    "sentence": "We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation.",
                    "sentence_kor": "우리는 지식 그래프의 구조적 및 의미적 정보를 모두 활용하는 것이 상식 인식 텍스트 생성을 촉진한다고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "53-3",
                    "sentence": "In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph.",
                    "sentence_kor": "본 논문에서, 우리는 외부 상식 지식 그래프에서 추출한 다중 관계 경로에 동적 다중 홉 추론을 사용하여 사전 훈련된 모델을 가능하게 하는 다중 홉 추론 흐름을 이용한 생성을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "53-4",
                    "sentence": "We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge.",
                    "sentence_kor": "우리는 우리의 모델이 상식적인 지식을 뛰어넘는 추론이 필요한 세 가지 텍스트 생성 작업에서 기존 기준을 능가한다는 것을 경험적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "53-5",
                    "sentence": "We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.",
                    "sentence_kor": "우리는 또한 생성에 근거를 제공하는 모델에 의해 추론된 추론 경로를 가진 동적 다중 홉 추론 모듈의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "54",
            "abstractID": "EMNLP_abs-54",
            "text": [
                {
                    "index": "54-0",
                    "sentence": "Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs.",
                    "sentence_kor": "현대의 NLP는 스타일 전송 작업을 의미론을 크게 변경하지 않고 주어진 문장의 스타일을 수정하는 것으로 정의한다. 이는 스타일 전송 시스템의 출력이 입력의 의역이어야 함을 의미한다.",
                    "tag": "1"
                },
                {
                    "index": "54-1",
                    "sentence": "However, many existing systems purportedly designed for style transfer inherently warp the input’s meaning through attribute transfer, which changes semantic properties such as sentiment.",
                    "sentence_kor": "그러나, 알려진 바에 따르면 스타일 전송을 위해 설계된 많은 기존 시스템은 속성 전송을 통해 입력의 의미를 본질적으로 왜곡하며, 이는 감정과 같은 의미적 특성을 변화시킨다.",
                    "tag": "1"
                },
                {
                    "index": "54-2",
                    "sentence": "In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data.",
                    "sentence_kor": "본 논문에서 우리는 비지도 스타일 전송을 패러프레이즈 생성 문제로 재구성하고, 자동으로 생성된 패러프레이즈 데이터에서 사전 훈련된 언어 모델을 미세 조정하는 간단한 방법론을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "54-3",
                    "sentence": "Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations.",
                    "sentence_kor": "간단함에도 불구하고, 우리의 방법은 인간과 자동 평가 모두에서 최첨단 스타일 전송 시스템을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "54-4",
                    "sentence": "We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants.",
                    "sentence_kor": "또한 23가지 스타일의 전송 논문을 조사하여 기존 자동 메트릭스를 쉽게 게이밍할 수 있음을 발견하고 고정 변형을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "54-5",
                    "sentence": "Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.",
                    "sentence_kor": "마지막으로, 15M 문장의 대규모 데이터 세트를 11개의 다양한 스타일로 수집하여 보다 실제적인 스타일 전송 설정으로 피벗한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "55",
            "abstractID": "EMNLP_abs-55",
            "text": [
                {
                    "index": "55-0",
                    "sentence": "Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation.",
                    "sentence_kor": "법원의 견해 생성은 판결 예측 결과의 해석성을 향상시키고 자동으로 법률 문서를 생성하는 것을 목표로 하는, 법률 AI에 있어 참신하지만 필수적인 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "55-1",
                    "sentence": "While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes.",
                    "sentence_kor": "이전 텍스트 대 텍스트 자연어 생성(NLG) 접근방식을 사용하여 이 문제를 해결할 수 있지만 데이터 생성 메커니즘의 교란 편향을 무시하면 모델 성능을 제한할 수 있으며 편향은 학습 결과를 오염시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "55-2",
                    "sentence": "In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders.",
                    "sentence_kor": "본 논문에서 우리는 주의 인코더와 한 쌍의 혁신적인 반효과 디코더로 구성된 새로운 주의 및 반효과 기반 자연어 생성(AC-NLG) 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "55-3",
                    "sentence": "The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized.",
                    "sentence_kor": "주의 인코더는 원고의 주장과 사실 설명을 입력으로 활용하여 청구 관련 정보를 실제로 강조할 수 있는 주장 인식 인코더를 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "55-4",
                    "sentence": "The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court’s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model.",
                    "sentence_kor": "반사실적 디코더는 데이터의 교란 편향을 제거하고 시너지론적 판단 예측 모델과 통합하여 판단 차별적 법원의 견해(지지적 견해와 비지지적 견해 모두)를 생성하기 위해 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "55-5",
                    "sentence": "Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.",
                    "sentence_kor": "종합적인 실험은 정량적 및 정성적 평가 지표 모두에서 우리 방법의 효과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "56",
            "abstractID": "EMNLP_abs-56",
            "text": [
                {
                    "index": "56-0",
                    "sentence": "Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content.",
                    "sentence_kor": "사전 교육을 받은 트랜스포머는 길고 유창한 텍스트를 생성하는 데 있어 인상적인 혁신을 가능케 했지만, 그들의 출력물은 일관되게 배열된 콘텐츠 없이 종종 \"혼란\"된다.",
                    "tag": "1"
                },
                {
                    "index": "56-1",
                    "sentence": "In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART.",
                    "sentence_kor": "이 작업에서는 대규모 모델인 BART를 기반으로 하는 계획 및 반복 개선 기능을 갖춘 새로운 콘텐츠 제어 텍스트 생성 프레임워크 PAURE를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "56-2",
                    "sentence": "We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions.",
                    "sentence_kor": "먼저 BERT 모델을 적용하여 핵심 문구 할당과 해당 문장 수준 위치로 구성된 콘텐츠 계획을 자동으로 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "56-3",
                    "sentence": "The BART model is employed for generation without modifying its structure.",
                    "sentence_kor": "BART 모델은 구조를 수정하지 않고 생성에 사용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "56-4",
                    "sentence": "We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework.",
                    "sentence_kor": "그런 다음 시퀀스 투 시퀀스 프레임워크 내에서 생성 품질을 점진적으로 향상시키는 개선 알고리즘을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "56-5",
                    "sentence": "Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements.",
                    "sentence_kor": "자동 측정 기준을 사용한 평가에 따르면 계획을 추가하면 평균 20 BLEU 포인트와 12 METEO 포인트 개선으로 3개의 서로 다른 도메인에서 발전 품질이 일관되게 개선된다.",
                    "tag": "3"
                },
                {
                    "index": "56-6",
                    "sentence": "In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.",
                    "sentence_kor": "또한, 인간 판사는 계획 없는 비교보다 시스템 출력이 더 적절하고 일관성이 있다고 평가한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "57",
            "abstractID": "EMNLP_abs-57",
            "text": [
                {
                    "index": "57-0",
                    "sentence": "Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future.",
                    "sentence_kor": "일상적 인간 인식의 핵심 능력인 귀납적 및 반실제적 추론은 상대적인 과거와 미래의 여러 맥락에 따라 조건화하면서 시간 t에 일어났을 수 있는 것에 대한 추론을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "57-1",
                    "sentence": "However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling.",
                    "sentence_kor": "그러나 생성 언어 모델(LM)을 사용하여 과거와 미래의 컨텍스트를 동시에 통합하는 것은 어려울 수 있다. 이는 컨텍스트를 과거 컨텍스트를 통해서만 조건화하거나 좁은 범위의 텍스트 주입을 수행하도록 훈련되기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "57-2",
                    "sentence": "In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision.",
                    "sentence_kor": "본 논문에서, 우리는 기성품 왼쪽에서 오른쪽 언어 모델만 사용하고 감독 없이 과거와 미래의 컨텍스트를 유연하게 통합할 수 있는 새로운 비지도 디코딩 알고리즘인 DeLorean을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "57-3",
                    "sentence": "The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters.",
                    "sentence_kor": "알고리즘의 핵심 직관은 역전파를 통해 미래를 통합하는 것이며, 그 동안 모델 매개 변수를 수정하는 동안 출력의 내부 표현만 업데이트한다.",
                    "tag": "3"
                },
                {
                    "index": "57-4",
                    "sentence": "By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts.",
                    "sentence_kor": "전진 및 후진 전파를 번갈아 사용함으로써 DeLorean은 왼쪽 및 오른쪽 컨텍스트를 모두 반영하는 출력 표현을 디코딩할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "57-5",
                    "sentence": "We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.",
                    "sentence_kor": "우리는 우리의 접근 방식이 일반적이며 두 가지 비 단조적 추론 과제, 즉 DeLorean이 자동 및 인간 평가를 기반으로 감독되지 않은 다양한 방법 및 감독된 방법을 능가하는 유도 텍스트 생성과 반사실적 스토리 개정에 적용 가능하다는 것을 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "58",
            "abstractID": "EMNLP_abs-58",
            "text": [
                {
                    "index": "58-0",
                    "sentence": "Learning to fuse vision and language information and representing them is an important research problem with many applications.",
                    "sentence_kor": "시각과 언어 정보를 융합하고 표현하는 것을 배우는 것은 많은 응용 분야에서 중요한 연구 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "58-1",
                    "sentence": "Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images.",
                    "sentence_kor": "최근 진행은 트랜스포머의 사전 교육(언어 모델링) 및 주의 계층 아이디어를 활용하여 이미지를 설명하는 언어 표현식에 정렬된 이미지를 포함하는 데이터 세트에서 표현을 학습했다.",
                    "tag": "1"
                },
                {
                    "index": "58-2",
                    "sentence": "In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets.",
                    "sentence_kor": "본 논문에서, 우리는 이러한 데이터 세트에서 자동으로 마이닝되는 이미지와 텍스트 사이의 일련의 시각적 기반 표현으로부터 표현을 학습할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "58-3",
                    "sentence": "In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded.",
                    "sentence_kor": "특히, 표기 그래프를 사용하여 특정 개념(예: 이미지를 설명하는 문장)이 시각적으로도 기초가 되는 추상적이고 일반적인 개념(예: 짧은 문구)과 어떻게 연결될 수 있는지를 나타낸다.",
                    "tag": "3"
                },
                {
                    "index": "58-4",
                    "sentence": "This type of generic-to-specific relations can be discovered using linguistic analysis tools.",
                    "sentence_kor": "이러한 유형의 일반-특정 관계는 언어 분석 도구를 사용하여 발견할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "58-5",
                    "sentence": "We propose methods to incorporate such relations into learning representation.",
                    "sentence_kor": "우리는 그러한 관계를 학습 표현에 통합하는 방법을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "58-6",
                    "sentence": "We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations.",
                    "sentence_kor": "우리는 자동으로 수집된 구조적 관계를 활용하여 최첨단 멀티모달 학습 모델을 더욱 개선할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "58-7",
                    "sentence": "The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition.",
                    "sentence_kor": "표현은 교차 모달 이미지 검색, 참조 표현 및 구성 속성-객체 인식의 다운스트림 작업에 대한 더 강력한 경험적 결과로 이어진다.",
                    "tag": "4"
                },
                {
                    "index": "58-8",
                    "sentence": "Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.",
                    "sentence_kor": "Flickr30K 및 COCO 데이터 세트에서 코드와 추출된 표시 그래프 모두 https://sha-lab.github.io/DG에서 공개적으로 사용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "59",
            "abstractID": "EMNLP_abs-59",
            "text": [
                {
                    "index": "59-0",
                    "sentence": "Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering.",
                    "sentence_kor": "표현적 교차 모달 상호 작용을 모델링하는 것은 시각적 질문 답변과 같은 다중 모드 작업에서 매우 중요한 것으로 보인다.",
                    "tag": "1"
                },
                {
                    "index": "59-1",
                    "sentence": "However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data.",
                    "sentence_kor": "그러나 때로는 고성능 블랙박스 알고리즘이 대부분 데이터의 단일 신호를 이용하는 것으로 밝혀지기도 한다.",
                    "tag": "1"
                },
                {
                    "index": "59-2",
                    "sentence": "We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task.",
                    "sentence_kor": "우리는 교차 모달 상호 작용이 주어진 작업에서 주어진 모델의 성능을 향상시키는지 여부를 분리하기 위한 새로운 진단 도구인 경험적 다중 부가 함수 투영(EMAP)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "59-3",
                    "sentence": "This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure.",
                    "sentence_kor": "이 함수 투영은 모델 예측을 수정하여 교차 모달 상호 작용이 제거되도록 하여 추가 단일 모달 구조를 격리합니다.",
                    "tag": "3"
                },
                {
                    "index": "59-4",
                    "sentence": "For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation.",
                    "sentence_kor": "7개의 이미지+텍스트 분류 작업(각각 새로운 최첨단 벤치마크를 설정함)의 경우, 많은 경우 교차 모달 상호 작용을 제거하면 성능 저하가 거의 또는 전혀 발생하지 않는다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "59-5",
                    "sentence": "Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions.",
                    "sentence_kor": "놀랍게도 이는 상호작용을 고려할 수 있는 능력이 있는 표현형 모델이 표현형 모델이 덜 표현형 모델을 능가하는 경우에도 유지된다. 따라서 성능 향상은 종종 교차 모달 특징 상호작용의 고려에 기인한다고 볼 수 없다.",
                    "tag": "4"
                },
                {
                    "index": "59-6",
                    "sentence": "We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.",
                    "sentence_kor": "따라서 멀티모달 머신러닝 연구자는 단일모달 기준선뿐만 아니라 가장 성능이 좋은 모델의 EMAP의 성능도 보고할 것을 권고한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "60",
            "abstractID": "EMNLP_abs-60",
            "text": [
                {
                    "index": "60-0",
                    "sentence": "While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d.",
                    "sentence_kor": "시각적 질문 답변 리더보드에서 진전이 이루어졌지만, 모델은 종종 i.i.d에 따른 데이터 세트의 잘못된 상관 관계와 선행 사례를 이용한다.",
                    "tag": "1"
                },
                {
                    "index": "60-1",
                    "sentence": "setting.",
                    "sentence_kor": "세팅의",
                    "tag": "1"
                },
                {
                    "index": "60-2",
                    "sentence": "As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization.",
                    "sentence_kor": "이와 같이 유통 외 시험 표본에 대한 평가가 일반화의 대리인으로 부상했다.",
                    "tag": "1"
                },
                {
                    "index": "60-3",
                    "sentence": "In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge.",
                    "sentence_kor": "본 논문에서 우리는 VQA-CP 과제와 같은 OOD 일반화를 개선하기 위해 모델을 지각적으로 유사하지만 의미론적으로 구별되는 입력 돌연변이에 노출시키는 훈련 패러다임인 MUAT를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "60-4",
                    "sentence": "Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer).",
                    "sentence_kor": "이 패러다임에 따라 모델은 입력(질문-이미지 쌍)의 의미적 변화가 출력(답변)에 미치는 영향을 이해하기 위해 일관성이 제한된 훈련 목표를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "60-5",
                    "sentence": "Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions.",
                    "sentence_kor": "VQA-CP의 기존 방법과 달리, MUTERT는 훈련 및 테스트 답변 분포의 특성에 대한 지식에 의존하지 않는다.",
                    "tag": "4"
                },
                {
                    "index": "60-6",
                    "sentence": "MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement.",
                    "sentence_kor": "MUTERT는 10.57% 향상된 VQA-CP에 대한 새로운 최첨단 정확도를 확립한다.",
                    "tag": "4"
                },
                {
                    "index": "60-7",
                    "sentence": "Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.",
                    "sentence_kor": "우리의 연구는 문제의 OOD 일반화를 위한 의미 입력 돌연변이의 사용을 위한 길을 열어준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "61",
            "abstractID": "EMNLP_abs-61",
            "text": [
                {
                    "index": "61-0",
                    "sentence": "Dialogue systems play an increasingly important role in various aspects of our daily life.",
                    "sentence_kor": "대화 시스템은 일상 생활의 다양한 측면에서 점점 더 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "61-1",
                    "sentence": "It is evident from recent research that dialogue systems trained on human conversation data are biased.",
                    "sentence_kor": "인간 대화 데이터에 대해 훈련된 대화 시스템이 편향되어 있다는 것은 최근의 연구를 통해 명백하다.",
                    "tag": "2"
                },
                {
                    "index": "61-2",
                    "sentence": "In particular, they can produce responses that reflect people’s gender prejudice.",
                    "sentence_kor": "특히, 그들은 사람들의 성별 편견을 반영하는 반응을 만들어 낼 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "61-3",
                    "sentence": "Many debiasing methods have been developed for various NLP tasks, such as word embedding.",
                    "sentence_kor": "단어 임베딩과 같은 다양한 NLP 작업을 위해 많은 디바이징 방법이 개발되었다.",
                    "tag": "3"
                },
                {
                    "index": "61-4",
                    "sentence": "However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders.",
                    "sentence_kor": "그러나 대화 모델은 성별에 따라 유사한 반응을 일으키도록 강제할 가능성이 높기 때문에 대화 시스템에 직접 적용할 수 없다.",
                    "tag": "4"
                },
                {
                    "index": "61-5",
                    "sentence": "This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models.",
                    "sentence_kor": "이는 생성된 응답의 다양성을 크게 저하시키고 대화 모델의 성능을 크게 해친다.",
                    "tag": "4"
                },
                {
                    "index": "61-6",
                    "sentence": "In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance.",
                    "sentence_kor": "본 논문에서, 우리는 대화 모델을 성과는 유지하면서 성 편견으로부터 자유롭게 훈련시키기 위해 새로운 적대적 학습 프레임워크인 Debiased-Chat을 제안한다.",
                    "tag": "5"
                },
                {
                    "index": "61-7",
                    "sentence": "Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.",
                    "sentence_kor": "두 개의 실제 대화 데이터 세트에 대한 광범위한 실험은 우리의 프레임워크가 응답 품질을 유지하면서 대화 모델에서 성별 편향을 크게 감소시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "62",
            "abstractID": "EMNLP_abs-62",
            "text": [
                {
                    "index": "62-0",
                    "sentence": "We explore the task of improving persona consistency of dialogue agents.",
                    "sentence_kor": "우리는 대화 에이전트의 성격 일관성을 개선하는 과제를 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "62-1",
                    "sentence": "Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency.",
                    "sentence_kor": "일관성을 다루는 최근 모델은 일관성을 유지하기 위해 추가 자연어 추론(NLI) 레이블을 사용하여 훈련하거나 훈련된 추가 모듈을 생성 에이전트에 부착하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "62-2",
                    "sentence": "However, such additional labels and training can be demanding.",
                    "sentence_kor": "그러나 그러한 추가 라벨과 교육은 까다로울 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "62-3",
                    "sentence": "Also, we find even the best-performing persona-based agents are insensitive to contradictory words.",
                    "sentence_kor": "또한, 우리는 가장 잘 하는 페르소나 기반 에이전트들조차 모순된 말들에 둔감하다는 것을 발견한다.",
                    "tag": "2"
                },
                {
                    "index": "62-4",
                    "sentence": "Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener.",
                    "sentence_kor": "사회적 인식과 실용주의에 영감을 받아, 우리는 기존의 대화 대리인들에게 상상의 청취자를 통해 공공의 자의식을 즉시 부여한다.",
                    "tag": "3"
                },
                {
                    "index": "62-5",
                    "sentence": "Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction.",
                    "sentence_kor": "Rational Speech Acts 프레임워크(Frank and Goodman, 2012)에 기초한 우리의 접근법은 대화 대리인들이 모순을 말하는 것을 자제하도록 강제할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "62-6",
                    "sentence": "We further extend the framework by learning the distractor selection, which has been usually done manually or randomly.",
                    "sentence_kor": "우리는 보통 수동으로 또는 무작위로 수행된 신연자 선택을 학습하여 프레임워크를 확장한다.",
                    "tag": "3"
                },
                {
                    "index": "62-7",
                    "sentence": "Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models.",
                    "sentence_kor": "Dialogue NLI(Welleck et al., 2019) 및 PersonaChat(Zhang et al., 2018) 데이터 세트에 대한 결과는 우리의 접근 방식이 모순을 줄이고 기존 대화 모델의 일관성을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "62-8",
                    "sentence": "Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.",
                    "sentence_kor": "또한 대화에서 개인 이상의 상황 일관성을 개선하기 위해 일반화할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "63",
            "abstractID": "EMNLP_abs-63",
            "text": [
                {
                    "index": "63-0",
                    "sentence": "The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice.",
                    "sentence_kor": "일반 텍스트와 작업 지향 대화 사이의 언어 패턴의 근본적인 차이는 기존의 사전 훈련된 언어 모델을 실무에서 덜 유용하게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "63-1",
                    "sentence": "In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling.",
                    "sentence_kor": "본 연구에서는 언어 모델링을 위해 인간-인간 및 다중 턴 작업 지향 대화 데이터 세트 9개를 통합한다.",
                    "tag": "2"
                },
                {
                    "index": "63-2",
                    "sentence": "To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling.",
                    "sentence_kor": "사전 교육 중 대화 동작을 더 잘 모델링하기 위해 사용자와 시스템 토큰을 마스킹 언어 모델링에 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "63-3",
                    "sentence": "We propose a contrastive objective function to simulate the response selection task.",
                    "sentence_kor": "우리는 대응 선택 작업을 시뮬레이션하기 위한 대조적인 목표 함수를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "63-4",
                    "sentence": "Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection.",
                    "sentence_kor": "사전 훈련된 작업 지향 대화 BERT(TOD-BERT)는 의도 인식, 대화 상태 추적, 대화 행동 예측 및 대응 선택을 포함한 네 가지 다운스트림 작업 지향 대화 애플리케이션에서 BERT와 같은 강력한 기준을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "63-5",
                    "sentence": "We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.",
                    "sentence_kor": "또한 TOD-BERT는 작업 중심의 대화를 위해 데이터 부족 문제를 완화할 수 있는 더 강력한 퓨샷 기능을 가지고 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "64",
            "abstractID": "EMNLP_abs-64",
            "text": [
                {
                    "index": "64-0",
                    "sentence": "Large-scale dialogue datasets have recently become available for training neural dialogue agents.",
                    "sentence_kor": "신경 대화제 훈련을 위해 대규모 대화 데이터 세트를 최근 사용할 수 있게 되었다.",
                    "tag": "1"
                },
                {
                    "index": "64-1",
                    "sentence": "However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs.",
                    "sentence_kor": "그러나 이러한 데이터 세트에는 허용할 수 없는 발화 쌍이 포함된 것으로 보고되었다.",
                    "tag": "1"
                },
                {
                    "index": "64-2",
                    "sentence": "In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness.",
                    "sentence_kor": "본 논문에서 우리는 발화 쌍의 연결성과 관련성 측면에서 품질을 평가하는 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "64-3",
                    "sentence": "The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities.",
                    "sentence_kor": "제안된 채점 방법은 대화 및 언어학 연구 커뮤니티에서 널리 공유되는 연구 결과를 기반으로 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "64-4",
                    "sentence": "We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality.",
                    "sentence_kor": "우리는 그것이 대화 질에 대한 인간의 판단과 비교적 좋은 상관관계를 가지고 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "64-5",
                    "sentence": "Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality.",
                    "sentence_kor": "또한 이 방법은 대규모 잡음이 많은 대화 말뭉치에서 잠재적으로 허용되지 않는 발화 쌍을 필터링하여 품질을 보장하는 데 적용된다.",
                    "tag": "4"
                },
                {
                    "index": "64-6",
                    "sentence": "We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.",
                    "sentence_kor": "제안된 방법으로 필터링된 훈련 데이터가 응답 생성 시 신경 대화제의 품질을 향상시킨다는 것을 실험적으로 확인한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "65",
            "abstractID": "EMNLP_abs-65",
            "text": [
                {
                    "index": "65-0",
                    "sentence": "Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model.",
                    "sentence_kor": "접촉 현상이 표준 트리 모델의 적용을 방해하기 때문에 방언의 진화를 분석하는 것은 여전히 어려운 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "65-1",
                    "sentence": "Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations.",
                    "sentence_kor": "이 문제에 대한 이전의 통계적 접근법은 혼합 분석에 의존하며, 여기서 각 방언은 잠재 조상 집단의 혼합으로 보인다.",
                    "tag": "1"
                },
                {
                    "index": "65-2",
                    "sentence": "However, such ancestral populations are hardly interpretable in the context of the tree model.",
                    "sentence_kor": "그러나 이러한 조상 모집단은 트리 모델의 맥락에서 거의 해석할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "65-3",
                    "sentence": "In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions.",
                    "sentence_kor": "본 논문에서 우리는 잠재적 요인을 지리적 분포로 나타내는 확률론적 생성 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "65-4",
                    "sentence": "We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions.",
                    "sentence_kor": "우리는 제안된 모델이 트리 모델과 더 높은 친화도를 갖는다고 주장한다. 왜냐하면 나무는 지리적 분포의 집합으로 대체적으로 표현될 수 있기 때문이다.",
                    "tag": "5"
                },
                {
                    "index": "65-5",
                    "sentence": "Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.",
                    "sentence_kor": "합성 데이터와 실제 데이터를 사용한 실험에 따르면 제안된 방법이 양적 및 질적으로 혼합 모델보다 우수하다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "66",
            "abstractID": "EMNLP_abs-66",
            "text": [
                {
                    "index": "66-0",
                    "sentence": "Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories.",
                    "sentence_kor": "워드 클래스 유연성은 단일 단어 형태가 서로 다른 문법 범주에 걸쳐 사용되는 현상을 말한다.",
                    "tag": "1"
                },
                {
                    "index": "66-1",
                    "sentence": "Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties.",
                    "sentence_kor": "언어 유형학에 대한 광범위한 연구는 언어 간 단어 클래스의 유연성을 특징짓기 위해 노력해왔지만, 이 현상을 정확하고 규모에 맞게 수량화하는 것은 어려움으로 가득 차 있다.",
                    "tag": "1"
                },
                {
                    "index": "66-2",
                    "sentence": "We propose a principled methodology to explore regularity in word class flexibility.",
                    "sentence_kor": "우리는 단어 클래스 유연성의 규칙성을 탐구하기 위한 원칙적인 방법론을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "66-3",
                    "sentence": "Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages.",
                    "sentence_kor": "우리의 방법은 단어 클래스 간의 의미 이동(예: 명사 대 동사 대 명사)을 정량화하기 위한 상황별 단어 임베딩의 최근 작업을 기반으로 하며, 이 방법을 37개 언어에 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "66-4",
                    "sentence": "We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages.",
                    "sentence_kor": "우리는 상황별 임베딩이 영어 단어 내의 클래스 변화에 대한 인간의 판단을 포착할 뿐만 아니라 언어 간 클래스 유연성에서 공유되는 경향을 발견한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "66-5",
                    "sentence": "Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process.",
                    "sentence_kor": "특히, 우리는 단어 클래스 유연성이 방향적 프로세스라는 견해를 뒷받침하는 유연한 레마를 그것의 지배적인 단어 클래스에 사용할 때 더 큰 의미적 변화를 발견한다.",
                    "tag": "3"
                },
                {
                    "index": "66-6",
                    "sentence": "Our work highlights the utility of deep contextualized models in linguistic typology.",
                    "sentence_kor": "우리의 연구는 언어 유형학에서 심층 상황별 모델의 유용성을 강조한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "67",
            "abstractID": "EMNLP_abs-67",
            "text": [
                {
                    "index": "67-0",
                    "sentence": "Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming.",
                    "sentence_kor": "딥 인코더는 신경 기계 변환(NMT) 시스템을 개선하는 데 효과적인 것으로 입증되었지만, 극도로 딥 인코더를 훈련하는 데는 시간이 많이 걸린다.",
                    "tag": "1"
                },
                {
                    "index": "67-1",
                    "sentence": "Moreover, why deep models help NMT is an open question.",
                    "sentence_kor": "게다가, 왜 심층적인 모델이 NMT를 돕는지는 미지수이다.",
                    "tag": "1"
                },
                {
                    "index": "67-2",
                    "sentence": "In this paper, we investigate the behavior of a well-tuned deep Transformer system.",
                    "sentence_kor": "본 논문에서는 잘 조정된 딥 트랜스포머 시스템의 동작을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "67-3",
                    "sentence": "We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly.",
                    "sentence_kor": "우리는 층 쌓기가 NMT 모델의 표현 능력을 향상시키는 데 도움이 되며 인접한 층도 이와 유사한 성능을 보인다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "67-4",
                    "sentence": "This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models.",
                    "sentence_kor": "이는 얕은 모델을 쌓아서 심층 모델을 학습하는 얕은~깊은 훈련 방법을 개발하도록 영감을 준다.",
                    "tag": "3"
                },
                {
                    "index": "67-5",
                    "sentence": "In this way, we successfully train a Transformer system with a 54-layer encoder.",
                    "sentence_kor": "이러한 방식으로 54 레이어 인코더를 사용하여 트랜스포머 시스템을 성공적으로 교육합니다.",
                    "tag": "3"
                },
                {
                    "index": "67-6",
                    "sentence": "Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks.",
                    "sentence_kor": "WMT'16 영어-독일어 및 WMT'14 영어-프랑스어 번역 작업에 대한 실험 결과는 처음부터 교육하는 것보다 1:4 빠르며 두 작업에서 30:33과 43:29의 BLEU 점수를 획득한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "67-7",
                    "sentence": "The code is publicly available at https://github.com/libeineu/SDT-Training.",
                    "sentence_kor": "이 코드는 https://github.com/libeineu/SDT-Training에서 공개적으로 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "68",
            "abstractID": "EMNLP_abs-68",
            "text": [
                {
                    "index": "68-0",
                    "sentence": "We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space.",
                    "sentence_kor": "우리는 순전히 연속 공간에서 번역을 반복적으로 조정하는 비 자기 회귀 기계 번역에 대한 효율적인 추론 절차를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "68-1",
                    "sentence": "Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead.",
                    "sentence_kor": "기계 변환을 위한 연속 잠재 변수 모델(Shu 등, 2020)이 주어지면, 우리는 대신 잠재 변수를 사용하여 목표 문장의 한계 로그 확률의 기울기를 근사하도록 추론 네트워크를 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "68-2",
                    "sentence": "This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability.",
                    "sentence_kor": "이를 통해 추론 시간에 한계 확률을 대략 최대화하는 목표 문장을 찾기 위해 구배 기반 최적화를 사용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "68-3",
                    "sentence": "As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space.",
                    "sentence_kor": "각 개선 단계는 낮은 차원의 잠재 공간(실험에서 8을 사용함)에서만 계산을 포함하므로 토큰 공간에서 종종 세분화하는 기존 비 자기 회귀 추론 절차에 의해 발생하는 계산 오버헤드를 피한다.",
                    "tag": "4"
                },
                {
                    "index": "68-4",
                    "sentence": "We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables.",
                    "sentence_kor": "우리는 우리의 접근방식을 이산 변수와 연속 변수로 구성된 하이브리드 공간에서 최적화하는 최근 제안된 전자파 유사 추론 절차(Shu 등, 2020)와 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "68-5",
                    "sentence": "We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps.",
                    "sentence_kor": "우리는 WMT'14 En→De, WMT'16 Ro→En 및 IWSLT'16 De→En에 대한 접근 방식을 평가하고, (1) 계산적으로 효율적이라는 것과 (2) 각 개선 단계가 두 배 빠르고, 따라서 한계 확률과 BLEU가 더 높다는 두 가지 장점을 관찰한다.",
                    "tag": "3"
                },
                {
                    "index": "68-6",
                    "sentence": "On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).",
                    "sentence_kor": "예를 들어 WMT'14 En→De에서 우리의 접근 방식은 변환 품질(0.9 BLEU)의 최소 저하로 자기 회귀 모델보다 6.2배 더 빠르게 디코딩할 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "69",
            "abstractID": "EMNLP_abs-69",
            "text": [
                {
                    "index": "69-0",
                    "sentence": "With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better.",
                    "sentence_kor": "컴퓨팅 파워의 성장에 따라 신경 기계 변환(NMT) 모델도 그에 따라 성장하고 개선된다.",
                    "tag": "1"
                },
                {
                    "index": "69-1",
                    "sentence": "However, they also become harder to deploy on edge devices due to memory constraints.",
                    "sentence_kor": "그러나 메모리 제약으로 인해 에지 장치에도 배포하기가 어려워집니다.",
                    "tag": "1"
                },
                {
                    "index": "69-2",
                    "sentence": "To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S).",
                    "sentence_kor": "이 문제에 대처하기 위해, 일반 관행은 크고 정확하게 훈련된 교사 네트워크(T)에서 콤팩트 학생 네트워크(S)로 지식을 증류하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "69-3",
                    "sentence": "Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative.",
                    "sentence_kor": "대부분의 경우 지식 증류(KD)가 유용하지만, 우리의 연구는 기존 KD 기법이 심층 NMT 엔진에 충분히 적합하지 않을 수 있다는 것을 보여주므로 우리는 새로운 대안을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "69-4",
                    "sentence": "In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese→English, Turkish→English, and English→German directions.",
                    "sentence_kor": "우리 모델에는 T와 S 예측과 일치하는 것 외에도 T에서 S로 레이어 레벨 감독을 주입하는 조합 메커니즘이 있다. 본 문서에서는 리소스 부족 설정을 대상으로 포르투갈어→영어, 터키어→영어 및 영어→독일어 길찾기에 대한 번역 엔진을 평가합니다.",
                    "tag": "3"
                },
                {
                    "index": "69-5",
                    "sentence": "Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.",
                    "sentence_kor": "이 기술을 사용하여 교육을 받은 학생은 매개 변수가 50% 더 적으며 여전히 12계층 교사의 매개 변수와 유사한 결과를 제공할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "70",
            "abstractID": "EMNLP_abs-70",
            "text": [
                {
                    "index": "70-0",
                    "sentence": "While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area.",
                    "sentence_kor": "단일 언어 데이터는 이중 언어 신경 기계 번역(NMT)을 개선하는 데 유용한 것으로 나타났지만, 다국어 NMT(Multi-language NMT) 시스템의 단일 언어 데이터를 효과적이고 효율적으로 활용하는 것은 덜 탐구된 영역이다.",
                    "tag": "1"
                },
                {
                    "index": "70-1",
                    "sentence": "In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data.",
                    "sentence_kor": "본 연구에서, 우리는 단일 언어 데이터에 대한 변환 작업과 단일 언어 데이터에 대한 두 가지 노이즈 제거 작업을 함께 모델을 훈련시키는 다중 작업 학습(MTL) 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "70-2",
                    "sentence": "We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets.",
                    "sentence_kor": "우리는 WMT 데이터 세트의 10개 언어 쌍을 사용하여 MNMT 시스템에 대한 광범위한 경험적 연구를 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "70-3",
                    "sentence": "We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models.",
                    "sentence_kor": "제안된 접근 방식이 높은 자원 언어와 낮은 자원 언어 모두에 대한 번역 품질을 효과적으로 개선하여 개별 이중 언어 모델보다 훨씬 더 나은 결과를 얻을 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "70-4",
                    "sentence": "We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data.",
                    "sentence_kor": "우리는 또한 한입 교육 데이터가 없는 언어 쌍에 대한 제로샷 설정에서 제안된 접근 방식의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "70-5",
                    "sentence": "Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.",
                    "sentence_kor": "또한 NMT 및 언어 간 전송 학습 NLU 과제 모두에 대한 사전 교육 접근법에 대한 MTL의 효과를 보여준다. 제안된 접근법은 단일 과제에 대해 훈련된 대규모 모델을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "71",
            "abstractID": "EMNLP_abs-71",
            "text": [
                {
                    "index": "71-0",
                    "sentence": "There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT).",
                    "sentence_kor": "주파수가 다른 토큰이 나타나 신경기계번역(NMT)에서 토큰에 대한 학습 난이도가 다르기 때문에 자연어에는 토큰 불균형 현상이 존재한다.",
                    "tag": "1"
                },
                {
                    "index": "71-1",
                    "sentence": "The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution.",
                    "sentence_kor": "바닐라 NMT 모델은 일반적으로 주파수가 다른 대상 토큰에 대해 사소한 등가중치 목표를 채택하고 황금 토큰 분포에 비해 더 많은 고주파 토큰과 적은 저주파 토큰을 생성하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "71-2",
                    "sentence": "However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.",
                    "sentence_kor": "그러나 저주파 토큰은 일단 무시되면 번역 품질에 영향을 미칠 중요한 의미 정보를 전달할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "71-3",
                    "sentence": "In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.",
                    "sentence_kor": "본 논문에서 우리는 훈련 중 각 대상 토큰에 적절한 가중치를 할당하기 위해 토큰 빈도를 기반으로 대상 토큰 수준의 적응 목표를 조사했다.",
                    "tag": "2"
                },
                {
                    "index": "71-4",
                    "sentence": "We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens.",
                    "sentence_kor": "우리는 모델이 이러한 토큰에 더 많은 주의를 기울이도록 장려하기 위해 의미가 있지만 상대적으로 빈도가 낮은 단어들을 목표에서 더 큰 가중치로 할당할 수 있다는 것을 목표로 했다.",
                    "tag": "3"
                },
                {
                    "index": "71-5",
                    "sentence": "Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively.",
                    "sentence_kor": "우리의 방법은 ZH-EN, EN-RO 및 EN-DE 번역 작업, 특히 기준선에 비해 각각 1.68, 1.02 및 0.52 BLEU가 증가할 수 있는 저주파 토큰이 더 많이 포함된 문장에서 번역 품질이 지속적으로 향상된다.",
                    "tag": "3"
                },
                {
                    "index": "71-6",
                    "sentence": "Further analyses show that our method can also improve the lexical diversity of translation.",
                    "sentence_kor": "추가 분석에 따르면 우리의 방법은 번역의 어휘적 다양성도 개선할 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "72",
            "abstractID": "EMNLP_abs-72",
            "text": [
                {
                    "index": "72-0",
                    "sentence": "Transformer models achieve remarkable success in Neural Machine Translation.",
                    "sentence_kor": "트랜스포머 모델은 신경 기계 번역에서 괄목할 만한 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "72-1",
                    "sentence": "Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention.",
                    "sentence_kor": "여러 병렬 유닛에 대한 조사는 거의 관심을 끌지 못하는 반면, 여러 유닛(즉, 다중 헤드 어텐션과 FFN의 조합)을 계단식으로 쌓아 트랜스포머를 심화시키는 데 많은 노력을 기울였다.",
                    "tag": "1"
                },
                {
                    "index": "72-2",
                    "sentence": "In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.",
                    "sentence_kor": "본 논문에서, 우리는 다양하고 보완적인 유닛을 도입하여 트랜스포머의 표현성을 촉진하는 것을 목표로 하는 다중 유닛 트랜스포머(MUTE)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "72-3",
                    "sentence": "Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.",
                    "sentence_kor": "특히, 우리는 여러 병렬 장치를 사용하며 여러 장치를 사용한 모델링이 모델 성능을 향상시키고 다양성을 도입한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "72-4",
                    "sentence": "Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.",
                    "sentence_kor": "또한 다중 단위 설정의 장점을 더 잘 활용하기 위해, 우리는 다른 단위들 간의 상호보완성을 안내하고 장려하는 편향된 모듈과 순차적 의존성을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "72-5",
                    "sentence": "Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%).",
                    "sentence_kor": "NIST 중국어-영어, WMT'14 영어-독일어 및 WMT'18 중국어-영어 등 세 가지 기계 번역 작업에 대한 실험 결과는 MUT 모델이 속도 저하만으로 트랜스포머-베이스보다 최대 +1.52, +1.90 및 +1.10 BLEU 포인트 높은 성능을 발휘한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "72-6",
                    "sentence": "In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters.",
                    "sentence_kor": "또한 우리의 방법은 매개 변수의 54%만 있는 Transformer-Big 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "72-7",
                    "sentence": "These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.",
                    "sentence_kor": "이러한 결과는 MUTE의 효과와 추론 프로세스 및 매개변수 사용에서의 효율성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "73",
            "abstractID": "EMNLP_abs-73",
            "text": [
                {
                    "index": "73-0",
                    "sentence": "Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources.",
                    "sentence_kor": "현대의 신경 기계 변환(NMT) 모델은 많은 매개 변수를 사용하며, 이는 심각한 과잉 매개 변수화를 초래하고 일반적으로 계산 자원의 활용도를 저하시킨다.",
                    "tag": "1"
                },
                {
                    "index": "73-1",
                    "sentence": "In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance.",
                    "sentence_kor": "이 문제에 대응하여, 우리는 더 나은 성능을 달성하기 위해 중복 매개변수를 재사용할 수 있는지 경험적으로 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "73-2",
                    "sentence": "Experiments and analyses are systematically conducted on different datasets and NMT architectures.",
                    "sentence_kor": "실험과 분석은 서로 다른 데이터 세트와 NMT 아키텍처에 대해 체계적으로 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "73-3",
                    "sentence": "We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.",
                    "sentence_kor": "우리는 1) 가지치기된 매개 변수를 회춘하여 기본 모델을 최대 +0.8 BLEU 포인트까지 개선할 수 있으며, 2) 회춘된 매개 변수를 재할당하여 낮은 수준의 어휘 정보 모델링 능력을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "74",
            "abstractID": "EMNLP_abs-74",
            "text": [
                {
                    "index": "74-0",
                    "sentence": "In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs.",
                    "sentence_kor": "본 연구에서는 대상 출력 간의 로컬 의존성을 포착하기 위해 비 자기 회귀 변환(NAT) 모델에 새로운 로컬 자기 회귀 변환(LAT) 메커니즘을 도입한다.",
                    "tag": "2"
                },
                {
                    "index": "74-1",
                    "sentence": "Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way.",
                    "sentence_kor": "특히, 각 대상 디코딩 위치에 대해 하나의 토큰이 아닌 짧은 토큰 시퀀스를 자기 회귀 방식으로 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "74-2",
                    "sentence": "We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence.",
                    "sentence_kor": "또한 출력 조각을 하나의 최종 출력 시퀀스로 정렬하고 병합하는 효율적인 병합 알고리즘을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "74-3",
                    "sentence": "We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding.",
                    "sentence_kor": "LAT를 조건부 마스크 언어 모델(CMLM)에 통합하고(Gazvininejad 등,2019) 마찬가지로 반복 디코딩을 채택한다.",
                    "tag": "3"
                },
                {
                    "index": "74-4",
                    "sentence": "Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup.",
                    "sentence_kor": "5가지 변환 작업에 대한 경험적 결과에 따르면, CMLM과 비교하여 우리의 방법은 디코딩 반복 횟수가 적으면서 비교 가능하거나 더 나은 성능을 달성하여 2.5배 속도를 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "74-5",
                    "sentence": "Further analysis indicates that our method reduces repeated translations and performs better at longer sentences.",
                    "sentence_kor": "추가 분석에 따르면 우리의 방법은 반복 번역을 줄이고 긴 문장에서도 더 잘 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "74-6",
                    "sentence": "Our code will be released to the public.",
                    "sentence_kor": "우리의 코드는 대중에게 공개될 것이다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "75",
            "abstractID": "EMNLP_abs-75",
            "text": [
                {
                    "index": "75-0",
                    "sentence": "Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans.",
                    "sentence_kor": "최근의 연구는 신경 기계 번역(NMT) 훈련이 인간의 학습 과정을 모방함으로써 촉진될 수 있다는 것을 증명했다.",
                    "tag": "1"
                },
                {
                    "index": "75-1",
                    "sentence": "Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity.",
                    "sentence_kor": "그럼에도 불구하고 이러한 종류의 커리큘럼 학습의 성과는 문장 길이 또는 단어 희귀성과 같은 수작업으로 작성한 인위적인 일정의 품질에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "75-2",
                    "sentence": "We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.",
                    "sentence_kor": "우리는 NMT 모델이 1) 훈련 예제에 대한 학습 신뢰도를 자동으로 정량화할 수 있고 2) 각 반복 단계에서 손실을 조절하여 학습을 유연하게 제어하는 자기 진행식 학습을 제안함으로써 이 절차를 보다 유연한 방식으로 개선한다.",
                    "tag": "2+3"
                },
                {
                    "index": "75-3",
                    "sentence": "Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.",
                    "sentence_kor": "여러 변환 작업에 대한 실험 결과는 제안된 모델이 강력한 기준선 및 변환 품질과 수렴 속도 모두에 대해 인간이 설계한 커리큘럼으로 훈련된 모델보다 더 나은 성능을 산출한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "76",
            "abstractID": "EMNLP_abs-76",
            "text": [
                {
                    "index": "76-0",
                    "sentence": "Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity.",
                    "sentence_kor": "많은 문서 수준 신경 기계 변환(NMT) 시스템은 일반적으로 증가하는 매개 변수와 계산 복잡성이 필요한 상황 인식 아키텍처의 유용성을 탐구해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "76-1",
                    "sentence": "However, few attention is paid to the baseline model.",
                    "sentence_kor": "그러나 기준 모델에는 거의 관심이 없다.",
                    "tag": "1"
                },
                {
                    "index": "76-2",
                    "sentence": "In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation.",
                    "sentence_kor": "본 논문에서, 우리는 문서 수준 변환에서 표준 변압기의 장단점을 광범위하게 연구하며, 자동 회귀 특성이 오류 누적의 일관성과 단점을 동시에 가져올 수 있다는 것을 발견했다.",
                    "tag": "2"
                },
                {
                    "index": "76-3",
                    "sentence": "Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors.",
                    "sentence_kor": "따라서 표준 변압기 위에 놀라울 정도로 간단한 장기 마스킹 셀프 어텐션을 제안하여 장거리 의존성을 효과적으로 포착하고 오류 전파를 줄일 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "76-4",
                    "sentence": "We examine our approach on the two publicly available document-level datasets.",
                    "sentence_kor": "공개적으로 사용 가능한 두 개의 문서 수준 데이터 세트에 대한 접근 방식을 검토한다.",
                    "tag": "4"
                },
                {
                    "index": "76-5",
                    "sentence": "We can achieve a strong result in BLEU and capture discourse phenomena.",
                    "sentence_kor": "우리는 BLEU에서 강력한 결과를 얻을 수 있고 담화 현상을 포착할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "77",
            "abstractID": "EMNLP_abs-77",
            "text": [
                {
                    "index": "77-0",
                    "sentence": "Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation.",
                    "sentence_kor": "번역 품질의 향상에도 불구하고 신경 기계 번역(NMT)은 종종 생성의 다양성 부족으로 어려움을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "77-1",
                    "sentence": "In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference.",
                    "sentence_kor": "본 논문에서, 우리는 추론을 위해 베이지안 모델링과 표본 추출 모델을 사용하여 많은 수의 가능한 모델을 도출하여 다양한 변환을 생성할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "77-2",
                    "sentence": "The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling.",
                    "sentence_kor": "가능한 모델은 NMT 모델에 구체적인 드롭아웃을 적용하여 얻으며, 각 모델은 예측에 대한 특정 신뢰도를 가지며, 이는 베이지안 모델링 원칙의 특정 훈련 데이터에 따른 사후 모델 분포에 해당한다.",
                    "tag": "3"
                },
                {
                    "index": "77-3",
                    "sentence": "With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled.",
                    "sentence_kor": "변동 추론을 사용하면 사후 모형 분포는 추론을 위한 최종 모형이 샘플링되는 변동 분포로 근사할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "77-4",
                    "sentence": "We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.",
                    "sentence_kor": "우리는 중국어-영어 및 영어-독일어 번역 작업에 대한 실험을 수행했고, 그 결과는 우리의 방법이 다양성과 정확성 사이에서 더 나은 균형을 만든다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "78",
            "abstractID": "EMNLP_abs-78",
            "text": [
                {
                    "index": "78-0",
                    "sentence": "This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming.",
                    "sentence_kor": "본 논문은 동적 프로그래밍으로 잠재 정렬을 모델링하는 비 자기 회귀 기계 변환을 위한 두 가지 강력한 방법인 CTC와 Imputer를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "78-1",
                    "sentence": "We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates.",
                    "sentence_kor": "기계 변환을 위해 CTC를 다시 살펴보고, 간단한 CTC 모델이 이전 연구에서 알 수 있는 것과 달리 단일 단계 비 자기 회귀 기계 변환을 위한 최첨단 기술을 달성할 수 있음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "78-2",
                    "sentence": "In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline.",
                    "sentence_kor": "또한 비 자기 회귀 기계 변환에 임퓨터 모델을 적용하고 4세대 단계만으로 임퓨터가 자기 회귀 변환기 기준선의 성능과 일치할 수 있음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "78-3",
                    "sentence": "Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model.",
                    "sentence_kor": "잠재 정렬 모델은 기존의 많은 비 자기 회귀 변환 기준선보다 간단하다. 예를 들어, 목표 길이 예측 또는 자기 회귀 모델을 통한 재점수가 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "78-4",
                    "sentence": "On the competitive WMT’14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps.",
                    "sentence_kor": "경쟁력 있는 WMT'14 En→De 과제에서 CTC 모델은 단일 세대 단계로 25.7 BLEU를 달성하고 Imputer는 2세대 단계로 27.5 BLEU를 달성하며, 28.0 BLEU는 4세대 단계로 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "78-5",
                    "sentence": "This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.",
                    "sentence_kor": "이는 27.8 BLEU의 자기 회귀 변압기 기준선과 유리하게 비교된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "79",
            "abstractID": "EMNLP_abs-79",
            "text": [
                {
                    "index": "79-0",
                    "sentence": "Many extractive question answering models are trained to predict start and end positions of answers.",
                    "sentence_kor": "많은 추출적 질문 답변 모델은 답변의 시작 및 종료 위치를 예측하도록 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "79-1",
                    "sentence": "The choice of predicting answers as positions is mainly due to its simplicity and effectiveness.",
                    "sentence_kor": "답을 포지션으로 예측하는 선택은 주로 단순성과 효과성 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "79-2",
                    "sentence": "In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions.",
                    "sentence_kor": "이 연구에서 우리는 답변 위치의 분포가 훈련 세트에서 심하게 치우쳐 있을 때(예: 각 구절의 k번째 문장에만 답이 있음) 위치를 예측한 QA 모델이 잘못된 위치 신호를 학습할 수 있고 다른 위치에서 답변을 제공하지 못할 수 있다고 가정한다.",
                    "tag": "1+2"
                },
                {
                    "index": "79-3",
                    "sentence": "We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT.",
                    "sentence_kor": "먼저 BiDAF 및 BERT와 같은 인기 추출 QA 모델에서 이 위치 편향을 설명하고 위치 편향이 BERT의 각 계층을 통해 어떻게 전파되는지 철저히 검토한다.",
                    "tag": "3"
                },
                {
                    "index": "79-4",
                    "sentence": "To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling.",
                    "sentence_kor": "위치 편향 없이 위치 정보를 안전하게 전달하기 위해 엔트로피 정규화 및 편향 앙상블링을 포함한 다양한 편파 제거 방법을 사용하여 모델을 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "79-5",
                    "sentence": "Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.",
                    "sentence_kor": "그 중, 우리는 편향된 SQuAD 데이터 세트에 대해 훈련했을 때 BERT의 성능을 37.48%에서 81.64%로 회복하면서, 답변 위치의 이전 분포를 바이어스 모델로 사용하는 것이 위치 편향을 줄이는 데 매우 효과적이라는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "80",
            "abstractID": "EMNLP_abs-80",
            "text": [
                {
                    "index": "80-0",
                    "sentence": "BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks.",
                    "sentence_kor": "BERT 시대의 질문 응답 시스템은 최근 몇 가지 질문 응답(QA) 작업에서 인상적인 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "80-1",
                    "sentence": "These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data.",
                    "sentence_kor": "이러한 시스템은 방대한 양의 데이터를 사용하여 단어 마스킹 및 문장 수반과 같은 자체 감독 작업에 대해 사전 교육된 표현을 기반으로 한다.",
                    "tag": "1"
                },
                {
                    "index": "80-2",
                    "sentence": "Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance.",
                    "sentence_kor": "그럼에도 불구하고, 합성 QA 쌍에 대한 훈련과 같이 최종 작업에 더 가까운 추가 사전 교육은 성능을 향상시키는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "80-3",
                    "sentence": "While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention.",
                    "sentence_kor": "최근 연구에서는 레이블이 지정된 데이터를 확대하고 레이블이 지정되지 않은 대규모 데이터 세트를 활용하여 합성 QA 데이터를 생성하는 것을 고려했지만, 대상 데이터에 직접 적응하는 것은 거의 주목을 받지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "80-4",
                    "sentence": "In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation.",
                    "sentence_kor": "본 논문에서 우리는 감독되지 않은 자가 적응을 실현하기 위한 방법으로 합성 QA 쌍의 반복 생성을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "80-5",
                    "sentence": "Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data.",
                    "sentence_kor": "생성된 QA 쌍을 필터링하기 위한 왕복 일관성 방법의 성공에 자극을 받아 적응 데이터의 확률에 대한 하한 근사치를 최대화하는 접근 방식의 반복 일반화를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "80-6",
                    "sentence": "By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.",
                    "sentence_kor": "대상 데이터에 생성된 합성 QA 쌍에 적응함으로써, 우리의 방법은 기존 증강 접근법보다 훨씬 적은 합성 데이터와 훈련 계산을 사용하여 QA 시스템을 크게 개선할 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "81",
            "abstractID": "EMNLP_abs-81",
            "text": [
                {
                    "index": "81-0",
                    "sentence": "AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph.",
                    "sentence_kor": "AMR-텍스트 생성은 입력 AMR 그래프와 동일한 의미를 포함하는 텍스트를 복구하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "81-1",
                    "sentence": "Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs.",
                    "sentence_kor": "현재 연구는 AMR 그래프를 더 잘 나타내기 위해 점점 더 강력한 그래프 인코더를 개발하고 있으며, 출력 생성에 표준 언어 모델링을 기반으로 하는 디코더가 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "81-2",
                    "sentence": "We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation.",
                    "sentence_kor": "텍스트 생성 중에 대상 문장에 예상되는 AMR 그래프를 역예측하는 디코더를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "81-3",
                    "sentence": "As the result, our outputs can better preserve the input meaning than standard decoders.",
                    "sentence_kor": "결과적으로, 우리의 출력은 표준 디코더보다 입력 의미를 더 잘 보존할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "81-4",
                    "sentence": "Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.",
                    "sentence_kor": "두 개의 AMR 벤치마크에 대한 실험은 그래프 트랜스포머를 기반으로 한 이전 최첨단 시스템에 비해 우리 모델의 우수성을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "82",
            "abstractID": "EMNLP_abs-82",
            "text": [
                {
                    "index": "82-0",
                    "sentence": "Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence.",
                    "sentence_kor": "다양한 방식으로 이야기와 절차와 같은 긴 형식의 내러티브를 만들어내는 것은 인공지능의 오랜 꿈이었다.",
                    "tag": "1"
                },
                {
                    "index": "82-1",
                    "sentence": "In this regard, there is often crucial subtext that is derived from the surrounding contexts.",
                    "sentence_kor": "이런 점에서, 종종 주변 문맥에서 파생된 중요한 하위 텍스트가 있다.",
                    "tag": "1"
                },
                {
                    "index": "82-2",
                    "sentence": "The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts.",
                    "sentence_kor": "일반적인 seq2seq 훈련 방법은 이러한 인접 컨텍스트 간의 간격을 메우려고 시도하는 동안 모델을 부족하게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "82-3",
                    "sentence": "In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images.",
                    "sentence_kor": "본 논문에서, 우리는 일련의 이미지에서 텍스트 설명을 생성하는 동시에 서술에서 누락된 단계를 예측하는 주입 기법을 사용하여 이 문제를 해결한다.",
                    "tag": "2+3"
                },
                {
                    "index": "82-4",
                    "sentence": "We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies.",
                    "sentence_kor": "또한 총 46,200개의 절차와 약 340k개의 쌍별 이미지 및 텍스트 설명이 포함된 새로운 대규모 시각적 절차 표시(ViPT) 데이터 세트를 제시하며, 이러한 컨텍스트 종속성이 풍부하다.",
                    "tag": "4"
                },
                {
                    "index": "82-5",
                    "sentence": "Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts.",
                    "sentence_kor": "주입 기법을 사용하여 단계를 생성하는 것은 보다 일관성 있는 텍스트가 있는 시각적 절차의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "82-6",
                    "sentence": "We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling.",
                    "sentence_kor": "우리는 시각적 스토리텔링의 최첨단보다 높은 절차에서 27.51의 METEOR 점수를 결정적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "82-7",
                    "sentence": "We also demonstrate the effects of interposing new text with missing images during inference.",
                    "sentence_kor": "또한 추론 중에 누락된 이미지와 새 텍스트를 인터핑하는 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "82-8",
                    "sentence": "The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.",
                    "sentence_kor": "코드 및 데이터 세트는 https://visual-narratives.github.io/Visual-Narratives/에서 공개적으로 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "83",
            "abstractID": "EMNLP_abs-83",
            "text": [
                {
                    "index": "83-0",
                    "sentence": "We propose a new task in the area of computational creativity: acrostic poem generation in English.",
                    "sentence_kor": "우리는 컴퓨터 창의성 분야의 새로운 과제인 영어의 4행시 생성을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "83-1",
                    "sentence": "Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase.",
                    "sentence_kor": "삼행시는 숨겨진 메시지를 담고 있는 시이다; 전형적으로, 각 행의 첫 글자는 단어나 짧은 구절을 철자한다.",
                    "tag": "1"
                },
                {
                    "index": "83-2",
                    "sentence": "We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme.",
                    "sentence_kor": "우리는 작업을 여러 제약 조건이 있는 생성 작업으로 정의한다. 입력 단어 1) 각 행의 첫 글자는 제공된 단어를 철자해야 하며 2) 시의 의미론도 그것과 관련되어야 하며 3) 시는 운율 체계에 따라야 한다.",
                    "tag": "3"
                },
                {
                    "index": "83-3",
                    "sentence": "We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model.",
                    "sentence_kor": "우리는 또한 신경 운율 모델과 결합된 조건부 신경 언어 모델로 구성된 작업을 위한 기본 모델을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "83-4",
                    "sentence": "Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems.",
                    "sentence_kor": "4행시 생성을 위한 전용 데이터 세트가 없기 때문에 먼저 주제 주석 시 집합에 대한 별도의 주제 예측 모델을 교육한 다음 추가 시에 대한 주제를 예측하여 작업에 대한 훈련 데이터를 만든다.",
                    "tag": "3"
                },
                {
                    "index": "83-5",
                    "sentence": "Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints.",
                    "sentence_kor": "우리의 실험은 기준선에 의해 생성된 삼행시가 인간들에게 잘 받아들여지고 추가적인 제약으로 인해 품질이 크게 떨어지지 않는다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "83-6",
                    "sentence": "Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.",
                    "sentence_kor": "마지막으로, 우리는 우리 모델에 의해 생성된 시가 제공된 프롬프트와 밀접한 관련이 있으며 위키피디아에 대한 사전 교육이 성과를 향상시킬 수 있다는 것을 확인한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "84",
            "abstractID": "EMNLP_abs-84",
            "text": [
                {
                    "index": "84-0",
                    "sentence": "Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data.",
                    "sentence_kor": "명명된 개체 인식(NER)은 심층 언어 이해의 첫 단계 중 하나이지만 현재 NER 모델은 사람 주석 데이터에 크게 의존하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "84-1",
                    "sentence": "In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other.",
                    "sentence_kor": "본 연구에서는 레이블링된 데이터에 대한 의존도를 완화하기 위해 준감독 NER에 대한 LADA(Local Additivity based Data Augmentation) 방법을 제안한다. 이 방법은 서로 가까운 시퀀스를 보간하여 가상 샘플을 만든다.",
                    "tag": "2"
                },
                {
                    "index": "84-2",
                    "sentence": "Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate.",
                    "sentence_kor": "우리의 접근 방식에는 두 가지 변형이 있습니다. 내부 LADA와 내부 LADA는 한 문장 내에서 토큰 간에 보간을 수행하고, 내부 LADA는 보간하기 위해 서로 다른 문장을 샘플링한다.",
                    "tag": "3"
                },
                {
                    "index": "84-3",
                    "sentence": "Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning.",
                    "sentence_kor": "LADA는 샘플링된 교육 데이터 간의 선형 추가를 통해 무한한 양의 레이블링 데이터를 생성하고 엔터티 및 컨텍스트 학습을 모두 개선합니다.",
                    "tag": "3"
                },
                {
                    "index": "84-4",
                    "sentence": "We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data.",
                    "sentence_kor": "라벨이 부착되지 않은 데이터에 대한 새로운 일관성 손실을 설계하여 LADA를 준감독 설정으로 확장한다.",
                    "tag": "3"
                },
                {
                    "index": "84-5",
                    "sentence": "Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines.",
                    "sentence_kor": "두 개의 NER 벤치마크에 대해 수행된 실험은 몇 가지 강력한 기준선에 대한 방법의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "84-6",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/LADA",
                    "sentence_kor": "우리는 https://github.com/GT-SALT/LADA에서 우리의 코드를 공개했습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "85",
            "abstractID": "EMNLP_abs-85",
            "text": [
                {
                    "index": "85-0",
                    "sentence": "Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks.",
                    "sentence_kor": "언어 모델은 NLP 전반에 걸쳐 중심 구성요소로 등장했으며, 많은 진전이 새로운 영역과 작업에 저렴하게 적응할 수 있는 능력(예: 미세 조정을 통해)에 달려 있다.",
                    "tag": "1"
                },
                {
                    "index": "85-1",
                    "sentence": "A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation.",
                    "sentence_kor": "언어 모델의 어휘(일반적으로 훈련 전에 선택되고 나중에 영구적으로 고정됨)는 그 크기에 영향을 미치며 그러한 적응에 저항하는 것의 일부이다.",
                    "tag": "1"
                },
                {
                    "index": "85-2",
                    "sentence": "Prior work has used compositional input embeddings based on surface forms to ameliorate this issue.",
                    "sentence_kor": "이전 연구에서는 이 문제를 개선하기 위해 표면 형태에 기초한 구성 입력 임베딩을 사용했다.",
                    "tag": "1"
                },
                {
                    "index": "85-3",
                    "sentence": "In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions.",
                    "sentence_kor": "본 연구에서는 한 단계 더 나아가 언어 모델을 위한 완전한 구성 출력 임베딩 레이어를 제안한다. 이 레이어는 구조화된 어휘(WordNet)의 정보, 즉 의미론적으로 관련된 단어와 자유 텍스트 정의를 기반으로 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "85-4",
                    "sentence": "To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary.",
                    "sentence_kor": "우리가 아는 바로는, 그 결과는 훈련 어휘에 의존하지 않는 크기의 첫 번째 단어 수준 언어 모델이다.",
                    "tag": "4"
                },
                {
                    "index": "85-5",
                    "sentence": "We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches.",
                    "sentence_kor": "우리는 기존의 언어 모델링과 개방형 어휘로 도전적인 교차 도메인 설정에 대한 모델을 평가하여 이전의 최첨단 출력 임베딩 방법 및 적응 접근법과 일치하거나 능가한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "85-6",
                    "sentence": "Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.",
                    "sentence_kor": "분석 결과, 샘플 효율성의 향상으로 귀속되는데, 이 모델은 저주파 단어에 대해 더 정확하다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "86",
            "abstractID": "EMNLP_abs-86",
            "text": [
                {
                    "index": "86-0",
                    "sentence": "Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples.",
                    "sentence_kor": "교육 영역에서 우수한 성능을 발휘하는 모델은 도메인 외(OOD) 예로 일반화하지 못하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "86-1",
                    "sentence": "Data augmentation is a common method used to prevent overfitting and improve OOD generalization.",
                    "sentence_kor": "데이터 확대는 OOD 일반화를 개선하고 과적합을 방지하기 위해 사용되는 일반적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "86-2",
                    "sentence": "However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold.",
                    "sentence_kor": "그러나 자연어에서는 기본 데이터 매니폴드에 머물러 있는 새로운 예를 생성하는 것이 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "86-3",
                    "sentence": "We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold.",
                    "sentence_kor": "손상 및 재구성 함수 쌍을 사용하여 데이터 매니폴드에서 무작위로 이동함으로써 합성 훈련 예제를 생성하는 데이터 증강 방법인 SSMBA를 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "86-4",
                    "sentence": "We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models.",
                    "sentence_kor": "우리는 마스크된 언어 모델로 손상된 텍스트를 재구성하기 위해 다양체 가정을 활용하여 자연어 영역에서 SSMBA의 사용을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "86-5",
                    "sentence": "In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.",
                    "sentence_kor": "3개의 작업과 9개의 데이터 세트에 걸친 견고성 벤치마크 실험에서 SSMBA는 OOD Amazon 검토에서 0.8%, OOD MNLI에서 1.8%, 도메인 IWSLT14 독일-영어에서 1.4 BLEU의 정확도를 달성하면서 도메인 내 데이터 확대 방법과 기준 모델을 일관되게 능가한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "87",
            "abstractID": "EMNLP_abs-87",
            "text": [
                {
                    "index": "87-0",
                    "sentence": "For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high.",
                    "sentence_kor": "예를 들어 정서 분류와 같은 많은 실제 분류 문제의 경우, 불균형 비율(IR)이 높을 때 대부분의 기존 기계 학습 방법이 다수 계층에 치우친다.",
                    "tag": "1"
                },
                {
                    "index": "87-1",
                    "sentence": "To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 분류자가 나중에 균형 잡힌 클래스 분포에 대해 훈련될 수 있도록 각 클래스에 대해 단일 대표를 추출하는 세트 컨볼루션(SetConv) 운영과 일시적인 훈련 전략을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "87-2",
                    "sentence": "We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.",
                    "sentence_kor": "우리는 우리가 제안한 알고리즘이 입력 순서에도 불구하고 순열 불변이라는 것을 증명하며, 여러 대규모 벤치마크 텍스트 데이터 세트에 대한 실험은 다른 SOTA 방법과 비교할 때 우리가 제안한 프레임워크의 우수성을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "88",
            "abstractID": "EMNLP_abs-88",
            "text": [
                {
                    "index": "88-0",
                    "sentence": "Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale.",
                    "sentence_kor": "외부 지식(예: 지식 그래프)으로 질문 답변(QA) 모델을 보강하는 기존 작업은 다중 홉 관계를 효율적으로 모델링하는 데 어려움을 겪거나 모델의 예측 근거에 대한 투명성이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "88-1",
                    "sentence": "In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN).",
                    "sentence_kor": "본 논문에서 우리는 사전 훈련된 언어 모델(PTLM)에 다중 홉 그래프 관계 네트워크(MHGRN)라는 다중 홉 관계 추론 모듈을 장착하는 새로운 지식 인식 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "88-2",
                    "sentence": "It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs.",
                    "sentence_kor": "외부 지식 그래프에서 추출한 하위 그래프에 대해 다중 홉, 다중 관계 추론을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "88-3",
                    "sentence": "The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability.",
                    "sentence_kor": "제안된 추론 모듈은 경로 기반 추론 방법과 그래프 신경망을 통합하여 더 나은 해석성과 확장성을 달성한다.",
                    "tag": "3"
                },
                {
                    "index": "88-4",
                    "sentence": "We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.",
                    "sentence_kor": "우리는 또한 상식에 대한 효과와 확장성을 경험적으로 보여준다.QA 및 오픈북QA 데이터 세트와 발표된 실험 코드를 사용하여 사례 연구를 통해 동작을 해석한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "89",
            "abstractID": "EMNLP_abs-89",
            "text": [
                {
                    "index": "89-0",
                    "sentence": "This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words.",
                    "sentence_kor": "본 논문은 단일 언어 사전 유도 과제를 설계하고 희귀 단어에 대한 이중 언어 사전 유도 정확도의 저하를 수반하는 두 가지 요인을 관찰한다.",
                    "tag": "2"
                },
                {
                    "index": "89-1",
                    "sentence": "First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency.",
                    "sentence_kor": "첫째, 저주파 영역에서 유사점 간의 차이가 줄어들고 둘째, 저주파에서 허브성이 악화되었다.",
                    "tag": "4"
                },
                {
                    "index": "89-2",
                    "sentence": "Based on the observation, we further propose two methods to address these two factors, respectively.",
                    "sentence_kor": "관찰을 바탕으로, 우리는 이 두 가지 요인을 각각 해결하기 위한 두 가지 방법을 추가로 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "89-3",
                    "sentence": "The larger issue is hubness.",
                    "sentence_kor": "더 큰 문제는 허브성입니다.",
                    "tag": "4"
                },
                {
                    "index": "89-4",
                    "sentence": "Addressing that improves induction accuracy significantly, especially for low-frequency words.",
                    "sentence_kor": "특히 저주파 워드의 경우 유도 정확도를 크게 향상시키는 어드레싱.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "90",
            "abstractID": "EMNLP_abs-90",
            "text": [
                {
                    "index": "90-0",
                    "sentence": "The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models.",
                    "sentence_kor": "VAE의 도입은 생성 주제 모델을 포함한 생성 모델의 학습을 위한 효율적인 프레임워크를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "90-1",
                    "sentence": "However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable.",
                    "sentence_kor": "그러나, 주제 모델이 잠재 디리클레 할당(LDA) 모델인 경우, 재측정 트릭인 VAE의 중심 기법이 적용되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "90-2",
                    "sentence": "This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick.",
                    "sentence_kor": "이는 디리클레 분포의 재매개화 형태가 현재까지 재매개화 트릭을 사용할 수 있는 것으로 알려져 있지 않기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "90-3",
                    "sentence": "In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models.",
                    "sentence_kor": "본 연구에서는 VAE-LDA 모델 학습을 위해 디리클레 분포를 재측정하는 RRT(Rounded Repameterization Trick)라는 새로운 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "90-4",
                    "sentence": "This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.",
                    "sentence_kor": "VAE-LDA 모델에 적용할 때 이 방법은 여러 벤치마크 데이터 세트와 합성 데이터 세트에서 기존 신경 주제 모델을 능가하는 것으로 실험적으로 보여진다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "91",
            "abstractID": "EMNLP_abs-91",
            "text": [
                {
                    "index": "91-0",
                    "sentence": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization.",
                    "sentence_kor": "미세 조정된 사전 교육 언어 모델은 과도한 매개 변수화로 인해 배포 내 및 배포 외(OD) 데이터 모두에 대해 심각한 오보정을 겪을 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "91-1",
                    "sentence": "To mitigate this issue, we propose a regularized fine-tuning method.",
                    "sentence_kor": "이 문제를 완화하기 위해 정규화된 미세 조정 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "91-2",
                    "sentence": "Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold.",
                    "sentence_kor": "우리의 방법은 더 나은 보정을 위해 (1) 데이터 다양체 내에서 보간을 통해 유사 온매니폴드 샘플을 생성하는 온매니폴드 정규화라는 두 가지 유형의 정규화를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "91-3",
                    "sentence": "Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration.",
                    "sentence_kor": "이러한 유사 샘플을 사용한 증강 훈련은 분포 내 교정을 개선하기 위해 평활도 정규화를 시행한다.",
                    "tag": "4"
                },
                {
                    "index": "91-4",
                    "sentence": "(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.",
                    "sentence_kor": "(2) OOD 데이터에 대한 과신뢰 문제를 해결하기 위해 모델이 유사 오프매니폴드 샘플에 대해 균일한 분포를 출력하도록 권장하는 Off-manifold 정규화.",
                    "tag": "4"
                },
                {
                    "index": "91-5",
                    "sentence": "Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets.",
                    "sentence_kor": "우리의 실험은 제안된 방법이 6개의 데이터 세트에서 기대 교정 오류, 잘못된 분류 감지 및 OOD 검출 측면에서 텍스트 분류에 대한 기존 교정 방법을 능가한다는 것을 입증한다.",
                    "tag": "4+5"
                },
                {
                    "index": "91-6",
                    "sentence": "Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.",
                    "sentence_kor": "코드는 https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "92",
            "abstractID": "EMNLP_abs-92",
            "text": [
                {
                    "index": "92-0",
                    "sentence": "The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure.",
                    "sentence_kor": "숨겨진 마르코프 모델(HMM)은 숨겨진 상태와 방출 구조를 깨끗하게 분리하는 시퀀스 모델링의 기본 도구입니다.",
                    "tag": "1"
                },
                {
                    "index": "92-1",
                    "sentence": "However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models.",
                    "sentence_kor": "그러나 이러한 분리는 현대 NLP의 대규모 데이터 세트에 HMM을 맞추기 어렵게 하며, 완전히 관찰된 모델에 비해 성능이 매우 좋지 않아 사용이 중단되었다.",
                    "tag": "1"
                },
                {
                    "index": "92-2",
                    "sentence": "This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling.",
                    "sentence_kor": "이 연구는 HMM을 언어 모델링 데이터 세트로 확장하여 신경 모델링에 대한 최신 접근 방식의 아이디어를 얻는 과제에 대해 재검토한다.",
                    "tag": "1"
                },
                {
                    "index": "92-3",
                    "sentence": "We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization.",
                    "sentence_kor": "우리는 효율적인 정확한 추론, 간단한 매개 변수화 및 효과적인 정규화를 유지하면서 HMM을 대규모 상태 공간으로 확장하는 방법을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "92-4",
                    "sentence": "Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.",
                    "sentence_kor": "실험에 따르면 이 접근 방식은 이전 HMMs 및 n그램 기반 방법보다 훨씬 더 정확한 모델로 이어져 최첨단 NN 모델의 성능을 향해 진보하고 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "93",
            "abstractID": "EMNLP_abs-93",
            "text": [
                {
                    "index": "93-0",
                    "sentence": "Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs.",
                    "sentence_kor": "NLP(Natural Language Processing) 작업은 일반적으로 텍스트 입력에 대해 낱말 단위로 수행된다.",
                    "tag": "1"
                },
                {
                    "index": "93-1",
                    "sentence": "We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs.",
                    "sentence_kor": "임의 기호를 사용하여 단어의 언어적 의미를 나타내고 이러한 기호를 입력으로 사용할 수 있습니다.",
                    "tag": "2"
                },
                {
                    "index": "93-2",
                    "sentence": "As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding.",
                    "sentence_kor": "텍스트 표현에 대한 \"대안\"으로 Soundex, MetaPhone, NYSIIS, Logogram to NLP를 소개하고 Huffman 코딩을 사용한 고정 출력 길이 코딩 및 확장 기능을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "93-3",
                    "sentence": "Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords.",
                    "sentence_kor": "이러한 각각의 코드는 서로 다른 문자/디지털 시퀀스를 결합하고 코드 워드에 기반한 새로운 어휘를 구성합니다.",
                    "tag": "3"
                },
                {
                    "index": "93-4",
                    "sentence": "We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs.",
                    "sentence_kor": "이러한 코드 워드와 텍스트의 통합은 텍스트 단독 입력보다 이중화를 통해 신경망 기반 NLP 시스템에 더 신뢰할 수 있는 입력을 제공한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "93-5",
                    "sentence": "Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging.",
                    "sentence_kor": "실험은 우리의 접근 방식이 기계 번역, 언어 모델링 및 음성 부분 태그 적용에 있어 최첨단 모델을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "93-6",
                    "sentence": "The source code is available at https://github.com/abdulrafae/coding_nmt.",
                    "sentence_kor": "소스 코드는 https://github.com/abdulrafae/coding_nmt에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "94",
            "abstractID": "EMNLP_abs-94",
            "text": [
                {
                    "index": "94-0",
                    "sentence": "Typically, machine learning systems solve new tasks by training on thousands of examples.",
                    "sentence_kor": "일반적으로 기계 학습 시스템은 수천 가지 예제를 교육함으로써 새로운 과제를 해결한다.",
                    "tag": "1"
                },
                {
                    "index": "94-1",
                    "sentence": "In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two.",
                    "sentence_kor": "이와는 대조적으로, 인간은 몇 가지 지시사항을 읽음으로써 새로운 과제를 해결할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "94-2",
                    "sentence": "To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area.",
                    "sentence_kor": "이 격차를 해소하기 위해, 우리는 이 영역의 이전 작업을 종합하여 설명서를 읽은 후 새로운 작업을 해결하는 NLP 시스템 개발을 위한 프레임워크를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "94-3",
                    "sentence": "We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks.",
                    "sentence_kor": "우리는 보이지 않는 작업에 대한 작업 지향 평가를 위해 구성된 새로운 영어 데이터 세트인 ZEST로 이 프레임을 인스턴스화한다.",
                    "tag": "3"
                },
                {
                    "index": "94-4",
                    "sentence": "Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model’s ability to solve each task.",
                    "sentence_kor": "작업 설명을 질문으로 공식화하여 각 입력이 가능한 많은 입력에 적용될 수 있을 만큼 충분히 일반적이므로 각 작업을 해결할 수 있는 모델의 능력을 종합적으로 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "94-5",
                    "sentence": "Moreover, the dataset’s structure tests specific types of systematic generalization.",
                    "sentence_kor": "또한 데이터 세트의 구조는 특정 유형의 체계적인 일반화를 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "94-6",
                    "sentence": "We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.",
                    "sentence_kor": "우리는 최첨단 T5 모델이 ZEST에서 12%의 점수를 달성하여 NLP 연구자들에게 중요한 과제를 남긴다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "95",
            "abstractID": "EMNLP_abs-95",
            "text": [
                {
                    "index": "95-0",
                    "sentence": "Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content.",
                    "sentence_kor": "명명된 개체 인식에 대한 기존 접근법은 짧은 텍스트와 비공식 텍스트, 특히 사용자가 생성한 소셜 미디어 콘텐츠에 대해 수행할 때 데이터 희소성 문제를 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "95-1",
                    "sentence": "Semantic augmentation is a potential way to alleviate this problem.",
                    "sentence_kor": "의미론적 확대는 이 문제를 완화할 수 있는 잠재적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "95-2",
                    "sentence": "Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation.",
                    "sentence_kor": "풍부한 의미 정보가 사전 훈련된 단어 임베딩에 암묵적으로 보존된다는 점을 감안할 때, 의미 증대를 위한 잠재적인 이상적인 자원이다.",
                    "tag": "1"
                },
                {
                    "index": "95-3",
                    "sentence": "In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account.",
                    "sentence_kor": "본 논문에서, 우리는 로컬(실행 텍스트)과 증강 의미론을 모두 고려하는 소셜 미디어 텍스트에 대한 NER에 대한 신경 기반 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "95-4",
                    "sentence": "In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.",
                    "sentence_kor": "특히, 대규모 말뭉치에서 증강 의미 정보를 얻고, 이러한 정보를 각각 인코딩하고 집계하기 위한 주의 깊은 의미 증강 모듈과 게이트 모듈을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "95-5",
                    "sentence": "Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.",
                    "sentence_kor": "영국과 중국 소셜 미디어 플랫폼에서 수집된 세 가지 벤치마크 데이터 세트에 대해 광범위한 실험을 수행하며, 그 결과는 세 데이터 세트 모두에 대한 이전 연구에 대한 접근 방식의 우수성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "96",
            "abstractID": "EMNLP_abs-96",
            "text": [
                {
                    "index": "96-0",
                    "sentence": "The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV).",
                    "sentence_kor": "소셜 미디어를 널리 사용하면 대규모 루머가 빠르게 퍼질 수 있으므로 자동 루머 검증(RV)의 필요성이 대두된다.",
                    "tag": "1"
                },
                {
                    "index": "96-1",
                    "sentence": "A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods.",
                    "sentence_kor": "많은 이전 연구는 자세 분류를 활용하여 다중 작업 학습(MTL) 방법으로 RV를 향상시키는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "96-2",
                    "sentence": "However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task.",
                    "sentence_kor": "그러나 이러한 방법의 대부분은 BERT와 같은 사전 훈련된 상황별 임베딩을 채택하지 못했고 RV 작업을 개선하기 위해 예측된 자세 레이블을 사용하여 작업 간 종속성을 이용하지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "96-3",
                    "sentence": "Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads.",
                    "sentence_kor": "따라서 본 논문에서는 BERT를 확장하여 스레드 표현을 얻기 위해 먼저 각 긴 스레드를 더 짧은 서브스레드로 나누고 BERT를 사용하여 각 서브스레드를 별도로 나타낸 다음 글로벌 트랜스포머 계층을 사용하여 모든 서브스레드를 인코딩하는 계층적 트랜스포머를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "96-4",
                    "sentence": "We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively.",
                    "sentence_kor": "또한 작업 간 상호 작용을 포착하기 위한 커플링 트랜스포머 모듈과 RV에 대해 예측된 자세 레이블을 사용하기 위한 사후 수준 주의 레이어를 각각 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "96-5",
                    "sentence": "Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.",
                    "sentence_kor": "두 벤치마크 데이터 세트에 대한 실험은 기존 MTL 접근 방식에 비해 결합된 계층적 트랜스포머 모델의 우수성을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "97",
            "abstractID": "EMNLP_abs-97",
            "text": [
                {
                    "index": "97-0",
                    "sentence": "Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval.",
                    "sentence_kor": "트위터와 같은 소셜 미디어 사이트는 정치적 의견과, 더 구체적으로, 정치 행위자들의 승인을 측정하는 설문조사를 보완할 수 있는 잠재력을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "97-1",
                    "sentence": "However, new challenges related to the reliability and validity of social-media-based estimates arise.",
                    "sentence_kor": "그러나 소셜 미디어 기반 추정치의 신뢰성과 타당성과 관련된 새로운 과제가 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "97-2",
                    "sentence": "Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on social media.",
                    "sentence_kor": "소셜 미디어에서 사용자의 정치적 의견을 측정하는 다양한 정서 분석 및 자세 감지 방법이 개발되어 이전 연구에서 사용되었습니다.",
                    "tag": "1"
                },
                {
                    "index": "97-3",
                    "sentence": "In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets.",
                    "sentence_kor": "본 연구에서는 여러 데이터 세트에 걸쳐 다양한 정치 행위자의 승인을 벤치마킹하여 표적이 없는 정서, 표적화된 정서 및 자세 감지 방법의 효과를 측정하려고 한다.",
                    "tag": "2"
                },
                {
                    "index": "97-4",
                    "sentence": "We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data.",
                    "sentence_kor": "또한 최소한의 사용자 지정 데이터에 대해 훈련된 모델 세트와 기성품(OTS) 방식으로 사용할 수 있는 이러한 사전 훈련된 방법의 성능을 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "97-5",
                    "sentence": "We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust.",
                    "sentence_kor": "우리는 OTS 방법이 보이지 않고 익숙한 대상에 대해 일반화 가능성이 낮은 반면 저자원 사용자 지정 모델은 더 강력하다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "97-6",
                    "sentence": "Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from tweets.",
                    "sentence_kor": "우리의 연구는 트위터에서 정치인들의 승인을 이해하기 위해 제안된 기존 방법의 강점과 한계를 조명한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "98",
            "abstractID": "EMNLP_abs-98",
            "text": [
                {
                    "index": "98-0",
                    "sentence": "Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models.",
                    "sentence_kor": "기계 판독 이해(MRC)는 주로 대규모 사전 훈련된 언어 모델 때문에 최근 몇 년 동안 개방형 도메인에서 상당한 진전을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "98-1",
                    "sentence": "However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect.",
                    "sentence_kor": "그러나 광범위한 훈련 데이터의 부족과 전문적인 구조 지식 무시로 인해 의료 분야와 같은 특정 영역에서 훨씬 더 나쁜 성과를 거두고 있다.",
                    "tag": "1"
                },
                {
                    "index": "98-2",
                    "sentence": "As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China.",
                    "sentence_kor": "이를 위해 먼저 중국 국가 면허 약사 시험을 위한 대규모 의료 다중 선택 질문 데이터 세트(21k건 이상 사례)를 수집한다.",
                    "tag": "1"
                },
                {
                    "index": "98-3",
                    "sentence": "It is a challenging medical examination with a passing rate of less than 14.2% in 2018.",
                    "sentence_kor": "2018년 합격률이 14.2% 미만인 어려운 건강검진이다.",
                    "tag": "1"
                },
                {
                    "index": "98-4",
                    "sentence": "Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books).",
                    "sentence_kor": "그런 다음 구조적 의학 지식(즉, 의학 지식 그래프)과 참조 의학 일반 텍스트(즉, 참고서에서 검색한 텍스트 조각)를 완전히 활용할 수 있는 새로운 독서 이해 모델 KMQA를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "98-5",
                    "sentence": "The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.",
                    "sentence_kor": "실험 결과에 따르면 KMQA는 큰 마진을 가진 기존 경쟁 모델을 능가하고 테스트 세트에서 61.8%의 정확도로 시험에 합격했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "99",
            "abstractID": "EMNLP_abs-99",
            "text": [
                {
                    "index": "99-0",
                    "sentence": "Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment.",
                    "sentence_kor": "의료 이미징은 진단 및 치료를 위한 임상 실습 및 시험에 자주 사용됩니다.",
                    "tag": "1"
                },
                {
                    "index": "99-1",
                    "sentence": "Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists.",
                    "sentence_kor": "영상 보고서 작성은 시간이 많이 걸리고 경험이 없는 방사선 전문의에게는 오류가 발생하기 쉽다.",
                    "tag": "1"
                },
                {
                    "index": "99-2",
                    "sentence": "Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain.",
                    "sentence_kor": "따라서 방사선 전문의의 작업부하를 줄이고 그에 따라 의료 영역에 인공지능을 적용하는 필수적인 작업인 임상 자동화를 촉진하기 위해 방사선학 보고서를 자동으로 생성하는 것이 매우 바람직하다.",
                    "tag": "1"
                },
                {
                    "index": "99-3",
                    "sentence": "In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer.",
                    "sentence_kor": "본 논문에서는 관계형 메모리가 생성 프로세스의 주요 정보를 기록하도록 설계되고 메모리 기반 조건부 계층 정규화가 트랜스포머의 디코더에 메모리를 통합하는 데 적용되는 메모리 기반 Transformer로 방사선 보고서를 생성할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "99-4",
                    "sentence": "Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations.",
                    "sentence_kor": "IU X-Ray와 MIMIC-CXR이라는 두 개의 일반적인 방사선 보고서 데이터 세트에 대한 실험 결과는 제안된 접근 방식이 언어 생성 메트릭과 임상 평가 모두에서 이전 모델을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "99-5",
                    "sentence": "Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge.",
                    "sentence_kor": "특히, 이것은 MIMIC-CXR에 대한 생성 결과를 우리가 아는 한 가장 먼저 보고하는 작업이다.",
                    "tag": "4"
                },
                {
                    "index": "99-6",
                    "sentence": "Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.",
                    "sentence_kor": "또한 추가 분석을 통해 우리의 접근 방식이 의미 있는 이미지-텍스트 주의 매핑뿐만 아니라 필요한 의학 용어로 긴 보고서를 생성할 수 있음을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "100",
            "abstractID": "EMNLP_abs-100",
            "text": [
                {
                    "index": "100-0",
                    "sentence": "Existing approaches to disfluency detection heavily depend on human-annotated data.",
                    "sentence_kor": "불용성 탐지에 대한 기존의 접근 방식은 사람 주석 데이터에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "100-1",
                    "sentence": "Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data.",
                    "sentence_kor": "라벨링된 데이터에 대한 의존도를 완화하기 위해 데이터 확대 방법의 수가 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "100-2",
                    "sentence": "However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies.",
                    "sentence_kor": "그러나 무작위 삽입 또는 반복과 같은 현재의 증강 접근법은 훈련 말뭉치와 잘 닮지 못하며 대개 부자연스럽고 제한된 유형의 불용성을 초래했다.",
                    "tag": "1"
                },
                {
                    "index": "100-3",
                    "sentence": "In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments.",
                    "sentence_kor": "본 연구에서, 우리는 자연적이고 다양한 불용성 텍스트를 증강 데이터로 생성하기 위한 간단한 불용성 생성 모델을 제안한다. 여기서 플래너는 불용성 세그먼트를 삽입할 위치를 결정하고 발전기는 해당 불용성 세그먼트를 생성하기 위한 예측을 따른다.",
                    "tag": "2+3"
                },
                {
                    "index": "100-4",
                    "sentence": "We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection.",
                    "sentence_kor": "우리는 또한 이 증강 데이터를 사전 훈련에 활용하고 불능 감지 작업에 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "100-5",
                    "sentence": "Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.",
                    "sentence_kor": "실험에 따르면 우리의 2단계 불용도 생성 모델은 기존 기준선을 능가한다. 이러한 불용도 문장은 불용도 검출 작업에 크게 도움이 되었고 스위치보드 말뭉치의 최첨단 성능을 이끌어냈다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "101",
            "abstractID": "EMNLP_abs-101",
            "text": [
                {
                    "index": "101-0",
                    "sentence": "Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks.",
                    "sentence_kor": "임상시험은 종종 참을 수 없는 비용과 위험을 수반하지만 증거 기반 의학의 실행을 위한 필수적인 지침을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "101-1",
                    "sentence": "To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task.",
                    "sentence_kor": "임상시험 설계를 최적화하기 위해 새로운 임상시험 결과 예측(CRP) 과제를 도입한다.",
                    "tag": "2"
                },
                {
                    "index": "101-2",
                    "sentence": "In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population.",
                    "sentence_kor": "CRP 프레임워크에서 모델은 PICO 형식의 임상시험 제안서를 배경으로 결과를 예측한다. 즉, 개입 그룹이 연구 모집단에서 측정된 결과의 관점에서 비교 그룹과 비교되는 방법을 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "101-3",
                    "sentence": "While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence.",
                    "sentence_kor": "구조화된 임상 증거는 수동 수집에 엄청나게 비싸지만, 우리는 PICO와 결과를 증거로 암시적으로 포함하는 의학 문헌의 대규모 비정형 문장을 이용한다.",
                    "tag": "3"
                },
                {
                    "index": "101-4",
                    "sentence": "Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets.",
                    "sentence_kor": "특히, 우리는 그러한 암묵적 증거로부터 분리된 결과를 예측하고 다운스트림 데이터 세트의 제한된 데이터로 모델을 미세 조정하도록 모델을 사전 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "101-5",
                    "sentence": "Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1.",
                    "sentence_kor": "벤치마크 증거 통합 데이터 세트에 대한 실험은 제안된 모델이 예를 들어 BioB에 비해 10.7%의 상대적 이득으로 기준선을 크게 능가한다는 것을 보여준다.매크로 F1의 ERT.",
                    "tag": "4"
                },
                {
                    "index": "101-6",
                    "sentence": "Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.",
                    "sentence_kor": "또한 COVID-19와 관련된 임상시험으로 구성된 또 다른 데이터셋에서도 성능 개선이 검증된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "102",
            "abstractID": "EMNLP_abs-102",
            "text": [
                {
                    "index": "102-0",
                    "sentence": "Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians.",
                    "sentence_kor": "임상 예측 모델은 종종 구조화된 변수를 사용하며 임상의가 쉽게 해석할 수 없는 결과를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "102-1",
                    "sentence": "Further, free-text medical notes may contain information not immediately available in structured variables.",
                    "sentence_kor": "또한 자유 텍스트 의료 노트에는 구조화된 변수에서 즉시 사용할 수 없는 정보가 포함될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "102-2",
                    "sentence": "We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively.",
                    "sentence_kor": "패혈증과 사망률 예측에서 각각 0.75와 0.78의 AUROC를 달성하는 해석 가능한 다중 작업 임상 언어 모델로 명시적인 주의를 기울이는 계층적 CNN-변환기 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "102-3",
                    "sentence": "We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis.",
                    "sentence_kor": "또한 투영 가중치 표준 상관 분석을 사용하여 구조화된 변수와 구조화되지 않은 변수에서 학습된 특징 간의 관계를 탐구한다.",
                    "tag": "3"
                },
                {
                    "index": "102-4",
                    "sentence": "Finally, we outline a protocol to evaluate model usability in a clinical decision support context.",
                    "sentence_kor": "마지막으로, 임상 의사 결정 지원 맥락에서 모델 사용성을 평가하기 위한 프로토콜을 개략적으로 설명한다.",
                    "tag": "3"
                },
                {
                    "index": "102-5",
                    "sentence": "From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.",
                    "sentence_kor": "도메인 전문가 평가를 통해, 우리의 모델은 유망한 실제 응용 프로그램을 가진 정보적 합리성을 생성한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "103",
            "abstractID": "EMNLP_abs-103",
            "text": [
                {
                    "index": "103-0",
                    "sentence": "Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing.",
                    "sentence_kor": "본문의 의료 언급을 지식 기반 실체와 연결하는 의료 실체 정규화는 의료 자연어 처리에서 중요한 연구 주제이다.",
                    "tag": "1"
                },
                {
                    "index": "103-1",
                    "sentence": "In this paper, we focus on Chinese medical procedure entity normalization.",
                    "sentence_kor": "본 논문에서 우리는 중국 의료 절차 실체 정상화에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "103-2",
                    "sentence": "However, nonstandard Chinese expressions and combined procedures present challenges in our problem.",
                    "sentence_kor": "그러나 비표준 중국어 표현과 결합된 절차는 우리의 문제에 난제를 제기한다.",
                    "tag": "3"
                },
                {
                    "index": "103-3",
                    "sentence": "The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions.",
                    "sentence_kor": "차별적 모델에 의존하는 기존 전략은 결합된 절차 언급의 정규화에 대처하기 어렵다.",
                    "tag": "3"
                },
                {
                    "index": "103-4",
                    "sentence": "We propose a sequence generative framework to directly generate all the corresponding medical procedure entities.",
                    "sentence_kor": "우리는 모든 해당 의료 절차 실체를 직접 생성하기 위한 시퀀스 생성 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "103-5",
                    "sentence": "we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results.",
                    "sentence_kor": "비현실적인 결과를 피하기 위해 범주 기반 제약 조건 디코딩과 범주 기반 모델 정제라는 두 가지 전략을 채택한다.",
                    "tag": "3"
                },
                {
                    "index": "103-6",
                    "sentence": "The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.",
                    "sentence_kor": "이 방법은 언급에 여러 절차 개념이 포함될 때 실체를 연결할 수 있으며, 포괄적인 실험을 통해 제안된 모델이 기존 기준선에 비해 현저한 개선을 달성할 수 있음을 입증했으며, 특히 다중 복제 중국 의료 절차의 경우 더욱 중요하다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "104",
            "abstractID": "EMNLP_abs-104",
            "text": [
                {
                    "index": "104-0",
                    "sentence": "The extraction of labels from radiology text reports enables large-scale training of medical imaging models.",
                    "sentence_kor": "방사선학 텍스트 보고서에서 레이블을 추출하면 의료 영상 모델의 대규모 교육이 가능하다.",
                    "tag": "1"
                },
                {
                    "index": "104-1",
                    "sentence": "Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts.",
                    "sentence_kor": "보고서 라벨링에 대한 기존 접근방식은 일반적으로 의료 영역 지식을 기반으로 한 정교한 기능 엔지니어링 또는 전문가의 수동 주석에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "104-2",
                    "sentence": "In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations.",
                    "sentence_kor": "본 연구에서는 사용 가능한 규칙 기반 시스템의 규모와 전문가 주석의 품질을 모두 활용하는 의료 이미지 보고서 라벨링에 대한 BERT 기반 접근방식을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "104-3",
                    "sentence": "We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation.",
                    "sentence_kor": "우리는 먼저 규칙 기반 라벨러의 주석에 대해 훈련된 다음 자동 역번역으로 보강된 작은 전문가 주석 세트에서 미세 조정된 생체공학 사전 훈련된 BERT 모델의 우수한 성능을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "104-4",
                    "sentence": "We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.",
                    "sentence_kor": "우리는 최종 모델인 Chexbert가 통계적인 중요성으로 이전의 최고 규칙 기반 라벨러를 능가할 수 있다는 것을 발견하여 흉부 X선 데이터 세트 중 하나에 대한 보고서 라벨링을 위한 새로운 SOTA를 설정했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "105",
            "abstractID": "EMNLP_abs-105",
            "text": [
                {
                    "index": "105-0",
                    "sentence": "Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories.",
                    "sentence_kor": "사건 이해에 대한 전산 및 인지 연구는 일련의 사건들에 대한 구조화된 표현과 그것의 구성요소를 (소프트) 사건 범주로 개념화(추출)하는 것에 달려 있음을 시사한다.",
                    "tag": "1"
                },
                {
                    "index": "105-1",
                    "sentence": "Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”.",
                    "sentence_kor": "따라서, \"자동차 구입\"과 같은 알려진 과정에 대한 지식은 \"주택 구입\"과 같은 새롭고 유사한 과정의 맥락에서 사용될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "105-2",
                    "sentence": "Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction.",
                    "sentence_kor": "그럼에도 불구하고, NLP에서 대부분의 사건 이해 작업은 여전히 현장 수준에 있으며 추상화를 고려하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "105-3",
                    "sentence": "In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes.",
                    "sentence_kor": "본 논문에서, 우리는 프로세스 간의 유사성과 하위 이벤트 인스턴스의 개념화를 활용하여 이전에 보이지 않았던 오픈 도메인 프로세스의 전체 하위 이벤트 시퀀스를 예측하는 유사 프로세스 구조 유도(APSI) 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "105-4",
                    "sentence": "As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.",
                    "sentence_kor": "우리의 실험과 분석에서 알 수 있듯이, APSI는 보이지 않는 프로세스에 대한 의미 있는 하위 이벤트 시퀀스의 생성을 지원하고 누락된 이벤트를 예측하는 데 도움이 될 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "106",
            "abstractID": "EMNLP_abs-106",
            "text": [
                {
                    "index": "106-0",
                    "sentence": "We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner.",
                    "sentence_kor": "우리는 완전한 자기 지도 방식으로 담화 언어 표현을 학습하기 위한 새로운 사전 훈련 목표인 문장 수준 언어 모델링을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "106-1",
                    "sentence": "Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other.",
                    "sentence_kor": "NLP의 최근 사전 훈련 방법은 하위 또는 최상위 언어 표현 학습에 초점을 맞춘다. 한 극단에서 언어 모델 목표로부터 파생된 상황별 단어 표현과 다른 한 극단에서 주어진 두 텍스트 세그먼트의 순서 분류로 학습한 전체 시퀀스 표현이다.",
                    "tag": "1"
                },
                {
                    "index": "106-2",
                    "sentence": "However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them.",
                    "sentence_kor": "그러나 이러한 모델은 문장 및 그 사이의 관계와 같은 자연어로 존재하는 중간 크기 구조의 표현을 포착하도록 직접적으로 권장되지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "106-3",
                    "sentence": "To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering.",
                    "sentence_kor": "이를 위해 입력 문장의 순서를 섞고 원래 순서를 재구성하기 위해 계층적 변압기 모델을 훈련하여 상황별 문장 수준 표현의 학습을 장려하는 새로운 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "106-4",
                    "sentence": "Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.",
                    "sentence_kor": "GLUE, SQuAD 및 DiscoEval과 같은 다운스트림 작업에 대한 실험을 통해 모델의 이 기능이 원래 BERT의 성능을 큰 폭으로 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "107",
            "abstractID": "EMNLP_abs-107",
            "text": [
                {
                    "index": "107-0",
                    "sentence": "Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.",
                    "sentence_kor": "언어 간 NLP 및 다국어 말뭉치 분석에 있어 서로 다른 언어로 전달되는 콘텐츠의 미세한 차이를 감지하는 것은 중요하지만, 주석이 비싸고 확장하기 어렵기 때문에 어려운 기계 학습 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "107-1",
                    "sentence": "This work improves the prediction and annotation of fine-grained semantic divergences.",
                    "sentence_kor": "이 연구는 세분화된 의미 격차의 예측과 주석을 개선한다.",
                    "tag": "2"
                },
                {
                    "index": "107-2",
                    "sentence": "We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity.",
                    "sentence_kor": "우리는 다양한 세분성의 합성 발산 예제의 순위를 매기는 방법을 배워 다국어 BERT 모델에 대한 훈련 전략을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "107-3",
                    "sentence": "We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.",
                    "sentence_kor": "우리는 이 작업과 함께 공개된 새로운 데이터 세트인 Rationalized English-French Semantic Divergences에 대한 모델을 평가한다. 이 데이터 세트는 의미론적 차이 클래스와 토큰 수준 합리성으로 주석을 단 영불 문장 쌍으로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "107-4",
                    "sentence": "Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.",
                    "sentence_kor": "순위를 매기는 학습은 강력한 문장 수준 유사성 모델보다 세분화된 문장 수준 차이를 더 정확하게 감지하는 데 도움이 되는 반면, 토큰 수준 예측은 거친 차이와 세분화된 차이를 더욱 구별할 수 있는 잠재력을 가지고 있다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "108",
            "abstractID": "EMNLP_abs-108",
            "text": [
                {
                    "index": "108-0",
                    "sentence": "Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences.",
                    "sentence_kor": "의미론적 문장 임베딩 모델은 자연어 문장을 벡터로 인코딩하여 임베딩 공간의 폐쇄성이 문장 사이의 의미론적 친밀성을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "108-1",
                    "sentence": "Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific.",
                    "sentence_kor": "이중 언어 데이터는 그러한 임베딩을 학습하는 데 유용한 신호를 제공한다. 번역 쌍에서 두 문장이 공유하는 속성은 의미론일 가능성이 있는 반면, 상이한 속성은 문체적 또는 언어별일 가능성이 있다.",
                    "tag": "1"
                },
                {
                    "index": "108-2",
                    "sentence": "We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors.",
                    "sentence_kor": "우리는 병렬 문장에서 소스 분리를 수행하고, 잠재 의미 벡터에서 공통점을 분리하며, 언어별 잠재 벡터로 남는 것을 설명하는 심층 잠재 변수 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "108-3",
                    "sentence": "Our proposed approach differs from past work on semantic sentence encoding in two ways.",
                    "sentence_kor": "우리가 제안한 접근 방식은 두 가지 면에서 의미론적 문장 인코딩에 대한 과거 연구와는 다르다.",
                    "tag": "3"
                },
                {
                    "index": "108-4",
                    "sentence": "First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time.",
                    "sentence_kor": "첫째, 가변 확률론적 프레임워크를 사용하여 소스 분리를 장려하고 모델의 후부를 사용하여 테스트 시 단일 언어 데이터에 대한 문장 임베딩을 예측할 수 있는 사전 사례를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "108-5",
                    "sentence": "Second, we use high-capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings.",
                    "sentence_kor": "둘째, 대용량 변압기를 데이터 생성 분포와 추론 네트워크로 모두 사용하여 문장 임베딩에 대한 대부분의 과거 작업과 대조적이다.",
                    "tag": "3"
                },
                {
                    "index": "108-6",
                    "sentence": "In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations.",
                    "sentence_kor": "실험에서, 우리의 접근 방식은 감독되지 않은 의미 유사성 평가의 표준 제품군에서 최첨단보다 훨씬 뛰어나다.",
                    "tag": "4"
                },
                {
                    "index": "108-7",
                    "sentence": "Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.",
                    "sentence_kor": "또한, 우리는 우리의 접근 방식이 단순한 단어 중복이 유사성의 좋은 지표가 아닌 이러한 평가의 더 어려운 부분 집합에서 가장 큰 이득을 산출한다는 것을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "109",
            "abstractID": "EMNLP_abs-109",
            "text": [
                {
                    "index": "109-0",
                    "sentence": "Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them.",
                    "sentence_kor": "추상적 의미 표현(AMR)은 노드가 개념이고 가장자리가 그 사이의 관계인 그래프 기반 의미 형식주의이다.",
                    "tag": "1"
                },
                {
                    "index": "109-1",
                    "sentence": "Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence.",
                    "sentence_kor": "대부분의 AMR 구문 분석 방법은 그래프의 노드와 문장의 단어 사이의 정렬을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "109-2",
                    "sentence": "However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages.",
                    "sentence_kor": "그러나 이 정렬은 수동 주석에서는 제공되지 않으며 사용 가능한 자동 정렬기는 다른 언어에서는 제대로 작동하지 않고 영어에만 초점을 맞춥니다.",
                    "tag": "1"
                },
                {
                    "index": "109-3",
                    "sentence": "Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair.",
                    "sentence_kor": "이러한 격차를 해소하기 위해, 우리는 의미론적으로 더 일치하는 단어 개념 쌍을 기반으로 포르투갈어를 위한 정렬 방법을 개발했다.",
                    "tag": "2"
                },
                {
                    "index": "109-4",
                    "sentence": "We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.",
                    "sentence_kor": "우리는 내재적 및 외적 평가를 모두 수행했으며 우리의 정렬 접근 방식이 영어를 위해 개발된 정렬 전략을 능가하여 AMR 파서를 개선하고 포르투갈어를 위해 설계된 파서로 경쟁 결과를 달성한다는 것을 보여주었다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "110",
            "abstractID": "EMNLP_abs-110",
            "text": [
                {
                    "index": "110-0",
                    "sentence": "BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming.",
                    "sentence_kor": "BERT는 시간이 많이 걸리는 많은 문장 쌍을 조합하여 평가해야 하므로 클러스터링이나 의미 검색과 같은 문장 쌍 작업에 비효율적이다.",
                    "tag": "1"
                },
                {
                    "index": "110-1",
                    "sentence": "Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed.",
                    "sentence_kor": "문장 BERT(SBERT)는 유사성 비교에 쉽게 접근할 수 있도록 의미론적으로 의미 있는 단일 문장의 표현을 학습하여 이 문제를 해결하려고 시도했다.",
                    "tag": "1"
                },
                {
                    "index": "110-2",
                    "sentence": "However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce.",
                    "sentence_kor": "그러나 SBERT는 고품질 라벨 문장 쌍이 있는 말뭉치에 대해 훈련되며, 이는 라벨링 데이터가 극히 부족한 작업으로 적용을 제한한다.",
                    "tag": "1"
                },
                {
                    "index": "110-3",
                    "sentence": "In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner.",
                    "sentence_kor": "본 논문에서 우리는 BERT 위에 경량 확장을 제안하고 상호 정보 최대화 전략에 기초한 새로운 자체 감독 학습 목표를 제안하여 비지도 방식으로 의미 있는 문장 임베딩을 도출한다.",
                    "tag": "2+3"
                },
                {
                    "index": "110-4",
                    "sentence": "Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus.",
                    "sentence_kor": "SBERT와 달리, 우리의 방법은 다른 도메인별 말뭉치에 적용될 수 있도록 라벨링된 데이터의 가용성에 의해 제한되지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "110-5",
                    "sentence": "Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks.",
                    "sentence_kor": "실험 결과에 따르면 제안된 방법은 공통 의미론적 텍스트 유사성(STS) 작업과 다운스트림 감독 작업에서 다른 비지도 문장 임베딩 기준선을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "110-6",
                    "sentence": "It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.",
                    "sentence_kor": "또한 도메인 내 라벨링된 데이터를 사용할 수 없는 설정에서 SBERT를 능가하고 다양한 작업에 대한 감독 방법을 사용하여 성능 경쟁력을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "111",
            "abstractID": "EMNLP_abs-111",
            "text": [
                {
                    "index": "111-0",
                    "sentence": "Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition.",
                    "sentence_kor": "구문 정렬은 의역 및 텍스트 수반 인식과 같은 문장 쌍 상호 작용을 모델링하기 위한 기초이다.",
                    "tag": "1"
                },
                {
                    "index": "111-1",
                    "sentence": "Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases.",
                    "sentence_kor": "대부분의 구문 정렬은 구문 쌍의 정렬이 하위 구문의 정렬을 기반으로 구성되는 구성 프로세스입니다.",
                    "tag": "1"
                },
                {
                    "index": "111-2",
                    "sentence": "Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice.",
                    "sentence_kor": "그럼에도 불구하고, 연구는 장거리 문구 재정렬과 관련된 비구성적 정렬이 실제로 널리 퍼져있다는 것을 밝혀냈다.",
                    "tag": "1"
                },
                {
                    "index": "111-3",
                    "sentence": "We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations.",
                    "sentence_kor": "문장의 유사성 분포를 강력한 상황별 표현에 명시적으로 내장하는 순서 없는 트리 매핑 알고리즘과 구문 표현 모델링을 결합하여 구문 정렬 문제를 해결한다.",
                    "tag": "2+3"
                },
                {
                    "index": "111-4",
                    "sentence": "Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments.",
                    "sentence_kor": "실험 결과는 우리의 방법이 구성 및 비구성 전역 구문 정렬을 효과적으로 처리한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "111-5",
                    "sentence": "Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.",
                    "sentence_kor": "우리의 방법은 이전 연구에서 사용한 방법을 크게 능가하고 숙련된 인간 주석자의 방법과 비교할 수 있는 성능을 달성한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "112",
            "abstractID": "EMNLP_abs-112",
            "text": [
                {
                    "index": "112-0",
                    "sentence": "Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning.",
                    "sentence_kor": "표와 같은 반구조적 증거에서 사실을 검증하려면 구조 정보를 인코딩하고 기호적 추론을 수행할 수 있는 능력이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "112-1",
                    "sentence": "Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information.",
                    "sentence_kor": "테이블을 시퀀스로 선형화하는 것만으로도 셀 정렬 정보가 손실되기 때문에 자연어에 대해 훈련된 언어 모델은 테이블을 인코딩하는 데 직접 적용할 수 없었다.",
                    "tag": "1"
                },
                {
                    "index": "112-2",
                    "sentence": "To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer.",
                    "sentence_kor": "사전 훈련된 변압기를 테이블 표현에 더 잘 활용하기 위해, 우리는 테이블 구조 정보를 셀프 어텐션 레이어의 마스크에 주입하는 구조 인식 변압기(SAT)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "112-3",
                    "sentence": "A method to combine symbolic and linguistic reasoning is also explored for this task.",
                    "sentence_kor": "이 작업을 위해 상징적 추론과 언어적 추론을 결합하는 방법도 탐구된다.",
                    "tag": "3"
                },
                {
                    "index": "112-4",
                    "sentence": "Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.",
                    "sentence_kor": "우리의 방법은 대규모 테이블 검증 데이터 세트인 TabFact에서 4.93%로 기준선을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "113",
            "abstractID": "EMNLP_abs-113",
            "text": [
                {
                    "index": "113-0",
                    "sentence": "Document-level relation extraction aims to extract relations among entities within a document.",
                    "sentence_kor": "문서 수준 관계 추출은 문서 내 엔티티 간의 관계를 추출하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "113-1",
                    "sentence": "Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs.",
                    "sentence_kor": "문장 수준 관계 추출과는 달리, 단락에 걸쳐 여러 문장에 대한 추론이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "113-2",
                    "sentence": "In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs.",
                    "sentence_kor": "본 논문에서 우리는 긴 문단에 대해 그러한 관계를 인식하는 방법인 그래프 집계 및 참조 네트워크(GAIN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "113-3",
                    "sentence": "GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG).",
                    "sentence_kor": "GAIN은 이종 언급 수준 그래프(MG)와 엔티티 수준 그래프(EG)의 두 그래프를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "113-4",
                    "sentence": "The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities.",
                    "sentence_kor": "전자는 서로 다른 언급들 사이의 복잡한 상호작용을 포착하고 후자는 동일한 실체에 기초하는 언급을 집계한다.",
                    "tag": "3"
                },
                {
                    "index": "113-5",
                    "sentence": "Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities.",
                    "sentence_kor": "그래프를 바탕으로 개체 간의 관계를 추론하기 위한 새로운 경로 추론 메커니즘을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "113-6",
                    "sentence": "Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art.",
                    "sentence_kor": "공개 데이터 세트 DocRED에 대한 실험 결과 GAIN은 이전 최신 기술에 비해 성능이 크게 향상되었다(F1의 경우 2.85).",
                    "tag": "4"
                },
                {
                    "index": "113-7",
                    "sentence": "Our code is available at https://github.com/PKUnlp-icler/GAIN.",
                    "sentence_kor": "우리의 코드는 https://github.com/PKUnlp-icler/GAIN에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "114",
            "abstractID": "EMNLP_abs-114",
            "text": [
                {
                    "index": "114-0",
                    "sentence": "Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts.",
                    "sentence_kor": "이벤트 추출(EE)은 텍스트의 이벤트 정보를 추출하는 것을 목표로 하는 중요한 정보 추출 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "114-1",
                    "sentence": "Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem.",
                    "sentence_kor": "EE에 대한 이전 방법은 일반적으로 분류 작업으로 모델링되며, 일반적으로 데이터 부족 문제가 발생하기 쉽다.",
                    "tag": "1"
                },
                {
                    "index": "114-2",
                    "sentence": "In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC).",
                    "sentence_kor": "본 논문에서 우리는 EE를 기계 판독 이해 문제(MRC)로 명시적으로 캐스팅하여 EE의 새로운 학습 패러다임을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "114-3",
                    "sentence": "Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results.",
                    "sentence_kor": "우리의 접근 방식에는 이벤트 스키마를 일련의 자연스러운 질문으로 전송할 수 있는 감독되지 않은 질문 생성 프로세스가 포함되어 있으며, 이어 EE 결과로 답변을 검색하기 위한 BERT 기반 질문 답변 프로세스가 뒤따른다.",
                    "tag": "1+2"
                },
                {
                    "index": "114-4",
                    "sentence": "This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC.",
                    "sentence_kor": "이 학습 패러다임은 MRC에 정교한 모델을 도입하여 EE의 추론 프로세스를 강화하고 MRC에 대규모 데이터 세트를 도입하여 데이터 부족 문제를 완화할 수 있도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "114-5",
                    "sentence": "The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods.",
                    "sentence_kor": "경험적 결과는 i) 우리의 접근 방식이 이전 방법에 비해 상당한 여유로 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "114-6",
                    "sentence": "ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method.",
                    "sentence_kor": "ii) 우리 모델은 예를 들어 이전 방법의 2.2%와 비교하여 1%의 데이터만으로 사건 인수 추출을 위해 F1에서 49.8%를 얻는 등 데이터 부족 시나리오에서 탁월하다.",
                    "tag": "4+5"
                },
                {
                    "index": "114-7",
                    "sentence": "iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.",
                    "sentence_kor": "iii) 당사의 모델은 또한 제로샷 시나리오에도 적합하여 EE 교육 데이터를 사용하지 않고 두 데이터 세트에서 F1의 37.0%와 16%를 달성합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "115",
            "abstractID": "EMNLP_abs-115",
            "text": [
                {
                    "index": "115-0",
                    "sentence": "Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text.",
                    "sentence_kor": "이벤트 트리거 단어를 식별하고 이벤트 유형을 분류하는 것을 의미하는 이벤트 감지(ED)는 일반 텍스트에서 이벤트 지식을 추출하기 위한 첫 번째이자 가장 기본적인 단계이다.",
                    "tag": "1"
                },
                {
                    "index": "115-1",
                    "sentence": "Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity.",
                    "sentence_kor": "대부분의 기존 데이터 세트에는 ED의 추가 개발을 제한하는 다음과 같은 문제가 있습니다. (1) 데이터 부족.",
                    "tag": "1"
                },
                {
                    "index": "115-2",
                    "sentence": "Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods.",
                    "sentence_kor": "기존의 소규모 데이터 세트는 점점 더 정교해지는 현대 신경 방법을 훈련하고 안정적으로 벤치마킹하기에 충분하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "115-3",
                    "sentence": "(2) Low coverage.",
                    "sentence_kor": "(2) 커버리지가 낮다.",
                    "tag": "1"
                },
                {
                    "index": "115-4",
                    "sentence": "Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models.",
                    "sentence_kor": "기존 데이터 세트의 제한된 이벤트 유형은 일반 도메인 이벤트를 잘 처리할 수 없어 ED 모델의 응용 프로그램이 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "115-5",
                    "sentence": "To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types.",
                    "sentence_kor": "이러한 문제를 완화하기 위해 4,480개의 위키피디아 문서, 118,732개의 이벤트 언급 인스턴스 및 168개의 이벤트 유형을 포함하는 MAVENT 탐지 데이터 세트(MAVENT Detection Detection Detaset)를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "115-6",
                    "sentence": "MAVEN alleviates the data scarcity problem and covers much more general event types.",
                    "sentence_kor": "MAVEN은 데이터 부족 문제를 완화하고 훨씬 더 일반적인 이벤트 유형을 다룹니다.",
                    "tag": "3"
                },
                {
                    "index": "115-7",
                    "sentence": "We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN.",
                    "sentence_kor": "우리는 최신 ED 모델을 재현하고 MAVEN에 대한 철저한 평가를 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "115-8",
                    "sentence": "The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts.",
                    "sentence_kor": "실험 결과는 기존 ED 방법이 소규모 데이터 세트처럼 MAVEN에서 유망한 결과를 얻을 수 없다는 것을 보여주며, 이는 현실 세계의 ED가 여전히 어려운 과제이며 추가 연구 노력이 필요하다는 것을 시사한다.",
                    "tag": "4+5"
                },
                {
                    "index": "115-9",
                    "sentence": "We also discuss further directions for general domain ED with empirical analyses.",
                    "sentence_kor": "또한 경험적 분석을 통해 일반 영역 ED에 대한 추가 방향을 논의한다.",
                    "tag": "5"
                },
                {
                    "index": "115-10",
                    "sentence": "The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.",
                    "sentence_kor": "소스 코드와 데이터 세트는 https://github.com/THU-KEG/MAVEN-dataset에서 얻을 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "116",
            "abstractID": "EMNLP_abs-116",
            "text": [
                {
                    "index": "116-0",
                    "sentence": "Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration.",
                    "sentence_kor": "지식 그래프(KG) 정렬은 지식 융합과 통합에 중요한 서로 다른 KG의 엔티티를 일치시키는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "116-1",
                    "sentence": "Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results.",
                    "sentence_kor": "최근 KG 정렬에 대한 많은 임베딩 기반 접근법이 제안되어 유망한 결과를 얻었다.",
                    "tag": "1"
                },
                {
                    "index": "116-2",
                    "sentence": "These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations.",
                    "sentence_kor": "이러한 접근법은 먼저 실체를 저차원 벡터 공간에 임베드한 다음 벡터 표현에 대한 계산을 통해 실체 정렬을 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "116-3",
                    "sentence": "Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory.",
                    "sentence_kor": "최근 연구에서 지속적인 개선이 이루어졌지만 기존 접근 방식의 성과는 여전히 만족스럽지 못하다.",
                    "tag": "1"
                },
                {
                    "index": "116-4",
                    "sentence": "In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment.",
                    "sentence_kor": "본 연구에서는 KG 정렬을 위한 개체 쌍 임베딩을 직접 학습하는 새로운 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "116-5",
                    "sentence": "Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities.",
                    "sentence_kor": "우리의 접근 방식은 먼저 노드가 개체 쌍이고 가장자리가 관계 쌍에 해당하는 두 KG의 쌍별 연결 그래프(PCG)를 생성한 다음 PCG의 노드(엔티 쌍) 임베딩을 학습하며, 이는 개체의 동등한 관계를 예측하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "116-6",
                    "sentence": "To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs.",
                    "sentence_kor": "바람직한 임베딩을 얻기 위해, 컨볼루션 신경망을 사용하여 속성에서 개체 쌍의 유사성 특징을 생성하고, 그래프 신경망을 사용하여 유사성 특징을 전파하고 개체 쌍의 최종 임베딩을 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "116-7",
                    "sentence": "Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.",
                    "sentence_kor": "5개의 실제 데이터 세트에 대한 실험을 통해 우리의 접근 방식이 최첨단 KG 정렬 결과를 달성할 수 있음을 알 수 있다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "117",
            "abstractID": "EMNLP_abs-117",
            "text": [
                {
                    "index": "117-0",
                    "sentence": "Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs.",
                    "sentence_kor": "퓨샷 지식 그래프(KG) 완성은 현재 연구의 초점으로, 각 과제는 퓨샷 기준 개체 쌍이 주어진 관계의 보이지 않는 사실을 쿼리하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "117-1",
                    "sentence": "Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries.",
                    "sentence_kor": "최근의 시도는 개체와 참조의 동적 특성을 무시하고 정적 표현을 학습함으로써 이 문제를 해결한다. 즉, 개체는 작업 관계 내에서 다양한 역할을 나타낼 수 있으며 참조는 쿼리에 서로 다른 기여를 할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "117-2",
                    "sentence": "This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations.",
                    "sentence_kor": "본 연구는 적응형 실체와 참조 표현을 학습하여 퓨샷 KG 완료를 위한 적응형 주의 네트워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "117-3",
                    "sentence": "Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions.",
                    "sentence_kor": "특히 엔티티는 작업 지향 역할을 식별하기 위해 적응형 이웃 인코더에 의해 모델링되는 반면 참조는 기여도를 차별화하기 위해 적응형 쿼리 인식 집계기에 의해 모델링된다.",
                    "tag": "3"
                },
                {
                    "index": "117-4",
                    "sentence": "Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations.",
                    "sentence_kor": "주의 메커니즘을 통해 개체와 참조 모두 세분화된 의미 의미를 포착할 수 있으므로 더 표현력이 높은 표현을 제공할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "117-5",
                    "sentence": "This will be more predictive for knowledge acquisition in the few-shot scenario.",
                    "sentence_kor": "이는 퓨샷 시나리오에서 지식 습득에 대해 보다 예측이 가능합니다.",
                    "tag": "3"
                },
                {
                    "index": "117-6",
                    "sentence": "Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes.",
                    "sentence_kor": "두 개의 공개 데이터 세트에 대한 링크 예측 평가를 통해 우리의 접근 방식이 다양한 퓨샷 크기로 새로운 최첨단 결과를 달성한다는 것을 알 수 있다.",
                    "tag": "4+5"
                },
                {
                    "index": "117-7",
                    "sentence": "The source code is available at https://github.com/JiaweiSheng/FAAN.",
                    "sentence_kor": "소스 코드는 https://github.com/JiaweiSheng/FAAN에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "118",
            "abstractID": "EMNLP_abs-118",
            "text": [
                {
                    "index": "118-0",
                    "sentence": "In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task.",
                    "sentence_kor": "본 논문에서는 개체 관계 추출 작업을 위해 사전 훈련된 인코더에 스팬 관련 정보를 통합한다.",
                    "tag": "1+2"
                },
                {
                    "index": "118-1",
                    "sentence": "Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model.",
                    "sentence_kor": "범용 문장 인코더(예: 기존의 범용 사전 교육 모델)를 사용하는 대신 스팬 인코더와 스팬 쌍 인코더를 사전 교육 네트워크에 도입하여 사전 교육된 모델로 스팬 내 및 스팬 간 정보를 더 쉽게 가져올 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "118-2",
                    "sentence": "To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs.",
                    "sentence_kor": "인코더를 배우기 위해 토큰, 스팬 및 스팬 쌍을 대상으로 하는 다양한 관점에서 세 가지 맞춤형 사전 교육 목표를 고안한다.",
                    "tag": "3"
                },
                {
                    "index": "118-3",
                    "sentence": "In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss.",
                    "sentence_kor": "특히 스팬 인코더는 한 스팬의 토큰 셔플링을 복구하도록 훈련되며, 스팬 쌍 인코더는 대조 손실을 사용하여 동일한 문장의 양성 쌍과 다른 문장의 음수 쌍을 예측하도록 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "118-4",
                    "sentence": "Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).",
                    "sentence_kor": "실험 결과에 따르면 제안된 사전 훈련 방법은 멀리서 감독되는 사전 훈련을 능가하고 두 개의 개체 관계 추출 벤치마크 데이터 세트(ACE05, SCIERC)에서 유망한 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "119",
            "abstractID": "EMNLP_abs-119",
            "text": [
                {
                    "index": "119-0",
                    "sentence": "Named entity recognition and relation extraction are two important fundamental problems.",
                    "sentence_kor": "명명된 개체 인식과 관계 추출은 두 가지 중요한 근본적인 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "119-1",
                    "sentence": "Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem.",
                    "sentence_kor": "두 과제를 동시에 해결하기 위해 공동 학습 알고리즘이 제안되었으며, 이들 중 다수는 공동 과제를 테이블 채우기 문제로 삼았다.",
                    "tag": "1"
                },
                {
                    "index": "119-2",
                    "sentence": "However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space.",
                    "sentence_kor": "그러나 일반적으로 동일한 공간 내에서 두 작업에 필요한 정보를 캡처하기 위해 단일 인코더(일반적으로 표의 형태로 표현 학습)를 학습하는 데 초점을 맞췄다.",
                    "tag": "1"
                },
                {
                    "index": "119-3",
                    "sentence": "We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.",
                    "sentence_kor": "우리는 학습 과정에서 서로 다른 두 가지 유형의 정보를 캡처하도록 두 개의 별개의 인코더를 설계하는 것이 유익할 수 있다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "119-4",
                    "sentence": "In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.",
                    "sentence_kor": "본 연구에서는 표 인코더와 시퀀스 인코더라는 두 개의 서로 다른 인코더가 표현 학습 프로세스에서 서로를 돕도록 설계된 새로운 표 시퀀스 인코더를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "119-5",
                    "sentence": "Our experiments confirm the advantages of having two encoders over one encoder.",
                    "sentence_kor": "우리의 실험은 하나의 인코더에 비해 두 개의 인코더가 있다는 장점을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "119-6",
                    "sentence": "On several standard datasets, our model shows significant improvements over existing approaches.",
                    "sentence_kor": "여러 표준 데이터 세트에서 우리 모델은 기존 접근 방식에 비해 상당히 개선되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "120",
            "abstractID": "EMNLP_abs-120",
            "text": [
                {
                    "index": "120-0",
                    "sentence": "Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past.",
                    "sentence_kor": "문서의 언어 모델에서 쿼리를 생성하는 작업으로 문서 순위를 보는 정보 검색용 생성 모델은 과거에 다양한 IR 작업에서 매우 성공적이었다.",
                    "tag": "1"
                },
                {
                    "index": "120-1",
                    "sentence": "However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead.",
                    "sentence_kor": "그러나 현대의 심층 신경망의 출현으로 관심은 문서와 쿼리의 의미적 유사성을 대신 모델링하는 차별적 순위 함수로 옮겨갔다.",
                    "tag": "1"
                },
                {
                    "index": "120-2",
                    "sentence": "Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet.",
                    "sentence_kor": "최근 GPT2와 BART와 같은 심층 생성 모델은 우수한 텍스트 생성기로 나타났지만 랭커로서의 효과는 아직 입증되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "120-3",
                    "sentence": "In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task.",
                    "sentence_kor": "본 연구에서, 우리는 정보 검색을 위한 생성 프레임워크를 다시 검토하고 우리의 생성 접근법이 답변 선택 작업에 대한 최첨단 의미 유사성 기반 차별 모델만큼 효과적이라는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "120-4",
                    "sentence": "Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.",
                    "sentence_kor": "또한, 우리는 IR에 대한 비우도 손실의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "121",
            "abstractID": "EMNLP_abs-121",
            "text": [
                {
                    "index": "121-0",
                    "sentence": "Topic models are a useful analysis tool to uncover the underlying themes within document collections.",
                    "sentence_kor": "주제 모델은 문서 모음 내의 기본 테마를 파악하는 데 유용한 분석 도구입니다.",
                    "tag": "1"
                },
                {
                    "index": "121-1",
                    "sentence": "The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words.",
                    "sentence_kor": "지배적인 접근법은 생성적 스토리를 가정하는 확률론적 주제 모델을 사용하는 것이지만, 본 논문에서 우리는 주제를 얻기 위한 대안적인 방법을 제안한다. 즉, 가중 군집화를 위한 문서 정보를 통합하고 상위 단어의 순위를 재조정하는 동시에 사전 훈련된 단어 임베딩을 클러스터링하는 것이다.",
                    "tag": "1+2"
                },
                {
                    "index": "121-2",
                    "sentence": "We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA.",
                    "sentence_kor": "우리는 다양한 단어 임베딩과 클러스터링 알고리즘의 조합에 대한 벤치마크를 제공하고 PCA를 통한 차원 감소에 따른 성능을 분석한다.",
                    "tag": "3"
                },
                {
                    "index": "121-3",
                    "sentence": "The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.",
                    "sentence_kor": "우리의 접근 방식을 위한 최상의 성능 조합은 고전적인 주제 모델만큼 성능이 뛰어나지만 런타임과 계산 복잡성은 낮다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "122",
            "abstractID": "EMNLP_abs-122",
            "text": [
                {
                    "index": "122-0",
                    "sentence": "While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS).",
                    "sentence_kor": "신경 시퀀스 학습 방법은 단일 문서 요약(SDS)에서 상당한 진전을 이루었지만 다중 문서 요약(MDS)에서는 만족스럽지 못한 결과를 산출한다.",
                    "tag": "1"
                },
                {
                    "index": "122-1",
                    "sentence": "We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle.",
                    "sentence_kor": "우리는 SDS 진보를 MDS에 적용할 때 (1) MDS는 더 큰 검색 공간과 더 제한된 훈련 데이터를 포함하며, 적절한 표현을 학습하기 위한 신경 방법의 장애물을 설정한다. (2) MDS는 소스 문서 간의 정보 중복성을 해결해야 하는데, 이는 SDS 방법이 처리하기에 덜 효과적이다.",
                    "tag": "1"
                },
                {
                    "index": "122-2",
                    "sentence": "To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.",
                    "sentence_kor": "격차를 줄이기 위해 기존 MDS에 사용된 고급 신경 SDS 방법과 통계 측정을 통합하는 RL-MMR, MDS에 대한 최대 여유 관련성 유도 강화 학습을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "122-3",
                    "sentence": "RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning.",
                    "sentence_kor": "RL-MMR은 더 적은 수의 유망한 후보자에게 MMR 지침을 부여하여 검색 공간을 제한하고 따라서 더 나은 표현 학습을 유도한다.",
                    "tag": "4"
                },
                {
                    "index": "122-4",
                    "sentence": "Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy.",
                    "sentence_kor": "또한 MMR의 명시적 중복성 측정은 요약을 신경적으로 표현하여 중복성을 더 잘 포착할 수 있도록 돕는다.",
                    "tag": "4"
                },
                {
                    "index": "122-5",
                    "sentence": "Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets.",
                    "sentence_kor": "광범위한 실험에 따르면 RL-MMR은 벤치마크 MDS 데이터 세트에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "122-6",
                    "sentence": "In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.",
                    "sentence_kor": "특히 학습 효과와 효율성 측면에서 SDS를 MDS에 적용할 때 MMR을 엔드 투 엔드 학습에 통합하는 이점을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "123",
            "abstractID": "EMNLP_abs-123",
            "text": [
                {
                    "index": "123-0",
                    "sentence": "Topic models are often used to identify human-interpretable topics to help make sense of large document collections.",
                    "sentence_kor": "주제 모델은 대규모 문서 모음을 이해하기 위해 사람이 해석할 수 있는 주제를 식별하는 데 자주 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "123-1",
                    "sentence": "We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers.",
                    "sentence_kor": "우리는 지식 증류를 사용하여 확률론적 주제 모델과 사전 훈련된 변압기의 최상의 속성을 결합한다.",
                    "tag": "2+3"
                },
                {
                    "index": "123-2",
                    "sentence": "Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence.",
                    "sentence_kor": "우리의 모듈식 방법은 주제 품질을 개선하기 위해 모든 신경 주제 모델에 직접적으로 적용될 수 있으며, 우리는 서로 다른 구조를 가진 두 모델을 사용하여 최첨단 주제 일관성을 얻는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "123-3",
                    "sentence": "We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.",
                    "sentence_kor": "우리는 우리의 적응형 프레임워크가 일반적으로 보고되는 바와 같이 모든 추정 주제에 걸쳐 총체적인 성능을 향상시킬 뿐만 아니라 정렬된 주제에 대한 정면 비교에서도 향상된다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "124",
            "abstractID": "EMNLP_abs-124",
            "text": [
                {
                    "index": "124-0",
                    "sentence": "Topic models have been prevailing for many years on discovering latent semantics while modeling long documents.",
                    "sentence_kor": "토픽 모델은 긴 문서를 모델링하면서 잠재 의미론을 발견하는 데 수 년 동안 널리 사용되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "124-1",
                    "sentence": "However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality.",
                    "sentence_kor": "그러나 짧은 텍스트의 경우 단어 동시 발생이 극도로 제한되므로 품질이 낮은 반복적이거나 사소한 주제를 생성하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "124-2",
                    "sentence": "In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts.",
                    "sentence_kor": "본 논문에서는 이 문제를 해결하기 위해 짧은 텍스트 모델링에 더 적합한 피크 분포를 생성하는 새로운 주제 분포 양자화 접근법으로 자동 인코딩 프레임워크에서 새로운 신경 주제 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "124-3",
                    "sentence": "Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics.",
                    "sentence_kor": "인코딩 외에도 디코딩 측면에서 이 문제를 해결하기 위해 반복적인 주제를 산출하지 않도록 음성 샘플에서 학습하는 새로운 음성 샘플링 디코더를 추가로 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "124-4",
                    "sentence": "We observe that our model can highly improve short text topic modeling performance.",
                    "sentence_kor": "우리는 우리 모델이 짧은 텍스트 주제 모델링 성능을 크게 향상시킬 수 있다는 것을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "124-5",
                    "sentence": "Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.",
                    "sentence_kor": "실제 데이터 세트에 대한 광범위한 실험을 통해, 우리는 우리의 모델이 극단적인 데이터 희소성 장면에서 강력한 기존 기준선과 신경 기준선을 모두 능가하여 고품질 주제를 생성할 수 있음을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "125",
            "abstractID": "EMNLP_abs-125",
            "text": [
                {
                    "index": "125-0",
                    "sentence": "Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval.",
                    "sentence_kor": "웹 상의 오픈 도메인 키프레이즈 추출(KPE)은 정보 검색 분야에서 광범위한 실제 응용을 가진 근본적이지만 복잡한 NLP 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "125-1",
                    "sentence": "In contrast to other document types, web page designs are intended for easy navigation and information finding.",
                    "sentence_kor": "다른 문서 유형과 달리 웹 페이지 설계는 탐색 및 정보 찾기가 용이합니다.",
                    "tag": "1"
                },
                {
                    "index": "125-2",
                    "sentence": "Effective designs encode within the layout and formatting signals that point to where the important information can be found.",
                    "sentence_kor": "효과적인 설계는 중요한 정보를 찾을 수 있는 위치를 가리키는 레이아웃 및 형식 지정 신호 내에서 인코딩됩니다.",
                    "tag": "1"
                },
                {
                    "index": "125-3",
                    "sentence": "In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task.",
                    "sentence_kor": "본 연구에서는 이러한 다중 모달 신호를 활용하여 KPE 작업을 지원하는 모델링 접근 방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "125-4",
                    "sentence": "In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection.",
                    "sentence_kor": "특히, 마이크로 레벨에서 어휘 및 시각적 특징(예: 크기, 글꼴, 위치)을 모두 활용하여 효과적인 전략 유도 및 매크로 레벨에서 페이지를 기술하여 전략 선택을 돕는다.",
                    "tag": "3"
                },
                {
                    "index": "125-5",
                    "sentence": "Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models.",
                    "sentence_kor": "우리의 평가는 KPE 작업에 대한 이 접근법 내에서 효과적인 전략 유도 및 전략 선택의 조합이 최첨단 모델을 능가한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "125-6",
                    "sentence": "A qualitative post-hoc analysis illustrates how these features function within the model.",
                    "sentence_kor": "정성적 사후 분석은 모델 내에서 이러한 특징들이 어떻게 작동하는지 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "126",
            "abstractID": "EMNLP_abs-126",
            "text": [
                {
                    "index": "126-0",
                    "sentence": "Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice.",
                    "sentence_kor": "불용성 탐지에 대한 대부분의 기존 접근법은 실제로 얻는 데 비용이 많이 드는 인간 주석 말뭉치에 크게 의존하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "126-1",
                    "sentence": "There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora.",
                    "sentence_kor": "예를 들어, 자체 지도 학습 기법으로 이 문제를 완화하기 위한 몇 가지 제안이 있었지만, 여전히 인간 주석 말뭉치가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "126-2",
                    "sentence": "In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain.",
                    "sentence_kor": "이 작업에서는 더 저렴하고 얻기 쉬운 라벨이 부착되지 않은 텍스트 말뭉치와 잠재적으로 작동할 수 있는 비지도 학습 패러다임을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "126-3",
                    "sentence": "Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training.",
                    "sentence_kor": "우리 모델은 자체 훈련 아이디어를 확장하는 준지도 학습 접근 방식인 소음 학생 훈련에 대한 최근 연구를 기반으로 한다.",
                    "tag": "3"
                },
                {
                    "index": "126-4",
                    "sentence": "Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).",
                    "sentence_kor": "일반적으로 사용되는 영어 스위치보드 테스트 세트에 대한 실험 결과는 우리의 접근 방식이 상황에 맞는 단어 임베딩(예: BERT 및 ELCTRA)을 사용하는 이전의 최첨단 감독 시스템과 비교하여 경쟁력 있는 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "127",
            "abstractID": "EMNLP_abs-127",
            "text": [
                {
                    "index": "127-0",
                    "sentence": "The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language.",
                    "sentence_kor": "인간의 언어는 목소리의 톤, 얼굴의 제스처, 그리고 구어를 포함하여, 양식이라고 알려진 여러 정보 소스를 통해 표현될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "127-1",
                    "sentence": "Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability.",
                    "sentence_kor": "감정 분석 및 감정 인식과 같은 인간 중심 과제에서 강력한 성과를 보이는 최근의 다중 모드 학습은 해석성이 매우 제한적인 블랙박스인 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "127-2",
                    "sentence": "In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample.",
                    "sentence_kor": "본 논문에서 우리는 입력 양식과 출력 표현 사이의 가중치를 각 입력 샘플에 대해 다르게 동적으로 조정하는 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "127-3",
                    "sentence": "Multimodal routing can identify relative importance of both individual modalities and cross-modality factors.",
                    "sentence_kor": "다중 모드 라우팅은 개별 양식과 교차 양식 요소 모두의 상대적 중요성을 식별할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "127-4",
                    "sentence": "Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.",
                    "sentence_kor": "또한 라우팅에 의한 가중치 할당은 첨단 방법과 비교하여 경쟁력 있는 성능을 유지하면서 각 단일 입력 샘플에 대해 전역적으로(즉, 전체 데이터 세트에 대한 일반적인 추세) 양식 예측 관계를 해석할 수 있게 해준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "128",
            "abstractID": "EMNLP_abs-128",
            "text": [
                {
                    "index": "128-0",
                    "sentence": "Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript).",
                    "sentence_kor": "오픈 도메인 비디오에 대한 멀티모달 요약은 멀티 소스 정보(비디오, 오디오, 스크립트)에서 요약을 생성하는 것을 목표로 하는 새로운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "128-1",
                    "sentence": "Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs.",
                    "sentence_kor": "이 작업에 대한 최근의 멀티엔코더-디코더 프레임워크의 성공에도 불구하고, 기존 방법은 멀티소스 입력의 세분화된 다중 모드 상호 작용이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "128-2",
                    "sentence": "Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise.",
                    "sentence_kor": "또한 다른 다중 모드 작업과 달리 이 작업은 중복성과 노이즈가 더 많은 다중 모드 시퀀스가 더 길다.",
                    "tag": "1"
                },
                {
                    "index": "128-3",
                    "sentence": "To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module.",
                    "sentence_kor": "이 두 가지 문제를 해결하기 위해, 우리는 다단계 융합 스키마를 통해 양식 간의 미세한 상호 작용을 모델링하고 망각 모듈을 통해 다중 모드 긴 시퀀스 간의 중복 정보 흐름을 제어함으로써 이 접근 방식을 기반으로 하는 융합 망각 게이트 모듈을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "128-4",
                    "sentence": "Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance.",
                    "sentence_kor": "How2 데이터 세트에 대한 실험 결과는 제안된 모델이 새로운 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "128-5",
                    "sentence": "Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures.",
                    "sentence_kor": "종합적인 분석은 여러 인코더-디코더 아키텍처에서 퓨전 스키마와 망각 모듈의 효과를 경험적으로 검증한다.",
                    "tag": "4"
                },
                {
                    "index": "128-6",
                    "sentence": "Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.",
                    "sentence_kor": "특히, 높은 소음 ASR 스크립트(WER>30%)를 사용할 때, 우리 모델은 여전히 실제 실제 스크립트 모델에 가까운 성능을 달성하여 수동 주석 비용을 절감한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "129",
            "abstractID": "EMNLP_abs-129",
            "text": [
                {
                    "index": "129-0",
                    "sentence": "Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns.",
                    "sentence_kor": "비디오 기반 대화는 (i) 공간적 및 시간적 변화를 모두 포함하는 비디오의 복잡성 및 (ii) 다중 대화 턴을 통해 비디오의 다른 세그먼트 및/또는 다른 개체를 쿼리하는 사용자 발언의 복잡성 때문에 매우 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "129-1",
                    "sentence": "However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos.",
                    "sentence_kor": "그러나 비디오 기반 대화에 대한 기존의 접근 방식은 종종 피상적인 시간적 수준의 시각적 단서에 초점을 맞추지만 비디오의 보다 세분화된 공간 신호는 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "129-2",
                    "sentence": "To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues.",
                    "sentence_kor": "이 단점을 해결하기 위해 텍스트 단서를 기반으로 비디오의 고해상도 쿼리를 위한 비전 언어 신경 프레임워크인 양방향 시공간 학습(BiST)을 제안했다.",
                    "tag": "2"
                },
                {
                    "index": "129-3",
                    "sentence": "Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning.",
                    "sentence_kor": "특히, 우리의 접근 방식은 공간 및 시간 수준 정보를 모두 활용할 뿐만 아니라 공간 대 시간 및 시간 대 공간 추론을 통해 두 특징 공간 사이의 동적 정보 확산을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "129-4",
                    "sentence": "The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting.",
                    "sentence_kor": "양방향 전략은 대화 설정에서 사용자 쿼리의 진화하는 의미론을 다루는 것을 목표로 한다.",
                    "tag": "3"
                },
                {
                    "index": "129-5",
                    "sentence": "The retrieved visual cues are used as contextual information to construct relevant responses to the users.",
                    "sentence_kor": "검색된 시각적 단서는 사용자에게 관련 응답을 구성하기 위한 상황별 정보로 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "129-6",
                    "sentence": "Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark.",
                    "sentence_kor": "우리의 경험적 결과와 종합적인 정성 분석은 BiST가 경쟁력 있는 성능을 달성하고 대규모 AVSD 벤치마크에서 합리적인 응답을 생성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "129-7",
                    "sentence": "We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.",
                    "sentence_kor": "또한 BiST 모델을 비디오 QA 설정에 맞게 조정하고 TGIF-QA 벤치마크에서 이전 접근 방식을 크게 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "130",
            "abstractID": "EMNLP_abs-130",
            "text": [
                {
                    "index": "130-0",
                    "sentence": "Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons.",
                    "sentence_kor": "다중 도메인 작업 지향 대화를 위한 엔드 투 엔드 대화 에이전트를 구축하는 것은 두 가지 주요 이유 때문에 개방적인 과제였다.",
                    "tag": "1"
                },
                {
                    "index": "130-1",
                    "sentence": "First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only.",
                    "sentence_kor": "첫째, 여러 도메인의 대화 상태를 추적하는 것은 대화 에이전트가 모든 관련 도메인에서 전체 상태를 얻어야 하기 때문에 중요하지 않으며, 그 중 일부는 특정 도메인에 대해서만 고유한 슬롯뿐만 아니라 도메인 간에 공유 슬롯을 가질 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "130-2",
                    "sentence": "Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users.",
                    "sentence_kor": "둘째, 대화 에이전트는 사용자에게 자연스러운 응답을 생성하기 위해 대화 컨텍스트, 대화 상태 및 데이터베이스를 포함한 다양한 유형의 정보를 도메인 전반에 걸쳐 처리해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "130-3",
                    "sentence": "Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously.",
                    "sentence_kor": "흔히 각 모듈을 개별적으로 훈련하도록 설계된 기존 접근 방식과 달리, 우리는 다중 도메인 작업 지향 대화에서 엔드 투 엔드 대화 시스템을 위한 새로운 통합 신경 아키텍처인 \"UniConv\"를 제안한다. 이 아키텍처는 (i) 양쪽 슬로에서 신호를 학습하여 대화 상태를 추적하는 바이 레벨 상태 추적기를 공동으로 훈련하도록 설계되었다.t 및 도메인 수준 독립적으로, 그리고 (ii) 다양한 입력 요소의 정보를 통합하고 대화 행위와 목표 응답을 동시에 모델링하는 공동 대화법 및 대응 생성기.",
                    "tag": "2+3"
                },
                {
                    "index": "130-4",
                    "sentence": "We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.",
                    "sentence_kor": "우리는 MultiWOZ 2.1 벤치마크에서 대화 상태 추적, 컨텍스트 대 텍스트 및 엔드 투 엔드 설정에 대한 포괄적인 실험을 수행하여 경쟁 기준선보다 우수한 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "131",
            "abstractID": "EMNLP_abs-131",
            "text": [
                {
                    "index": "131-0",
                    "sentence": "End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs.",
                    "sentence_kor": "엔드 투 엔드 작업 지향 대화 시스템은 일반 텍스트 입력에서 직접 시스템 응답을 생성하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "131-1",
                    "sentence": "There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history.",
                    "sentence_kor": "이러한 시스템에는 두 가지 과제가 있다. 하나는 외부 지식 기반(KB)을 학습 프레임워크에 효과적으로 통합하는 방법과 다른 하나는 대화 이력의 의미를 정확하게 포착하는 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "131-2",
                    "sentence": "In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue.",
                    "sentence_kor": "본 논문에서는 대화의 의존성 구문 분석 트리와 지식 기반에서 그래프 구조 정보를 활용하여 이 두 가지 과제를 해결한다.",
                    "tag": "2+3"
                },
                {
                    "index": "131-3",
                    "sentence": "To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs.",
                    "sentence_kor": "대화 이력에서 구조 정보를 효과적으로 활용하기 위해 그래프에 표현 학습을 허용하는 새로운 반복 셀 아키텍처를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "131-4",
                    "sentence": "To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure.",
                    "sentence_kor": "KB 단위 실체 간의 관계를 이용하기 위해, 모델은 그래프 구조에 기초한 다중 홉 추론 능력을 결합한다.",
                    "tag": "3"
                },
                {
                    "index": "131-5",
                    "sentence": "Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.",
                    "sentence_kor": "실험 결과에 따르면 제안된 모델은 서로 다른 두 작업 지향 대화 데이터 세트에서 최첨단 모델에 비해 일관된 개선을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "132",
            "abstractID": "EMNLP_abs-132",
            "text": [
                {
                    "index": "132-0",
                    "sentence": "Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics.",
                    "sentence_kor": "하나 또는 일련의 대화에서 의미 있는 구조적 표현을 유도하는 것은 컴퓨터 언어학에서 중요하지만 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "132-1",
                    "sentence": "Advancement made in this area is critical for dialogue system design and discourse analysis.",
                    "sentence_kor": "이 분야에서 이루어진 발전은 대화 시스템 설계와 담화 분석에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "132-2",
                    "sentence": "It can also be extended to solve grammatical inference.",
                    "sentence_kor": "문법적 추론을 풀기 위해 확장될 수도 있다.",
                    "tag": "1"
                },
                {
                    "index": "132-3",
                    "sentence": "In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion.",
                    "sentence_kor": "본 연구에서는 비지도 방식으로 대화 구조를 학습하기 위해 구조화된 주의 계층을 이산 잠재 상태를 가진 가변 반복 신경망(VRNN) 모델에 통합할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "132-4",
                    "sentence": "Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias.",
                    "sentence_kor": "바닐라 VRNN과 비교하여 구조화된 주의를 통해 모델은 구조적 유도 편향을 적용하면서 소스 문장 임베딩의 다른 부분에 초점을 맞출 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "132-5",
                    "sentence": "Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus.",
                    "sentence_kor": "실험에 따르면 쌍방 대화 데이터 세트에서 구조화된 주의를 가진 VRNN은 이 대화 말뭉치를 생성하는 데 사용되는 템플릿과 유사한 의미 구조를 학습한다.",
                    "tag": "4"
                },
                {
                    "index": "132-6",
                    "sentence": "While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.",
                    "sentence_kor": "다자간 대화 데이터 세트에서 우리 모델은 스피커나 주소를 구별하는 능력을 입증하는 대화형 구조를 학습하여 명시적인 인간 주석 없이 자동으로 대화를 중단시킨다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "133",
            "abstractID": "EMNLP_abs-133",
            "text": [
                {
                    "index": "133-0",
                    "sentence": "In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation.",
                    "sentence_kor": "지난 몇 년 동안 서로 다른 분야의 관객들은 대화 내용 생성을 강화하기 위해 시퀀스 투 시퀀스 모델(예: LSTM+어텐션, 포인터 생성기 네트워크 및 트랜스포머)의 성과를 목격한다.",
                    "tag": "1"
                },
                {
                    "index": "133-1",
                    "sentence": "While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored.",
                    "sentence_kor": "콘텐츠의 유창성과 정확성이 모델 훈련의 주요 지표로 작용하는 반면, 일부 특정 영역에 대한 중요한 정보를 전달하는 대화 논리는 종종 무시된다.",
                    "tag": "1"
                },
                {
                    "index": "133-2",
                    "sentence": "Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.",
                    "sentence_kor": "고객 서비스와 법정 토론 대화를 예로 들어, 서로 다른 대화 사례에서 호환 가능한 논리를 관찰할 수 있으며, 이 정보는 발언 생성에 중요한 증거를 제공할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "133-3",
                    "sentence": "In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances’ logical structure simultaneously.",
                    "sentence_kor": "본 논문에서 우리는 현재 대화 컨텍스트와 유사한 대화 인스턴스의 논리적 구조를 동시에 탐색하기 위한 새로운 네트워크 아키텍처인 CCN(Cross Copy Networks)을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "133-4",
                    "sentence": "Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.",
                    "sentence_kor": "법정 토론과 고객 서비스 콘텐츠 생성이라는 두 가지 과제를 이용한 실험은 제안된 알고리즘이 기존의 최첨단 콘텐츠 생성 모델보다 우수하다는 것을 입증했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "134",
            "abstractID": "EMNLP_abs-134",
            "text": [
                {
                    "index": "134-0",
                    "sentence": "Multi-turn response selection is a task designed for developing dialogue agents.",
                    "sentence_kor": "다중 회전 응답 선택은 대화 에이전트를 개발하기 위해 설계된 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "134-1",
                    "sentence": "The performance on this task has a remarkable improvement with pre-trained language models.",
                    "sentence_kor": "이 작업의 성능은 사전 훈련된 언어 모델을 통해 현저하게 향상되었다.",
                    "tag": "1"
                },
                {
                    "index": "134-2",
                    "sentence": "However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns.",
                    "sentence_kor": "그러나 이러한 모델은 단순히 대화 이력의 턴을 입력으로 연결하고 턴 간의 의존성을 대부분 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "134-3",
                    "sentence": "In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations.",
                    "sentence_kor": "본 논문에서, 우리는 대화 이력을 의존 관계를 기반으로 스레드로 변환하는 대화 추출 알고리즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "134-4",
                    "sentence": "Each thread can be regarded as a self-contained sub-dialogue.",
                    "sentence_kor": "각 스레드는 자급자족하는 하위 대화로 간주될 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "134-5",
                    "sentence": "We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer.",
                    "sentence_kor": "또한 사전 훈련된 트랜스포머에 의해 스레드 및 후보를 콤팩트 표현으로 인코딩하고 마지막으로 주의 계층을 통해 일치 점수를 얻는 스레드-인코더 모델을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "134-6",
                    "sentence": "The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.",
                    "sentence_kor": "실험은 의존 관계가 대화 상황 이해에 도움이 되며, 우리 모델은 우분투V2에 대한 경쟁 결과와 함께 DSTC7과 DSTC8* 모두에서 최첨단 기준선을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "135",
            "abstractID": "EMNLP_abs-135",
            "text": [
                {
                    "index": "135-0",
                    "sentence": "The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models.",
                    "sentence_kor": "동일한 턴과 다른 턴에 걸친 시스템과 사용자 발언 사이의 의존성은 기존 다중 도메인 대화 상태 추적(MDST) 모델에서 완전히 고려되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "135-1",
                    "sentence": "In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies.",
                    "sentence_kor": "이 연구에서 우리는 MDST 설계에 이러한 종속성의 통합이 중요하다고 주장하고 이러한 종속성을 모델링하기 위해 PIN(Parallel Interactive Networks)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "135-2",
                    "sentence": "Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies.",
                    "sentence_kor": "특히, 대화형 인코더를 통합하여 인턴 의존성과 교차 턴 의존성을 공동으로 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "135-3",
                    "sentence": "The slot-level context is introduced to extract more expressive features for different slots.",
                    "sentence_kor": "슬롯 레벨 컨텍스트는 다양한 슬롯에 대해 보다 표현적인 기능을 추출하기 위해 도입됩니다.",
                    "tag": "4"
                },
                {
                    "index": "135-4",
                    "sentence": "And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances.",
                    "sentence_kor": "그리고 분산 복사 메커니즘은 과거 시스템 발언 또는 과거 사용자 발언에서 단어를 선택적으로 복사하는 데 사용됩니다.",
                    "tag": "4"
                },
                {
                    "index": "135-5",
                    "sentence": "Empirical studies demonstrated the superiority of the proposed PIN model.",
                    "sentence_kor": "경험적 연구는 제안된 PIN 모델의 우수성을 입증했다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "136",
            "abstractID": "EMNLP_abs-136",
            "text": [
                {
                    "index": "136-0",
                    "sentence": "Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system.",
                    "sentence_kor": "슬롯 채우기 및 의도 감지는 구어 이해(SLU) 시스템의 두 가지 주요 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "136-1",
                    "sentence": "In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling.",
                    "sentence_kor": "본 논문에서 우리는 공동 의도 감지 및 슬롯 채우기를 위한 SlotRefine이라는 새로운 비 자기 회귀 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "136-2",
                    "sentence": "Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model.",
                    "sentence_kor": "또한, 우리는 비 자기 회귀 모델의 조건부 독립성으로 인한 조정되지 않은 슬롯 문제를 처리하기 위해 새로운 2패스 반복 메커니즘을 설계한다.",
                    "tag": "1+2"
                },
                {
                    "index": "136-3",
                    "sentence": "Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77).",
                    "sentence_kor": "실험에 따르면 우리 모델은 슬롯 채우기 작업에서 이전 모델을 크게 능가하는 동시에 디코딩 속도를 상당히 향상시킨다(최대 x10.77).",
                    "tag": "4"
                },
                {
                    "index": "136-4",
                    "sentence": "In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.",
                    "sentence_kor": "심층 분석에 따르면 1) 사전 훈련 계획은 모델을 더욱 향상시킬 수 있다. 2) 2-통과 메커니즘은 조정되지 않은 슬롯을 실제로 개선할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "137",
            "abstractID": "EMNLP_abs-137",
            "text": [
                {
                    "index": "137-0",
                    "sentence": "Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale.",
                    "sentence_kor": "언어 이해를 위한 복잡한 모델의 결정은 제공된 입력을 원본 텍스트의 관련 후속(논리)으로 제한함으로써 설명할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "137-1",
                    "sentence": "Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context.",
                    "sentence_kor": "간결한 이론적 근거에 따라 예측을 조건화하는 모델은 해석 가능하지만 전체 컨텍스트를 사용할 수 있는 모델보다 정확도가 떨어지는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "137-2",
                    "sentence": "In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective.",
                    "sentence_kor": "본 논문에서 우리는 정보 병목 현상(IB) 목표에 대한 한계를 최적화함으로써 간결한 설명과 높은 작업 정확도 사이의 균형을 더 잘 관리할 수 있음을 보여준다.",
                    "tag": "1+2"
                },
                {
                    "index": "137-3",
                    "sentence": "Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences.",
                    "sentence_kor": "우리의 접근 방식은 명시적 감독 없이 입력 문장에 대한 희박한 이진 마스크를 예측하는 설명자와 잔여 문장만 고려하는 최종 작업 예측자를 공동으로 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "137-4",
                    "sentence": "Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior.",
                    "sentence_kor": "IB를 사용하여 조정 가능한 희소성 이전을 통해 마스크 희소성 수준을 직접 제어할 수 있는 학습 목표를 도출한다.",
                    "tag": "3"
                },
                {
                    "index": "137-5",
                    "sentence": "Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales.",
                    "sentence_kor": "지우개 벤치마크에 대한 실험은 작업 수행과 인간 이성과의 합의 모두에서 이전 작업에 비해 상당한 이득을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "137-6",
                    "sentence": "Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.",
                    "sentence_kor": "또한 준감독 설정에서 적당한 양의 금 합리화(금 마스크가 있는 훈련 예제의 25%)가 전체 입력을 사용하는 모델과의 성능 격차를 좁힐 수 있다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "138",
            "abstractID": "EMNLP_abs-138",
            "text": [
                {
                    "index": "138-0",
                    "sentence": "Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks.",
                    "sentence_kor": "사전 훈련된 언어 모델, 특히 마스크된 언어 모델(MLM)은 많은 NLP 작업에서 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "138-1",
                    "sentence": "However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations.",
                    "sentence_kor": "그러나, 그들이 훈련 받은 말뭉치에 의심할 여지 없이 존재하는 문화적 편견을 사용한다는 충분한 증거가 있으며, 편향된 표현으로 해를 암시적으로 만들어낸다.",
                    "tag": "1"
                },
                {
                    "index": "138-2",
                    "sentence": "To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs).",
                    "sentence_kor": "미국의 보호된 인구 통계 그룹에 대한 언어 모델의 일부 형태의 사회적 편견을 측정하기 위해 크라우드소싱 고정관념 쌍 벤치마크(CrowS-Pairs)를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "138-3",
                    "sentence": "CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age.",
                    "sentence_kor": "CrowS-Pairs는 인종, 종교, 나이와 같은 9가지 유형의 편견을 다루는 1508개의 예를 가지고 있다.",
                    "tag": "3"
                },
                {
                    "index": "138-4",
                    "sentence": "In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.",
                    "sentence_kor": "CrowS-Pairs에서 모델은 두 개의 문장으로 제시됩니다: 하나는 더 고정관념적인 문장이고 다른 하나는 덜 고정관념적인 문장이다.",
                    "tag": "3"
                },
                {
                    "index": "138-5",
                    "sentence": "The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.",
                    "sentence_kor": "이 자료는 역사적으로 혜택 받지 못한 집단에 대한 고정관념에 초점을 맞추고 있으며 이 집단들과 대조된다.",
                    "tag": "3"
                },
                {
                    "index": "138-6",
                    "sentence": "We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs.",
                    "sentence_kor": "우리는 우리가 평가하는 널리 사용되는 세 개의 MLM 모두 CrowS-Pair의 모든 범주에서 고정관념을 표현하는 문장을 상당히 선호한다는 것을 발견했다.",
                    "tag": "4+5"
                },
                {
                    "index": "138-7",
                    "sentence": "As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",
                    "sentence_kor": "덜 편향된 모델을 구축하는 작업이 진행됨에 따라, 이 데이터 세트는 진행 상황을 평가하기 위한 벤치마크로 사용될 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "139",
            "abstractID": "EMNLP_abs-139",
            "text": [
                {
                    "index": "139-0",
                    "sentence": "Machine learning techniques have been widely used in natural language processing (NLP).",
                    "sentence_kor": "기계 학습 기술은 자연어 처리(NLP)에 널리 사용되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "139-1",
                    "sentence": "However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.",
                    "sentence_kor": "그러나 많은 최근 연구에서 밝혀진 바와 같이 기계 학습 모델은 종종 데이터의 사회적 편견을 계승하고 증폭시킨다.",
                    "tag": "1"
                },
                {
                    "index": "139-2",
                    "sentence": "Various metrics have been proposed to quantify biases in model predictions.",
                    "sentence_kor": "모델 예측에서 편견을 정량화하기 위해 다양한 측정기준이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "139-3",
                    "sentence": "In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus.",
                    "sentence_kor": "특히 이들 중 일부는 시험 말뭉치의 보호 그룹과 장점 그룹 간의 모델 성능 차이를 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "139-4",
                    "sentence": "However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model.",
                    "sentence_kor": "그러나 우리는 말뭉치 수준에서 편견을 평가하는 것은 모델에 편견이 어떻게 내재되어 있는지를 이해하는 데 충분하지 않다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "139-5",
                    "sentence": "In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region.",
                    "sentence_kor": "실제로 전체 데이터의 서로 다른 그룹 간에 집계된 성능이 유사한 모델은 로컬 영역의 인스턴스에서 다르게 동작할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "139-6",
                    "sentence": "To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering.",
                    "sentence_kor": "이러한 로컬 편향을 분석하고 탐지하기 위해 클러스터링을 기반으로 하는 새로운 편향 탐지 기술인 LOGAN을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "139-7",
                    "sentence": "Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.",
                    "sentence_kor": "독성 분류 및 물체 분류 작업에 대한 실험은 LOGAN이 국소 영역의 편향을 식별하고 모델 예측의 편향을 더 잘 분석할 수 있도록 한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "140",
            "abstractID": "EMNLP_abs-140",
            "text": [
                {
                    "index": "140-0",
                    "sentence": "Recurrent neural networks empirically generate natural language with high syntactic fidelity.",
                    "sentence_kor": "반복 신경망은 경험적으로 구문 충실도가 높은 자연어를 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "140-1",
                    "sentence": "However, their success is not well-understood theoretically.",
                    "sentence_kor": "하지만, 그들의 성공은 이론적으로 잘 이해되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "140-2",
                    "sentence": "We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax.",
                    "sentence_kor": "우리는 이러한 성공에 대한 이론적 통찰력을 제공하여 RNN이 자연어 구문의 발판을 반영하는 경계 계층 언어를 효율적으로 생성할 수 있음을 유한 정밀도 설정으로 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "140-3",
                    "sentence": "We introduce Dyck-(k,m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax.",
                    "sentence_kor": "자연어 구문의 제한된 메모리 요구와 장거리 의존성을 반영하여 잘 중첩된 괄호(k 유형)와 m 경계 중첩 깊이의 언어인 Dick-(k,m)를 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "140-4",
                    "sentence": "The best known results use O(km⁄2) memory (hidden units) to generate these languages.",
                    "sentence_kor": "가장 잘 알려진 결과는 O(km²2) 메모리(숨겨진 단위)를 사용하여 이러한 언어를 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "140-5",
                    "sentence": "We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction.",
                    "sentence_kor": "우리는 O(m log k) 은닉 단위가 있는 RNN이 명시적 구성에 의해 메모리 지수 감소로 충분하다는 것을 증명한다.",
                    "tag": "3+4"
                },
                {
                    "index": "140-6",
                    "sentence": "Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.",
                    "sentence_kor": "마지막으로, 무한 연산을 하더라도 어떤 알고리즘도 o(m log k) 은닉 단위를 충분히 사용할 수 없다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "141",
            "abstractID": "EMNLP_abs-141",
            "text": [
                {
                    "index": "141-0",
                    "sentence": "We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”.",
                    "sentence_kor": "우리는 최첨단 구문 분석가들이 \"hers\"와 \"thers\"를 대명사로 일관되게 식별하지 못했지만 남성 등가물인 \"his\"를 식별했다고 보고한다.",
                    "tag": "1"
                },
                {
                    "index": "141-1",
                    "sentence": "We find that the same biases exist in recent language models like BERT.",
                    "sentence_kor": "우리는 BERT와 같은 최근의 언어 모델에 동일한 편견이 존재한다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "141-2",
                    "sentence": "While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models.",
                    "sentence_kor": "일부 편향은 성별 불균형이 있는 훈련 데이터와 같이 알려진 출처에서 비롯되지만, 우리는 언어 모델에서 편향이 _증가되었으며 본질적으로 편향되지 않은 영어 대명사 간의 언어 차이가 일부 기계 학습 모델에서 편향이 될 수 있다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "141-3",
                    "sentence": "We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.",
                    "sentence_kor": "베이지안 근사치를 사용하여 모델 자체에서 부분적으로 합성 데이터를 생성하는 모델의 편향 측정을 위한 새로운 기법을 소개한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "142",
            "abstractID": "EMNLP_abs-142",
            "text": [
                {
                    "index": "142-0",
                    "sentence": "Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems.",
                    "sentence_kor": "인간은 현대의 NLP 시스템에 비해 한 번에 훨씬 제한된 데이터 샘플에 대한 액세스로 지속적으로 언어를 습득한다.",
                    "tag": "1"
                },
                {
                    "index": "142-1",
                    "sentence": "To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes.",
                    "sentence_kor": "이러한 인간과 유사한 언어 습득 능력을 연구하기 위해 스트리밍 시각적 장면에서 구성 구문을 지속적으로 습득하는 것을 시뮬레이션하는 시각적 기반 언어 학습 과제인 VisCOL을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "142-2",
                    "sentence": "In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets.",
                    "sentence_kor": "이 작업에서 모델은 객체 분포가 이동하는 쌍 이미지 캡처 스트림에 대해 훈련되며, 보류 테스트 세트에서 시각적으로 바탕을 둔 마스킹 언어 예측 작업에 의해 지속적으로 평가된다.",
                    "tag": "3"
                },
                {
                    "index": "142-3",
                    "sentence": "VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions).",
                    "sentence_kor": "VisCOL은 지속적인 학습(즉, 지속적으로 이동하는 데이터 분포로부터 학습)과 구성 일반화(즉, 새로운 구성으로 일반화)의 과제를 복합적으로 다룬다.",
                    "tag": "3"
                },
                {
                    "index": "142-4",
                    "sentence": "To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods.",
                    "sentence_kor": "VisCOL에 대한 연구를 용이하게 하기 위해 COCO-shift와 Flickr-shift라는 두 데이터 세트를 구성하고 서로 다른 연속 학습 방법을 사용하여 벤치마킹한다.",
                    "tag": "3"
                },
                {
                    "index": "142-5",
                    "sentence": "Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible.",
                    "sentence_kor": "결과에 따르면 가능한 모든 구성의 예제를 저장하는 것은 불가능하기 때문에 SoTA의 지속적인 학습 접근 방식은 VisCOL에 거의 또는 전혀 개선되지 않습니다.",
                    "tag": "4"
                },
                {
                    "index": "142-6",
                    "sentence": "We conduct further ablations and analysis to guide future work.",
                    "sentence_kor": "향후 작업을 안내하기 위해 추가적인 목욕 및 분석을 수행합니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "143",
            "abstractID": "EMNLP_abs-143",
            "text": [
                {
                    "index": "143-0",
                    "sentence": "Phrase localization is a task that studies the mapping from textual phrases to regions of an image.",
                    "sentence_kor": "구문 로컬라이제이션은 텍스트 구문에서 이미지 영역으로의 매핑을 연구하는 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "143-1",
                    "sentence": "Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision.",
                    "sentence_kor": "규모에 따라 구문 대 객체 데이터 세트에 주석을 달기가 어려운 경우, 우리는 보다 널리 사용 가능한 캡션 이미지 데이터 세트를 활용하기 위해 MAF(Multimodal Alignment Framework)를 개발하여 약한 감시의 형태로 사용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "143-2",
                    "sentence": "We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations.",
                    "sentence_kor": "먼저 세분화된 시각적 표현과 시각적 인식 언어 표현을 활용하여 구문-객체 관련성을 모델링하는 알고리즘을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "143-3",
                    "sentence": "By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios.",
                    "sentence_kor": "대조적인 목표를 채택함으로써, 우리 방법은 캡션-이미지 쌍의 정보를 사용하여 약하게 감독되는 시나리오에서 성능을 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "143-4",
                    "sentence": "Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods.",
                    "sentence_kor": "널리 채택된 Flickr30k 데이터 세트에 대해 수행된 실험은 약하게 감독된 기존 방법에 비해 크게 개선된 것으로 나타났다.",
                    "tag": "3+4"
                },
                {
                    "index": "143-5",
                    "sentence": "With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%.",
                    "sentence_kor": "시각 인식 언어 표현의 도움을 받아 이전의 최상의 감독되지 않은 결과를 5.56% 개선할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "143-6",
                    "sentence": "We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.",
                    "sentence_kor": "우리는 우리의 새로운 모델과 약하게 감독되는 전략 둘 다 우리의 강력한 결과에 크게 기여한다는 것을 보여주기 위해 절제 연구를 수행한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "144",
            "abstractID": "EMNLP_abs-144",
            "text": [
                {
                    "index": "144-0",
                    "sentence": "Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations.",
                    "sentence_kor": "이미지는 단어의 맥락적 의미에 대한 통찰력을 제공할 수 있지만, 현재의 이미지 텍스트 접지 접근법에는 상세한 주석이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "144-1",
                    "sentence": "Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts.",
                    "sentence_kor": "이러한 세분화된 주석은 드물고 비싸며 대부분의 도메인별 컨텍스트에서 사용할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "144-2",
                    "sentence": "In contrast, unlabeled multi-image, multi-sentence documents are abundant.",
                    "sentence_kor": "반대로 레이블이 지정되지 않은 다중 이미지, 다중 문장 문서는 풍부합니다.",
                    "tag": "1"
                },
                {
                    "index": "144-3",
                    "sentence": "Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap?",
                    "sentence_kor": "어휘적 기초는 어휘적 및 시각적 겹침이 상당하더라도 그러한 문서에서 학습할 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "144-4",
                    "sentence": "Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as “kitchen” and “bedroom”, and introduce metrics to assess this document similarity.",
                    "sentence_kor": "부동산 리스트의 사례 연구 데이터 세트로 작업하면서, 우리는 \"부엌\"과 \"침실\"과 같은 고도로 상관된 기반 용어를 구별하는 과제를 입증하고 이 문서의 유사성을 평가하기 위한 메트릭스를 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "144-5",
                    "sentence": "We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset.",
                    "sentence_kor": "데이터 세트의 레이블링된 하위 집합에서 평가할 때 개체 감지 및 이미지 태그 기준선을 넘어 정밀도와 호출을 향상시키는 간단한 비지도 클러스터링 기반 방법을 제시한다.",
                    "tag": "3+4"
                },
                {
                    "index": "144-6",
                    "sentence": "The proposed method is particularly effective for local contextual meanings of a word, for example associating “granite” with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.",
                    "sentence_kor": "제안된 방법은 예를 들어 부동산 데이터 세트의 카운터톱과 \"granite\"를 연결하거나 Wikipedia 데이터 세트의 록 풍경과 같은 단어의 로컬 컨텍스트 의미에 특히 효과적이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "145",
            "abstractID": "EMNLP_abs-145",
            "text": [
                {
                    "index": "145-0",
                    "sentence": "We present HERO, a novel framework for large-scale video+language omni-representation learning.",
                    "sentence_kor": "대규모 비디오+언어 전표현 학습을 위한 새로운 프레임워크인 HERO를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "145-1",
                    "sentence": "HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer.",
                    "sentence_kor": "HERO는 다중 모드 입력을 계층 구조로 인코딩한다. 여기서 비디오 프레임의 로컬 컨텍스트는 다중 모드 퓨전을 통해 크로스 모드 트랜스포머에 의해 캡처되고 글로벌 비디오 컨텍스트는 시간적 트랜스포머에 의해 캡처된다.",
                    "tag": "3"
                },
                {
                    "index": "145-2",
                    "sentence": "In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames.",
                    "sentence_kor": "표준 마스킹 언어 모델링(MLM)과 마스킹 프레임 모델링(MFM) 목표 외에도, 우리는 (i) 모델이 전역 및 로컬 시간 정렬을 예측하는 VSM(Video-Subtitle Matching)과 모델이 혼합된 비디오의 올바른 순서를 예측하는 FOM(Frame Order Modeling)이라는 두 가지 새로운 사전 교육 작업을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "145-3",
                    "sentence": "HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions.",
                    "sentence_kor": "HERO는 HowTo100M과 대규모 TV 데이터 세트에 대해 공동으로 교육되어 다중 문자 상호작용을 통한 복잡한 사회 역학을 깊이 이해한다.",
                    "tag": "4+5"
                },
                {
                    "index": "145-4",
                    "sentence": "Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains.",
                    "sentence_kor": "종합적인 실험은 HERO가 텍스트 기반 비디오/비디오 순간 검색, 비디오 질문 답변(QA), 비디오 및 언어 추론 및 비디오 캡션 작업에 대한 여러 벤치마크에서 새로운 최첨단 기술을 달성한다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "145-5",
                    "sentence": "We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.",
                    "sentence_kor": "또한 두 가지 새로운 도전 벤치마크 방법2를 소개합니다.비디오 QA 및 검색을 위한 QA 및 How2R. 멀티모달리티에 대한 다양한 비디오 콘텐츠에서 수집됩니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "146",
            "abstractID": "EMNLP_abs-146",
            "text": [
                {
                    "index": "146-0",
                    "sentence": "Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world.",
                    "sentence_kor": "인간은 듣기, 말하기, 쓰기, 읽기, 그리고 다중 모드 현실 세계와의 상호작용을 통해 언어를 배운다.",
                    "tag": "1"
                },
                {
                    "index": "146-1",
                    "sentence": "Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper.",
                    "sentence_kor": "기존의 언어 사전 훈련 프레임워크는 본 논문에서 시각적으로 감독되는 언어 모델의 아이디어를 탐구하는 동안 텍스트 전용 자체 감시의 효과를 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "146-2",
                    "sentence": "We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora.",
                    "sentence_kor": "우리는 이 탐사를 방해하는 주요 이유가 시각 기반 언어 데이터 세트와 순수 언어 말뭉치 사이의 크기와 분포의 큰 차이 때문이라는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "146-3",
                    "sentence": "Therefore, we develop a technique named “vokenization” that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call “vokens”).",
                    "sentence_kor": "따라서 언어 토큰을 관련 이미지(우리가 \"vokens\"라고 함)에 상황에 맞게 매핑하여 언어 전용 데이터에 대한 다중 모드 정렬을 추론하는 \"vokenization\"이라는 기술을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "146-4",
                    "sentence": "The “vokenizer” is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora.",
                    "sentence_kor": "\"vokenizer\"는 비교적 작은 이미지 캡션 데이터 세트에 대해 교육된 다음 이를 적용하여 대규모 언어 말뭉치를 위한 voken을 생성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "146-5",
                    "sentence": "Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.",
                    "sentence_kor": "이러한 상황에 맞게 생성된 voken으로 훈련된 시각적 감독 언어 모델은 GLUE, SQuAD 및 SWAG와 같은 여러 순수 언어 작업에서 자체 감독 대안보다 일관된 개선을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "147",
            "abstractID": "EMNLP_abs-147",
            "text": [
                {
                    "index": "147-0",
                    "sentence": "Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem.",
                    "sentence_kor": "일반 국민을 오도하거나 속이기 위한 허위 정보의 대규모 온라인 유포는 주요 사회 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "147-1",
                    "sentence": "Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism.",
                    "sentence_kor": "이미지, 비디오 및 자연어 생성 모델의 급속한 발전은 이러한 상황을 악화시키고 효과적인 방어 메커니즘에 대한 우리의 필요성을 강화시킬 뿐이다.",
                    "tag": "1"
                },
                {
                    "index": "147-2",
                    "sentence": "While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors.",
                    "sentence_kor": "신경성 가짜 뉴스를 방어하기 위해 기존 접근법이 제안되었지만, 일반적으로 제목과 작성자와 같은 텍스트와 메타데이터만 있는 매우 제한된 설정으로 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "147-3",
                    "sentence": "In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions.",
                    "sentence_kor": "본 논문에서는 이미지와 캡션을 포함하는 기계 생성 뉴스를 방어하는 보다 현실적이고 도전적인 과제를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "147-4",
                    "sentence": "To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset.",
                    "sentence_kor": "적들이 이용할 수 있는 약점을 식별하기 위해, 우리는 네 가지 유형의 생성된 기사로 구성된 NeuralNews 데이터 세트를 만들고 이 데이터 세트를 기반으로 일련의 인간 사용자 연구 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "147-5",
                    "sentence": "Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.",
                    "sentence_kor": "시각적 의미적 불일치 탐지에 기초한 비교적 효과적인 접근방식을 제공하는 것과 함께, 사용자 연구 실험에서 얻은 귀중한 통찰력은 결과적으로 본 논문은 기계 생성 허위 정보를 방어하는 데 있어 효과적인 첫 번째 방어선이자 향후 작업에 귀중한 참고 자료 역할을 할 것이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "148",
            "abstractID": "EMNLP_abs-148",
            "text": [
                {
                    "index": "148-0",
                    "sentence": "Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on.",
                    "sentence_kor": "측면 용어 추출(ATE)은 사용자가 의견을 표시한 검토 문장에서 측면 용어를 추출하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "148-1",
                    "sentence": "Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level.",
                    "sentence_kor": "기존 연구는 대부분 토큰 수준에서 언어적 특징을 추출하기 위한 신경 시퀀스 태그 설계에 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "148-2",
                    "sentence": "However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure.",
                    "sentence_kor": "그러나 측면 용어와 맥락 단어는 대개 긴 꼬리 분포를 나타내기 때문에 이러한 태그는 표본 노출이 충분하지 않은 열등한 상태로 수렴되는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "148-3",
                    "sentence": "In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes.",
                    "sentence_kor": "본 논문에서, 우리는 소프트 프로토타입을 통해 단어들을 서로 연관시킴으로써 이 문제를 해결할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "148-4",
                    "sentence": "These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms.",
                    "sentence_kor": "소프트 검색 프로세스에 의해 생성된 이러한 프로토타입은 내부 또는 외부 데이터로부터 글로벌 지식을 도입하고 측면 용어를 발견하는 데 도움이 되는 증거 역할을 할 수 있습니다.",
                    "tag": "5"
                },
                {
                    "index": "148-5",
                    "sentence": "Our proposed model is a general framework and can be combined with almost all sequence taggers.",
                    "sentence_kor": "우리가 제안한 모델은 일반적인 프레임워크이며 거의 모든 시퀀스 태그와 결합할 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "148-6",
                    "sentence": "Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.",
                    "sentence_kor": "4개의 SemEval 데이터 세트에 대한 실험에 따르면 우리 모델은 세 가지 일반적인 AT 방법의 성능을 큰 폭으로 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "149",
            "abstractID": "EMNLP_abs-149",
            "text": [
                {
                    "index": "149-0",
                    "sentence": "Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted.",
                    "sentence_kor": "다른 영역과 달리, 의학 텍스트는 불가피하게 개인 정보를 동반하기 때문에, 이러한 텍스트들을 공유하거나 복사하는 것은 엄격히 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "149-1",
                    "sentence": "However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection.",
                    "sentence_kor": "그러나 의료 관계 추출 모델을 교육하려면 이러한 개인 정보 취급 텍스트를 수집하여 하나의 기계에 저장해야 하며 이는 개인 정보 보호와 상충된다.",
                    "tag": "1"
                },
                {
                    "index": "149-2",
                    "sentence": "In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged.",
                    "sentence_kor": "본 논문에서, 우리는 공유 또는 교환되는 개인 로컬 데이터의 단일 부분 없이 중앙 모델을 훈련할 수 있는 연합 학습을 기반으로 하는 개인 정보 보호 의료 관계 추출 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "149-3",
                    "sentence": "Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters.",
                    "sentence_kor": "연합 학습은 프라이버시 보호에서 뚜렷한 이점을 가지고 있지만 주로 번거로운 로컬 매개변수를 업로드해야 하기 때문에 발생하는 통신 병목 현상을 겪고 있다.",
                    "tag": "3"
                },
                {
                    "index": "149-4",
                    "sentence": "To overcome this bottleneck, we leverage a strategy based on knowledge distillation.",
                    "sentence_kor": "이러한 병목 현상을 극복하기 위해 지식 증류를 기반으로 하는 전략을 활용합니다.",
                    "tag": "3"
                },
                {
                    "index": "149-5",
                    "sentence": "Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters.",
                    "sentence_kor": "이러한 전략은 앙상블 로컬 모델의 업로드된 예측을 사용하여 로컬 매개 변수를 업로드하지 않고도 중앙 모델을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "149-6",
                    "sentence": "Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.",
                    "sentence_kor": "공개적으로 사용 가능한 세 가지 의료 관계 추출 데이터 세트에 대한 실험은 우리 방법의 효과를 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "150",
            "abstractID": "EMNLP_abs-150",
            "text": [
                {
                    "index": "150-0",
                    "sentence": "Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval.",
                    "sentence_kor": "제품 속성 값은 고객 서비스 로봇, 제품 추천 및 제품 검색과 같은 많은 전자 상거래 시나리오에서 필수적입니다.",
                    "tag": "1"
                },
                {
                    "index": "150-1",
                    "sentence": "While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications.",
                    "sentence_kor": "실제 세계에서는 제품의 속성 값이 대개 불완전하고 시간이 지남에 따라 달라지기 때문에 실제 적용에 큰 방해가 된다.",
                    "tag": "1"
                },
                {
                    "index": "150-2",
                    "sentence": "In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images.",
                    "sentence_kor": "본 논문에서는 제품 이미지를 사용하여 제품 속성을 공동으로 예측하고 텍스트 제품 설명에서 값을 추출하는 멀티모달 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "150-3",
                    "sentence": "We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given.",
                    "sentence_kor": "우리는 제품 속성과 값이 높은 상관관계를 가지고 있다고 주장한다. 예를 들어, 제품 속성이 주어진다는 조건으로 값을 추출하는 것이 더 쉬워질 것이다.",
                    "tag": "3"
                },
                {
                    "index": "150-4",
                    "sentence": "Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values.",
                    "sentence_kor": "따라서, 우리는 속성과 가치 사이의 상호작용에 대한 여러 측면에서 속성 예측과 가치 추출 작업을 공동으로 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "150-5",
                    "sentence": "Moreover, product images have distinct effects on our tasks for different product attributes and values.",
                    "sentence_kor": "또한 제품 이미지는 서로 다른 제품 속성과 가치에 대한 작업에 뚜렷한 영향을 미칩니다.",
                    "tag": "4"
                },
                {
                    "index": "150-6",
                    "sentence": "Thus, we selectively draw useful visual information from product images to enhance our model.",
                    "sentence_kor": "따라서 제품 이미지에서 유용한 시각적 정보를 선별적으로 추출하여 모델을 개선합니다.",
                    "tag": "3+4"
                },
                {
                    "index": "150-7",
                    "sentence": "We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task.",
                    "sentence_kor": "우리는 87,194개의 인스턴스를 포함하는 멀티모달 제품 속성 값 데이터 세트에 주석을 달며, 이 데이터 세트에 대한 실험 결과는 속성과 값 사이의 관계를 명시적으로 모델링하는 방법을 용이하게 하며, 시각적 제품 정보를 선택적으로 활용하는 것이 필요하다는 것을 보여준다.그 일에 적합하다",
                    "tag": "3+4"
                },
                {
                    "index": "150-8",
                    "sentence": "Our code and dataset are available at https://github.com/jd-aig/JAVE.",
                    "sentence_kor": "코드 및 데이터 세트는 https://github.com/jd-aig/JAVE에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "151",
            "abstractID": "EMNLP_abs-151",
            "text": [
                {
                    "index": "151-0",
                    "sentence": "Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks.",
                    "sentence_kor": "기존 OIE(Open Information Extraction) 알고리즘은 중복 작업이 많을 정도로 서로 독립적이다. 특징적인 전략은 재사용할 수 없으며 새로운 작업에 적응할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "151-1",
                    "sentence": "This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies.",
                    "sentence_kor": "본 논문은 OIE 시스템을 구축하기 위한 새로운 파이프라인을 제안한다. 여기서 OIX(Open-Domain Information eExpression) 작업은 모든 OIE 전략을 위한 플랫폼을 제공하기 위해 제안된다.",
                    "tag": "2"
                },
                {
                    "index": "151-2",
                    "sentence": "The OIX is an OIE friendly expression of a sentence without information loss.",
                    "sentence_kor": "OIX는 OIE에 친숙한 문장의 표현으로 정보가 손실되지 않는다.",
                    "tag": "4"
                },
                {
                    "index": "151-3",
                    "sentence": "The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems.",
                    "sentence_kor": "OIX 생성 절차에는 OIE 알고리즘의 공유 작업이 포함되어 있어 OIE 전략이 더 중요한 문제에 초점을 맞춘 추론 작업으로 OIX 플랫폼에서 개발될 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "151-4",
                    "sentence": "Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased.",
                    "sentence_kor": "동일한 OIX 플랫폼을 기반으로 OIE 전략은 재사용 가능하며, 사람들은 적응성이 크게 향상될 수 있도록 특정 작업에 대한 알고리즘을 조립하는 일련의 전략을 선택할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "151-5",
                    "sentence": "This paper focuses on the task of OIX and propose a solution – Open Information Annotation (OIA).",
                    "sentence_kor": "본 논문은 OOX의 과제에 초점을 맞추고 솔루션을 제안한다. OIA(Open Information Annotation)",
                    "tag": "4"
                },
                {
                    "index": "151-6",
                    "sentence": "OIA is a predicate-function-argument annotation for sentences.",
                    "sentence_kor": "OIA는 문장에 대한 술어 함수 인수 주석이다.",
                    "tag": "4"
                },
                {
                    "index": "151-7",
                    "sentence": "We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences.",
                    "sentence_kor": "우리는 문장-OIA 쌍의 데이터 세트에 레이블을 지정하고 문장으로부터 OIA 주석을 생성하기 위한 의존성 기반 규칙 시스템을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "151-8",
                    "sentence": "The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.",
                    "sentence_kor": "평가 결과에 따르면 자연어 문장의 복잡성으로 인해 문장에서 OIA를 배우는 것은 어려운 일이며, 연구계의 관심을 끌 가치가 있다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "152",
            "abstractID": "EMNLP_abs-152",
            "text": [
                {
                    "index": "152-0",
                    "sentence": "We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model.",
                    "sentence_kor": "구문 거리를 활용하여 구문 구성 요소와 종속성 연결을 언어 모델로 인코딩할 것을 제안하여 최종 작업을 용이하게 하기 위해 구조 인식 트랜스포머 언어 모델을 개량하는 것을 고려한다.",
                    "tag": "1+2"
                },
                {
                    "index": "152-1",
                    "sentence": "A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme.",
                    "sentence_kor": "중간 계층 구조 학습 전략은 구조 통합을 위해 활용되며, 다중 작업 학습 체계에 따른 주요 의미론적 작업 훈련으로 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "152-2",
                    "sentence": "Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases.",
                    "sentence_kor": "실험 결과는 수정된 구조 인식 트랜스포머 언어 모델이 정확한 구문 구문을 유도하는 동시에 난해도를 개선한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "152-3",
                    "sentence": "By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.",
                    "sentence_kor": "구조 인식 미세 조정을 수행함으로써, 우리 모델은 의미론적 및 구문론적 종속적 작업 모두에서 상당한 개선을 달성한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "153",
            "abstractID": "EMNLP_abs-153",
            "text": [
                {
                    "index": "153-0",
                    "sentence": "AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text.",
                    "sentence_kor": "AMR-텍스트 생성은 추상적 의미 표현 구조(AMR)를 텍스트로 변환하는 데 사용됩니다.",
                    "tag": "1"
                },
                {
                    "index": "153-1",
                    "sentence": "A key challenge in this task is to efficiently learn effective graph representations.",
                    "sentence_kor": "이 작업의 핵심 과제는 효과적인 그래프 표현을 효율적으로 학습하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "153-2",
                    "sentence": "Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme.",
                    "sentence_kor": "이전에는 입력 AMR을 인코딩하는 데 GCN(Graph Convolution Networks)이 사용되었지만, 바닐라 GCN은 로컬 정보가 아닌 정보를 캡처할 수 없으며, 또한 로컬(1차) 정보 집계 체계를 따른다.",
                    "tag": "1"
                },
                {
                    "index": "153-3",
                    "sentence": "To account for these issues, larger and deeper GCN models are required to capture more complex interactions.",
                    "sentence_kor": "이러한 문제를 해결하려면 더 복잡한 상호 작용을 포착하기 위해 더 크고 깊은 GCN 모델이 필요합니다.",
                    "tag": "1"
                },
                {
                    "index": "153-4",
                    "sentence": "In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs.",
                    "sentence_kor": "본 논문에서 우리는 입력 그래프에서 고차 정보를 합성하여 더 풍부한 비국소적 상호작용을 포착하는 경량 동적 그래프 컨볼루션 네트워크(LDGCN)를 제안하는 동적 융합 메커니즘을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "153-5",
                    "sentence": "We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity.",
                    "sentence_kor": "우리는 메모리 사용과 모델 복잡성을 줄이기 위해 그룹 그래프 컨볼루션과 가중치 연결 컨볼루션을 기반으로 두 가지 새로운 매개 변수 절약 전략을 추가로 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "153-6",
                    "sentence": "With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity.",
                    "sentence_kor": "이러한 전략의 도움으로, 우리는 모델 용량을 유지하면서 더 적은 매개 변수로 모델을 교육할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "153-7",
                    "sentence": "Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.",
                    "sentence_kor": "실험에 따르면 LDGCN은 매개 변수가 상당히 적은 AMR-텍스트 생성을 위한 두 가지 벤치마크 데이터 세트에서 최첨단 모델을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "154",
            "abstractID": "EMNLP_abs-154",
            "text": [
                {
                    "index": "154-0",
                    "sentence": "Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results.",
                    "sentence_kor": "놀랍게도 신경 언어 생성기의 정확한 최대 사후 디코딩은 종종 낮은 품질의 결과로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "154-1",
                    "sentence": "Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate.",
                    "sentence_kor": "오히려 언어 생성 작업에 대한 대부분의 최신 결과는 압도적으로 높은 검색 오류율에도 불구하고 빔 검색을 사용하여 달성된다.",
                    "tag": "1"
                },
                {
                    "index": "154-2",
                    "sentence": "This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question?",
                    "sentence_kor": "이는 MAP 목표만으로는 우리가 원하는 속성을 텍스트로 표현하지 못한다는 것을 의미하며, 이는 빔 검색이 정답이라면 어떤 질문이었는가?라는 질문을 받을 가치가 있다.",
                    "tag": "1"
                },
                {
                    "index": "154-3",
                    "sentence": "We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy.",
                    "sentence_kor": "모델만으로는 타당성을 나타내지 않을 수 있는 높은 확률에 대한 통찰력을 얻기 위해 빔 검색을 다른 디코딩 목표에 대한 정확한 솔루션으로 정의한다.",
                    "tag": "1+2"
                },
                {
                    "index": "154-4",
                    "sentence": "We find that beam search enforces uniform information density in text, a property motivated by cognitive science.",
                    "sentence_kor": "우리는 빔 검색이 인지 과학에 의해 동기 부여되는 특성인 텍스트에서 균일한 정보 밀도를 강제한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "154-5",
                    "sentence": "We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models.",
                    "sentence_kor": "우리는 이 속성을 명시적으로 적용하는 일련의 디코딩 목표를 제안하고 이러한 목표를 가진 정확한 디코딩이 잘못 보정된 언어 생성 모델을 디코딩할 때 발생하는 문제를 완화한다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "154-6",
                    "sentence": "Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.",
                    "sentence_kor": "또한 다양한 디코딩 전략을 사용하여 생성된 텍스트를 분석하고 신경 기계 변환 실험에서 이 특성이 BLEU와 강하게 상관된다는 것을 확인한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "155",
            "abstractID": "EMNLP_abs-155",
            "text": [
                {
                    "index": "155-0",
                    "sentence": "Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data.",
                    "sentence_kor": "잠재 구조 모델은 언어 데이터를 모델링하는 강력한 도구입니다. 파이프라인 시스템의 오류 전파 및 주석 병목 현상을 완화하는 동시에 데이터에 대한 언어 통찰력을 발견할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "155-1",
                    "sentence": "One challenge with end-to-end training of these models is the argmax operation, which has null gradient.",
                    "sentence_kor": "이러한 모델의 엔드 투 엔드 훈련과 관련된 한 가지 과제는 null 그라데이션이 있는 argmax 연산이다.",
                    "tag": "1"
                },
                {
                    "index": "155-2",
                    "sentence": "In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem.",
                    "sentence_kor": "본 논문에서 우리는 이 문제를 다루기 위한 일반적인 전략인 대리 그레이디언트에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "155-3",
                    "sentence": "We explore latent structure learning through the angle of pulling back the downstream learning objective.",
                    "sentence_kor": "우리는 다운스트림 학습 목표를 후퇴시키는 각도를 통해 잠재 구조 학습을 탐구한다.",
                    "tag": "3"
                },
                {
                    "index": "155-4",
                    "sentence": "In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT – a variant of STE for structured models.",
                    "sentence_kor": "이 패러다임에서 우리는 스트레이트 스루 추정기(STE)와 구조화된 모델을 위한 STE의 변형인 최근 제안된 SPIGOT 모두에 대한 원칙적인 동기를 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "155-5",
                    "sentence": "Our perspective leads to new algorithms in the same family.",
                    "sentence_kor": "우리의 관점은 같은 계열의 새로운 알고리즘으로 이어진다.",
                    "tag": "4"
                },
                {
                    "index": "155-6",
                    "sentence": "We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.",
                    "sentence_kor": "알려진 추정기와 새로운 철수 추정기를 일반적인 대안과 경험적으로 비교하여 실무자에게 새로운 통찰력을 제공하고 흥미로운 실패 사례를 드러낸다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "156",
            "abstractID": "EMNLP_abs-156",
            "text": [
                {
                    "index": "156-0",
                    "sentence": "Recent work raises concerns about the use of standard splits to compare natural language processing models.",
                    "sentence_kor": "최근 연구는 자연어 처리 모델을 비교하기 위한 표준 분할 사용에 대한 우려를 제기하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "156-1",
                    "sentence": "We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results.",
                    "sentence_kor": "우리는 한 모델이 다른 모델을 능가하거나 두 모델이 실질적으로 동일한 결과를 산출할 가능성을 추정하기 위해 여러 데이터 세트에 걸쳐 k배 교차 검증을 사용하는 베이지안 통계 모델 비교 기법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "156-2",
                    "sentence": "We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.",
                    "sentence_kor": "우리는 이 기술을 사용하여 두 데이터 세트와 세 가지 평가 메트릭에서 6개의 영어 음성 부분 태그거의 순위를 매긴다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "157",
            "abstractID": "EMNLP_abs-157",
            "text": [
                {
                    "index": "157-0",
                    "sentence": "Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL.",
                    "sentence_kor": "이전 연구에 따르면 계층적 다중 작업 학습(MTL)은 인코더를 쌓아서 작업 의존성을 활용하고 민주적 MTL을 능가한다.",
                    "tag": "1"
                },
                {
                    "index": "157-1",
                    "sentence": "However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks.",
                    "sentence_kor": "그러나 스택킹 인코더는 형상 표현의 종속성만 고려하고 논리적으로 종속된 작업에서 레이블 종속성은 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "157-2",
                    "sentence": "Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks.",
                    "sentence_kor": "또한 라벨의 적절한 활용 방법은 작업 간의 단계적 오류 때문에 여전히 문제로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "157-3",
                    "sentence": "In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models.",
                    "sentence_kor": "본 논문에서 우리는 논리적으로 종속적인 MTL을 인과 추론의 관점에서 보고 기존 MTL 모델의 교란 가정 대신 중재 가정을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "157-4",
                    "sentence": "We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors.",
                    "sentence_kor": "우리는 모든 하위 수준의 작업의 레이블을 활용하기 위한 각 작업의 레이블 전송(LT)과 계단식 오류를 처리하기 위한 검벨 샘플링(GS)이라는 두 가지 주요 메커니즘을 포함하는 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "157-5",
                    "sentence": "In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL.",
                    "sentence_kor": "인과 추론 분야에서, 우리 모델의 GS는 본질적으로 작업 간의 인과 효과를 추정하고 MTL을 개선하는 데 활용하려는 반사실적 추론 프로세스이다.",
                    "tag": "3"
                },
                {
                    "index": "157-6",
                    "sentence": "We conduct experiments on two English datasets and one Chinese dataset.",
                    "sentence_kor": "우리는 두 개의 영어 데이터 세트와 한 개의 중국어 데이터 세트에 대해 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "157-7",
                    "sentence": "Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions’ consistency.",
                    "sentence_kor": "실험 결과에 따르면 우리 모델은 7개의 하위 작업 중 6개에서 최첨단 기술을 달성하고 예측의 일관성을 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "158",
            "abstractID": "EMNLP_abs-158",
            "text": [
                {
                    "index": "158-0",
                    "sentence": "We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning.",
                    "sentence_kor": "미세 조정을 통해 수정하는 대신 사전 훈련된 가중치에 대한 선택적 이진 마스크를 학습하는 사전 훈련된 언어 모델을 활용하는 효율적인 방법을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "158-1",
                    "sentence": "Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred.",
                    "sentence_kor": "BERT, RoBERTA 및 DisilB 마스킹에 대한 광범위한 평가11개의 다양한 NLP 작업에 대한 ERT는 마스킹 체계가 미세 조정에 버금가는 성능을 제공하지만 여러 작업을 추론해야 할 때 메모리 설치 공간이 훨씬 더 적다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "158-2",
                    "sentence": "Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks.",
                    "sentence_kor": "내재적 평가에 따르면 이진 마스킹 언어 모델에서 계산된 표현은 다운스트림 작업을 해결하는 데 필요한 정보를 인코딩한다.",
                    "tag": "4"
                },
                {
                    "index": "158-3",
                    "sentence": "Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy.",
                    "sentence_kor": "손실 풍경을 분석하면 마스킹 및 미세 조정은 거의 일정한 테스트 정확도로 라인 세그먼트로 연결할 수 있는 최소값에 상주하는 모델을 생성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "158-4",
                    "sentence": "This confirms that masking can be utilized as an efficient alternative to finetuning.",
                    "sentence_kor": "이를 통해 마스킹이 미세 조정에 대한 효율적인 대안으로 활용될 수 있음을 확인할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "159",
            "abstractID": "EMNLP_abs-159",
            "text": [
                {
                    "index": "159-0",
                    "sentence": "Document-level neural machine translation has yielded attractive improvements.",
                    "sentence_kor": "문서 수준의 신경 기계 변환은 매력적인 개선을 가져왔다.",
                    "tag": "1"
                },
                {
                    "index": "159-1",
                    "sentence": "However, majority of existing methods roughly use all context sentences in a fixed scope.",
                    "sentence_kor": "그러나 기존 방법의 대부분은 고정된 범위의 모든 문맥 문장을 대략적으로 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "159-2",
                    "sentence": "They neglect the fact that different source sentences need different sizes of context.",
                    "sentence_kor": "그들은 다른 출처 문장이 다른 크기의 맥락을 필요로 한다는 사실을 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "159-3",
                    "sentence": "To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations.",
                    "sentence_kor": "이 문제를 해결하기 위해, 문서 수준 변환 모델이 더 유용한 선택된 컨텍스트 문장을 활용하여 더 나은 번역을 생성할 수 있도록 동적 컨텍스트를 선택하는 효과적인 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "159-4",
                    "sentence": "Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence.",
                    "sentence_kor": "특히, 우리는 각 후보 문맥 문장을 채점하기 위해 번역 모듈과는 독립적인 선택 모듈을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "159-5",
                    "sentence": "Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module.",
                    "sentence_kor": "그런 다음, 다양한 수의 컨텍스트 문장을 명시적으로 선택하여 번역 모듈에 제공하는 두 가지 전략을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "159-6",
                    "sentence": "We train the two modules end-to-end via reinforcement learning.",
                    "sentence_kor": "우리는 강화 학습을 통해 두 모듈을 종단 간 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "159-7",
                    "sentence": "A novel reward is proposed to encourage the selection and utilization of dynamic context sentences.",
                    "sentence_kor": "동적 컨텍스트 문장의 선택과 활용을 장려하기 위해 새로운 보상이 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "159-8",
                    "sentence": "Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.",
                    "sentence_kor": "실험을 통해 우리의 접근 방식이 서로 다른 원본 문장에 대한 적응형 컨텍스트 문장을 선택할 수 있으며 문서 수준 번역 방법의 성능을 크게 향상시킬 수 있음을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "160",
            "abstractID": "EMNLP_abs-160",
            "text": [
                {
                    "index": "160-0",
                    "sentence": "Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models.",
                    "sentence_kor": "대규모 훈련 데이터 세트는 최근 신경 기계 변환(NMT) 모델의 성공에 핵심을 두고 있다.",
                    "tag": "1"
                },
                {
                    "index": "160-1",
                    "sentence": "However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.",
                    "sentence_kor": "그러나 대규모 데이터의 복잡한 패턴과 잠재적 잡음은 NMT 모델을 훈련시키는 것을 어렵게 한다.",
                    "tag": "1"
                },
                {
                    "index": "160-2",
                    "sentence": "In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution.",
                    "sentence_kor": "본 연구에서는 모델 성능에 덜 기여하는 비활성 훈련 예를 식별하고 비활성 예제의 존재가 데이터 분포에 따라 달라진다는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "160-3",
                    "sentence": "We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples.",
                    "sentence_kor": "또한 비활성 예를 활용하여 대규모 데이터 세트에 대한 NMT 모델 교육을 개선하기 위해 데이터 회춘을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "160-4",
                    "sentence": "The proposed framework consists of three phases.",
                    "sentence_kor": "제안된 프레임워크는 세 단계로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "160-5",
                    "sentence": "First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities.",
                    "sentence_kor": "첫째, 원래 훈련 데이터에 대한 식별 모델을 교육하고 이를 사용하여 비활성 예제와 활성 예제를 문장 수준 출력 확률로 구별한다.",
                    "tag": "3"
                },
                {
                    "index": "160-6",
                    "sentence": "Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation.",
                    "sentence_kor": "그런 다음 활성 예제에 대해 회춘 모델을 교육하며, 이는 비활성 예제에 전진 번역으로 레이블을 다시 지정하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "160-7",
                    "sentence": "Finally, the rejuvenated examples and the active examples are combined to train the final NMT model.",
                    "sentence_kor": "마지막으로, 회춘된 예와 활동적인 예들은 최종 NMT 모델을 훈련시키기 위해 결합된다.",
                    "tag": "3"
                },
                {
                    "index": "160-8",
                    "sentence": "Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models.",
                    "sentence_kor": "WMT14 영어-독일어 및 영어-프랑스어 데이터 세트에 대한 실험 결과는 제안된 데이터 회춘이 몇 가지 강력한 NMT 모델의 성능을 일관되고 크게 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "160-9",
                    "sentence": "Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.",
                    "sentence_kor": "광범위한 분석에 따르면 우리의 접근 방식은 NMT 모델의 훈련 과정을 안정화시키고 가속화하여 더 나은 일반화 기능을 가진 최종 모델을 만든다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "161",
            "abstractID": "EMNLP_abs-161",
            "text": [
                {
                    "index": "161-0",
                    "sentence": "Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training.",
                    "sentence_kor": "널리 사용되는 신경 기계 번역 모델 훈련은 BLEU 점수를 향상시키기 위해 역번역과 같은 전략을 사용하여 많은 양의 추가 데이터와 훈련을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "161-1",
                    "sentence": "We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model.",
                    "sentence_kor": "우리는 훈련된 기계 번역 모델을 미세 조정하는 데 사용하는 조건부 생성-차별적 하이브리드 손실 클래스를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "161-2",
                    "sentence": "Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data.",
                    "sentence_kor": "모델이 제대로 학습하지 못한 훈련 데이터의 목표 미세 조정 목표와 직관적인 재사용의 조합을 통해, 우리는 추가 데이터를 사용하지 않고 문장 수준과 상황별 모델 모두의 모델 성능을 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "161-3",
                    "sentence": "We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset.",
                    "sentence_kor": "우리는 미세 조정을 통해 대명사 번역의 개선을 목표로 하고 대명사 벤치마크 테스트 세트에서 모델을 평가한다.",
                    "tag": "4"
                },
                {
                    "index": "161-4",
                    "sentence": "Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation.",
                    "sentence_kor": "문장 수준 모델은 WMT14와 IWSLT13 De-En 테스트셋에서 모두 0.5 BLEU가 개선된 반면, 상황별 모델은 WMT14 De-En 테스트셋에서 31.81에서 32 BLEU로, IWSLT13 De에서 32.10에서 33.13으로 개선되어 최상의 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "161-5",
                    "sentence": "We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.",
                    "sentence_kor": "또한 Fr-En과 Cs-En이라는 두 개의 추가 언어 쌍에 대한 개선 사항을 재현하여 우리 방법의 일반화 가능성을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "162",
            "abstractID": "EMNLP_abs-162",
            "text": [
                {
                    "index": "162-0",
                    "sentence": "Balancing accuracy and latency is a great challenge for simultaneous translation.",
                    "sentence_kor": "정확도와 대기 시간의 균형을 맞추는 것은 동시 변환의 큰 과제입니다.",
                    "tag": "1"
                },
                {
                    "index": "162-1",
                    "sentence": "To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency.",
                    "sentence_kor": "높은 정확도를 달성하기 위해 모델은 일반적으로 변환 전에 더 많은 스트리밍 텍스트를 기다려야 하므로 대기 시간이 증가합니다.",
                    "tag": "1"
                },
                {
                    "index": "162-2",
                    "sentence": "However, keeping low latency would probably hurt accuracy.",
                    "sentence_kor": "그러나 대기 시간을 짧게 유지하면 정확도가 저하될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "162-3",
                    "sentence": "Therefore, it is essential to segment the ASR output into appropriate units for translation.",
                    "sentence_kor": "따라서 변환하려면 ASR 출력을 적절한 단위로 분할해야 합니다.",
                    "tag": "1"
                },
                {
                    "index": "162-4",
                    "sentence": "Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation.",
                    "sentence_kor": "인간 해석가로부터 영감을 받아 동시 번역을 위한 새로운 적응 세분화 정책을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "162-5",
                    "sentence": "The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation.",
                    "sentence_kor": "이 정책은 번역 모델에 의해 생성된 가능한 번역을 고려하여 원본 텍스트를 분할하는 방법을 학습하여 분할과 번역 간의 일관성을 유지한다.",
                    "tag": "4"
                },
                {
                    "index": "162-6",
                    "sentence": "Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.",
                    "sentence_kor": "중국어-영어 및 독일어-영어 번역에 대한 실험 결과는 우리 방법이 최근 제안된 최첨단 방법에 비해 더 나은 정확도-지연성 절충을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "163",
            "abstractID": "EMNLP_abs-163",
            "text": [
                {
                    "index": "163-0",
                    "sentence": "Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks.",
                    "sentence_kor": "최근 다국어 사전 교육 언어 모델(MPLM)의 등장으로 다양한 다운스트림 언어 간 전송(CLT) 작업에 획기적인 발전이 가능해졌다.",
                    "tag": "1"
                },
                {
                    "index": "163-1",
                    "sentence": "However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks.",
                    "sentence_kor": "그러나 MPLM 기반 방법은 일반적으로 두 가지 문제를 포함한다. (1) 단순히 미세 조정만으로 범용 다국어 표현을 저자원 언어에서 작업 인식으로 적용할 수 없다. (2) 언어 간 적응이 다운스트림 작업에 어떻게 발생하는지는 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "163-2",
                    "sentence": "To address the issues, we propose a meta graph learning (MGL) method.",
                    "sentence_kor": "문제를 해결하기 위해 메타 그래프 학습(MTL) 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "163-3",
                    "sentence": "Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages.",
                    "sentence_kor": "처음부터 전송하는 이전 작업과 달리, MPL은 과거 CLT 경험(작업)에서 메타 지식을 추출하여 언어 간 전송을 배울 수 있으므로 MPLM은 저자원 언어에 둔감하다.",
                    "tag": "4"
                },
                {
                    "index": "163-4",
                    "sentence": "Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer.",
                    "sentence_kor": "또한, 각 CLT 작업에 대해, MTL은 그것의 전송 과정을 동적 그래프를 통한 정보 전파로 공식화한다. 여기서 기하학적 구조는 언어 간 전송을 명시적으로 안내하기 위해 고유 언어 관계를 자동으로 캡처할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "163-5",
                    "sentence": "Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.",
                    "sentence_kor": "경험적으로 공개 데이터 세트와 실제 데이터 세트 모두에 대한 광범위한 실험은 ML 방법의 효과를 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "164",
            "abstractID": "EMNLP_abs-164",
            "text": [
                {
                    "index": "164-0",
                    "sentence": "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality.",
                    "sentence_kor": "다국어 의존성 파싱의 최근 발전은 진정한 보편적 파서의 아이디어를 현실에 더 가깝게 만들었다.",
                    "tag": "1"
                },
                {
                    "index": "164-1",
                    "sentence": "However, cross-language interference and restrained model capacity remain major obstacles.",
                    "sentence_kor": "그러나 언어 간 간섭과 제한된 모델 용량은 여전히 주요 장애물로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "164-2",
                    "sentence": "To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules.",
                    "sentence_kor": "이를 해결하기 위해 상황별 매개 변수 생성 및 어댑터 모듈을 기반으로 한 새로운 다국어 작업 적응 방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "164-3",
                    "sentence": "This approach enables to learn adapters via language embeddings while sharing model parameters across languages.",
                    "sentence_kor": "이 접근 방식을 사용하면 언어 임베딩을 통해 어댑터를 학습하는 동시에 언어 간에 모델 매개 변수를 공유할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "164-4",
                    "sentence": "It also allows for an easy but effective integration of existing linguistic typology features into the parsing network.",
                    "sentence_kor": "또한 기존 언어 유형 기능을 구문 분석 네트워크에 쉽고 효과적으로 통합할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "164-5",
                    "sentence": "The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach.",
                    "sentence_kor": "그 결과 파서인 UDapter는 대부분의 고자원 및 저자원(제로샷) 언어에서 강력한 단일 언어 및 다국어 기준선을 능가하여 제안된 적응 접근법의 성공을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "164-6",
                    "sentence": "Our in-depth analyses show that soft parameter sharing via typological features is key to this success.",
                    "sentence_kor": "우리의 심층 분석은 유형학적 특징을 통한 부드러운 매개 변수 공유가 이러한 성공의 핵심이라는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "165",
            "abstractID": "EMNLP_abs-165",
            "text": [
                {
                    "index": "165-0",
                    "sentence": "Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks.",
                    "sentence_kor": "라벨 디코딩을 위한 조건부 무작위 필드(CRF)는 시퀀스 라벨링 작업에서 어디서나 볼 수 있게 되었다.",
                    "tag": "1"
                },
                {
                    "index": "165-1",
                    "sentence": "However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved.",
                    "sentence_kor": "그러나 로컬 레이블 종속성과 비효율적인 Viterbi 디코딩은 항상 해결해야 할 문제였다.",
                    "tag": "1"
                },
                {
                    "index": "165-2",
                    "sentence": "In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient.",
                    "sentence_kor": "본 연구에서는 훨씬 더 계산 효율적이면서 장기적인 라벨 의존성을 모델링하기 위한 새로운 2단계 라벨 디코딩 프레임워크를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "165-3",
                    "sentence": "A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction.",
                    "sentence_kor": "기본 모델은 먼저 초안 레이블을 예측한 다음 새로운 2스트림 셀프 어텐션 모델은 장거리 레이블 종속성을 기반으로 이러한 초안 예측을 개선하여 더 빠른 예측을 위해 병렬 디코딩을 달성할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "165-4",
                    "sentence": "In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation.",
                    "sentence_kor": "또한 부정확한 초안 라벨의 부작용을 완화하기 위해 베이지안 신경망을 사용하여 오류 발생 가능성이 높은 라벨을 표시하여 오류 전파를 방지하는 데 크게 도움이 될 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "165-5",
                    "sentence": "The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.",
                    "sentence_kor": "세 가지 시퀀스 라벨링 벤치마크에 대한 실험 결과는 제안된 방법이 CRF 기반 방법을 능가할 뿐만 아니라 추론 프로세스를 크게 가속화한다는 것을 보여주었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "166",
            "abstractID": "EMNLP_abs-166",
            "text": [
                {
                    "index": "166-0",
                    "sentence": "Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years.",
                    "sentence_kor": "효과적인 적대적 공격자를 구축하고 자연어 처리(NLP)를 위한 적대적 공격에 대한 대책을 상세히 설명하는 것은 최근 많은 연구를 유치했다.",
                    "tag": "1"
                },
                {
                    "index": "166-1",
                    "sentence": "However, most of the existing approaches focus on classification problems.",
                    "sentence_kor": "그러나 기존 접근법의 대부분은 분류 문제에 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "166-2",
                    "sentence": "In this paper, we investigate attacks and defenses for structured prediction tasks in NLP.",
                    "sentence_kor": "본 논문에서 우리는 NLP의 구조화된 예측 작업에 대한 공격과 방어를 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "166-3",
                    "sentence": "Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input.",
                    "sentence_kor": "개별 단어를 교란시키는 어려움과 NLP 작업에서 공격자가 직면하는 문장 유창성 문제 외에도 구조화된 예측 모델의 공격자에게는 특정한 과제가 있다. 구조화된 예측 모델의 구조화된 출력은 입력의 작은 동요에 민감하다.",
                    "tag": "3"
                },
                {
                    "index": "166-4",
                    "sentence": "To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task.",
                    "sentence_kor": "이러한 문제를 해결하기 위해, 우리는 동일한 구조화된 예측 작업의 여러 참조 모델에서 피드백을 받아 시퀀스 투 시퀀스 모델을 사용하여 구조화된 예측 모델을 공격하는 방법을 배우는 새롭고 통합된 프레임워크를 제안한다.",
                    "tag": "3+4"
                },
                {
                    "index": "166-5",
                    "sentence": "Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate.",
                    "sentence_kor": "제안된 공격을 기반으로, 우리는 적대적 훈련으로 피해자 모델을 더욱 강화하여 예측이 더욱 강력하고 정확하도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "166-6",
                    "sentence": "We evaluate the proposed framework in dependency parsing and part-of-speech tagging.",
                    "sentence_kor": "종속성 구문 분석 및 음성 부분 태깅에서 제안된 프레임워크를 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "166-7",
                    "sentence": "Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.",
                    "sentence_kor": "자동 및 인간 평가에 따르면 제안된 프레임워크는 최첨단 구조화된 예측 모델을 공격하고 적대적 훈련을 통해 모델을 강화하는데 모두 성공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "167",
            "abstractID": "EMNLP_abs-167",
            "text": [
                {
                    "index": "167-0",
                    "sentence": "Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment.",
                    "sentence_kor": "ASTE(Aspect Mentrium Triplet Extraction)는 대상 실체의 세 쌍둥이, 관련 감정 및 감정의 이유를 설명하는 의견 범위를 추출하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "167-1",
                    "sentence": "Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.",
                    "sentence_kor": "기존의 연구 노력은 대부분 파이프라인 접근 방식을 사용하여 이 문제를 해결하며, 이는 삼중 추출 프로세스를 여러 단계로 나눕니다.",
                    "tag": "1"
                },
                {
                    "index": "167-2",
                    "sentence": "Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach.",
                    "sentence_kor": "우리의 관찰은 세 쌍둥이 안에 있는 세 가지 요소는 서로 매우 관련이 깊다는 것이고, 이것은 시퀀스 태그 방식을 사용하여 세 쌍둥이를 추출하기 위한 공동 모델을 만들도록 동기를 부여한다.",
                    "tag": "1"
                },
                {
                    "index": "167-3",
                    "sentence": "However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question.",
                    "sentence_kor": "그러나 요소 간의 풍부한 상호작용을 포착할 수 있는 세 쌍둥이를 추출하기 위해 태그 지정 접근 방식을 효과적으로 설계하는 방법은 어려운 연구 질문이다.",
                    "tag": "1"
                },
                {
                    "index": "167-4",
                    "sentence": "In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets.",
                    "sentence_kor": "본 연구에서, 우리는 세 쌍둥이를 공동으로 추출할 수 있는 새로운 위치 인식 태그 체계를 가진 첫 번째 엔드 투 엔드 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "167-5",
                    "sentence": "Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches.",
                    "sentence_kor": "여러 기존 데이터 세트에 대한 실험 결과는 우리의 접근 방식을 사용하여 세 배의 요소를 공동으로 캡처하면 기존 접근 방식보다 성능이 향상된다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "167-6",
                    "sentence": "We also conducted extensive experiments to investigate the model effectiveness and robustness.",
                    "sentence_kor": "우리는 또한 모델 효과와 견고성을 조사하기 위해 광범위한 실험을 수행했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "168",
            "abstractID": "EMNLP_abs-168",
            "text": [
                {
                    "index": "168-0",
                    "sentence": "Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible.",
                    "sentence_kor": "동시 기계 변환(SiMT)은 연속 입력 텍스트 스트림을 가장 낮은 대기 시간과 가능한 최고 품질의 다른 언어로 변환하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "168-1",
                    "sentence": "The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation.",
                    "sentence_kor": "따라서 번역은 불완전한 원본 텍스트로 시작해야 하며, 이는 점차적으로 읽혀져 예상할 필요가 있다.",
                    "tag": "1"
                },
                {
                    "index": "168-2",
                    "sentence": "In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context.",
                    "sentence_kor": "본 논문에서는 시각적 정보를 추가하면 누락된 소스 컨텍스트를 보완할 수 있는지 여부를 이해하려고 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "168-3",
                    "sentence": "To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks.",
                    "sentence_kor": "이를 위해 다양한 다중 모드 접근 방식과 시각적 특징이 최첨단 SiMT 프레임워크에 미치는 영향을 분석한다.",
                    "tag": "3"
                },
                {
                    "index": "168-4",
                    "sentence": "Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios.",
                    "sentence_kor": "우리의 결과는 시각적 맥락이 유용하며 명시적 객체 영역 정보를 기반으로 한 시각적 기반 모델이 일반적으로 사용되는 전역 특징보다 훨씬 우수하여 낮은 지연 시간 시나리오에서 최대 3 BLEU 포인트 향상에 도달한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "168-5",
                    "sentence": "Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.",
                    "sentence_kor": "우리의 정성 분석은 영어와 프랑스어 사이의 형용사-명사 배치와 같은 어순의 차이를 다룰 뿐만 아니라 멀티모달 시스템만이 영어에서 성별 표시 언어로 올바르게 번역할 수 있는 경우를 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "169",
            "abstractID": "EMNLP_abs-169",
            "text": [
                {
                    "index": "169-0",
                    "sentence": "In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects.",
                    "sentence_kor": "인간의 언어 능력을 시뮬레이션하기 위해서는 자연어 처리 시스템이 가능한 원인과 결과를 포함하여 일상 상황의 역학을 추론할 수 있어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "169-1",
                    "sentence": "Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences.",
                    "sentence_kor": "게다가, 그들은 습득한 세계 지식을 새로운 언어, 모듈로 문화적 차이에 일반화할 수 있어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "169-2",
                    "sentence": "Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks.",
                    "sentence_kor": "기계 추론 및 언어 간 전송의 발전은 까다로운 평가 벤치마크의 가용성에 달려 있다.",
                    "tag": "1"
                },
                {
                    "index": "169-3",
                    "sentence": "Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole.",
                    "sentence_kor": "두 요구 모두에 자극을 받아 동부 아푸리맥 케추아 및 아이티 크레올과 같은 자원이 부족한 언어를 포함하는 11개 언어의 인과 상식 추론을 위한 유형학적으로 다양한 다국어 데이터 세트인 XCOPA를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "169-4",
                    "sentence": "We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer.",
                    "sentence_kor": "우리는 이 새로운 데이터 세트에서 다양한 최신 모델을 평가하여 다국어 사전 훈련 및 제로샷 미세 조정에 기반한 현재 방법의 성능이 번역 기반 전송에 비해 부족하다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "169-5",
                    "sentence": "Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline.",
                    "sentence_kor": "마지막으로, 우리는 다국어 모델을 작은 말뭉치 또는 이중 언어 사전만 사용할 수 있는 표본 외 자원 임대 언어에 적응시키고 무작위 기준선을 통해 상당한 개선을 보고하는 전략을 제안한다.",
                    "tag": "5"
                },
                {
                    "index": "169-6",
                    "sentence": "The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.",
                    "sentence_kor": "XCOPA 데이터 세트는 github.com/cambridgeltl/xcopa에서 무료로 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "170",
            "abstractID": "EMNLP_abs-170",
            "text": [
                {
                    "index": "170-0",
                    "sentence": "Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces.",
                    "sentence_kor": "교차 언어 NLP 작업의 성능은 현재 언어의 유사성에 의해 영향을 받는다. 예를 들어, 이전 연구에서는 이중 언어 어휘 유도(BLI)의 예상 성공과 단일 언어 임베딩 공간 사이의 (대략) 동형 가정 사이에 연관성이 있다고 제안했다.",
                    "tag": "1"
                },
                {
                    "index": "170-1",
                    "sentence": "In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT.",
                    "sentence_kor": "이 연구에서 우리는 수천 개의 언어 쌍과 네 가지 다른 작업인 BLI, 구문 분석, POS 태그 지정 및 MT를 다루는 단일 언어 임베딩 공간 유사성과 작업 성능 사이의 상관관계에 초점을 맞춘 대규모 연구를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "170-2",
                    "sentence": "We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned.",
                    "sentence_kor": "우리는 각 단일 언어 임베딩 공간의 스펙트럼 통계가 얼마나 잘 정렬될 수 있는지를 나타낸다고 가정한다.",
                    "tag": "3"
                },
                {
                    "index": "170-3",
                    "sentence": "We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra.",
                    "sentence_kor": "그런 다음 개별 스펙트럼의 관련 통계를 기반으로 두 임베딩 공간 사이에 몇 가지 동형성 측정을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "170-4",
                    "sentence": "We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret.",
                    "sentence_kor": "우리는 (1) 그러한 스펙트럼 동형 측정에서 도출된 언어 유사성 점수가 서로 다른 언어 간 작업에서 관찰된 성능과 강하게 연관되어 있음을 경험적으로 보여준다. (2) 우리의 스펙트럼 기반 측정치는 계산적으로 더 다루기 쉽고 상호간 용이하면서 이전 표준 동형 측정보다 일관되게 성능이 뛰어나다.시치미를 떼다",
                    "tag": "3+4"
                },
                {
                    "index": "170-5",
                    "sentence": "Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.",
                    "sentence_kor": "마지막으로, 우리의 측정은 유형학적으로 구동되는 언어 거리 측정에 대한 보완 정보를 캡처하고, 두 계열의 측정값을 조합하면 훨씬 더 높은 작업 성능 상관 관계를 산출한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "171",
            "abstractID": "EMNLP_abs-171",
            "text": [
                {
                    "index": "171-0",
                    "sentence": "Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation.",
                    "sentence_kor": "언어 유형학 데이터베이스의 희박한 언어 벡터와 다국어 기계 번역과 같은 작업에서 학습된 임베딩은 서로의 언어 특성화의 이점을 분석할 수 있는 방법을 분석하지 않고 단독으로 조사되었다.",
                    "tag": "1"
                },
                {
                    "index": "171-1",
                    "sentence": "We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source.",
                    "sentence_kor": "우리는 단일 벡터 표준 상관 분석을 사용하여 두 가지 관점을 융합하고 각 출처에서 유도되는 정보의 종류를 연구할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "171-2",
                    "sentence": "By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships.",
                    "sentence_kor": "유형학적 특징과 언어 계통 발생을 유추함으로써, 우리는 우리의 표현이 유형학을 포함하고 언어 관계와의 상관관계를 강화한다는 것을 관찰한다.",
                    "tag": "3+4"
                },
                {
                    "index": "171-3",
                    "sentence": "We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer.",
                    "sentence_kor": "그런 다음 다국어 기계 번역에 다중 뷰 언어 벡터 공간을 활용한다. 여기서 언어 클러스터링 및 다국어 전송을 위한 후보 순위 지정과 같은 언어 유사성에 대한 정보가 필요한 작업에서 경쟁력 있는 전체 번역 정확도를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "171-4",
                    "sentence": "With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.",
                    "sentence_kor": "이 방법을 사용하면 관련 접근 방식의 주요 단점인 대규모 다국어 또는 순위 모델의 값비싼 재교육 없이 새로운 언어를 쉽게 투영하고 평가할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "172",
            "abstractID": "EMNLP_abs-172",
            "text": [
                {
                    "index": "172-0",
                    "sentence": "Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping.",
                    "sentence_kor": "오늘날 많은 전자상거래 사이트에서 제품 관련 질문 응답 플랫폼이 널리 사용되고 있어 잠재 고객이 온라인 쇼핑 중에 자신의 우려를 해결할 수 있는 편리한 방법을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "172-1",
                    "sentence": "However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business.",
                    "sentence_kor": "그러나 이러한 플랫폼의 답변에서 잘못된 정보는 사용자가 신뢰할 수 있고 진실된 제품 정보를 얻는데 전례가 없는 도전을 제기하며, 이는 전자상거래 사업에서 상업적 손실을 초래할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "172-2",
                    "sentence": "To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums.",
                    "sentence_kor": "이 문제를 해결하기 위해 본 논문에서 답변의 정확성을 예측하고 제품 질문 답변 포럼에서 대규모 사실 확인 데이터 세트인 AnswerFact를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "172-3",
                    "sentence": "Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings.",
                    "sentence_kor": "각 답변에는 진실성 라벨과 관련 증거 문장이 첨부되어 QA 설정에서 증거 기반 사실 확인 작업을 위한 귀중한 테스트 베드를 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "172-4",
                    "sentence": "We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem.",
                    "sentence_kor": "우리는 또한 관련 답변 진실성 예측 문제를 처리하기 위해 맞춤형 증거 순위 구성 요소를 갖춘 새로운 신경 모델을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "172-5",
                    "sentence": "Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.",
                    "sentence_kor": "제안된 모델과 다양한 기존 사실 확인 방법을 사용하여 광범위한 실험을 수행하며, 우리의 방법이 이 작업에 대한 모든 기준선을 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "173",
            "abstractID": "EMNLP_abs-173",
            "text": [
                {
                    "index": "173-0",
                    "sentence": "Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage.",
                    "sentence_kor": "추출 QA 모델은 주어진 구절에 대한 질문에 대한 정답을 예측하는 데 매우 유망한 성능을 보여 주었다.",
                    "tag": "1"
                },
                {
                    "index": "173-1",
                    "sentence": "However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question.",
                    "sentence_kor": "그러나, 그들은 때때로 주어진 질문과는 무관한 맥락에서 정답 텍스트를 예측하는 결과를 낳는다.",
                    "tag": "1"
                },
                {
                    "index": "173-2",
                    "sentence": "This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases.",
                    "sentence_kor": "이 불일치는 한 구절에서 답변 텍스트의 발생 횟수가 증가함에 따라 특히 중요해진다.",
                    "tag": "1"
                },
                {
                    "index": "173-3",
                    "sentence": "To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task.",
                    "sentence_kor": "이 문제를 해결하기 위해 다중 작업 학습 방식의 보조 작업으로 컨텍스트 예측과 컨텍스트 예측 작업을 학습하는 블록 어텐션 방법의 두 가지 주요 아이디어를 기반으로 BLANC(BLock AgentioN for Context 예측)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "173-4",
                    "sentence": "With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases.",
                    "sentence_kor": "읽기 이해에 대한 실험을 통해, 우리는 BLANC가 최첨단 QA 모델을 능가하며, 답변 텍스트 발생 횟수가 증가함에 따라 성능 격차가 증가한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "173-5",
                    "sentence": "We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.",
                    "sentence_kor": "또한 SQuAD를 사용하여 모델을 교육하고 HotpotQA에 대한 지원 사실을 예측하는 실험을 수행하며 이 제로샷 설정에서 BLAN이 모든 기준 모델을 능가한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "174",
            "abstractID": "EMNLP_abs-174",
            "text": [
                {
                    "index": "174-0",
                    "sentence": "Document interpretation and dialog understanding are the two major challenges for conversational machine reading.",
                    "sentence_kor": "문서 해석과 대화 이해는 대화 기계 판독의 두 가지 주요 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "174-1",
                    "sentence": "In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog.",
                    "sentence_kor": "본 연구에서는 연결을 강화하고 문서와 대화 상자 모두에 대한 이해를 높이기 위해 담화 인식 수반 추론 네트워크인 \"Discern\"을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "174-2",
                    "sentence": "Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation.",
                    "sentence_kor": "특히 사전 훈련된 담화 세분화 모델을 사용하여 문서를 절과 같은 기본 담화 단위(EDU)로 나누고, 대화에서 각 EDU가 사용자 피드백에 의해 수반되는지 여부를 예측하기 위해 모델을 약하게 훈련한다.",
                    "tag": "3"
                },
                {
                    "index": "174-3",
                    "sentence": "Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information.",
                    "sentence_kor": "학습된 EDU 및 수반 표현에 기초하여 사용자에게 초기 질문에 대한 최종 결정을 \"예/아니오/무관계\"로 회신하거나 추가 정보를 조회하기 위한 후속 질문을 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "174-4",
                    "sentence": "Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation.",
                    "sentence_kor": "ShARC 벤치마크(블라인드, 홀드아웃 테스트 세트)에 대한 우리의 실험은 Dispect가 의사 결정 시 78.3%의 매크로 평균 정확도와 후속 질문 생성 시 64.0 BLEU1의 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "174-5",
                    "sentence": "Code and models are released at https://github.com/Yifan-Gao/Discern.",
                    "sentence_kor": "코드와 모델은 https://github.com/Yifan-Gao/Discern에서 공개된다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "175",
            "abstractID": "EMNLP_abs-175",
            "text": [
                {
                    "index": "175-0",
                    "sentence": "Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models.",
                    "sentence_kor": "기계 생성 텍스트를 자동으로 식별하는 작업인 딥페이크 탐지는 최근 자연어 생성 모델의 발전과 함께 점점 더 중요해지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "175-1",
                    "sentence": "Existing approaches to deepfake detection typically represent documents with coarse-grained representations.",
                    "sentence_kor": "딥페이크 탐지에 대한 기존 접근 방식은 일반적으로 거친 표현으로 문서를 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "175-2",
                    "sentence": "However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis.",
                    "sentence_kor": "그러나 통계 분석에 따르면 기계가 작성한 텍스트와 사람이 작성한 텍스트를 구별하는 요소인 문서의 사실 구조를 포착하기 위해 고군분투한다.",
                    "tag": "1"
                },
                {
                    "index": "175-3",
                    "sentence": "To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text.",
                    "sentence_kor": "이를 해결하기 위해 문서의 사실 구조를 활용하여 텍스트의 딥페이크 탐지를 위한 그래프 기반 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "175-4",
                    "sentence": "Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network.",
                    "sentence_kor": "우리의 접근 방식은 주어진 문서의 사실 구조를 개체 그래프로 나타내며, 그래프 신경망을 사용하여 문장 표현을 학습하는 데 추가로 활용된다.",
                    "tag": "3"
                },
                {
                    "index": "175-5",
                    "sentence": "Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled.",
                    "sentence_kor": "그런 다음 문장 표현은 예측을 위한 문서 표현으로 구성되며, 여기서 인접 문장 간의 일관된 관계가 순차적으로 모델링된다.",
                    "tag": "3"
                },
                {
                    "index": "175-6",
                    "sentence": "Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa.",
                    "sentence_kor": "두 개의 공개 딥페이크 데이터 세트에 대한 실험 결과는 우리의 접근 방식이 RoBERTa로 구축된 강력한 기본 모델을 크게 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "175-7",
                    "sentence": "Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.",
                    "sentence_kor": "모델 분석은 또한 우리의 모델이 기계 생성 텍스트와 사람이 작성한 텍스트 사이의 사실 구조의 차이를 구별할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "176",
            "abstractID": "EMNLP_abs-176",
            "text": [
                {
                    "index": "176-0",
                    "sentence": "We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English.",
                    "sentence_kor": "우리는 영어로 된 커뮤니티 질문 응답 포럼의 140개 소스 도메인에 대한 자체 지도 교육을 통해 대규모 텍스트 매칭 모델의 제로샷 전송 기능을 연구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "176-1",
                    "sentence": "We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines.",
                    "sentence_kor": "우리는 답변 선택과 질문 유사성 작업의 9개 벤치마크에서 모델 성능을 조사하고 140개 모델 모두가 놀랍게도 잘 전달된다는 것을 보여준다. 여기서 대부분의 모델은 일반적인 IR 기준선을 상당히 능가한다.",
                    "tag": "3+4"
                },
                {
                    "index": "176-2",
                    "sentence": "We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains.",
                    "sentence_kor": "또한 가장 크고 가장 유사한 도메인에 의존하는 표준 절차와 대조되는 최상의 제로샷 전송 성능을 얻기 위해서는 광범위한 소스 도메인을 고려하는 것이 중요하다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "176-3",
                    "sentence": "In addition, we extensively study how to best combine multiple source domains.",
                    "sentence_kor": "또한 여러 소스 도메인을 가장 잘 결합하는 방법을 광범위하게 연구한다.",
                    "tag": "4"
                },
                {
                    "index": "176-4",
                    "sentence": "We propose to incorporate self-supervised with supervised multi-task learning on all available source domains.",
                    "sentence_kor": "우리는 사용 가능한 모든 소스 도메인에 대해 감독되는 다중 작업 학습과 함께 자체 감독을 통합할 것을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "176-5",
                    "sentence": "Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks.",
                    "sentence_kor": "당사의 최고의 제로샷 전송 모델은 6개의 벤치마크에서 도메인 내 BERT 및 이전 기술 수준을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "176-6",
                    "sentence": "Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.",
                    "sentence_kor": "도메인 내 데이터로 모델을 미세 조정하면 추가로 큰 이득을 얻을 수 있으며 9개 벤치마크 모두에서 새로운 최첨단 기술을 달성할 수 있습니다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "177",
            "abstractID": "EMNLP_abs-177",
            "text": [
                {
                    "index": "177-0",
                    "sentence": "Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph.",
                    "sentence_kor": "추상적 의미 표현(AMR)은 문장의 의미를 의미 그래프로 나타내는 자연어의 대중적인 형식주의이다.",
                    "tag": "1"
                },
                {
                    "index": "177-1",
                    "sentence": "It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages.",
                    "sentence_kor": "문자열에서 의미를 도출하는 방법에 대해서는 불가지론이며, 이러한 이유로 언어 간 의미론 인코딩에 잘 적응한다.",
                    "tag": "1"
                },
                {
                    "index": "177-2",
                    "sentence": "However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting.",
                    "sentence_kor": "그러나 영어 이외의 언어에서는 훈련 데이터가 부족하고 기존의 영어 AMR 파서는 교차 언어 환경에서 사용하기에 직접적으로 적합하지 않기 때문에 교차 언어 AMR 파싱은 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "177-3",
                    "sentence": "In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR.",
                    "sentence_kor": "이 작업에서는 언어 간 AMR 구문 분석을 가능하게 하기 위해 이 두 가지 문제를 해결한다. 우리는 언어 간 자동 AMR 주석을 생성하기 위한 다양한 전송 학습 기술을 탐색하고 언어 간 AMR 분석기인 XL-AMR을 개발한다.",
                    "tag": "1+2"
                },
                {
                    "index": "177-4",
                    "sentence": "This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing.",
                    "sentence_kor": "이는 생성된 데이터에 대해 학습할 수 있으며 영어 AMR 구문 분석에서 흔히 사용되는 것처럼 AMR 얼라이너 또는 소스 복사 메커니즘에 의존하지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "177-5",
                    "sentence": "The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish.",
                    "sentence_kor": "XL-AMR의 결과는 이전에 중국어, 독일어, 이탈리아어, 스페인어로 보고된 것을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "177-6",
                    "sentence": "Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages.",
                    "sentence_kor": "마지막으로, 언어에 걸친 AMR의 적합성을 조명하는 정성 분석을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "177-7",
                    "sentence": "We release XL-AMR at github.com/SapienzaNLP/xl-amr.",
                    "sentence_kor": "XL-AMR은 github.com/SapienzaNLP/xl-amr에서 출시됩니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "178",
            "abstractID": "EMNLP_abs-178",
            "text": [
                {
                    "index": "178-0",
                    "sentence": "In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance.",
                    "sentence_kor": "문헌에서 추상적 의미 표현(AMR) 구문 분석에 대한 연구는 우수한 성능의 AMR 분석기를 구축하는 데 중요한 인간 곡선 데이터 세트의 크기에 의해 크게 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "178-1",
                    "sentence": "To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing.",
                    "sentence_kor": "이러한 데이터 크기 제한을 완화하기 위해 사전 훈련된 모델은 AMR 구문 분석에서 점점 더 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "178-2",
                    "sentence": "However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing.",
                    "sentence_kor": "그러나 BERT와 같은 이전의 사전 교육 모델은 AMR 구문 분석이라는 특정 작업에 예상대로 작동하지 않을 수 있는 일반적인 목적으로 구현된다.",
                    "tag": "1"
                },
                {
                    "index": "178-3",
                    "sentence": "In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself.",
                    "sentence_kor": "본 논문에서 우리는 시퀀스 투 시퀀스(seq2seq) AMR 파싱에 초점을 맞추고 세 가지 관련 작업, 즉 기계 번역, 구문 분석 및 AMR 구문 분석 자체에서 단일 및 공동 방식으로 사전 훈련된 모델을 구축하기 위한 seq2seq 사전 훈련 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "178-4",
                    "sentence": "Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models.",
                    "sentence_kor": "또한 사전 훈련된 모델의 반응을 보존하기 위해 노력하는 동시에 AMR 구문 분석 성능을 최적화하는 멀티태스킹 학습 미세 조정 방법으로 바닐라 미세 조정 방법을 확장한다.",
                    "tag": "3+4"
                },
                {
                    "index": "178-5",
                    "sentence": "Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art.",
                    "sentence_kor": "두 개의 영어 벤치마크 데이터 세트에 대한 광범위한 실험 결과는 단일 및 공동 사전 교육 모델 모두 최신 기술에 도달하는 성능을 크게 향상시킨다는 것을 보여준다(예: AMR 2.0의 71.5에서 80.2까지).",
                    "tag": "4"
                },
                {
                    "index": "178-6",
                    "sentence": "The result is very encouraging since we achieve this with seq2seq models rather than complex models.",
                    "sentence_kor": "복잡한 모델이 아닌 seq2seq 모델로 이를 달성했기 때문에 결과는 매우 고무적이다.",
                    "tag": "5"
                },
                {
                    "index": "178-7",
                    "sentence": "We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.",
                    "sentence_kor": "코드 및 모델은 https:// github.com/xdqkid/S2S-AMR-Parser에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "179",
            "abstractID": "EMNLP_abs-179",
            "text": [
                {
                    "index": "179-0",
                    "sentence": "The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion.",
                    "sentence_kor": "인종, 성별, 종교에 대한 편견 없는 사회에서 함축되어 있기 때문에 소셜 미디어 콘텐츠에서 자동적으로 혐오 발언을 하고 모욕적인 언어를 탐지하는 일은 가장 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "179-1",
                    "sentence": "Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics.",
                    "sentence_kor": "그러나 이 분야의 기존 연구는 주로 영어에 초점을 맞추고 있어 특정 인구통계학적 적용가능성을 제한하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "179-2",
                    "sentence": "Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task.",
                    "sentence_kor": "Roman Urdu(RU)는 널리 보급되었지만 이 작업에 대한 언어 리소스, 주석이 달린 데이터 세트 및 언어 모델이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "179-3",
                    "sentence": "In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available.",
                    "sentence_kor": "본 연구에서 우리는 (1) RU의 혐오 단어 어휘를 제시하고 (2) 증오 음성 및 모욕 언어의 거친 레이블과 세분화된 레이블을 모두 사용하여 RU의 10,012개 트윗으로 구성된 주석이 달린 RUHSOLD라는 데이터 세트를 개발한다. (3) 기존 임베딩 모델 5개를 RU, (4)신설로 이전할 수 있는 가능성을 탐구한다. 증오 음성 및 모욕적인 언어 탐지를 위해 CNN-그램이라는 학습 아키텍처가 그 성능을 RUHSOLD 데이터 세트의 현재 7가지 기본 접근법과 비교하고, (5) 470만 개 이상의 트윗에 도메인별 임베딩을 교육하여 공개적으로 사용할 수 있도록 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "179-4",
                    "sentence": "We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.",
                    "sentence_kor": "우리는 전송 학습이 처음부터 임베딩된 교육에 비해 더 유익하고 제안된 모델이 기준선에 비해 더 큰 견고성을 보인다는 결론을 내린다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "180",
            "abstractID": "EMNLP_abs-180",
            "text": [
                {
                    "index": "180-0",
                    "sentence": "Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data.",
                    "sentence_kor": "편파적 발언의 편견에 대한 연구는 일반적으로 데이터의 품질을 상대적으로 간과하면서 분류 성과를 향상시키는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "180-1",
                    "sentence": "We examine selection bias in hate speech in a language and label independent fashion.",
                    "sentence_kor": "우리는 언어에서 편파적 발언의 선택 편향을 조사하고 독립적인 방식에 라벨을 붙인다.",
                    "tag": "2"
                },
                {
                    "index": "180-2",
                    "sentence": "We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora.",
                    "sentence_kor": "우리는 먼저 주제 모델을 사용하여 11개의 혐오 발언 말뭉치에서 잠재 의미론을 발견한 다음, 주제와 말뭉치를 구축하는 데 자주 사용되는 검색어 사이의 의미적 유사성에 기초한 두 가지 편향 평가 지표를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "180-3",
                    "sentence": "We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.",
                    "sentence_kor": "데이터 세트를 비교하고 대조적인 사례 연구를 분석하여 데이터 수집 프로세스를 수정할 수 있는 가능성에 대해 논의한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "181",
            "abstractID": "EMNLP_abs-181",
            "text": [
                {
                    "index": "181-0",
                    "sentence": "In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions.",
                    "sentence_kor": "사이버 왕따의 컴퓨터 탐지에서 기존 작업은 주로 소셜 미디어 세션의 텍스트 분석에만 의존하는 일반 분류기를 구축하는 데 초점을 맞췄다.",
                    "tag": "1"
                },
                {
                    "index": "181-1",
                    "sentence": "Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying.",
                    "sentence_kor": "이러한 경험적 성공에도 불구하고, 우리는 중요한 누락 부분이 모델 설명 가능성, 즉 특정 미디어 세션이 사이버 폭력으로 감지되는 이유라고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "181-2",
                    "sentence": "In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection.",
                    "sentence_kor": "따라서 본 논문에서 우리는 설명할 수 있는 사이버 왕따 탐지를 위한 새로운 심층 모델인 HENIN(Heterigonal Neural Interaction Networks)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "181-3",
                    "sentence": "HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors.",
                    "sentence_kor": "HENIN에는 주석 인코더, 주석 후 공동 주의 하위 네트워크, 세션 세션 및 사후 상호 작용 추출기의 구성 요소가 포함되어 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "181-4",
                    "sentence": "Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.",
                    "sentence_kor": "실제 데이터 세트에 대해 수행된 광범위한 실험은 HENIN의 유망한 성능을 보여줄 뿐만 아니라 미디어 세션이 사이버 왕따로 식별되는 이유를 이해할 수 있도록 증거 코멘트를 강조한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "182",
            "abstractID": "EMNLP_abs-182",
            "text": [
                {
                    "index": "182-0",
                    "sentence": "Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data.",
                    "sentence_kor": "비아냥 탐지는 많은 양의 라벨링된 데이터가 필요한 정서적 컴퓨팅에서 중요한 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "182-1",
                    "sentence": "We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques.",
                    "sentence_kor": "우리는 온라인 대화의 역학을 활용하여 기존 데이터 수집 기법의 한계를 극복하는 새로운 데이터 수집 방법인 반응성 감독을 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "182-2",
                    "sentence": "We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features.",
                    "sentence_kor": "우리는 비꼬는 투시 레이블과 새로운 상황적 특징을 가진 최초의 대규모 트윗 데이터 세트를 만들고 출시하기 위해 새로운 방법을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "182-3",
                    "sentence": "The dataset is expected to advance sarcasm detection research.",
                    "sentence_kor": "그 데이터 세트는 비꼬는 탐지기 연구를 진보시킬 것으로 기대된다.",
                    "tag": "5"
                },
                {
                    "index": "182-4",
                    "sentence": "Our method can be adapted to other affective computing domains, thus opening up new research opportunities.",
                    "sentence_kor": "우리의 방법은 다른 정서적 컴퓨팅 영역에 적응될 수 있으므로 새로운 연구 기회를 열 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "183",
            "abstractID": "EMNLP_abs-183",
            "text": [
                {
                    "index": "183-0",
                    "sentence": "Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle.",
                    "sentence_kor": "자체 감독 신경 기계 변환(SSNMT)은 두 작업이 선순환에서 서로를 지원하는 방식으로 비교 가능한 (병렬이 아닌) 말뭉치에서 적절한 훈련 데이터를 식별하고 선택하는 방법을 공동으로 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "183-1",
                    "sentence": "In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training.",
                    "sentence_kor": "본 연구에서는 SSNMT 모델이 교육 중에 선택하는 샘플링에 대한 심층 분석을 제공합니다.",
                    "tag": "2"
                },
                {
                    "index": "183-2",
                    "sentence": "We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum.",
                    "sentence_kor": "우리는 모델이 (iii) 노이즈 제거 커리큘럼 수행과 함께 (i) 복잡성 증가 및 (ii) 작업 관련성 샘플을 스스로 선택하는 방법을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "183-3",
                    "sentence": "We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance.",
                    "sentence_kor": "우리는 두 시스템 내부 표현 유형의 상호 감독 신호의 역학은 추출 및 변환 성능에 필수적이라는 것을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "183-4",
                    "sentence": "We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.",
                    "sentence_kor": "SSNMT는 Gunning-Fog 가독성 지수와 관련하여 고등학생에게 적합한 Wikipedia 데이터를 추출하고 학습하기 시작하여 학부 1학년 학생에게 적합한 콘텐츠로 빠르게 이동하고 있음을 보여줍니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "184",
            "abstractID": "EMNLP_abs-184",
            "text": [
                {
                    "index": "184-0",
                    "sentence": "Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train.",
                    "sentence_kor": "문자 수준에 트랜스포머 아키텍처를 적용하는 것은 보통 훈련하기 어렵고 느린 매우 심층적인 아키텍처를 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "184-1",
                    "sentence": "These problems can be partially overcome by incorporating a segmentation into tokens in the model.",
                    "sentence_kor": "모델의 토큰에 세그먼트화를 통합하여 이러한 문제를 부분적으로 해결할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "184-2",
                    "sentence": "We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation.",
                    "sentence_kor": "우리는 처음에 하위 단어 모델을 교육한 다음 문자로 미세 조정함으로써 토큰 분할 없이 문자 수준에서 작동하는 신경 기계 변환 모델을 얻을 수 있음을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "184-3",
                    "sentence": "We use only the vanilla 6-layer Transformer Base architecture.",
                    "sentence_kor": "우리는 바닐라 6계층 트랜스포머 베이스 아키텍처만 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "184-4",
                    "sentence": "Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality.",
                    "sentence_kor": "우리의 문자 수준 모델은 형태학적 현상을 더 잘 포착하고 다소 더 나쁜 전체 번역 품질을 희생시키면서 노이즈에 대한 더 견고성을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "184-5",
                    "sentence": "Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.",
                    "sentence_kor": "우리의 연구는 고성능이며 매우 크지 않은 캐릭터 기반 모델을 훈련하기 쉬운 중요한 단계이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "185",
            "abstractID": "EMNLP_abs-185",
            "text": [
                {
                    "index": "185-0",
                    "sentence": "Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages.",
                    "sentence_kor": "mBERT 및 XLM-RoBERTa와 같은 다국어 변압기 모델은 다양한 언어의 많은 NLP 작업에 대해 크게 개선되었다.",
                    "tag": "1"
                },
                {
                    "index": "185-1",
                    "sentence": "However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios.",
                    "sentence_kor": "그러나 최근 연구는 또한 고자원 언어의 결과가 현실적이고 저자원 시나리오로 쉽게 전달될 수 없다는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "185-2",
                    "sentence": "In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification.",
                    "sentence_kor": "본 연구에서는 Hausa, isiXhosa 세 아프리카 언어와 NER 및 주제 분류 모두에 대해 서로 다른 양의 가용 리소스에 대한 성능 동향을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "185-3",
                    "sentence": "We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data.",
                    "sentence_kor": "우리는 이러한 모델이 전송 학습 또는 원격 감독과 결합하여 훨씬 더 많은 지도 훈련 데이터가 있는 기준선과 동일한 성능을 10개에서 100개 정도의 레이블링된 문장으로 달성할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "185-4",
                    "sentence": "However, we also find settings where this does not hold.",
                    "sentence_kor": "그러나 이 설정이 유지되지 않는 설정도 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "185-5",
                    "sentence": "Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.",
                    "sentence_kor": "시간과 하드웨어 제한과 같은 가정에 대한 논의와 추가 실험은 저자원 학습의 도전과 기회를 강조한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "186",
            "abstractID": "EMNLP_abs-186",
            "text": [
                {
                    "index": "186-0",
                    "sentence": "The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations.",
                    "sentence_kor": "번역 품질 추정(QE) 과제, 특히 미터법 과제로서의 QE는 참조 번역을 사용하지 않고 번역 및 원본 문장을 기반으로 번역의 일반적인 품질을 평가하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "186-1",
                    "sentence": "Supervised learning of this QE task requires human evaluation of translation quality as training data.",
                    "sentence_kor": "이 QE 과제의 감독 학습을 위해서는 훈련 데이터로서 번역 품질에 대한 인체 평가가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "186-2",
                    "sentence": "Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations.",
                    "sentence_kor": "번역 품질에 대한 인간 평가는 번역에 절대 점수를 할당하거나 다른 번역의 순위를 매기는 등 다양한 방법으로 수행될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "186-3",
                    "sentence": "In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations.",
                    "sentence_kor": "지도 학습에 서로 다른 유형의 인간 평가 데이터를 활용하기 위해, 우리는 번역 점수를 매기고 번역 순위를 매기는 두 가지 과제를 공동으로 학습하는 다중 작업 학습 QE 모델을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "186-4",
                    "sentence": "Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models.",
                    "sentence_kor": "QE 모델은 사전 훈련된 다국어 언어 모델의 교차 언어 문장 임베딩을 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "186-5",
                    "sentence": "We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.",
                    "sentence_kor": "메트릭 작업으로 WMT 2019 QE에 대한 새로운 최첨단 결과를 얻었으며, SentB를 능가한다.WMT 2019 메트릭스 작업에 대한 LEU.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "187",
            "abstractID": "EMNLP_abs-187",
            "text": [
                {
                    "index": "187-0",
                    "sentence": "The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system.",
                    "sentence_kor": "음성 변환(ST)에 대한 계단식 접근법은 자동 음성 인식(ASR) 시스템과 기계 변환(MT) 시스템을 연결하는 파이프라인을 기반으로 한다.",
                    "tag": "1"
                },
                {
                    "index": "187-1",
                    "sentence": "These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system.",
                    "sentence_kor": "이러한 시스템은 일반적으로 ASR 출력을 MT 시스템에 공급할 희망적이고 의미론적으로 자체 포함 청크로 분할하는 세그먼트기로 연결된다.",
                    "tag": "1"
                },
                {
                    "index": "187-2",
                    "sentence": "This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account.",
                    "sentence_kor": "이는 대기 시간 요구 사항도 고려해야 하는 스트리밍 ST의 경우 특히 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "187-3",
                    "sentence": "This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk.",
                    "sentence_kor": "이 연구는 텍스트뿐만 아니라 ASR 출력이 청크로 분할되는 시기를 결정하는 음향 정보를 포함하는 스트리밍 ST를 위한 새로운 분할 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "187-4",
                    "sentence": "An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario.",
                    "sentence_kor": "스트리밍 ST 시나리오에서 BLEU 점수 측면에서 분할 모델의 성능에 대한 음향 정보의 기여를 입증하기 위해 Europarl-ST 데이터 세트에 대해 광범위하고 철저한 실험 설정이 수행된다.",
                    "tag": "4"
                },
                {
                    "index": "187-5",
                    "sentence": "Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.",
                    "sentence_kor": "마지막으로, 이전 연구와의 비교 결과도 이 연구에서 제안된 분할 모델의 우수성을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "188",
            "abstractID": "EMNLP_abs-188",
            "text": [
                {
                    "index": "188-0",
                    "sentence": "Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources.",
                    "sentence_kor": "세계에서 7번째로 널리 사용되는 언어임에도 불구하고, 벵골어는 자원이 부족하기 때문에 기계 번역 문학에서 훨씬 덜 주목을 받았습니다.",
                    "tag": "1"
                },
                {
                    "index": "188-1",
                    "sentence": "Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them.",
                    "sentence_kor": "벵골어에 대해 공개적으로 사용할 수 있는 대부분의 병렬 말뭉치는 충분히 크지 않으며, 대부분 잘못된 문장 분할로 인한 잘못된 문장 정렬과 또한 벵골어에 존재하는 많은 소음 때문에 품질이 좋지 않다.",
                    "tag": "1"
                },
                {
                    "index": "188-2",
                    "sentence": "In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering.",
                    "sentence_kor": "본 연구에서는 벵골어를 위한 맞춤형 문장 분할기를 구축하고 저자원 설정에서 병렬 말뭉치를 생성하기 위한 두 가지 새로운 방법인 얼라이너 앙상블링과 배치 필터링을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "188-3",
                    "sentence": "With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before.",
                    "sentence_kor": "분할자와 두 방법을 결합하여, 우리는 275만 개의 문장 쌍으로 구성된 고품질 벵골어-영어 병렬 말뭉치를 컴파일한다. 이 중 200만 개 이상은 이전에는 사용할 수 없었다.",
                    "tag": "3"
                },
                {
                    "index": "188-4",
                    "sentence": "Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation.",
                    "sentence_kor": "신경 모델에 대한 훈련을 통해 벵골어-영어 기계 번역에 대한 이전 접근 방식보다 9 BLEU 이상의 향상을 달성했다.",
                    "tag": "4"
                },
                {
                    "index": "188-5",
                    "sentence": "We also evaluate on a new test set of 1000 pairs made with extensive quality control.",
                    "sentence_kor": "우리는 또한 광범위한 품질 관리를 통해 만들어진 1000쌍의 새로운 테스트 세트에 대해 평가한다.",
                    "tag": "4"
                },
                {
                    "index": "188-6",
                    "sentence": "We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status.",
                    "sentence_kor": "분할자, 병렬 말뭉치 및 평가 세트를 릴리스하여 벵골어를 저자원 상태에서 상승시킨다.",
                    "tag": "4"
                },
                {
                    "index": "188-7",
                    "sentence": "To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation.",
                    "sentence_kor": "우리가 아는 한, 이것은 벵골어-영어 기계 번역에 대한 최초의 대규모 연구이다.",
                    "tag": "5"
                },
                {
                    "index": "188-8",
                    "sentence": "We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages.",
                    "sentence_kor": "우리는 우리의 연구가 다른 저자원 언어들뿐만 아니라 벵골어-영어 기계 번역에 대한 향후 연구를 위한 길을 열어줄 것이라고 믿는다.",
                    "tag": "5"
                },
                {
                    "index": "188-9",
                    "sentence": "Our data and code are available at https://github.com/csebuetnlp/banglanmt.",
                    "sentence_kor": "데이터와 코드는 https://github.com/csebuetnlp/banglanmt에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "189",
            "abstractID": "EMNLP_abs-189",
            "text": [
                {
                    "index": "189-0",
                    "sentence": "This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT).",
                    "sentence_kor": "본 논문은 신경 기계 번역(NMT)을 위한 코드 전환 사전 훈련(CSP)이라는 새로운 사전 훈련 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "189-1",
                    "sentence": "Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language.",
                    "sentence_kor": "입력 문장의 일부 조각을 무작위로 가리는 기존의 사전 훈련 방법과 달리, 제안된 CSP는 소스 문장의 일부 단어를 대상 언어의 번역 단어로 임의로 대체한다.",
                    "tag": "3"
                },
                {
                    "index": "189-2",
                    "sentence": "Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons.",
                    "sentence_kor": "특히, 먼저 소스 언어와 대상 언어 간의 감독되지 않은 단어 임베딩 매핑으로 어휘를 유도한 다음 추출된 번역 어휘에 따라 입력 문장의 일부 단어를 임의로 번역 단어로 대체한다.",
                    "tag": "3"
                },
                {
                    "index": "189-3",
                    "sentence": "CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence.",
                    "sentence_kor": "CSP는 인코더-디코더 프레임워크를 채택한다. 인코더는 코드 혼합 문장을 입력으로 사용하고 디코더는 입력 문장의 교체된 부분을 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "189-4",
                    "sentence": "In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus.",
                    "sentence_kor": "이러한 방식으로 CSP는 소스 및 대상 단일 언어 말뭉치에서 추출한 정렬 정보를 명시적으로 활용하여 NMT 모델을 사전 교육할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "189-5",
                    "sentence": "Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].",
                    "sentence_kor": "또한 [마스크]와 같은 인공 기호로 인해 발생하는 사전 훈련과 최종 조정 불일치를 완화한다.",
                    "tag": "3"
                },
                {
                    "index": "189-6",
                    "sentence": "To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT.",
                    "sentence_kor": "제안된 방법의 효과를 검증하기 위해, 우리는 감독 및 감독되지 않은 NMT에 대한 광범위한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "189-7",
                    "sentence": "Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.",
                    "sentence_kor": "실험 결과에 따르면 CSP는 사전 교육이나 다른 사전 교육 방법 없이 기준선에 비해 크게 개선된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "190",
            "abstractID": "EMNLP_abs-190",
            "text": [
                {
                    "index": "190-0",
                    "sentence": "The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male doctors.",
                    "sentence_kor": "NLP의 성 편견에 대한 이전 연구에서 영어에 대한 일방적인 초점은 다른 언어의 기회를 놓치고 있다. GAP 및 WinoGender와 같은 영어 도전 데이터 세트는 예를 들어 남성 의사로서의 '의사'의 성별 모호성 발생과 같은 모델 선호도를 강조한다.",
                    "tag": "1"
                },
                {
                    "index": "190-1",
                    "sentence": "We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of ‘the doctor removed his mask’ is not ambiguous between a coreferential reading and a disjoint reading.",
                    "sentence_kor": "스웨덴어와 러시아어와 같이 B형 반사 신경화가 있는 언어의 경우, 명확하게 잘못된 모델 예측을 초래하는 성별 편향을 감지하기 위한 다중 작업 과제 데이터 세트를 구성할 수 있음을 보여준다. 이들 언어에서, '의사가 마스크를 벗었다'의 직역어는 핵심적 독서와 분리적 독해 사이에서 모호하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "190-2",
                    "sentence": "Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive.",
                    "sentence_kor": "대신, 중심미분 대명사는 비격차 대명사를 필요로 하고, 유전대명사는 반반향대명사를 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "190-3",
                    "sentence": "We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon.",
                    "sentence_kor": "우리는 4개 언어와 4개 NLP 작업에 걸쳐 있고 이 현상에만 초점을 맞춘 다국어 다중 작업 과제 데이터 세트를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "190-4",
                    "sentence": "We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.",
                    "sentence_kor": "우리는 모든 업무 언어 조합에 걸쳐 성별 편견에 대한 증거를 찾고 모델 편견을 국가 노동 시장 통계와 연관시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "191",
            "abstractID": "EMNLP_abs-191",
            "text": [
                {
                    "index": "191-0",
                    "sentence": "We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs?",
                    "sentence_kor": "기계 번역(MT)을 위해 다음 질문을 조사한다. 공통 시드 역할을 하는 단일 범용 MT 모델을 개발하고 임의 언어 쌍에 대한 파생 및 개선 모델을 얻을 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "191-1",
                    "sentence": "We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model.",
                    "sentence_kor": "범용 다국어 신경 기계 번역 모델을 사전 교육하는 접근 방식인 mRASP를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "191-2",
                    "sentence": "Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space.",
                    "sentence_kor": "mRASP의 핵심 아이디어는 무작위 정렬 대체의 새로운 기술로, 표현 공간에서 여러 언어에 걸쳐 유사한 의미의 단어와 구를 더 가까이 가져온다.",
                    "tag": "3"
                },
                {
                    "index": "191-3",
                    "sentence": "We pre-train a mRASP model on 32 language pairs jointly with only public datasets.",
                    "sentence_kor": "32개 언어 쌍에 대해 공개 데이터 세트만 함께 mRASP 모델을 사전 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "191-4",
                    "sentence": "The model is then fine-tuned on downstream language pairs to obtain specialized MT models.",
                    "sentence_kor": "그런 다음 이 모델은 다운스트림 언어 쌍에서 미세 조정되어 전문 MT 모델을 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "191-5",
                    "sentence": "We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs.",
                    "sentence_kor": "우리는 저, 중, 풍부한 자원을 포함한 다양한 환경에서 42개의 번역 방향에 대한 광범위한 실험을 수행하고 외래어 쌍으로 변환한다.",
                    "tag": "3"
                },
                {
                    "index": "191-6",
                    "sentence": "Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs.",
                    "sentence_kor": "실험 결과는 mRASP가 목표 쌍에 대한 직접 교육에 비해 상당한 성능 향상을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "191-7",
                    "sentence": "It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT.",
                    "sentence_kor": "여러 개의 저자원 언어 쌍을 사용하여 리치 리소스 MT를 개선할 수 있는지 확인한 것은 이번이 처음이다.",
                    "tag": "4"
                },
                {
                    "index": "191-8",
                    "sentence": "Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus.",
                    "sentence_kor": "놀랍게도, mRASP는 사전 훈련 말뭉치에서 결코 발생하지 않는 외래어 번역 품질을 향상시킬 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "191-9",
                    "sentence": "Code, data, and pre-trained models are available at https://github.",
                    "sentence_kor": "코드, 데이터 및 사전 교육 모델은 https://github에서 이용할 수 있습니다.",
                    "tag": "6"
                },
                {
                    "index": "191-10",
                    "sentence": "com/linzehui/mRASP.",
                    "sentence_kor": "com/linzehui/mRASP",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "192",
            "abstractID": "EMNLP_abs-192",
            "text": [
                {
                    "index": "192-0",
                    "sentence": "The attention mechanism is the crucial component of the transformer architecture.",
                    "sentence_kor": "주의 메커니즘은 변압기 아키텍처의 중요한 구성요소이다.",
                    "tag": "1"
                },
                {
                    "index": "192-1",
                    "sentence": "Recent research shows that most attention heads are not confident in their decisions and can be pruned.",
                    "sentence_kor": "최근 연구에 따르면 대부분의 주의력 헤드는 자신의 결정에 자신이 없으며 가지치기할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "192-2",
                    "sentence": "However, removing them before training a model results in lower quality.",
                    "sentence_kor": "그러나 모형을 교육하기 전에 제거할 경우 품질이 저하됩니다.",
                    "tag": "1"
                },
                {
                    "index": "192-3",
                    "sentence": "In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training.",
                    "sentence_kor": "본 논문에서, 우리는 훈련의 초기 단계에서 머리를 자르기 위해 복권 가설을 적용한다.",
                    "tag": "1+2"
                },
                {
                    "index": "192-4",
                    "sentence": "Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English.",
                    "sentence_kor": "기계 번역에 대한 우리의 실험은 터키어→영어용 BLEU의 평균 -0.1 변화로 초기 훈련 동안 최대 4분의 3의 어텐션 헤드를 변압기에서 제거할 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "192-5",
                    "sentence": "The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training.",
                    "sentence_kor": "가지치기된 모델은 비록 더 긴 훈련의 비용이 들지만 추론 속도는 1.5배 빠르다.",
                    "tag": "4"
                },
                {
                    "index": "192-6",
                    "sentence": "Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.",
                    "sentence_kor": "우리의 방법은 교사-교사와 같은 다른 접근방식을 보완하며, 영어→독일 학생 모델은 75%의 인코더 주의 제거와 0.2 BLEU 손실로 10%의 추가 속도를 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "193",
            "abstractID": "EMNLP_abs-193",
            "text": [
                {
                    "index": "193-0",
                    "sentence": "Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.",
                    "sentence_kor": "신경 기계 번역(NMT)은 고품질 문장 생성 능력으로 인해 큰 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "193-1",
                    "sentence": "Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers.",
                    "sentence_kor": "인간 번역과 비교하여 현재 NMT의 단점 중 하나는 특히 인간 독자의 경우, 번역이 정보를 생략하거나 관련 없는 부분을 생성하는 등 일반적으로 입력에 충실하지 않다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "193-2",
                    "sentence": "In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt).",
                    "sentence_kor": "본 논문에서, 우리는 NMT 모델(FEnmt라는 이름)의 충실도를 높이기 위한 다중 작업 학습 패러다임을 가진 새로운 훈련 전략을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "193-3",
                    "sentence": "During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated.",
                    "sentence_kor": "NMT 훈련 과정 동안, 우리는 훈련 세트에서 부분 집합을 추출하여 잘못 번역된 조각을 얻기 위해 그것들을 번역한다.",
                    "tag": "3"
                },
                {
                    "index": "193-4",
                    "sentence": "Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments.",
                    "sentence_kor": "그 후에 제안된 다중 작업 학습 패러다임을 인코더와 디코더 모두에 사용하여 NMT가 이러한 조각을 올바르게 번역하도록 안내한다.",
                    "tag": "3"
                },
                {
                    "index": "193-5",
                    "sentence": "Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.",
                    "sentence_kor": "자동 및 인간 평가 모두 FEnmt가 불성실한 번역을 효과적으로 줄임으로써 번역 품질을 향상시킬 수 있음을 검증한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "194",
            "abstractID": "EMNLP_abs-194",
            "text": [
                {
                    "index": "194-0",
                    "sentence": "We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements.",
                    "sentence_kor": "우리는 인간 판단과 새로운 최첨단 수준의 상관관계를 얻는 다국어 기계 번역 평가 모델을 훈련하기 위한 신경 프레임워크인 COMT를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "194-1",
                    "sentence": "Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality.",
                    "sentence_kor": "우리의 프레임워크는 MT 품질을 보다 정확하게 예측하기 위해 소스 입력과 대상 언어 참조 번역 모두의 정보를 이용하는 다국어 및 적응성이 높은 MT 평가 모델을 교차 언어 사전 교육 언어 모델링의 최근 혁신을 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "194-2",
                    "sentence": "To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric.",
                    "sentence_kor": "우리의 프레임워크를 보여주기 위해, 우리는 다른 유형의 인간 판단으로 세 가지 모델을 훈련시킨다. 직접 평가, 인간 매개 번역 편집 속도 및 다차원 품질 메트릭.",
                    "tag": "3"
                },
                {
                    "index": "194-3",
                    "sentence": "Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.",
                    "sentence_kor": "우리 모델은 WMT 2019 메트릭 공유 작업에서 새로운 최첨단 성능을 달성하고 고성능 시스템에 대한 견고성을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "195",
            "abstractID": "EMNLP_abs-195",
            "text": [
                {
                    "index": "195-0",
                    "sentence": "Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results.",
                    "sentence_kor": "비지도 신경 기계 번역(UNMT) 시스템을 초기화하기 위해 단일 언어 데이터가 큰 두 언어로 사전 훈련된 언어 모델(LM)을 사용하면 최첨단 결과를 얻을 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "195-1",
                    "sentence": "When limited data is available for one language, however, this method leads to poor translations.",
                    "sentence_kor": "그러나 한 언어에 대해 제한된 데이터를 사용할 수 있는 경우 이 방법은 잘못된 번역으로 이어집니다.",
                    "tag": "1"
                },
                {
                    "index": "195-2",
                    "sentence": "We present an effective approach that reuses an LM that is pretrained only on the high-resource language.",
                    "sentence_kor": "우리는 고자원 언어에서만 사전 교육을 받은 LM을 재사용하는 효과적인 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "195-3",
                    "sentence": "The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model.",
                    "sentence_kor": "단일 언어 LM은 두 언어 모두에서 미세 조정되며 UNMT 모델을 초기화하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "195-4",
                    "sentence": "To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language.",
                    "sentence_kor": "사전 교육을 받은 LM을 재사용하기 위해서는 사전 정의된 어휘를 수정하여 새로운 언어를 설명해야 합니다.",
                    "tag": "3"
                },
                {
                    "index": "195-5",
                    "sentence": "We therefore propose a novel vocabulary extension method.",
                    "sentence_kor": "그러므로 우리는 새로운 어휘 확장 방법을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "195-6",
                    "sentence": "Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.",
                    "sentence_kor": "우리의 접근 방식인 RE-LM은 영어-Macedonian(En-Mk) 및 영어-Albanian(En-Sq)에서 경쟁력 있는 언어 교차 사전 훈련 모델(XLM)을 능가하며, 네 가지 번역 방향 모두에 대해 +8.3 BLEU 이상의 점수를 산출한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "196",
            "abstractID": "EMNLP_abs-196",
            "text": [
                {
                    "index": "196-0",
                    "sentence": "Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic).",
                    "sentence_kor": "이중 언어 어휘 유도(BLI)에 대한 대부분의 성공적이고 지배적인 방법은 매핑 기반이며, 여기서 선형 매핑 함수는 다른 언어의 단어 임베딩 공간이 유사한 기하학적 구조를 나타낸다는 가정 하에 학습된다(즉, 거의 동형).",
                    "tag": "1"
                },
                {
                    "index": "196-1",
                    "sentence": "However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages.",
                    "sentence_kor": "그러나, 몇몇 최근의 연구들은 이 단순화된 가정을 비판했는데, 이는 밀접하게 관련된 언어에서도 일반적으로 성립되지 않는다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "196-2",
                    "sentence": "In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI.",
                    "sentence_kor": "본 연구에서는 BLI를 위한 교차 언어 단어 임베딩을 학습하는 새로운 준감독 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "196-3",
                    "sentence": "Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders.",
                    "sentence_kor": "우리 모델은 동형 가정과 독립적이며 두 개의 독립적으로 사전 훈련된 자동 인코더의 잠재 공간에서 비선형 매핑을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "196-4",
                    "sentence": "Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin.",
                    "sentence_kor": "서로 다른 두 데이터 세트의 자원이 풍부한 언어와 저자원 언어로 구성된 15개(15)개의 서로 다른 언어 쌍에 대한 광범위한 실험을 통해, 우리는 우리의 방법이 기존 모델을 상당한 차이로 능가한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "196-5",
                    "sentence": "Ablation studies show the importance of different model components and the necessity of non-linear mapping.",
                    "sentence_kor": "절제 연구는 다양한 모델 구성요소의 중요성과 비선형 매핑의 필요성을 보여줍니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "197",
            "abstractID": "EMNLP_abs-197",
            "text": [
                {
                    "index": "197-0",
                    "sentence": "As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other.",
                    "sentence_kor": "시퀀스 대 시퀀스 생성 작업으로서, 신경 기계 번역(NMT)은 자연스럽게 내재적 불확실성을 포함하며, 여기서 한 언어의 한 문장은 다른 언어의 여러 개의 유효한 대응물을 갖는다.",
                    "tag": "1"
                },
                {
                    "index": "197-1",
                    "sentence": "However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference.",
                    "sentence_kor": "그러나 NMT에 대한 지배적인 방법은 모델 훈련을 위해 병렬 말뭉치에서 이들 중 하나만 관찰하지만 추론 시 동일한 의미 하에서 적절한 변동을 처리해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "197-2",
                    "sentence": "This leads to a discrepancy of the data distribution between the training and the inference phases.",
                    "sentence_kor": "이는 훈련 단계와 추론 단계 사이의 데이터 분포의 불일치로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "197-3",
                    "sentence": "To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 불확실성 인식 의미 증대를 제안한다. 불확실성 인식 의미 증대는 의미론적으로 동일한 여러 소스 문장 사이의 보편적 의미 정보를 명시적으로 캡처하고 더 나은 번역을 위해 이 정보로 숨겨진 표현을 강화한다.",
                    "tag": "2+3"
                },
                {
                    "index": "197-4",
                    "sentence": "Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.",
                    "sentence_kor": "다양한 변환 작업에 대한 광범위한 실험은 우리의 접근 방식이 강력한 기준선과 기존 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "198",
            "abstractID": "EMNLP_abs-198",
            "text": [
                {
                    "index": "198-0",
                    "sentence": "A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts.",
                    "sentence_kor": "개핑 구조는 하나의 결막을 제외한 모든 결막에서 중복 요소가 용리되는 조정된 구조로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "198-1",
                    "sentence": "This paper proposes a method of parsing sentences with gapping to recover elided elements.",
                    "sentence_kor": "본 논문은 용출된 요소를 복구하기 위해 개핑이 있는 문장을 구문 분석하는 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "198-2",
                    "sentence": "The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements.",
                    "sentence_kor": "제안된 방법은 생략된 요소를 식별하는 데 유용한 문법 및 의미적 역할이 주석이 달린 구성 트리를 기반으로 한다.",
                    "tag": "3"
                },
                {
                    "index": "198-3",
                    "sentence": "Our method outperforms the previous method in terms of F-measure and recall.",
                    "sentence_kor": "우리의 방법은 F-측정 및 회수 측면에서 이전 방법을 능가한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "199",
            "abstractID": "EMNLP_abs-199",
            "text": [
                {
                    "index": "199-0",
                    "sentence": "We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures.",
                    "sentence_kor": "우리는 중첩되지 않은 구조를 포함하여 블록 2급 불연속 구성 트리의 스팬 기반 구문 분석을 위한 새로운 차트 기반 알고리즘을 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "199-1",
                    "sentence": "In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3).",
                    "sentence_kor": "특히, 우리는 O(n66)에서 O(n33)까지 더 작은 검색 공간과 시간 복잡성으로 파서의 변형을 구축할 수 있음을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "199-2",
                    "sentence": "The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers.",
                    "sentence_kor": "입방 시간 변형은 언어 트리 뱅크에서 관찰된 구성 요소의 98%를 포함하지만 연속 구성 요소 파서와 복잡성은 동일하다.",
                    "tag": "4"
                },
                {
                    "index": "199-3",
                    "sentence": "We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting.",
                    "sentence_kor": "우리는 독일어 및 영어 트리 뱅크(Negra, Tiger 및 DPTB)에 대한 접근 방식을 평가하고 완전히 감독된 환경에서 최첨단 결과를 보고한다.",
                    "tag": "4"
                },
                {
                    "index": "199-4",
                    "sentence": "We also experiment with pre-trained word embeddings and Bert-based neural networks.",
                    "sentence_kor": "또한 사전 훈련된 단어 임베딩과 버트 기반 신경망을 실험한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "200",
            "abstractID": "EMNLP_abs-200",
            "text": [
                {
                    "index": "200-0",
                    "sentence": "Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences.",
                    "sentence_kor": "(보편적) 의존성 구문 분석 성능의 언어 간 차이는 대부분 트리뱅크 크기, 평균 문장 길이, 평균 의존성 길이, 형태학적 복잡성 및 도메인 차이에 기인한다.",
                    "tag": "1"
                },
                {
                    "index": "200-1",
                    "sentence": "We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data?",
                    "sentence_kor": "앞에서 설명하지 않은 요소를 지적합니다. 단어 및 종속성 라벨에서 추출할 경우, 테스트 데이터의 그래프가 교육 데이터에서 몇 개 보여졌습니까?",
                    "tag": "1+2"
                },
                {
                    "index": "200-2",
                    "sentence": "We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.",
                    "sentence_kor": "그래프 동형을 계산하고, 트리뱅크 크기를 제쳐두고 훈련 그래프와 테스트 그래프 사이의 중복이 위와 같은 표준 설명보다 관찰된 변동을 더 많이 설명한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "201",
            "abstractID": "EMNLP_abs-201",
            "text": [
                {
                    "index": "201-0",
                    "sentence": "This paper reduces discontinuous parsing to sequence labeling.",
                    "sentence_kor": "본 논문은 시퀀스 라벨링에 대한 불연속 파싱을 줄인다.",
                    "tag": "1+2"
                },
                {
                    "index": "201-1",
                    "sentence": "It first shows that existing reductions for constituent parsing as labeling do not support discontinuities.",
                    "sentence_kor": "먼저 라벨링으로서의 성분 파싱에 대한 기존 감소가 불연속성을 지원하지 않는다는 것을 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "201-2",
                    "sentence": "Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence.",
                    "sentence_kor": "둘째, 이 간격을 메우고 트리 불연속성을 입력 시퀀스의 거의 순서가 정해진 순열로 인코딩할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "201-3",
                    "sentence": "Third, it studies whether such discontinuous representations are learnable.",
                    "sentence_kor": "셋째, 그러한 불연속적 표현을 배울 수 있는지 여부를 연구한다.",
                    "tag": "3"
                },
                {
                    "index": "201-4",
                    "sentence": "The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.",
                    "sentence_kor": "실험은 구조적 단순성에도 불구하고 올바른 표현 하에서 모델이 빠르고 정확하다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "202",
            "abstractID": "EMNLP_abs-202",
            "text": [
                {
                    "index": "202-0",
                    "sentence": "This paper focuses on tree-based modeling for the sentence classification task.",
                    "sentence_kor": "이 논문은 문장 분류 작업의 트리 기반 모델링에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "202-1",
                    "sentence": "In existing works, aggregating on a syntax tree usually considers local information of sub-trees.",
                    "sentence_kor": "기존 작업에서 구문 트리에서 집계하면 일반적으로 하위 트리의 로컬 정보를 고려합니다.",
                    "tag": "3"
                },
                {
                    "index": "202-2",
                    "sentence": "In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees.",
                    "sentence_kor": "대조적으로, 로컬 정보 외에도, 우리가 제안한 MSNN(Modularized Syntactic Neural Network)은 구문 범주 레이블을 활용하고 하위 트리를 모델링하는 동안 글로벌 컨텍스트를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "202-3",
                    "sentence": "In MSNN, each node of a syntax tree is modeled by a label-related syntax module.",
                    "sentence_kor": "MSNN에서 구문 트리의 각 노드는 레이블 관련 구문 모듈에 의해 모델링됩니다.",
                    "tag": "3"
                },
                {
                    "index": "202-4",
                    "sentence": "Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation.",
                    "sentence_kor": "각 구문 모듈은 하위 수준 모듈의 출력을 집계하고 마지막으로 루트 모듈은 문장 표현을 제공합니다.",
                    "tag": "3"
                },
                {
                    "index": "202-5",
                    "sentence": "We design a tree-parallel mini-batch strategy for efficient training and predicting.",
                    "sentence_kor": "우리는 효율적인 훈련과 예측을 위해 트리 병렬 미니 배치 전략을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "202-6",
                    "sentence": "Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.",
                    "sentence_kor": "4개의 벤치마크 데이터 세트에 대한 실험 결과는 MSNN이 문장 분류 작업에서 이전의 최첨단 트리 기반 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "203",
            "abstractID": "EMNLP_abs-203",
            "text": [
                {
                    "index": "203-0",
                    "sentence": "This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays.",
                    "sentence_kor": "본 논문은 논쟁적인 학생 에세이의 담화 요소를 모델링하기 위해 담화 수준에 자기 주의를 적응시킬 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "203-1",
                    "sentence": "Specifically, we focus on two issues.",
                    "sentence_kor": "특히, 우리는 두 가지 문제에 초점을 맞춥니다.",
                    "tag": "2"
                },
                {
                    "index": "203-2",
                    "sentence": "First, we propose structural sentence positional encodings to explicitly represent sentence positions.",
                    "sentence_kor": "첫째, 문장 위치를 명시적으로 나타내기 위한 구조적인 문장 위치 인코딩을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "203-3",
                    "sentence": "Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation.",
                    "sentence_kor": "둘째, 문장 간 주의를 사용하여 문장 상호 작용을 포착하고 문장 표현을 강화할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "203-4",
                    "sentence": "We conduct experiments on two datasets: a Chinese dataset and an English dataset.",
                    "sentence_kor": "우리는 중국어 데이터 세트와 영어 데이터 세트라는 두 데이터 세트에 대해 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "203-5",
                    "sentence": "We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.",
                    "sentence_kor": "우리는 (i) 문장 위치 인코딩이 담화 요소 식별을 위한 큰 개선으로 이어질 수 있다는 것을 발견했다. (iii) 문장의 구조적 상대적 위치 인코딩이 가장 효과적인 것으로 나타났다. (iii) 문장 간 주의 벡터는 담화 요소 식별을 위한 일종의 문장 표현으로 유용하다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "204",
            "abstractID": "EMNLP_abs-204",
            "text": [
                {
                    "index": "204-0",
                    "sentence": "Existing pre-trained large language models have shown unparalleled generative capabilities.",
                    "sentence_kor": "사전 교육을 받은 기존의 대형 언어 모델은 타의 추종을 불허하는 생성 능력을 보여 주었다.",
                    "tag": "1"
                },
                {
                    "index": "204-1",
                    "sentence": "However, they are not controllable.",
                    "sentence_kor": "하지만, 그것들은 통제할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "204-2",
                    "sentence": "In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base.",
                    "sentence_kor": "본 논문에서 우리는 대규모 언어 모델을 사용하고 외부 지식 기반을 통합하여 텍스트 생성을 제어하는 새로운 프레임워크인 MEGATRON-CNTRL을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "204-3",
                    "sentence": "Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator.",
                    "sentence_kor": "우리의 프레임워크는 키워드 예측기, 지식 검색기, 상황별 지식 순위기 및 조건부 텍스트 생성기로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "204-4",
                    "sentence": "As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding.",
                    "sentence_kor": "지식 랭커에 대한 실측 감시에 접근할 수 없기 때문에 문장 임베딩에서 약한 감시를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "204-5",
                    "sentence": "The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset.",
                    "sentence_kor": "경험적 결과는 우리 모델이 ROC 스토리 데이터 세트에 대한 이전 작업에 비해 반복이 적고 다양성이 높은 더 유창하고 일관되며 일관된 스토리를 생성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "204-6",
                    "sentence": "We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process.",
                    "sentence_kor": "우리는 스토리를 생성하는 데 사용되는 키워드를 대체하고 생성 프로세스를 다시 실행하여 모델의 제어 가능성을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "204-7",
                    "sentence": "Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords.",
                    "sentence_kor": "인간 평가 결과에 따르면 이러한 이야기의 77.5%가 새로운 키워드에 의해 성공적으로 통제되고 있다.",
                    "tag": "4"
                },
                {
                    "index": "204-8",
                    "sentence": "Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).",
                    "sentence_kor": "또한 모델을 1억 2400만 개에서 83억 개 매개 변수로 확장함으로써 우리는 대형 모델이 발전 품질(정합성의 경우 74.5%에서 93.0%로)과 제어 가능성(77.5%에서 91.5%) 모두를 향상시킨다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "205",
            "abstractID": "EMNLP_abs-205",
            "text": [
                {
                    "index": "205-0",
                    "sentence": "Recent years the task of incomplete utterance rewriting has raised a large attention.",
                    "sentence_kor": "최근 몇 년 동안 불완전한 발화 재작성 작업이 큰 관심을 불러 일으켰다.",
                    "tag": "1"
                },
                {
                    "index": "205-1",
                    "sentence": "Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism.",
                    "sentence_kor": "이전 작품들은 보통 이것을 기계 번역 작업으로 형성하고 복사 메커니즘이 있는 시퀀스 기반 아키텍처를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "205-2",
                    "sentence": "In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task.",
                    "sentence_kor": "본 논문에서, 우리는 그것을 의미론적 세분화 작업으로 공식화하는 새롭고 광범위한 접근방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "205-3",
                    "sentence": "Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix.",
                    "sentence_kor": "이러한 공식은 처음부터 생성하는 대신 편집 연산을 도입하고 문제를 단어 수준 편집 행렬의 예측으로 형상화합니다.",
                    "tag": "3"
                },
                {
                    "index": "205-4",
                    "sentence": "Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets.",
                    "sentence_kor": "로컬 및 글로벌 정보를 모두 캡처할 수 있는 우리의 접근 방식은 여러 공개 데이터 세트에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "205-5",
                    "sentence": "Furthermore, our approach is four times faster than the standard approach in inference.",
                    "sentence_kor": "또한, 우리의 접근 방식은 추론에서 표준 접근 방식보다 4배 더 빠르다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "206",
            "abstractID": "EMNLP_abs-206",
            "text": [
                {
                    "index": "206-0",
                    "sentence": "A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one.",
                    "sentence_kor": "신경망을 이용한 시퀀스 투 시퀀스(seq2seq) 학습은 경험적으로 오류가 있는 문장을 입력으로 받아들이고 수정된 문장을 출력하는 문법 오류 수정(GEC)에 효과적인 프레임워크임을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "206-1",
                    "sentence": "However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand.",
                    "sentence_kor": "그러나 seq2seq 프레임워크를 가진 GEC 모델의 성능은 현재 코퍼스의 크기와 품질에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "206-2",
                    "sentence": "We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set.",
                    "sentence_kor": "우리는 모델의 취약점을 지속적으로 식별하여 보다 의미 있고 가치 있는 훈련 사례를 생성하고 생성된 적대적 사례를 교육 세트에 점진적으로 추가하여 모델을 개선하기 위한 적대적 훈련에서 영감을 얻은 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "206-3",
                    "sentence": "Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.",
                    "sentence_kor": "광범위한 실험 결과는 그러한 적대적 훈련이 GEC 모델의 일반화 및 견고성 모두를 개선할 수 있다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "207",
            "abstractID": "EMNLP_abs-207",
            "text": [
                {
                    "index": "207-0",
                    "sentence": "Punning is a creative way to make conversation enjoyable and literary writing elegant.",
                    "sentence_kor": "펀닝은 대화를 즐겁게 하고 문학작품을 우아하게 만드는 창의적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "207-1",
                    "sentence": "In this paper, we focus on the task of generating a pun sentence given a pair of homophones.",
                    "sentence_kor": "본 논문에서 우리는 한 쌍의 동음이의어가 주어진 말장난 문장을 생성하는 작업에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "207-2",
                    "sentence": "We first find the constraint words supporting the semantic incongruity for a sentence.",
                    "sentence_kor": "우리는 먼저 문장의 의미적 부조화를 지원하는 제약 단어를 찾는다.",
                    "tag": "3"
                },
                {
                    "index": "207-3",
                    "sentence": "Then we rewrite the sentence with explicit positive and negative constraints.",
                    "sentence_kor": "그리고 나서 우리는 분명한 긍정과 부정의 제약조건으로 문장을 다시 쓴다.",
                    "tag": "3"
                },
                {
                    "index": "207-4",
                    "sentence": "Our model achieves the state-of-the-art results in both automatic and human evaluations.",
                    "sentence_kor": "우리 모델은 자동 및 인간 평가 모두에서 최첨단 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "207-5",
                    "sentence": "We further make an error analysis and discuss the challenges for the computational pun models.",
                    "sentence_kor": "또한 오류 분석을 수행하고 계산 말장난 모델에 대한 과제에 대해 논의한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "208",
            "abstractID": "EMNLP_abs-208",
            "text": [
                {
                    "index": "208-0",
                    "sentence": "Neural Natural Language Generation (NLG) systems are well known for their unreliability.",
                    "sentence_kor": "신경 자연어 생성(NLG) 시스템은 신뢰할 수 없는 것으로 잘 알려져 있다.",
                    "tag": "1"
                },
                {
                    "index": "208-1",
                    "sentence": "To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability.",
                    "sentence_kor": "이 문제를 극복하기 위해, 우리는 네트워크의 출력을 제한하고 신뢰성을 보장할 수 있는 데이터 확대 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "208-2",
                    "sentence": "While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task.",
                    "sentence_kor": "이 제한은 무작위로 샘플링할 때보다 생성의 다양성이 떨어진다는 것을 의미하지만, 우리는 둔하고 반복적인 텍스트를 생성하는 기존 신경 생성 접근법의 경향을 입증하는 실험을 포함하며, 이 작업의 다양성보다 신뢰성이 더 중요하다고 주장한다.",
                    "tag": "4"
                },
                {
                    "index": "208-3",
                    "sentence": "The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.",
                    "sentence_kor": "이 접근 방식을 사용하여 훈련된 시스템은 템플릿 시스템과 동일한 E2E NLG 챌린지 데이터 세트에서 의미적 정확도에서 100% 점수를 받았다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "209",
            "abstractID": "EMNLP_abs-209",
            "text": [
                {
                    "index": "209-0",
                    "sentence": "Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output.",
                    "sentence_kor": "구조화된 데이터에서 텍스트를 생성하는 것은 (i) 구조와 자연어(NL) 및 (ii) 의미론적으로 덜 규정된 입력과 완전히 지정된 NL 출력 사이의 간격을 메워야 하기 때문에 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "209-1",
                    "sentence": "Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties.",
                    "sentence_kor": "다국어 생성은 다양한 어순과 형태학적 특성을 가진 언어로 생성하는 것과 같은 추가적인 과제를 야기한다.",
                    "tag": "1"
                },
                {
                    "index": "209-2",
                    "sentence": "In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English.",
                    "sentence_kor": "본 연구에서는 구조적 입력으로서 추상적 의미 표현(AMR)에 초점을 맞추고 있으며, 이전 연구는 압도적으로 영어로만 생성하는 데 초점을 맞추고 있다.",
                    "tag": "2"
                },
                {
                    "index": "209-3",
                    "sentence": "We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages.",
                    "sentence_kor": "우리는 교차 언어 임베딩, 사전 교육 및 다국어 모델의 발전을 활용하여 21개의 다른 언어로 생성되는 다국어 AMR-텍스트 모델을 만든다.",
                    "tag": "3"
                },
                {
                    "index": "209-4",
                    "sentence": "Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics.",
                    "sentence_kor": "다국어 모델은 자동 측정 기준을 기반으로 18개 언어로 하나의 언어로 생성되는 기준선을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "209-5",
                    "sentence": "We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.",
                    "sentence_kor": "우리는 인간 평가를 사용하여 형태와 어순을 정확하게 포착하는 다국어 모델의 능력을 분석하고 원어민 화자들이 우리 세대가 유창하다고 판단한다는 것을 발견한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "210",
            "abstractID": "EMNLP_abs-210",
            "text": [
                {
                    "index": "210-0",
                    "sentence": "Bolukbasi et al.(2016) presents one of the first gender bias mitigation techniques for word embeddings.",
                    "sentence_kor": "Bolukbasi 등(2016)은 단어 임베딩에 대한 최초의 성 편견 완화 기법 중 하나를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "210-1",
                    "sentence": "Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings.",
                    "sentence_kor": "이 방법은 사전 훈련된 단어 임베딩을 입력으로 사용하고 임베딩의 성별 편향을 대부분 포착하는 선형 하위 공간을 분리하려고 시도한다.",
                    "tag": "1"
                },
                {
                    "index": "210-2",
                    "sentence": "As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings.",
                    "sentence_kor": "유사 평가 과제로 판단한 대로, 이들의 방법은 임베딩의 성별 편향을 사실상 제거한다.",
                    "tag": "1"
                },
                {
                    "index": "210-3",
                    "sentence": "However, an implicit and untested assumption of their method is that the bias subspace is actually linear.",
                    "sentence_kor": "그러나 방법의 암시적이고 검증되지 않은 가정은 치우침 부분공간이 실제로 선형이라는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "210-4",
                    "sentence": "In this work, we generalize their method to a kernelized, non-linear version.",
                    "sentence_kor": "이 작업에서는 커널화된 비선형 버전으로 방법을 일반화한다.",
                    "tag": "2"
                },
                {
                    "index": "210-5",
                    "sentence": "We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique.",
                    "sentence_kor": "커널 주성분 분석에서 영감을 얻어 비선형 바이어스 격리 기법을 도출한다.",
                    "tag": "4"
                },
                {
                    "index": "210-6",
                    "sentence": "We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear.",
                    "sentence_kor": "우리는 단어 임베딩에서 비선형 성 편견 완화에 대한 방법의 몇 가지 실질적인 단점을 논의하고 극복하며 편향 하위 공간이 실제로 선형인지 경험적으로 분석한다.",
                    "tag": "4"
                },
                {
                    "index": "210-7",
                    "sentence": "Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al.(2016).",
                    "sentence_kor": "우리의 분석은 성별 편견이 실제로 선형 하위 공간에 의해 잘 포착되어 Bolukbasi 등(2016)의 가정을 정당화한다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "211",
            "abstractID": "EMNLP_abs-211",
            "text": [
                {
                    "index": "211-0",
                    "sentence": "It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts.",
                    "sentence_kor": "다중 작업 상대와 비교하여 성능 저하 없이 다양한 작업에서 평생 언어 학습(LLL)을 수행하는 것은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "211-1",
                    "sentence": "To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation.",
                    "sentence_kor": "이 문제를 해결하기 위해 기존 LLL 아키텍처에 쉽게 적용할 수 있는 간단하지만 효율적인 방법인 평생 언어 지식 증류(L2KD)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "211-2",
                    "sentence": "Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation.",
                    "sentence_kor": "특히, LLL 모델이 새로운 과제에 대해 교육을 받을 때, 우리는 먼저 새로운 과제를 학습하고 지식 증류를 통해 지식을 LLL 모델에 전달하기 위해 교사 모델을 할당한다.",
                    "tag": "3"
                },
                {
                    "index": "211-3",
                    "sentence": "Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge.",
                    "sentence_kor": "따라서 LLL 모델은 이전에 학습한 지식을 유지하면서 새로운 작업에 더 잘 적응할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "211-4",
                    "sentence": "Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.",
                    "sentence_kor": "실험에 따르면 제안된 L2KD는 이전의 최첨단 모델을 일관되게 개선하고 LLL 작업에서 다중 작업 모델과 비교한 열화는 시퀀스 생성 및 텍스트 분류 작업 모두에서 잘 완화된다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "212",
            "abstractID": "EMNLP_abs-212",
            "text": [
                {
                    "index": "212-0",
                    "sentence": "To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems.",
                    "sentence_kor": "잠재 디리클레 할당과 같은 확률론적 주제 모델의 비모수 확장을 대규모 데이터 세트로 확장하기 위해, 실무자들은 병렬 및 분산 시스템에 점점 더 많이 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "212-1",
                    "sentence": "In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model.",
                    "sentence_kor": "본 연구에서는 계층적 디리클레 프로세스(HDP) 주제 모델에 대한 데이터 병렬 훈련을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "212-2",
                    "sentence": "Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model.",
                    "sentence_kor": "HDP 내 특정 조건부 분포의 표현을 기반으로, 우리는 HDP 주제 모델에 대한 이중 희소 데이터 병렬 샘플러를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "212-3",
                    "sentence": "This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient.",
                    "sentence_kor": "이 샘플러는 자연어로 발견된 모든 희소성의 원천을 활용하는데, 이는 계산을 효율적으로 하는 중요한 방법이다.",
                    "tag": "3"
                },
                {
                    "index": "212-4",
                    "sentence": "We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.",
                    "sentence_kor": "우리는 4일 이내에 단일 멀티 코어 기계를 사용하여 8m 문서와 768m 토큰이 있는 잘 알려진 말뭉치(PubMed)에서 우리의 방법을 벤치마킹한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "213",
            "abstractID": "EMNLP_abs-213",
            "text": [
                {
                    "index": "213-0",
                    "sentence": "Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples.",
                    "sentence_kor": "거의/제로샷 학습은 훈련 샘플이 거의 없거나 아예 없는 클래스의 인스턴스를 분류자가 인식해야 하는 많은 분류 작업의 큰 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "213-1",
                    "sentence": "It becomes more difficult in multi-label classification, where each instance is labelled with more than one class.",
                    "sentence_kor": "각 인스턴스는 둘 이상의 클래스로 라벨이 지정된 다중 라벨 분류에서 더욱 어려워진다.",
                    "tag": "1"
                },
                {
                    "index": "213-2",
                    "sentence": "In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification.",
                    "sentence_kor": "본 논문에서, 우리는 집계된 지식이 다중 레이블 제로/few-shot 문서 분류에 어떻게 도움이 될 수 있는지 연구하기 위해 서로 다른 의미 라벨 관계를 인코딩하는 다중 레이블 그래프의 지식을 융합하는 간단한 다중 그래프 집계 모델을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "213-3",
                    "sentence": "The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations.",
                    "sentence_kor": "이 모델은 사전 훈련된 단어 임베딩, 라벨 설명 및 사전 정의된 라벨 관계와 같은 세 가지 종류의 의미 정보를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "213-4",
                    "sentence": "Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.",
                    "sentence_kor": "두 개의 대규모 임상 데이터 세트(예: MIMIC-II 및 MIMIC-II)와 EU 법률 데이터 세트에서 도출된 실험 결과는 다중 그래프 지식 집계를 장착한 방법이 퓨/제로샷 레이블의 거의 모든 측정에서 상당한 성능 향상을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "214",
            "abstractID": "EMNLP_abs-214",
            "text": [
                {
                    "index": "214-0",
                    "sentence": "One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment.",
                    "sentence_kor": "텍스트 유사성을 평가하기 위한 한 가지 핵심 원칙은 단어 정렬을 고려하여 텍스트 간의 의미 중복 정도를 측정하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "214-1",
                    "sentence": "Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors.",
                    "sentence_kor": "그러한 정렬 기반 접근법은 직관적이고 해석 가능하다. 그러나 그것들은 범용 문장 벡터 사이의 단순한 코사인 유사성보다 경험적으로 열등하다.",
                    "tag": "1"
                },
                {
                    "index": "214-2",
                    "sentence": "We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity.",
                    "sentence_kor": "우리는 단어 벡터의 표준이 단어 중요성에 대한 좋은 대리점이고, 그것들의 각도가 단어 유사성에 대한 좋은 대리점이라는 사실에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "214-3",
                    "sentence": "However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance.",
                    "sentence_kor": "그러나 정렬 기반 접근법은 표준과 방향을 구분하지 않는 반면, 문장-벡터 접근법은 자동으로 표준을 단어의 중요성으로 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "214-4",
                    "sentence": "Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover’s distance (optimal transport), which we refer to as word rotator’s distance.",
                    "sentence_kor": "따라서 워드 벡터를 표준과 방향으로 분리한 다음 어스 무버의 거리(최적 전송)를 사용하여 정렬 기반 유사성을 계산할 것을 제안한다. 이를 워드 회전 장치 거리(Rotator)라고 한다.",
                    "tag": "4"
                },
                {
                    "index": "214-5",
                    "sentence": "Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method.",
                    "sentence_kor": "또한, 우리는 단어 벡터(벡터 변환기)의 표준과 방향을 키우는 방법을 시연한다. 이것은 제안된 방법의 성능을 크게 향상시킬 수 있는 문장-벡터 추정 방법에서 파생된 새로운 체계적인 접근법이다.",
                    "tag": "5"
                },
                {
                    "index": "214-6",
                    "sentence": "On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines.",
                    "sentence_kor": "여러 STS 벤치마크에서 제안된 방법은 정렬 기반 접근방식뿐만 아니라 강력한 기준선을 능가한다.",
                    "tag": "5"
                },
                {
                    "index": "214-7",
                    "sentence": "The source code is avaliable at https://github.com/eumesy/wrd",
                    "sentence_kor": "소스 코드는 https://github.com/eumesy/wrd에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "215",
            "abstractID": "EMNLP_abs-215",
            "text": [
                {
                    "index": "215-0",
                    "sentence": "Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data.",
                    "sentence_kor": "그래프 임베딩(GE) 방법은 그래프에 노드(및/또는 가장자리)를 저차원 의미 공간에 내장하고 다관계 데이터 모델링에서 그 효과를 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "215-1",
                    "sentence": "However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data.",
                    "sentence_kor": "그러나 기존 GE 모델은 들어오는 데이터의 스트리밍 특성을 간과했기 때문에 실제 적용에서는 실용적이지 않다.",
                    "tag": "1"
                },
                {
                    "index": "215-2",
                    "sentence": "To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 새로운 데이터에 대해 GE 모델을 지속적으로 훈련시키면서 오래된 학습된 지식을 비극적으로 잊어버리는 것을 방지하는 것을 목표로 하는 지속적인 그래프 표현 학습의 문제를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "215-3",
                    "sentence": "Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human’s ability to learn procedural knowledge.",
                    "sentence_kor": "또한, 절차적 지식을 학습하는 인간의 능력에 영감을 받은 DiCGRL(disentangle-based 연속 그래프 표현 학습) 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "215-4",
                    "sentence": "The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.",
                    "sentence_kor": "실험 결과는 DiCGRL이 치명적인 망각 문제를 효과적으로 완화하고 최첨단 연속 학습 모델을 능가할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "215-5",
                    "sentence": "The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.",
                    "sentence_kor": "코드와 데이터 세트는 https://github.com/KXY-PUBLIC/DiCGRL에서 공개된다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "216",
            "abstractID": "EMNLP_abs-216",
            "text": [
                {
                    "index": "216-0",
                    "sentence": "Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations.",
                    "sentence_kor": "준감독은 주석이 제한된 이중 언어 어휘 유도(BLI)에 대한 유망한 패러다임이다.",
                    "tag": "1"
                },
                {
                    "index": "216-1",
                    "sentence": "However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance.",
                    "sentence_kor": "그러나 이전의 반지도 방법은 주석이 달린 데이터와 주석이 없는 데이터에 숨겨진 지식을 완전히 활용하지 않아 성능 향상을 더욱 방해한다.",
                    "tag": "1"
                },
                {
                    "index": "216-2",
                    "sentence": "In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment.",
                    "sentence_kor": "본 논문에서, 우리는 감독된 신호와 감독되지 않은 정렬 사이의 상호작용을 장려하기 위한 새로운 준감독 BLI 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "216-3",
                    "sentence": "We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively.",
                    "sentence_kor": "우리는 주석이 달린 데이터와 주석이 없는 데이터 간에 지식을 전송하는 두 가지 메시지 전달 메커니즘을 설계하며, 각각 사전 최적 전송 및 양방향 어휘 업데이트로 명명되었다.",
                    "tag": "3"
                },
                {
                    "index": "216-4",
                    "sentence": "Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models.",
                    "sentence_kor": "그런 다음 모델을 업데이트하기 위해 주기적 또는 병렬 매개 변수 공급 루틴을 기반으로 준지도 학습을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "216-5",
                    "sentence": "Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport.",
                    "sentence_kor": "우리의 프레임워크는 최적의 운송을 기반으로 감독 및 감독되지 않은 BLI 방법을 통합할 수 있는 일반적인 프레임워크이다.",
                    "tag": "3"
                },
                {
                    "index": "216-6",
                    "sentence": "Experimental results on MUSE and VecMap datasets show significant improvement of our models.",
                    "sentence_kor": "MUSE 및 VecMap 데이터 세트에 대한 실험 결과는 모델이 크게 개선되었음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "216-7",
                    "sentence": "Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance.",
                    "sentence_kor": "절제 연구는 또한 감독된 신호와 감독되지 않은 정렬 사이의 양방향 상호작용이 전체 성능의 이득을 설명한다는 것을 입증한다.",
                    "tag": "5"
                },
                {
                    "index": "216-8",
                    "sentence": "Results on distant language pairs further illustrate the advantage and robustness of our proposed method.",
                    "sentence_kor": "원거리 언어 쌍에 대한 결과는 제안된 방법의 장점과 견고성을 더욱 잘 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "217",
            "abstractID": "EMNLP_abs-217",
            "text": [
                {
                    "index": "217-0",
                    "sentence": "One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned.",
                    "sentence_kor": "비대칭 영역의 일치 텍스트에 대한 한 가지 접근법은 일치 함수를 쉽게 정의하고 학습할 수 있는 특징 벡터로 입력 시퀀스를 공통 의미 공간에 투영하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "217-1",
                    "sentence": "In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable.",
                    "sentence_kor": "실제 일치 관행에서, 훈련이 진행되면서 다른 도메인에서 투영된 특징 벡터는 구별할 수 없는 경향이 있는 것으로 종종 관찰된다.",
                    "tag": "1"
                },
                {
                    "index": "217-2",
                    "sentence": "The phenomenon, however, is often overlooked in existing matching models.",
                    "sentence_kor": "그러나 이러한 현상은 기존 일치 모델에서 간과되는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "217-3",
                    "sentence": "As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions.",
                    "sentence_kor": "결과적으로 형상 벡터는 정규화 없이 구성되므로 다운스트림 매칭 기능을 학습하는 어려움이 불가피하다.",
                    "tag": "1"
                },
                {
                    "index": "217-4",
                    "sentence": "In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match.",
                    "sentence_kor": "본 논문에서 우리는 WD-Match라는 비대칭 도메인에서 텍스트 일치를 위해 맞춤화된 새로운 일치 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "217-5",
                    "sentence": "In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains.",
                    "sentence_kor": "WD-Match에서는 Wasserstein 거리 기반 정규화기가 정의되어 서로 다른 도메인에서 투영된 형상 벡터를 정규화한다.",
                    "tag": "3"
                },
                {
                    "index": "217-6",
                    "sentence": "As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated.",
                    "sentence_kor": "결과적으로, 이 방법은 형상 투영 함수가 서로 다른 도메인에 해당하는 벡터를 쉽게 구별할 수 없도록 벡터를 생성하도록 강제한다.",
                    "tag": "3"
                },
                {
                    "index": "217-7",
                    "sentence": "The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance.",
                    "sentence_kor": "WD-Match의 훈련 과정은 와서스테인 거리로 정규화된 매칭 손실을 최소화하는 게임에 해당한다.",
                    "tag": "4"
                },
                {
                    "index": "217-8",
                    "sentence": "WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model.",
                    "sentence_kor": "WD-Match는 메소드를 기본 일치 모델로 사용하여 서로 다른 텍스트 일치 방법을 개선하는 데 사용할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "217-9",
                    "sentence": "Four popular text matching methods have been exploited in the paper.",
                    "sentence_kor": "이 논문에서 네 가지 인기 있는 텍스트 일치 방법이 이용되었다.",
                    "tag": "3"
                },
                {
                    "index": "217-10",
                    "sentence": "Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.",
                    "sentence_kor": "공개 가능한 벤치마크 4개를 기반으로 한 실험 결과는 WD-Match가 기본 방법과 기준선을 지속적으로 능가하는 것으로 나타났다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "218",
            "abstractID": "EMNLP_abs-218",
            "text": [
                {
                    "index": "218-0",
                    "sentence": "Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages.",
                    "sentence_kor": "이중 언어 환경에서 감독되지 않은 언어 교차 임베딩에 대한 최근 발전은 여러 언어에 대한 공유 임베딩 공간을 학습하는 자극을 주었다.",
                    "tag": "1"
                },
                {
                    "index": "218-1",
                    "sentence": "A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space.",
                    "sentence_kor": "후자 문제를 해결하기 위한 인기 있는 프레임워크는 다음과 같은 두 가지 하위 문제를 공동으로 해결하는 것이다. 1) 여러 언어 쌍 간의 감독되지 않은 단어 정렬 학습 및 2) 모든 언어의 단일 언어 임베딩을 공유 다국어 공간에 매핑하는 방법을 학습하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "218-2",
                    "sentence": "In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques.",
                    "sentence_kor": "대조적으로, 우리는 위의 두 하위 문제를 분리하여 기존 기법을 사용하여 하나씩 개별적으로 해결하는 간단한 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "218-3",
                    "sentence": "We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing.",
                    "sentence_kor": "우리는 제안된 이 접근 방식이 이중 언어 어휘 유도, 교차 언어 단어 유사성, 다국어 문서 분류 및 다국어 종속성 구문 분석과 같은 작업에서 놀라울 정도로 우수한 성과를 거두었음을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "218-4",
                    "sentence": "When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.",
                    "sentence_kor": "원격 언어가 관련되면 제안된 접근법은 강력한 동작을 보이며 기존의 비지도 다국어 단어 임베딩 접근법보다 성능이 우수하다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "219",
            "abstractID": "EMNLP_abs-219",
            "text": [
                {
                    "index": "219-0",
                    "sentence": "We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes.",
                    "sentence_kor": "텍스트 기반 게임(TBG)을 해결하기 위한 강화 학습(RL) 방법은 특히 소규모 데이터 체제에서 보이지 않는 게임을 일반화하지 못하는 경우가 많다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "219-1",
                    "sentence": "To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization.",
                    "sentence_kor": "이 문제를 해결하기 위해 일반화를 개선하기 위해 관찰 텍스트에서 관련 없는 토큰 제거를 위한 상황 관련 에피소드 상태 잘라내기(CREST)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "219-2",
                    "sentence": "Our method first trains a base model using Q-learning, which typically overfits the training games.",
                    "sentence_kor": "우리의 방법은 먼저 Q-러닝을 사용하여 기본 모델을 훈련시키는데, 이는 일반적으로 훈련 게임에 지나치게 적합하다.",
                    "tag": "3"
                },
                {
                    "index": "219-3",
                    "sentence": "The base model’s action token distribution is used to perform observation pruning that removes irrelevant tokens.",
                    "sentence_kor": "기본 모델의 작업 토큰 배포는 관련 없는 토큰을 제거하는 관찰 가지치기를 수행하는 데 사용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "219-4",
                    "sentence": "A second bootstrapped model is then retrained on the pruned observation text.",
                    "sentence_kor": "그런 다음 가지치기된 관찰 텍스트에서 두 번째 부트스트랩 모델이 다시 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "219-5",
                    "sentence": "Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.",
                    "sentence_kor": "부트랩된 에이전트는 더 적은 수의 교육 에피소드를 요구함에도 불구하고 이전의 최첨단(SOTA) 방법에 비해 10배-20배 적은 훈련 게임을 사용하여 보이지 않는 TextWorld 게임을 해결하는 데 있어 향상된 일반화를 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "220",
            "abstractID": "EMNLP_abs-220",
            "text": [
                {
                    "index": "220-0",
                    "sentence": "Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks.",
                    "sentence_kor": "사전 훈련된 언어 모델(예: BERT)은 다양한 자연어 처리(NLP) 작업에서 상당한 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "220-1",
                    "sentence": "However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices.",
                    "sentence_kor": "그러나 높은 스토리지 및 계산 비용으로 인해 사전 훈련된 언어 모델이 리소스가 제한된 장치에 효과적으로 배치되는 데 방해가 됩니다.",
                    "tag": "1"
                },
                {
                    "index": "220-2",
                    "sentence": "In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers.",
                    "sentence_kor": "본 논문에서 우리는 각 중간 학생 계층이 중간 교사 계층에서 학습할 수 있도록 하는 다대다 계층 매핑을 기반으로 하는 새로운 BERT 증류 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "220-3",
                    "sentence": "In this way, our model can learn from different teacher layers adaptively for different NLP tasks.",
                    "sentence_kor": "이러한 방식으로 우리의 모델은 다양한 NLP 작업에 대해 적응적으로 다른 교사 계층으로부터 학습할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "220-4",
                    "sentence": "In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network.",
                    "sentence_kor": "또한, 우리는 EMD(Earth Mover's Distance)를 활용하여 교사 네트워크에서 학생 네트워크로 지식을 변환하기 위해 지불해야 하는 최소 누적 비용을 계산한다.",
                    "tag": "3"
                },
                {
                    "index": "220-5",
                    "sentence": "EMD enables effective matching for the many-to-many layer mapping.",
                    "sentence_kor": "EMD는 다대다 계층 매핑을 위한 효과적인 일치를 가능하게 한다.",
                    "tag": "4"
                },
                {
                    "index": "220-6",
                    "sentence": "Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model’s performance and accelerate convergence time.",
                    "sentence_kor": "또한 EMD에 사용되는 레이어 가중치를 자동으로 학습하는 비용 주의 메커니즘을 제안하며, 이는 모델의 성능을 더욱 향상시키고 컨버전스 시간을 가속화하도록 되어 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "220-7",
                    "sentence": "Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression",
                    "sentence_kor": "GLUE 벤치마크에 대한 광범위한 실험은 우리 모델이 정확성과 모델 압축 측면에서 강력한 경쟁사 대비 경쟁력 있는 성능을 달성한다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "221",
            "abstractID": "EMNLP_abs-221",
            "text": [
                {
                    "index": "221-0",
                    "sentence": "Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST).",
                    "sentence_kor": "도메인 온톨로지의 불완전성과 일부 값을 사용할 수 없는 것은 대화 상태 추적(DST)의 두 가지 불가피한 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "221-1",
                    "sentence": "Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence.",
                    "sentence_kor": "기존 접근법은 일반적으로 두 가지 극단으로 나뉜다. 즉, 온톨로지가 없는 모델을 선택하는 것과 온톨로지를 모델에 내장하는 것은 지나친 의존을 초래하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "221-2",
                    "sentence": "In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN.",
                    "sentence_kor": "본 논문에서는 SAVN이라고 하는 슬롯 어텐션(SA)과 가치 표준화(VN)로 구성된 온톨로지를 교묘하게 활용할 수 있는 새로운 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "221-3",
                    "sentence": "Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value.",
                    "sentence_kor": "또한, 우리는 MultiWOZ 2.1에 대한 지원 스팬 주석을 보완하는데, 이는 라벨링된 값을 지원하는 발화에서 가장 짧은 스팬이다.",
                    "tag": "2+3"
                },
                {
                    "index": "221-4",
                    "sentence": "SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span.",
                    "sentence_kor": "SA는 슬롯과 발화 간에 지식을 공유하며 지원 범위를 예측하기 위한 간단한 구조만 필요합니다.",
                    "tag": "3"
                },
                {
                    "index": "221-5",
                    "sentence": "VN is designed specifically for the use of ontology, which can convert supporting spans to the values.",
                    "sentence_kor": "VN은 지원 범위를 값으로 변환할 수 있는 온톨로지를 사용하도록 특별히 설계되었습니다.",
                    "tag": "3"
                },
                {
                    "index": "221-6",
                    "sentence": "Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1.",
                    "sentence_kor": "경험적 결과에 따르면 SAVN은 MultiWOZ 2.0에서 54.52%, MultiWOZ 2.1에서 54.86%의 최첨단 조인트 정확도를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "221-7",
                    "sentence": "Besides, we evaluate VN with incomplete ontology.",
                    "sentence_kor": "또한 불완전한 온톨로지로 VN을 평가한다.",
                    "tag": "4"
                },
                {
                    "index": "221-8",
                    "sentence": "The results show that even if only 30% ontology is used, VN can also contribute to our model.",
                    "sentence_kor": "그 결과 온톨로지가 30%만 사용되더라도 VN도 우리 모델에 기여할 수 있음을 알 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "222",
            "abstractID": "EMNLP_abs-222",
            "text": [
                {
                    "index": "222-0",
                    "sentence": "Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer.",
                    "sentence_kor": "개방형 도메인 질문 답변에 대한 대부분의 접근 방식은 후보 구절 세트를 선택하는 경량 검색기와 정답을 식별하기 위해 구절을 조사하는 계산 비용이 많이 드는 판독기로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "222-1",
                    "sentence": "Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader.",
                    "sentence_kor": "이전 작품들은 검색된 구절의 수가 증가할수록 독자의 수행도 증가한다는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "222-2",
                    "sentence": "However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost.",
                    "sentence_kor": "그러나 검색된 모든 구간의 중요도가 동일하다고 가정하고 동일한 양의 연산을 할당하여 계산 비용을 크게 증가시킨다.",
                    "tag": "1"
                },
                {
                    "index": "222-3",
                    "sentence": "To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read.",
                    "sentence_kor": "이 비용을 줄이기 위해, 우리는 읽을 구절에 할당된 계산 예산을 제어하기 위해 적응형 연산을 사용할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "222-4",
                    "sentence": "We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability.",
                    "sentence_kor": "우리는 먼저 개별 구간에 대해 독립적으로 작동하는 기술을 도입하는데, 이 기술은 언제든지 예측하고 초기 출구 확률의 계층별 추정에 의존한다.",
                    "tag": "3"
                },
                {
                    "index": "222-5",
                    "sentence": "We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning.",
                    "sentence_kor": "그런 다음 강화 학습을 통해 훈련된 자원 할당 정책을 기반으로 각 단계에서 계산을 할당할 구절을 동적으로 결정하는 접근 방식인 스카이라인 빌더를 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "222-6",
                    "sentence": "Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.",
                    "sentence_kor": "SQuAD-Open에 대한 우리의 결과는 글로벌 우선 순위를 가진 적응형 연산이 몇 가지 강력한 정적 및 적응형 방법에 비해 개선되어 전체 모델의 95% 성능을 유지하면서 4.3배의 계산 감소로 이어진다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "223",
            "abstractID": "EMNLP_abs-223",
            "text": [
                {
                    "index": "223-0",
                    "sentence": "Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives.",
                    "sentence_kor": "텍스트에 대한 복잡한 추론을 위해서는 자유형 술어와 논리적 연결어를 이해하고 연결해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "223-1",
                    "sentence": "Prior work has largely tried to do this either symbolically or with black-box transformers.",
                    "sentence_kor": "이전 연구에서는 이것을 상징적으로나 블랙박스 변압기로 주로 시도해왔다.",
                    "tag": "1"
                },
                {
                    "index": "223-2",
                    "sentence": "We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning.",
                    "sentence_kor": "우리는 이 두 극단 사이의 중간 지점을 제시한다. 즉, 연쇄 논리적 추론을 수행할 수 있는 신경 모듈 네트워크를 연상시키는 구성 모델이다.",
                    "tag": "2"
                },
                {
                    "index": "223-3",
                    "sentence": "This model first finds relevant sentences in the context and then chains them together using neural modules.",
                    "sentence_kor": "이 모델은 먼저 컨텍스트에서 관련 문장을 찾은 다음 신경 모듈을 사용하여 이들을 함께 연결한다.",
                    "tag": "3"
                },
                {
                    "index": "223-4",
                    "sentence": "Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.",
                    "sentence_kor": "우리 모델은 최근 도입된 복잡한 추론 데이터 세트인 ROPS에서 상당한 성능 향상(재순위자와 결합할 경우 최대 29%의 상대적 오류 감소)을 제공한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "224",
            "abstractID": "EMNLP_abs-224",
            "text": [
                {
                    "index": "224-0",
                    "sentence": "State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive.",
                    "sentence_kor": "최첨단 질문 답변(QA)은 라벨링에 시간이 많이 걸리고 따라서 비용이 많이 드는 대량의 교육 데이터에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "224-1",
                    "sentence": "For this reason, customizing QA systems is challenging.",
                    "sentence_kor": "이러한 이유로 QA 시스템을 사용자 정의하기가 어렵습니다.",
                    "tag": "1"
                },
                {
                    "index": "224-2",
                    "sentence": "As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme.",
                    "sentence_kor": "해결책으로 비용 효율적인 주석 정책과 준감독 주석 체계를 학습하는 QA 데이터 세트에 주석을 달기 위한 새로운 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "224-3",
                    "sentence": "The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations.",
                    "sentence_kor": "후자는 인적 노력을 감소시킨다. 즉, 기본 QA 시스템을 활용하여 잠재적인 후보 주석을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "224-4",
                    "sentence": "Human annotators then simply provide binary feedback on these candidates.",
                    "sentence_kor": "그런 다음 인간 주석자는 이러한 후보자에 대한 이진 피드백을 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "224-5",
                    "sentence": "Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost.",
                    "sentence_kor": "우리 시스템은 과거의 주석이 미래 성능을 지속적으로 향상시켜 전체 주석 비용을 향상시키도록 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "224-6",
                    "sentence": "To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost.",
                    "sentence_kor": "우리가 아는 한, 이 논문은 최소 주석 비용으로 질문에 주석을 다는 문제를 다루는 첫 번째 논문이다.",
                    "tag": "5"
                },
                {
                    "index": "224-7",
                    "sentence": "We compare our framework against traditional manual annotations in an extensive set of experiments.",
                    "sentence_kor": "광범위한 실험에서 프레임워크를 기존의 수동 주석과 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "224-8",
                    "sentence": "We find that our approach can reduce up to 21.1% of the annotation cost.",
                    "sentence_kor": "우리의 접근 방식은 주석 비용의 최대 21.1%를 절감할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "225",
            "abstractID": "EMNLP_abs-225",
            "text": [
                {
                    "index": "225-0",
                    "sentence": "This paper focuses on machine reading comprehension for narrative passages.",
                    "sentence_kor": "이 논문은 서술 구절에 대한 기계 독해력에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "225-1",
                    "sentence": "Narrative passages usually describe a chain of events.",
                    "sentence_kor": "이야기 구절들은 보통 일련의 사건들을 묘사한다.",
                    "tag": "1"
                },
                {
                    "index": "225-2",
                    "sentence": "When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively.",
                    "sentence_kor": "이런 글을 읽을 때, 인간은 자신의 사전 지식으로 글에 따라 장면을 복구하는 경향이 있기 때문에 그 내용을 종합적으로 이해하는 데 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "225-3",
                    "sentence": "Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension.",
                    "sentence_kor": "인간의 이러한 행동에 영감을 받아, 우리는 기계가 내러티브를 읽는 동안 장면을 상상하도록 하는 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "225-4",
                    "sentence": "Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph.",
                    "sentence_kor": "특히, 우리는 Atomic을 외부 지식으로 활용하여 장면 그래프를 구축하고 그래프를 인코딩하기 위한 새로운 GDIN(Graph Dimensional-Iteration Network)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "225-5",
                    "sentence": "We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice.",
                    "sentence_kor": "우리는 SCT(Story Cloze Test)의 데이터 세트인 ROCStories와 객관식의 데이터 세트인 CosmosQA에 대한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "225-6",
                    "sentence": "Our method achieves state-of-the-art.",
                    "sentence_kor": "우리의 방법은 최첨단이다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "226",
            "abstractID": "EMNLP_abs-226",
            "text": [
                {
                    "index": "226-0",
                    "sentence": "Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.",
                    "sentence_kor": "읽기 이해 모델(RC)은 학습 문제를 완화하고 텍스트를 명시적으로 생성하는 모델의 필요성을 피하기 위해 일반적으로 출력 공간을 입력에서 모든 연속적인 범위의 집합으로 제한한다.",
                    "tag": "1"
                },
                {
                    "index": "226-1",
                    "sentence": "However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.",
                    "sentence_kor": "그러나 답변을 단일 범위로 강제하는 것은 제한적일 수 있으며, 최근의 일부 데이터 세트에는 다중 범위 질문, 즉 텍스트의 비연속 범위 세트가 답인 질문도 포함된다.",
                    "tag": "1"
                },
                {
                    "index": "226-2",
                    "sentence": "Naturally, models that return single spans cannot answer these questions.",
                    "sentence_kor": "당연히 단일 범위를 반환하는 모델은 이러한 질문에 대답할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "226-3",
                    "sentence": "In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.",
                    "sentence_kor": "본 연구에서는 작업을 시퀀스 태그 문제로 캐스팅하여 다중 스팬 질문에 대답하는 간단한 아키텍처를 제안한다. 즉, 각 입력 토큰에 대해 출력의 일부가 되어야 하는지 여부를 예측한다.",
                    "tag": "2+3"
                },
                {
                    "index": "226-4",
                    "sentence": "Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.",
                    "sentence_kor": "우리 모델은 DROP와 Quoref의 스팬 추출 질문에 대한 성능을 각각 9.9점과 5.5EM점 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "227",
            "abstractID": "EMNLP_abs-227",
            "text": [
                {
                    "index": "227-0",
                    "sentence": "The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets.",
                    "sentence_kor": "비지도 임베딩 학습 알고리즘의 최근 성과에 힘입어 의미 모델의 도메인별 응용 프로그램의 성장은 도메인별 평가 데이터 세트를 요구한다.",
                    "tag": "1"
                },
                {
                    "index": "227-1",
                    "sentence": "In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks.",
                    "sentence_kor": "많은 경우, 콘텐츠 기반 추천자가 대표적인 예로서, 이러한 모델은 특정 개념에 대한 의미적 관련성에 따라 단어 또는 텍스트의 순위를 매기고, 특히 상위 순위에 초점을 맞추어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "227-2",
                    "sentence": "In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks.",
                    "sentence_kor": "본 연구에서는 이러한 요구사항을 해결하기 위해 세 가지 기여를 한다. (i) 우리는 사용 가능한 자원에 대해 맞춤화되고 최상위 평가에서 특히 정확하도록 최적화된 관련성 기반 평가 데이터 세트의 구성을 위한 프로토콜을 정의한다. (ii) 적절한 메트릭스, 확장 기능을 정의한다.잘 알려진 순위 상관 계수의 sions는 상위 순위의 더 큰 중요성을 고려하여 앞서 언급한 데이터 세트를 통해 의미론적 모델을 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "227-3",
                    "sentence": "Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.",
                    "sentence_kor": "마지막으로 (iii) 제안된 데이터 세트 구성 프로토콜의 효과를 확인하는 의미 중심 쌍별 비교를 시뮬레이션하기 위해 확률적 전이성 모델을 정의한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "228",
            "abstractID": "EMNLP_abs-228",
            "text": [
                {
                    "index": "228-0",
                    "sentence": "Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets.",
                    "sentence_kor": "사전 훈련된 신경 언어 모델은 작업별 훈련 세트에 대한 모델을 미세 조정함으로써 다양한 NLP 작업에 상당한 개선을 가져온다.",
                    "tag": "1"
                },
                {
                    "index": "228-1",
                    "sentence": "During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced.",
                    "sentence_kor": "미세 조정 중에 매개변수는 사전 훈련된 모델에서 직접 초기화되며, 이는 서로 다른 도메인에서 유사한 NLP 작업의 학습 프로세스가 어떻게 상관되고 상호 강화되는지를 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "228-2",
                    "sentence": "In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models.",
                    "sentence_kor": "본 논문에서는 신경 언어 모델에 대한 유사한 NLP 작업 그룹을 해결하기 위한 메타 학습자 역할을 하는 MFT(Meta Fine-Tuning)라는 효과적인 학습 절차를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "228-3",
                    "sentence": "Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge.",
                    "sentence_kor": "MFT는 단순히 모든 데이터 세트에 대한 멀티태스킹 교육 대신 다양한 도메인의 일반적인 인스턴스로부터만 학습하여 전송 가능성이 높은 지식을 습득한다.",
                    "tag": "3"
                },
                {
                    "index": "228-4",
                    "sentence": "It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.",
                    "sentence_kor": "또한 언어 모델이 일련의 새로운 도메인 손상 손실 함수를 최적화하여 도메인 불변 표현을 인코딩하도록 권장한다.",
                    "tag": "3"
                },
                {
                    "index": "228-5",
                    "sentence": "After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability.",
                    "sentence_kor": "MFT 이후, 모델은 더 나은 매개변수 초기화 및 더 높은 일반화 기능으로 각 도메인에 대해 미세 조정될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "228-6",
                    "sentence": "We implement MFT upon BERT to solve several multi-domain text mining tasks.",
                    "sentence_kor": "우리는 여러 다중 도메인 텍스트 마이닝 작업을 해결하기 위해 BERT에 MFT를 구현한다.",
                    "tag": "3"
                },
                {
                    "index": "228-7",
                    "sentence": "Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.",
                    "sentence_kor": "실험 결과는 MFT의 효과와 퓨샷 학습에 대한 유용성을 확인시켜 준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "229",
            "abstractID": "EMNLP_abs-229",
            "text": [
                {
                    "index": "229-0",
                    "sentence": "Generative neural networks have been shown effective on query suggestion.",
                    "sentence_kor": "생성 신경망은 쿼리 제안에 효과적인 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "229-1",
                    "sentence": "Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time.",
                    "sentence_kor": "일반적으로 조건부 생성 문제로 제기되는 이 작업은 검색 세션에서 사용자의 이전 입력을 활용하여 나중에 발생할 가능성이 높은 쿼리를 예측하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "229-2",
                    "sentence": "User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns.",
                    "sentence_kor": "사용자 입력은 쿼리 및 클릭과 같은 다양한 형태로 나타나며, 각 입력은 해당 행동 패턴을 통해 채널링되는 서로 다른 의미 신호를 암시할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "229-3",
                    "sentence": "This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice.",
                    "sentence_kor": "본 논문은 이러한 행동 편향을 쿼리 생성을 위한 가설로 유도하며, 여기서 일반 인코더-디코더 트랜스포머 프레임워크는 임의의 선택 가설을 집계하기 위해 제시된다.",
                    "tag": "2+3"
                },
                {
                    "index": "229-4",
                    "sentence": "Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.",
                    "sentence_kor": "우리의 실험 결과는 제안된 접근 방식이 최근 BART 모델에 비해 상위 K개 단어 오류율과 Bert F1 점수를 크게 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "230",
            "abstractID": "EMNLP_abs-230",
            "text": [
                {
                    "index": "230-0",
                    "sentence": "The causal relationships between emotions and causes in text have recently received a lot of attention.",
                    "sentence_kor": "텍스트에서 감정과 원인의 인과관계는 최근 많은 관심을 받고 있다.",
                    "tag": "1"
                },
                {
                    "index": "230-1",
                    "sentence": "Most of the existing works focus on the extraction of the causally related clauses from documents.",
                    "sentence_kor": "기존 저작의 대부분은 문서에서 인과관계 조항을 추출하는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "230-2",
                    "sentence": "However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related.",
                    "sentence_kor": "그러나 이러한 연구들 중 어느 것도 추출된 감정과 원인 조항 사이의 인과 관계가 특정한 맥락에서만 유효할 수 있는 가능성을 고려하지 않았으며, 이 상황에서 추출된 조항은 인과 관계가 없을 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "230-3",
                    "sentence": "To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset.",
                    "sentence_kor": "이러한 문제를 해결하기 위해, 우리는 감정과 원인의 입력 쌍이 다른 맥락에서 유효한 인과 관계를 가지고 있는지 여부를 판단하는 새로운 작업을 제안하고, 기존 벤치마크 데이터 세트를 기반으로 수동 주석과 마이너스 샘플링을 통해 해당 데이터 세트를 구성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "230-4",
                    "sentence": "Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses.",
                    "sentence_kor": "또한 입력 조항의 특성을 기반으로 예측 결과를 미세 조정하기 위해 계산 오버헤드가 낮은 예측 집계 모듈을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "230-5",
                    "sentence": "Experiments demonstrate the effectiveness and generality of our aggregation module.",
                    "sentence_kor": "실험은 집합 모듈의 효과와 일반성을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "231",
            "abstractID": "EMNLP_abs-231",
            "text": [
                {
                    "index": "231-0",
                    "sentence": "The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention.",
                    "sentence_kor": "원칙적이고 유용한 방식으로 상황별 단어 표현을 조사하는 방법에 대한 문제는 최근 상당한 관심을 보여 왔다.",
                    "tag": "1"
                },
                {
                    "index": "231-1",
                    "sentence": "In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume.",
                    "sentence_kor": "이 논의에 대한 우리의 기여에서, 우리는 먼저 탐사의 복잡성과 성능 사이의 균형을 반영하는 탐침 지표인 파레토 과부피에 대해 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "231-2",
                    "sentence": "To measure complexity, we present a number of parametric and non-parametric metrics.",
                    "sentence_kor": "복잡성을 측정하기 위해 여러 매개 변수 및 비모수 측정 기준을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "231-3",
                    "sentence": "Our experiments with such metrics show that probe’s performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones).",
                    "sentence_kor": "그러한 메트릭스를 사용한 우리의 실험은 탐색기의 성능 곡선이 언어 표현 간에 널리 인정받는 순위(예: 상황별 표현을 능가하는 비 상황적 표현 포함)와 일치하지 않는 경우가 많다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "231-4",
                    "sentence": "These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations.",
                    "sentence_kor": "이러한 결과는 둘째, POS 라벨링 및 의존성 아크 라벨링과 같은 일반적인 단순성 프로브 작업이 상황별 단어 표현으로 인코딩된 속성을 평가하기에 불충분하다고 주장하게 한다.",
                    "tag": "4"
                },
                {
                    "index": "231-5",
                    "sentence": "We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume.",
                    "sentence_kor": "우리는 예제 프로브 작업으로 완전한 의존성 파싱을 제안하고 파레토 하이퍼볼륨으로 그것을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "231-6",
                    "sentence": "In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.",
                    "sentence_kor": "우리의 주장을 뒷받침하기 위해, 이 예증적 실험의 결과는 문맥적 단어 표현 중 승인된 순위에 더 가깝다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "232",
            "abstractID": "EMNLP_abs-232",
            "text": [
                {
                    "index": "232-0",
                    "sentence": "To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input.",
                    "sentence_kor": "자연어 처리(NLP)를 위한 심층 신경망의 \"블랙박스\" 특성을 명확히 하기 위해 입력의 각 토큰을 지운 후 예측 확률의 변화를 측정하여 예측을 해석하는 몇 가지 방법이 제안되었다.",
                    "tag": "2"
                },
                {
                    "index": "232-1",
                    "sentence": "Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations.",
                    "sentence_kor": "기존 방법은 각 토큰을 사전 정의된 값(즉, 0)으로 대체하기 때문에 결과 문장은 훈련 데이터 분포에서 벗어나 잘못된 해석을 낳는다.",
                    "tag": "1"
                },
                {
                    "index": "232-2",
                    "sentence": "In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out.",
                    "sentence_kor": "본 연구에서는 기존 해석 방법에 의해 유발된 분포 이탈 문제를 제기하고 해결책을 제시한다. 우리는 각 토큰을 소외시킬 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "232-3",
                    "sentence": "We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.",
                    "sentence_kor": "우리는 제안된 방법을 사용하여 감정 분석과 자연어 추론을 위해 훈련된 다양한 NLP 모델을 해석한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "233",
            "abstractID": "EMNLP_abs-233",
            "text": [
                {
                    "index": "233-0",
                    "sentence": "Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic.",
                    "sentence_kor": "교차 언어 단어 벡터 공간을 정렬하는 기존 알고리즘은 벡터 공간이 거의 동형이라고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "233-1",
                    "sentence": "As a result, they perform poorly or fail completely on non-isomorphic spaces.",
                    "sentence_kor": "결과적으로 비동형 공간에서 성능이 떨어지거나 완전히 실패합니다.",
                    "tag": "1"
                },
                {
                    "index": "233-2",
                    "sentence": "Such non-isomorphism has been hypothesised to result from typological differences between languages.",
                    "sentence_kor": "그러한 비동형성은 언어들 간의 유형학적 차이에서 비롯된다고 가정되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "233-3",
                    "sentence": "In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces.",
                    "sentence_kor": "이 연구에서 우리는 비동형성이 결정적으로 퇴화된 단어 벡터 공간의 부호인지 여부를 묻는다.",
                    "tag": "2"
                },
                {
                    "index": "233-4",
                    "sentence": "We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g.“under-training”).",
                    "sentence_kor": "다양한 언어에 걸친 일련의 실험을 제시했는데, 이는 언어 쌍에 걸친 성능 차이가 유형적 차이 때문만이 아니라 사용 가능한 단일 언어 자원의 크기, 단일 언어 훈련의 속성 및 기간(예: \"훈련 미달\")에 기인한다고 볼 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "234",
            "abstractID": "EMNLP_abs-234",
            "text": [
                {
                    "index": "234-0",
                    "sentence": "Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable.",
                    "sentence_kor": "신경망은 많은 자연어 처리 애플리케이션에서 인상적인 성능을 달성할 수 있지만 일반적으로 훈련을 위해 라벨이 붙은 데이터가 많이 필요하며 쉽게 해석할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "234-1",
                    "sentence": "On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios.",
                    "sentence_kor": "반면에 정규 표현식과 같은 기호 규칙은 해석 가능하고 훈련이 필요하지 않으며 종종 적절한 정확도를 달성하지만 규칙은 사용 가능한 경우 라벨링된 데이터로부터 이익을 얻을 수 없으며 따라서 풍부한 자원 시나리오에서 신경망의 성능을 저하시킨다.",
                    "tag": "1"
                },
                {
                    "index": "234-2",
                    "sentence": "In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules.",
                    "sentence_kor": "본 논문에서, 우리는 신경망의 장점과 정규 표현 규칙을 결합한 FA-RNN이라는 순환 신경망의 유형을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "234-3",
                    "sentence": "An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios.",
                    "sentence_kor": "FA-RNN은 정규식에서 변환하여 제로샷 및 콜드 스타트 시나리오에 배치할 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "234-4",
                    "sentence": "It can also utilize labeled data for training to achieve improved prediction accuracy.",
                    "sentence_kor": "또한 레이블링된 데이터를 교육에 활용하여 예측 정확도를 향상시킬 수 있습니다.",
                    "tag": "5"
                },
                {
                    "index": "234-5",
                    "sentence": "After training, an FA-RNN often remains interpretable and can be converted back into regular expressions.",
                    "sentence_kor": "훈련 후 FA-RNN은 종종 해석 가능한 상태로 유지되며 정규식으로 다시 변환할 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "234-6",
                    "sentence": "We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.",
                    "sentence_kor": "우리는 FA-RNN을 텍스트 분류에 적용하고 FA-RNN이 제로샷 및 저자원 설정 모두에서 이전의 신경 접근 방식을 크게 능가하며 풍부한 자원 설정에서 매우 경쟁력을 유지하고 있음을 관찰한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "235",
            "abstractID": "EMNLP_abs-235",
            "text": [
                {
                    "index": "235-0",
                    "sentence": "Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers.",
                    "sentence_kor": "대형 트랜스포머 기반 모델은 적은 수의 자체 주의 헤드 및 레이어로 축소 가능한 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "235-1",
                    "sentence": "We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning.",
                    "sentence_kor": "우리는 구조적인 가지치기 및 크기 가지치기를 모두 사용하여 복권 가설의 관점에서 이 현상을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "235-2",
                    "sentence": "For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse.",
                    "sentence_kor": "미세 조정된 BERT의 경우 (a) 전체 모델과 유사한 성능을 달성하는 하위 네트워크를 찾을 수 있으며 (b) 나머지 모델에서 샘플링된 유사한 크기의 하위 네트워크의 성능이 더 나쁘다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "235-3",
                    "sentence": "Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful.",
                    "sentence_kor": "놀랍게도, 최악의 하위 네트워크도 구조화된 가지치기로 인해 훈련 가능성이 매우 높아 대부분의 사전 훈련된 BERT 가중치가 잠재적으로 유용함을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "235-4",
                    "sentence": "We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.",
                    "sentence_kor": "우리는 또한 \"좋은\" 하위 네트워크를 연구하여 그들의 성공이 우수한 언어지식에 기인할 수 있는지 확인하지만, 그것들이 불안정하고 의미 있는 자기 주의 패턴으로 설명되지 않는다는 것을 발견한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "236",
            "abstractID": "EMNLP_abs-236",
            "text": [
                {
                    "index": "236-0",
                    "sentence": "Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency.",
                    "sentence_kor": "트랜스포머 기반 모델의 성공을 감안할 때 개별 주의 헤드의 역할 해석과 효율화를 위한 모델 다운사이징이라는 두 가지 연구 방향이 등장했다.",
                    "tag": "1"
                },
                {
                    "index": "236-1",
                    "sentence": "Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads.",
                    "sentence_kor": "우리의 작업은 이 두 가지 흐름에 걸쳐 있습니다. 우리는 주의 헤드의 해석된 역할에 기초하여 가지치기 전략의 중요성을 분석한다.",
                    "tag": "2"
                },
                {
                    "index": "236-2",
                    "sentence": "We evaluate this on Transformer and BERT models on multiple NLP tasks.",
                    "sentence_kor": "여러 NLP 작업의 트랜스포머 및 BERT 모델에서 이를 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "236-3",
                    "sentence": "Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy.",
                    "sentence_kor": "첫째, 어텐션 헤드의 많은 부분이 정확도에 제한적인 영향을 미치면서 무작위로 가지치기될 수 있다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "236-4",
                    "sentence": "Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head.",
                    "sentence_kor": "둘째로, 트랜스포머의 경우, 머리의 위치와 중요성을 관련시키는 기존 연구를 기반으로 중요한 것으로 확인된 주의 헤드를 제거해도 이점을 찾을 수 없다.",
                    "tag": "4"
                },
                {
                    "index": "236-5",
                    "sentence": "On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance.",
                    "sentence_kor": "BERT 모델에서도 상단 또는 하단 레이어에 대한 선호도를 찾을 수 없지만 후자의 선호도는 더 높은 것으로 보고되었다.",
                    "tag": "4"
                },
                {
                    "index": "236-6",
                    "sentence": "However, strategies that avoid pruning middle layers and consecutive layers perform better.",
                    "sentence_kor": "그러나 중간 레이어와 연속 레이어의 가지치기를 피하는 전략은 더 나은 성능을 발휘한다.",
                    "tag": "4"
                },
                {
                    "index": "236-7",
                    "sentence": "Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads.",
                    "sentence_kor": "마지막으로, 미세 조정 중에 가지치기 주의 헤드에 대한 보상은 가지치기되지 않은 헤드에 거의 동일하게 분포된다.",
                    "tag": "4"
                },
                {
                    "index": "236-8",
                    "sentence": "Our results thus suggest that interpretation of attention heads does not strongly inform pruning.",
                    "sentence_kor": "따라서 우리의 결과는 주의 헤드의 해석이 가지치기에 강하게 영향을 주지 않음을 시사한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "237",
            "abstractID": "EMNLP_abs-237",
            "text": [
                {
                    "index": "237-0",
                    "sentence": "BERT and its variants have achieved state-of-the-art performance in various NLP tasks.",
                    "sentence_kor": "BERT와 그 변형은 다양한 NLP 작업에서 최첨단 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "237-1",
                    "sentence": "Since then, various works have been proposed to analyze the linguistic information being captured in BERT.",
                    "sentence_kor": "그 이후로 BERT에서 포착되는 언어 정보를 분석하기 위한 다양한 연구가 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "237-2",
                    "sentence": "However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering.",
                    "sentence_kor": "그러나 현재 연구는 BERT가 독해 기반 질문 답변 작업에서 인간 수준에 가까운 성능을 달성할 수 있는 방법에 대한 통찰력을 제공하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "237-3",
                    "sentence": "In this work, we attempt to interpret BERT for RCQA.",
                    "sentence_kor": "이 작업에서는 RCQA에 대한 BERT를 해석하려고 한다.",
                    "tag": "2"
                },
                {
                    "index": "237-4",
                    "sentence": "Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients.",
                    "sentence_kor": "BERT 계층에는 미리 정의된 역할이 없으므로 통합 그레이디언트를 사용하여 계층의 역할 또는 기능을 정의합니다.",
                    "tag": "3"
                },
                {
                    "index": "237-5",
                    "sentence": "Based on the defined roles, we perform a preliminary analysis across all layers.",
                    "sentence_kor": "정의된 역할을 기반으로 모든 계층에 걸쳐 예비 분석을 수행합니다.",
                    "tag": "3"
                },
                {
                    "index": "237-6",
                    "sentence": "We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction.",
                    "sentence_kor": "초기 계층은 질의-통과 상호작용에 초점을 맞추는 반면, 이후 계층은 상황 이해와 답변 예측 강화에 더 초점을 맞춘다는 것을 관찰했다.",
                    "tag": "4"
                },
                {
                    "index": "237-7",
                    "sentence": "Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly.",
                    "sentence_kor": "특히 정량화 질문(얼마/몇 개)의 경우, BERT가 후기 계층에서 혼란스러운 단어(즉, 단락의 다른 수치 수량에 관한)에 초점을 맞추고 있지만 여전히 정답을 정확하게 예측한다.",
                    "tag": "4"
                },
                {
                    "index": "237-8",
                    "sentence": "The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.",
                    "sentence_kor": "미세 조정 및 분석 스크립트는 https://github.com/iitmnlp/BERT-Analysis-RCQA에서 공개적으로 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "238",
            "abstractID": "EMNLP_abs-238",
            "text": [
                {
                    "index": "238-0",
                    "sentence": "Attribution methods assess the contribution of inputs to the model prediction.",
                    "sentence_kor": "귀인 방법은 모형 예측에 대한 입력의 기여도를 평가합니다.",
                    "tag": "1"
                },
                {
                    "index": "238-1",
                    "sentence": "One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction.",
                    "sentence_kor": "그렇게 하는 한 가지 방법은 소거입니다. 예측에 영향을 미치지 않고 제거할 수 있는 입력의 하위 집합은 무관한 것으로 간주됩니다.",
                    "tag": "1"
                },
                {
                    "index": "238-2",
                    "sentence": "Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models.",
                    "sentence_kor": "개념적으로는 간단하지만, 삭제의 목표는 다루기 어렵고 현대 심층 NLP 모델에서 대략적인 검색은 여전히 비싸다.",
                    "tag": "1"
                },
                {
                    "index": "238-3",
                    "sentence": "Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped.",
                    "sentence_kor": "소거 또한 사후 인식 편견에 취약하다. 입력이 삭제될 수 있다는 사실이 모델이 삭제될 수 있음을 '알고' 있다는 것을 의미하지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "238-4",
                    "sentence": "The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction.",
                    "sentence_kor": "결과 가지치기는 지나치게 공격적이며 모델이 예측에 도달하는 방법을 반영하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "238-5",
                    "sentence": "To deal with these challenges, we introduce Differentiable Masking.",
                    "sentence_kor": "이러한 과제를 해결하기 위해 차별화 가능한 마스킹을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "238-6",
                    "sentence": "DiffMask learns to mask-out subsets of the input while maintaining differentiability.",
                    "sentence_kor": "DiffMask는 미분성을 유지하면서 입력의 하위 집합을 마스크 아웃하는 방법을 학습합니다.",
                    "tag": "3"
                },
                {
                    "index": "238-7",
                    "sentence": "The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model.",
                    "sentence_kor": "입력 토큰을 포함하거나 무시하는 결정은 분석된 모델의 중간 숨겨진 레이어를 기반으로 하는 단순 모델을 사용하여 이루어진다.",
                    "tag": "3"
                },
                {
                    "index": "238-8",
                    "sentence": "First, this makes the approach efficient because we predict rather than search.",
                    "sentence_kor": "첫째, 검색보다는 예측하기 때문에 접근 방식이 효율적입니다.",
                    "tag": "4"
                },
                {
                    "index": "238-9",
                    "sentence": "Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers.",
                    "sentence_kor": "둘째, 검색 분류자와 마찬가지로 네트워크가 해당 계층에서 무엇을 '알고' 있는지 알 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "238-10",
                    "sentence": "This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers.",
                    "sentence_kor": "이를 통해 귀속 히트맵을 표시할 뿐만 아니라 네트워크 계층 간에 의사결정이 형성되는 방식을 분석할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "238-11",
                    "sentence": "We use DiffMask to study BERT models on sentiment classification and question answering.",
                    "sentence_kor": "우리는 감정 분류 및 질문 답변에 대한 BERT 모델을 연구하기 위해 DiffMask를 사용한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "239",
            "abstractID": "EMNLP_abs-239",
            "text": [
                {
                    "index": "239-0",
                    "sentence": "Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity.",
                    "sentence_kor": "기계 학습의 최근 발전은 구조적 복잡성 증가를 감수하고 인간 성능에 접근하는 모델을 도입했다.",
                    "tag": "1"
                },
                {
                    "index": "239-1",
                    "sentence": "Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques.",
                    "sentence_kor": "모델의 예측 뒤에 숨겨진 이성을 투명하게 만들기 위한 노력은 풍부한 새로운 설명 가능 기술에 영감을 주었습니다.",
                    "tag": "1"
                },
                {
                    "index": "239-2",
                    "sentence": "Provided with an already trained model, they compute saliency scores for the words of an input instance.",
                    "sentence_kor": "이미 훈련된 모델이 제공되면 입력 인스턴스의 단어에 대한 돌출성 점수를 계산한다.",
                    "tag": "1"
                },
                {
                    "index": "239-3",
                    "sentence": "However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique.",
                    "sentence_kor": "그러나, (i) 특정 애플리케이션 과제와 모델 아키텍처가 주어진 그러한 기법을 선택하는 방법, 그리고 (ii) 각각의 그러한 기법 사용의 장점과 단점에 대한 명확한 지침은 없다.",
                    "tag": "1"
                },
                {
                    "index": "239-4",
                    "sentence": "In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques.",
                    "sentence_kor": "본 논문에서, 우리는 기존의 설명 가능성 기법을 평가하기 위한 진단 속성의 포괄적인 목록을 개발한다.",
                    "tag": "2"
                },
                {
                    "index": "239-5",
                    "sentence": "We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures.",
                    "sentence_kor": "그런 다음 제안된 목록을 사용하여 다운스트림 텍스트 분류 작업과 신경망 아키텍처에 대한 다양한 설명 가능성 기술 세트를 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "239-6",
                    "sentence": "We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones.",
                    "sentence_kor": "또한 설명 가능성 기법에 의해 할당된 돌출 입력 영역의 인간 주석과 비교함으로써 모델의 성능과 인간과의 이론적 합치 사이의 관계를 찾는다.",
                    "tag": "3"
                },
                {
                    "index": "239-7",
                    "sentence": "Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.",
                    "sentence_kor": "전반적으로, 우리는 경사도 기반 설명이 작업 및 모델 아키텍처에서 가장 잘 수행된다는 것을 발견하고 검토한 설명 가능성 기법의 특성에 대한 추가 통찰력을 제시한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "240",
            "abstractID": "EMNLP_abs-240",
            "text": [
                {
                    "index": "240-0",
                    "sentence": "Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image.",
                    "sentence_kor": "차트 질문 답변(CQA)은 차트 이미지의 시각화에 대한 자연어 질문에 답하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "240-1",
                    "sentence": "Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure.",
                    "sentence_kor": "VQA 접근 방식에서 영감을 얻은 최근의 솔루션은 고유한 차트 구조를 무시한 채 질문/응답에 이미지 기반 주의를 기울인다.",
                    "tag": "1"
                },
                {
                    "index": "240-2",
                    "sentence": "We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach.",
                    "sentence_kor": "순차적 요소 현지화, 질문 인코딩 및 구조적 변압기 기반 학습 접근방식을 통해 질문/응답 기능을 개선하는 STL-CQA를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "240-3",
                    "sentence": "We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types.",
                    "sentence_kor": "사전 교육 작업, 방법론 및 다양한 유형의 보다 복잡하고 균형 잡힌 질문을 통해 개선된 데이터 세트를 제안하는 동시에 광범위한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "240-4",
                    "sentence": "The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset.",
                    "sentence_kor": "제안된 방법론은 다양한 차트 Q/A 데이터 세트의 최첨단 접근 방식에 비해 상당한 정확도가 향상되었으며 DVQA 데이터 세트의 인간 기준치도 능가했다.",
                    "tag": "4"
                },
                {
                    "index": "240-5",
                    "sentence": "We also demonstrate interpretability while examining different components in the inference pipeline.",
                    "sentence_kor": "또한 추론 파이프라인의 여러 구성 요소를 검사하면서 해석 가능성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "241",
            "abstractID": "EMNLP_abs-241",
            "text": [
                {
                    "index": "241-0",
                    "sentence": "In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data.",
                    "sentence_kor": "VQA(Visual Question Answering) 작업에서 대부분의 최첨단 모델은 교육 세트에서 잘못된 상관 관계를 학습하고 배포되지 않은 테스트 데이터에서 낮은 성능을 달성하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "241-1",
                    "sentence": "Some methods of generating counterfactual samples have been proposed to alleviate this problem.",
                    "sentence_kor": "이 문제를 완화하기 위해 반사실적 샘플을 생성하는 몇 가지 방법이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "241-2",
                    "sentence": "However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized.",
                    "sentence_kor": "그러나 대부분의 이전 방법에 의해 생성된 반사실적 샘플은 증강을 위한 훈련 데이터에 추가될 뿐 완전히 활용되지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "241-3",
                    "sentence": "Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples.",
                    "sentence_kor": "따라서 원본 샘플, 사실 샘플 및 반사실적 샘플 간의 관계를 학습하기 위해 새로운 자체 감독 대조 학습 메커니즘을 도입한다.",
                    "tag": "1"
                },
                {
                    "index": "241-4",
                    "sentence": "With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly.",
                    "sentence_kor": "보조 훈련 목표에서 학습한 더 나은 교차 모달 조인트 임베딩으로, VQA 모델의 추론 능력과 견고성이 크게 향상되었다.",
                    "tag": "1"
                },
                {
                    "index": "241-5",
                    "sentence": "We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.",
                    "sentence_kor": "우리는 VQA 모델의 견고성을 평가하기 위한 진단 벤치마크인 VQA-CP 데이터 세트에서 현재 최첨단 모델을 능가함으로써 우리 방법의 효과를 평가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "242",
            "abstractID": "EMNLP_abs-242",
            "text": [
                {
                    "index": "242-0",
                    "sentence": "Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction.",
                    "sentence_kor": "물리적 상식은 인간과 로봇 상호작용을 위한 로봇의 인지 능력에 필수적인 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "242-1",
                    "sentence": "Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization.",
                    "sentence_kor": "기계 학습 방법은 자연어 처리에서 물리적 상식 학습에 대한 유망한 결과를 보여주었지만 여전히 모델 일반화에 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "242-2",
                    "sentence": "In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples.",
                    "sentence_kor": "본 논문에서 우리는 물리적 상식 학습을 지식 그래프 완료 문제로 공식화하여 훈련 샘플 간의 잠재 관계를 더 잘 활용한다.",
                    "tag": "1"
                },
                {
                    "index": "242-3",
                    "sentence": "Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small.",
                    "sentence_kor": "일반 지식 그래프를 완성하는 것과 비교할 때, 물리적 상식 지식 그래프를 완성하는 것은 세 가지 독특한 특성이 있다. 즉, 훈련 데이터가 부족하고, 모든 사실을 기존 텍스트에서 캐낼 수 없으며, 관계의 수가 적다.",
                    "tag": "1"
                },
                {
                    "index": "242-4",
                    "sentence": "To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships.",
                    "sentence_kor": "이러한 문제를 처리하기 위해 먼저 사전 훈련 언어 모델 BERT를 사용하여 훈련 데이터를 보강한 다음 유형을 제한하고 부정적인 관계를 추가하여 복잡한 관계를 모델링하기 위해 제한된 터커 인수분해를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "242-5",
                    "sentence": "We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.",
                    "sentence_kor": "우리는 우리의 방법을 기존의 최첨단 지식 그래프 임베딩 방법과 비교하고 그것의 우수한 성능을 보여준다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "243",
            "abstractID": "EMNLP_abs-243",
            "text": [
                {
                    "index": "243-0",
                    "sentence": "Social media produces large amounts of contents every day.",
                    "sentence_kor": "소셜 미디어는 매일 많은 양의 콘텐츠를 생산한다.",
                    "tag": "1"
                },
                {
                    "index": "243-1",
                    "sentence": "To help users quickly capture what they need, keyphrase prediction is receiving a growing attention.",
                    "sentence_kor": "사용자가 필요한 내용을 신속하게 파악할 수 있도록 키프레이즈 예측에 대한 관심이 높아지고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "243-2",
                    "sentence": "Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images.",
                    "sentence_kor": "그럼에도 불구하고 대부분의 선행 노력은 텍스트 모델링에 초점을 맞추고 있으며, 일치하는 이미지에 포함된 풍부한 기능을 대부분 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "243-3",
                    "sentence": "In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post.",
                    "sentence_kor": "본 연구에서는 멀티미디어 게시물의 주요 문구를 예측할 때 텍스트와 이미지의 공동 효과를 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "243-4",
                    "sentence": "To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities.",
                    "sentence_kor": "소셜 미디어 스타일 텍스트와 이미지를 더 잘 정렬하기 위해 (1) 복잡한 교차 미디어 상호 작용을 캡처하는 새로운 다중 모드 다중 헤드 어텐션(M3H-At)과 (2) 광학 문자와 이미지 속성의 이미지 워딩, 두 가지 양식을 브리징할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "243-5",
                    "sentence": "Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages.",
                    "sentence_kor": "또한, 우리는 키프레이즈 분류 및 생성의 출력을 활용하고 그 장점을 결합하기 위한 통합 프레임워크를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "243-6",
                    "sentence": "Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms.",
                    "sentence_kor": "트위터에서 새로 수집한 대규모 데이터 세트에 대한 광범위한 실험은 우리 모델이 기존의 주의 메커니즘을 기반으로 한 이전 기술 수준을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "243-7",
                    "sentence": "Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.",
                    "sentence_kor": "추가 분석에 따르면 다중 헤드 주의는 다양한 측면의 정보에 참여할 수 있으며 다양한 시나리오에서 분류 또는 생성을 향상시킬 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "244",
            "abstractID": "EMNLP_abs-244",
            "text": [
                {
                    "index": "244-0",
                    "sentence": "Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history.",
                    "sentence_kor": "시각적 대화는 이미지 콘텐츠 및 대화 기록에 대한 추론을 통해 대화 에이전트가 일련의 질문에 답해야 하는 어려운 비전 언어 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "244-1",
                    "sentence": "Prior work has mostly focused on various attention mechanisms to model such intricate interactions.",
                    "sentence_kor": "이전 작업은 대부분 이러한 복잡한 상호작용을 모델링하기 위한 다양한 주의 메커니즘에 초점을 맞추었습니다.",
                    "tag": "1"
                },
                {
                    "index": "244-2",
                    "sentence": "By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks.",
                    "sentence_kor": "이와는 대조적으로, 본 연구에서는 Visual Dialog 작업에 사전 훈련된 BERT 언어 모델을 활용하는 통합 비전 대화 변환기의 간단하지만 효과적인 프레임워크인 VD-BERT를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "244-3",
                    "sentence": "The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture.",
                    "sentence_kor": "이 모델은 (1) 단일 스트림 Transformer 인코더를 사용하여 이미지와 멀티 턴 대화 상자 사이의 모든 상호 작용을 캡처하고 (2) 동일한 아키텍처를 통해 응답 순위와 응답 생성을 원활하게 지원한다는 점에서 통일된다.",
                    "tag": "3"
                },
                {
                    "index": "244-4",
                    "sentence": "More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training.",
                    "sentence_kor": "더 중요한 것은, 우리는 시각 기반 훈련을 통해 시각과 대화 내용의 효과적인 결합을 위해 BERT를 적응시킨다는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "244-5",
                    "sentence": "Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.",
                    "sentence_kor": "외부 비전 언어 데이터에 대한 사전 교육이 필요하지 않은 우리 모델은 새로운 최첨단 기술을 제공하여 시각적 대화 리더보드에서 단일 모델과 앙상블 설정(74.54 및 75.35 NDCG 점수) 모두에서 최상위 위치를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "244-6",
                    "sentence": "Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.",
                    "sentence_kor": "우리의 코드와 사전 교육된 모델은 https://github.com/salesforce/VD-BERT에서 공개된다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "245",
            "abstractID": "EMNLP_abs-245",
            "text": [
                {
                    "index": "245-0",
                    "sentence": "In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language.",
                    "sentence_kor": "본 논문에서 우리는 원래 자연어를 분석하도록 설계된 UGI(비지도 문법 유도) 기법을 사용하여 참조 게임에서 나타난 언어의 구문적 특성을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "245-1",
                    "sentence": "We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use.",
                    "sentence_kor": "우리는 고려된 UGI 기법이 새로운 언어를 분석하는 데 적절하다는 것을 보여주고 우리는 일반적인 참조 게임 설정에서 나타나는 언어들이 구문 구조를 보이는지 그리고 이것이 에이전트가 사용할 수 있는 최대 메시지 길이와 기호 수에 어느 정도 의존하는지 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "245-2",
                    "sentence": "Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language.",
                    "sentence_kor": "우리의 실험은 구조가 나타나기 위해서는 특정한 메시지 길이와 어휘 크기가 필요하다는 것을 증명하지만, 그것들은 또한 인간 언어에서 관찰된 것과 더 유사한 구문적 특성을 얻기 위해서는 더 정교한 게임 시나리오가 필요하다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "245-3",
                    "sentence": "We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.",
                    "sentence_kor": "우리는 UGI 기법이 최신 언어 분석을 위한 표준 툴킷의 일부가 되어야 하며 미래 연구자를 위해 그러한 분석을 용이하게 하기 위한 포괄적인 라이브러리를 공개해야 한다고 주장한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "246",
            "abstractID": "EMNLP_abs-246",
            "text": [
                {
                    "index": "246-0",
                    "sentence": "Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions.",
                    "sentence_kor": "시각 및 언어 탐색을 위해서는 에이전트가 자연어 지시에 따라 실제 3D 환경을 탐색해야 합니다.",
                    "tag": "1"
                },
                {
                    "index": "246-1",
                    "sentence": "Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences.",
                    "sentence_kor": "상당한 발전에도 불구하고, 시각 순서와 텍스트 순서 사이의 강한 대응성을 완전히 활용할 수 있는 이전 연구는 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "246-2",
                    "sentence": "Meanwhile, due to the lack of intermediate supervision, the agent’s performance at following each part of the instruction cannot be assessed during navigation.",
                    "sentence_kor": "한편, 중간감독이 부족하기 때문에 항법 시 지시사항의 각 부분에 대한 대리점의 수행능력을 평가할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "246-3",
                    "sentence": "In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction.",
                    "sentence_kor": "이 작업에서는 명령의 완료를 통한 에이전트의 추적 가능성뿐만 아니라 시각적 및 언어 시퀀스의 세분성에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "246-4",
                    "sentence": "We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time.",
                    "sentence_kor": "우리는 에이전트에게 훈련 중에 세분화된 주석을 제공하고 그들이 지시를 더 잘 따를 수 있고 시험 시간에 목표에 도달할 가능성이 더 높다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "246-5",
                    "sentence": "We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths.",
                    "sentence_kor": "벤치마크 데이터 세트 R2R(Room-to-Room)을 하위 명령과 해당 경로로 강화한다.",
                    "tag": "3"
                },
                {
                    "index": "246-6",
                    "sentence": "To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step.",
                    "sentence_kor": "이 데이터를 활용하기 위해 효과적인 하위 지침 주의와 각 시간 단계에서 단일 하위 지침을 선택하고 이를 처리하는 모듈 이동을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "246-7",
                    "sentence": "We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.",
                    "sentence_kor": "우리는 하위 명령 모듈을 기본 모델과 비교하여 4개의 최신 에이전트로 구현하고 제안된 방법이 4개 에이전트의 성능을 향상시킨다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "246-8",
                    "sentence": "We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",
                    "sentence_kor": "우리는 세분화된 R2R 데이터 세트(FGR2R)와 코드를 https://github.com/YicongHong/Fine-Grained-R2R에서 공개한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "247",
            "abstractID": "EMNLP_abs-247",
            "text": [
                {
                    "index": "247-0",
                    "sentence": "We study knowledge-grounded dialogue generation with pre-trained language models.",
                    "sentence_kor": "우리는 사전 훈련된 언어 모델을 사용하여 지식 기반 대화 생성을 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "247-1",
                    "sentence": "To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues.",
                    "sentence_kor": "용량 제약 하에서 중복 외부 지식을 활용하기 위해, 우리는 지식 선택 모듈을 가진 사전 훈련된 언어 모델에 의해 정의된 응답 생성과 라벨이 부착되지 않은 대화와 함께 지식 선택 및 응답 생성을 공동으로 최적화하는 비지도 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "247-2",
                    "sentence": "Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.",
                    "sentence_kor": "두 벤치마크에 대한 경험적 결과는 우리 모델이 자동 평가와 인간 판단 모두에서 최첨단 방법을 크게 능가할 수 있음을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "248",
            "abstractID": "EMNLP_abs-248",
            "text": [
                {
                    "index": "248-0",
                    "sentence": "In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data.",
                    "sentence_kor": "본 논문에서, 우리는 과제 지향 대화 시스템의 시스템 설계 프로세스를 단순화하고 주석이 달린 데이터에 대한 지나친 의존성을 완화하기 위해 MinTL(Minimalist Transfer Learning)을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "248-1",
                    "sentence": "MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.",
                    "sentence_kor": "MinTL은 사전 훈련된 seq2seq 모델을 플러그 앤 플레이하고 대화 상태 추적 및 대화 응답 생성을 공동으로 학습할 수 있는 간단하지만 효과적인 전송 학습 프레임워크이다.",
                    "tag": "1"
                },
                {
                    "index": "248-2",
                    "sentence": "Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length.",
                    "sentence_kor": "이전 대화 상태를 새로운 대화 상태로 \"이월\"하기 위해 복사 메커니즘을 사용하는 이전 접근법과 달리, 우리는 최소 생성 길이로 효율적인 대화 상태 추적을 가능하게 하는 레벤슈테인 신념 범위(Levenshtein 믿음 범위)를 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "248-3",
                    "sentence": "We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ.",
                    "sentence_kor": "사전 훈련된 두 백본으로 학습 프레임워크를 인스턴스화합니다. T5와 BART를 MultiWOZ에서 평가합니다.",
                    "tag": "3"
                },
                {
                    "index": "248-4",
                    "sentence": "Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.",
                    "sentence_kor": "광범위한 실험에 따르면 1) 우리 시스템은 엔드 투 엔드 응답 생성에 대한 새로운 최첨단 결과를 수립하고, 2) MinTL 기반 시스템은 낮은 리소스 설정에서 기본 방법보다 더 강력하며, 단 20%의 교육 데이터만으로 경쟁력 있는 결과를 달성하며 3) 레브는 추론 효율성을 크게 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "249",
            "abstractID": "EMNLP_abs-249",
            "text": [
                {
                    "index": "249-0",
                    "sentence": "Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks.",
                    "sentence_kor": "최근 연구에서는 심층 생성 모델에서 생성된 합성 샘플이 훈련 데이터 세트를 보완하는 생성 데이터 증가가 NLP 작업에 도움이 된다는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "249-1",
                    "sentence": "In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs.",
                    "sentence_kor": "이 작업에서는 이 접근 방식을 목표 지향 대화 상자의 대화 상태 추적 작업으로 확장한다.",
                    "tag": "1"
                },
                {
                    "index": "249-2",
                    "sentence": "Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features.",
                    "sentence_kor": "발화 및 관련 주석에 대한 목표 지향 대화 상자의 고유한 계층 구조 때문에 심층 생성 모델은 다양한 계층 구조 및 대화 상자 특징 유형 간의 일관성을 포착할 수 있어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "249-3",
                    "sentence": "We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals.",
                    "sentence_kor": "언어적 특징과 기본 구조화된 주석, 즉 화자 정보, 대화 상자 행동 및 목표를 포함하여 목표 지향 대화 상자의 전체 측면을 모델링하기 위한 VHDA(Variational Hierarchical Dialog Autoencoder)를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "249-4",
                    "sentence": "The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces.",
                    "sentence_kor": "제안된 아키텍처는 상호 연결된 잠재 변수를 사용하여 목표 지향 대화 상자의 각 측면을 모델링하도록 설계되었으며 잠재 공간에서 일관된 목표 지향 대화 상자를 생성하는 방법을 학습한다.",
                    "tag": "2+3"
                },
                {
                    "index": "249-5",
                    "sentence": "To overcome training issues that arise from training complex variational models, we propose appropriate training strategies.",
                    "sentence_kor": "복잡한 변형 모델 훈련에서 발생하는 훈련 문제를 극복하기 위해 적절한 훈련 전략을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "249-6",
                    "sentence": "Experiments on various dialog datasets show that our model improves the downstream dialog trackers’ robustness via generative data augmentation.",
                    "sentence_kor": "다양한 대화 상자 데이터 세트에 대한 실험은 우리 모델이 생성 데이터 증대를 통해 다운스트림 대화 상자 추적기의 견고성을 향상시킨다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "249-7",
                    "sentence": "We also discover additional benefits of our unified approach to modeling goal-oriented dialogs – dialog response generation and user simulation, where our model outperforms previous strong baselines.",
                    "sentence_kor": "또한 목표 지향 대화 상자 모델링(대화 응답 생성 및 사용자 시뮬레이션)에 대한 통합 접근 방식의 추가적인 이점을 발견했는데, 여기서 우리 모델은 이전의 강력한 기준선을 능가한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "250",
            "abstractID": "EMNLP_abs-250",
            "text": [
                {
                    "index": "250-0",
                    "sentence": "Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge.",
                    "sentence_kor": "지식 선택은 지식 기반 대화에서 중요한 역할을 하는데, 이는 외부 지식을 활용하여 보다 유익한 응답을 창출하는 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "250-1",
                    "sentence": "Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance.",
                    "sentence_kor": "최근 잠재 변수 모델은 지식보다 사전 및 사후 분포를 모두 사용하고 유망한 성과를 달성함으로써 지식 선택의 다양성을 다루기 위해 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "250-2",
                    "sentence": "However, these models suffer from a huge gap between prior and posterior knowledge selection.",
                    "sentence_kor": "그러나 이러한 모델은 사전 지식 선택과 사후 지식 선택 사이의 큰 격차를 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "250-3",
                    "sentence": "Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information.",
                    "sentence_kor": "첫째, 사전 선택 모듈은 필요한 사후 정보가 부족하기 때문에 지식을 올바르게 선택하는 방법을 배우지 못할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "250-4",
                    "sentence": "Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference.",
                    "sentence_kor": "둘째로, 잠재적 변수 모델은 대화 생성은 훈련 시 후방 분포에서 선택한 지식을 기반으로 하지만 추론 시 이전 분포에 기초한다는 노출 편향을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "250-5",
                    "sentence": "Here, we deal with these issues on two aspects: ",
                    "sentence_kor": "여기서는 두 가지 측면에서 이러한 문제를 다룹니다.",
                    "tag": "1"
                },
                {
                    "index": "250-6",
                    "sentence": "(1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); ",
                    "sentence_kor": "(1) 우리는 특수 설계된 PIPM(Postary Information Prediction Module)에서 얻은 필요한 후방 정보를 사용하여 사전 선택 모듈을 강화한다.",
                    "tag": "1"
                },
                {
                    "index": "250-7",
                    "sentence": "(2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection.",
                    "sentence_kor": "(2) 지식 증류 기반 교육 전략(KDB)을 제안한다.TS) 사전 배포에서 선택한 지식으로 디코더를 교육하여 지식 선택의 노출 편향을 제거합니다.",
                    "tag": "1"
                },
                {
                    "index": "250-8",
                    "sentence": "Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.",
                    "sentence_kor": "두 개의 지식 기반 대화 데이터 세트에 대한 실험 결과는 PIPM과 산은이 모두라는 것을 보여준다.TS는 최첨단 잠재 변수 모델에 비해 성능 향상을 달성하며 이들의 조합은 더욱 개선되었음을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "251",
            "abstractID": "EMNLP_abs-251",
            "text": [
                {
                    "index": "251-0",
                    "sentence": "Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses.",
                    "sentence_kor": "오픈 도메인 대화 생성은 방대한 잠재적 응답 규모로 인해 데이터 부족 문제를 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "251-1",
                    "sentence": "In this paper, we propose to explore potential responses by counterfactual reasoning.",
                    "sentence_kor": "본 논문에서, 우리는 반사실적 추론을 통해 잠재적인 반응을 탐구할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "251-2",
                    "sentence": "Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken.",
                    "sentence_kor": "관찰된 반응이 주어지면, 반사실적 추론 모델은 취할 수 있었던 대안 정책의 결과를 자동으로 추론한다.",
                    "tag": "1"
                },
                {
                    "index": "251-3",
                    "sentence": "The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch.",
                    "sentence_kor": "사후 판단에서 합성된 반사실적 반응은 처음부터 합성된 반응보다 품질이 더 높다.",
                    "tag": "4"
                },
                {
                    "index": "251-4",
                    "sentence": "Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space.",
                    "sentence_kor": "적대적 학습 프레임워크에서 반사실적 응답에 대한 교육은 잠재적 응답 공간의 높은 보상 영역을 탐색하는 데 도움이 된다.",
                    "tag": "5"
                },
                {
                    "index": "251-5",
                    "sentence": "An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.",
                    "sentence_kor": "DailyDialog 데이터 세트에 대한 경험적 연구는 우리의 접근 방식이 기존의 적대적 학습 접근 방식뿐만 아니라 HRED 모델도 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "252",
            "abstractID": "EMNLP_abs-252",
            "text": [
                {
                    "index": "252-0",
                    "sentence": "Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data.",
                    "sentence_kor": "개방형 도메인 대화 시스템의 최근 발전은 대규모 데이터에 대해 훈련된 신경 모델의 성공에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "252-1",
                    "sentence": "However, collecting large-scale dialogue data is usually time-consuming and labor-intensive.",
                    "sentence_kor": "그러나 대규모 대화 데이터를 수집하는 것은 일반적으로 시간이 많이 걸리고 노동 집약적이다.",
                    "tag": "1"
                },
                {
                    "index": "252-2",
                    "sentence": "To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data.",
                    "sentence_kor": "이 데이터 딜레마를 해결하기 위해 페어링되지 않은 데이터를 활용하여 개방형 도메인 대화 모델을 교육하기 위한 새로운 데이터 확대 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "252-3",
                    "sentence": "Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data.",
                    "sentence_kor": "특히, 페어링되지 않은 데이터에서 포스트와 응답을 모두 검색하는 증강 대화를 구성하기 위한 데이터 수준 증류 프로세스가 먼저 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "252-4",
                    "sentence": "A ranking module is employed to filter out low-quality dialogues.",
                    "sentence_kor": "낮은 품질의 대화를 걸러내기 위해 순위 모듈을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "252-5",
                    "sentence": "Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data.",
                    "sentence_kor": "또한 모델 수준 증류 프로세스를 사용하여 고품질 페어링 데이터에 대해 훈련된 교사 모델을 증강 대화 쌍으로 증류하여 대화 모델이 증강 데이터의 노이즈에 영향을 받지 않도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "252-6",
                    "sentence": "Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.",
                    "sentence_kor": "자동 및 수동 평가에 따르면 우리 방법은 다양한 콘텐츠를 가진 고품질 대화 쌍을 생성할 수 있으며, 제안된 데이터 수준 및 모델 수준 대화 증류는 경쟁 기준선의 성능을 향상시킬 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "253",
            "abstractID": "EMNLP_abs-253",
            "text": [
                {
                    "index": "253-0",
                    "sentence": "We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning.",
                    "sentence_kor": "우리는 과제 완료 대화 정책 학습을 위해 이중 Q 결투 네트워크(MCTS-DDU)를 사용한 몬테 카를로 트리 검색 프레임워크를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "253-1",
                    "sentence": "Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors.",
                    "sentence_kor": "배경 계획을 사용하고 낮은 품질의 시뮬레이션 경험을 겪을 수 있는 이전의 심층 모델 기반 강화 학습 방법과 달리 MCTS-DDU는 몬테카를로 시뮬레이션에 의해 구축된 대화 상태 검색 트리에 기초한 의사결정 시간 계획을 수행하고 시뮬레이션 오류에 강하다.",
                    "tag": "1"
                },
                {
                    "index": "253-2",
                    "sentence": "Such idea arises naturally in human behaviors, e.g. predicting others’ responses and then deciding our own actions.",
                    "sentence_kor": "그러한 생각은 다른 사람의 반응을 예측하고 우리 자신의 행동을 결정하는 것과 같은 인간의 행동에서 자연스럽게 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "253-3",
                    "sentence": "In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly.",
                    "sentence_kor": "시뮬레이션된 영화 티켓 예약 작업에서 우리의 방법은 배경 계획 접근법을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "253-4",
                    "sentence": "We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.",
                    "sentence_kor": "우리는 상세한 절제 연구에서 MCTS와 결투 네트워크의 효과를 입증하고 또한 이 두 가지 계획 방법의 성능 상한을 비교한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "254",
            "abstractID": "EMNLP_abs-254",
            "text": [
                {
                    "index": "254-0",
                    "sentence": "We study multi-turn response generation for open-domain dialogues.",
                    "sentence_kor": "우리는 오픈 도메인 대화를 위한 멀티 턴 응답 생성을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "254-1",
                    "sentence": "The existing state-of-the-art addresses the problem with deep neural architectures.",
                    "sentence_kor": "기존의 최첨단 기술은 심층 신경 아키텍처의 문제를 해결한다.",
                    "tag": "1"
                },
                {
                    "index": "254-2",
                    "sentence": "While these models improved response quality, their complexity also hinders the application of the models in real systems.",
                    "sentence_kor": "이러한 모델은 응답 품질을 향상시키지만, 복잡성은 실제 시스템에서 모델의 적용을 방해하기도 합니다.",
                    "tag": "1"
                },
                {
                    "index": "254-3",
                    "sentence": "In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation.",
                    "sentence_kor": "이 작업에서는 간단한 구조를 가지면서도 응답 생성을 위해 대화 컨텍스트를 효과적으로 활용할 수 있는 모델을 추구한다.",
                    "tag": "3"
                },
                {
                    "index": "254-4",
                    "sentence": "To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation.",
                    "sentence_kor": "이를 위해 워드 순서 복구, 발화 순서 복구, 마스킹된 단어 복구 및 마스킹 발화 복구를 포함한 네 가지 보조 작업을 제안하고 생성 가능성을 극대화하면서 이러한 작업의 목표를 최적화한다.",
                    "tag": "2+3"
                },
                {
                    "index": "254-5",
                    "sentence": "By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum.",
                    "sentence_kor": "이러한 방법으로, 상황 이해와 관련된 보조 과제는 발전 모델의 학습을 더 나은 국소 최적화를 달성하도록 안내할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "254-6",
                    "sentence": "Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.",
                    "sentence_kor": "세 가지 벤치마크를 사용한 경험적 연구에 따르면 우리 모델은 자동 평가와 인간 판단 모두에서 반응 품질 면에서 최첨단 생성 모델을 크게 능가할 수 있으며, 동시에 훨씬 빠른 디코딩 프로세스를 누리고 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "255",
            "abstractID": "EMNLP_abs-255",
            "text": [
                {
                    "index": "255-0",
                    "sentence": "Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response.",
                    "sentence_kor": "대화 문맥과 관련된 적절한 지식을 검색하는 것은 대화 시스템에서 사용자가 보다 유익한 응답을 하도록 하는 중요한 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "255-1",
                    "sentence": "Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance.",
                    "sentence_kor": "몇몇 최근 연구에서는 이 지식 선택 문제를 외부 지식 그래프(KG)를 통한 경로 횡단으로 공식화할 것을 제안하고 있지만, KG 구조의 제한된 활용만을 보여 주므로 성능 개선의 여지가 있다.",
                    "tag": "1"
                },
                {
                    "index": "255-2",
                    "sentence": "To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows.",
                    "sentence_kor": "이러한 취지로, 우리는 Attn을 제시한다.IO는 두 방향의 주의 흐름을 기반으로 KG의 풍부한 구조 정보를 최대한 활용하는 새로운 대화 조건 경로 횡단 모델이다.",
                    "tag": "2"
                },
                {
                    "index": "255-3",
                    "sentence": "Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context.",
                    "sentence_kor": "주의의 흐름을 통해, AttnIO는 광범위한 다중 홉 지식 경로를 탐색할 수 있을 뿐만 아니라 대화 상자 컨텍스트에 따라 참석할 수 있는 그럴듯한 노드와 에지의 다양한 범위를 유연하게 조정하는 방법을 배운다.",
                    "tag": "3"
                },
                {
                    "index": "255-4",
                    "sentence": "Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset.",
                    "sentence_kor": "경험적 평가는 Attn의 현저한 성능 향상을 나타낸다.OpenDialKG 데이터 세트의 모든 기준선과 비교한 IO입니다.",
                    "tag": "4"
                },
                {
                    "index": "255-5",
                    "sentence": "Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.",
                    "sentence_kor": "또한 경로를 사용할 수 없고 대상 노드만 레이블로 제공되어 실제 대화 시스템에 보다 쉽게 적용할 수 있는 경우에도 적절한 지식 경로를 생성하도록 모델을 훈련할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "256",
            "abstractID": "EMNLP_abs-256",
            "text": [
                {
                    "index": "256-0",
                    "sentence": "The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention.",
                    "sentence_kor": "지식 기반을 쿼리하고 과제 지향 대화 시스템에 대해 인간과 유사한 응답을 생성하여 과제 완료를 달성해야 하는 과제가 연구 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "256-1",
                    "sentence": "In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously.",
                    "sentence_kor": "본 논문에서, 우리는 정확한 KB 엔티티를 검색하고 인간과 유사한 응답을 동시에 생성하는 것을 목표로 과제 지향 대화를 위한 \"TTOS\" 학습 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "256-2",
                    "sentence": "TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network).",
                    "sentence_kor": "TTOS는 고품질 과제 지향 대화 시스템(학생 네트워크)을 구축하기 위한 포괄적인 지침을 제공하는 두 교사 네트워크의 지식을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "256-3",
                    "sentence": "Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network.",
                    "sentence_kor": "각 교사 네트워크는 목표를 향한 전문가로 간주될 수 있는 목표별 보상이 있는 강화 학습을 통해 훈련되고 전문적인 특성을 학생 네트워크에 이전한다.",
                    "tag": "3"
                },
                {
                    "index": "256-4",
                    "sentence": "Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student.",
                    "sentence_kor": "학생 네트워크의 출력을 교사 네트워크가 생성하는 소프트 타겟을 정확히 모방하도록 강제하는 고전적인 학생-교사 학습을 채택하는 대신, 우리는 두 명의 교사로부터 학생에게 지식을 이전하기 위해 생성적 적대 네트워크(GAN)에서와 같이 두 개의 판별자를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "256-5",
                    "sentence": "The usage of discriminators relaxes the rigid coupling between the student and teachers.",
                    "sentence_kor": "판별기의 사용은 학생과 교사 사이의 엄격한 결합을 완화시킨다.",
                    "tag": "4"
                },
                {
                    "index": "256-6",
                    "sentence": "Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.",
                    "sentence_kor": "두 가지 벤치마크 데이터 세트(즉, CamRest와 차내 보조 장치)에 대한 광범위한 실험은 TTOS가 기준 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "257",
            "abstractID": "EMNLP_abs-257",
            "text": [
                {
                    "index": "257-0",
                    "sentence": "Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks.",
                    "sentence_kor": "서로 다른 단어 임베딩에서 보완 정보를 결합하는 메타 임베딩 학습은 서로 다른 자연어 처리 작업에서 우수한 성능을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "257-1",
                    "sentence": "However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains.",
                    "sentence_kor": "그러나 도메인별 지식은 기존 메타 임베딩 방법에서는 여전히 무시되며, 이로 인해 특정 도메인에서 불안정한 성능이 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "257-2",
                    "sentence": "Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem.",
                    "sentence_kor": "또한, 일반 및 도메인 단어 임베딩의 중요성은 다운스트림 작업과 관련이 있다. 다운스트림 작업을 적응하기 위해 메타 임베딩을 정규화하는 방법은 해결되지 않은 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "257-3",
                    "sentence": "In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings.",
                    "sentence_kor": "본 논문에서는 도메인별 정보와 작업 지향 정보를 모두 메타 임베딩에 통합하는 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "257-4",
                    "sentence": "We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.",
                    "sentence_kor": "우리는 네 개의 텍스트 분류 데이터 세트에 대해 광범위한 실험을 수행했고 그 결과는 우리가 제안한 방법의 효과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "258",
            "abstractID": "EMNLP_abs-258",
            "text": [
                {
                    "index": "258-0",
                    "sentence": "State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models.",
                    "sentence_kor": "WSD(Word Sense Disambigization)를 위한 최첨단 방법은 두 가지 다른 특징, 즉 사전 훈련된 언어 모델의 힘과 해당 모델의 적용 범위를 확장하는 전파 방법을 결합한다.",
                    "tag": "1"
                },
                {
                    "index": "258-1",
                    "sentence": "This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet).",
                    "sentence_kor": "이 전파는 현재 의미 주석이 붙은 말뭉치가 기본 의미 인벤토리(일반적으로 WordNet)에서 많은 인스턴스에 대한 커버리지가 부족하기 때문에 필요합니다.",
                    "tag": "1"
                },
                {
                    "index": "258-2",
                    "sentence": "At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora.",
                    "sentence_kor": "이와 동시에 WordNet의 모든 단어에 대해 명확한 단어가 많은 부분을 차지하는 동시에 기존의 의미 있는 주석 말뭉치에서는 제대로 다루어지지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "258-3",
                    "sentence": "In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus.",
                    "sentence_kor": "본 논문에서, 우리는 대규모 말뭉치의 모호하지 않은 대부분의 단어에 주석을 제공하는 간단한 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "258-4",
                    "sentence": "We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.",
                    "sentence_kor": "우리는 UWA(모호한 단어 주석) 데이터 세트를 소개하고 최첨단 전파 기반 모델이 이를 사용하여 WSD에 대한 원래 결과를 개선하여 워드 센스 임베딩의 적용 범위와 품질을 상당한 폭으로 확장하는 방법을 보여준다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "259",
            "abstractID": "EMNLP_abs-259",
            "text": [
                {
                    "index": "259-0",
                    "sentence": "We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words.",
                    "sentence_kor": "단어 간의 어휘-의미적 관계를 인식하기 위한 새로운 관계 내 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "259-1",
                    "sentence": "Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation.",
                    "sentence_kor": "우리 모델은 관계형 및 분포형 신호를 통합하여 각 관계에 대한 효과적인 하위 공간 표현을 형성한다.",
                    "tag": "1"
                },
                {
                    "index": "259-2",
                    "sentence": "We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.",
                    "sentence_kor": "우리는 제안된 모델이 경쟁력 있고 다양한 벤치마크에서 다른 기준선을 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "260",
            "abstractID": "EMNLP_abs-260",
            "text": [
                {
                    "index": "260-0",
                    "sentence": "Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information.",
                    "sentence_kor": "문맥화된 단어 임베딩은 유용한 의미 정보를 전달하는 것으로 입증되었기 때문에 자연어 처리의 여러 작업에 걸쳐 효과적으로 사용되었다.",
                    "tag": "1"
                },
                {
                    "index": "260-1",
                    "sentence": "However, it is still hard to link them to structured sources of knowledge.",
                    "sentence_kor": "하지만, 그것들을 체계적인 지식의 원천과 연결시키는 것은 여전히 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "260-2",
                    "sentence": "In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors.",
                    "sentence_kor": "본 논문에서 우리는 문맥화된 단어 벡터와 유사한 공간에 있는 어휘적 지식 기반 내에서 어휘적 의미에 대한 감각 임베딩을 생성하는 준감독 접근 방식인 ARES(Context-Awre Embeddings of Sense)를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "260-3",
                    "sentence": "ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only.",
                    "sentence_kor": "ARES 표현은 간단한 1 근접 이웃 알고리즘이 영어 단어 의미 제거 작업뿐만 아니라 다국어 모델에서도 최첨단 모델을 능가하는 동시에 영어로만 표기된 데이터에 대한 교육을 가능하게 한다.",
                    "tag": "3"
                },
                {
                    "index": "260-4",
                    "sentence": "We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures.",
                    "sentence_kor": "우리는 또한 Word-in-Context 작업에서 임베딩의 품질을 평가한다. 여기서 외부 지식의 원천으로 사용될 경우 신경 모델의 성능을 지속적으로 향상시켜 다른 복잡한 아키텍처와 경쟁하도록 유도한다.",
                    "tag": "3"
                },
                {
                    "index": "260-5",
                    "sentence": "ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.",
                    "sentence_kor": "모든 WordNet 개념과 감지 표현을 만드는 데 사용되는 자동 검색 컨텍스트를 위한 ARS 임베딩은 http://sensembert.org/ares에서 무료로 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "261",
            "abstractID": "EMNLP_abs-261",
            "text": [
                {
                    "index": "261-0",
                    "sentence": "The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence.",
                    "sentence_kor": "측면 수준 감정 분류의 최첨단 방법은 문장의 구문 구조를 통합하기 위해 그래프 기반 모델을 활용했다.",
                    "tag": "1"
                },
                {
                    "index": "261-1",
                    "sentence": "While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like “nothing special”.",
                    "sentence_kor": "이러한 방법은 효과적이기는 하지만 언어학에서 \"특별한 것은 없다\"와 같은 연어를 반영하는 말뭉치 수준의 단어 동시 발생 정보를 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "261-2",
                    "sentence": "Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation “food-was” is treated equally as an adjectival complement relation “was-okay” in “food was okay”.",
                    "sentence_kor": "더욱이, 그들은 구문 의존성의 다른 유형을 구별하지 않는다. 예를 들어, 명목상의 주어 관계 \"food-was\"는 \"food was ok\"에서 형용사 보완 관계 \"was-ok\"로 동등하게 취급된다.",
                    "tag": "1"
                },
                {
                    "index": "261-3",
                    "sentence": "To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs.",
                    "sentence_kor": "위의 두 가지 한계를 해결하기 위해 계층적 구문 및 어휘 그래프에 대해 난해한 새로운 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "261-4",
                    "sentence": "Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information.",
                    "sentence_kor": "특히, 말뭉치 수준 단어 동시 발생 정보를 인코딩하기 위해 글로벌 어휘 그래프를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "261-5",
                    "sentence": "Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs.",
                    "sentence_kor": "또한, 우리는 다양한 유형의 의존 관계 또는 어휘 단어 쌍을 구별하기 위해 구문 및 어휘 그래프 모두에 개념 계층을 구축한다.",
                    "tag": "3"
                },
                {
                    "index": "261-6",
                    "sentence": "Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs.",
                    "sentence_kor": "마지막으로, 우리는 이 두 그래프를 완전히 활용하기 위해 2단계 대화형 그래프 컨볼루션 네트워크를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "261-7",
                    "sentence": "Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.",
                    "sentence_kor": "5개의 벤치 마크 데이터 세트에 대한 광범위한 실험은 우리의 방법이 최첨단 기준선을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "262",
            "abstractID": "EMNLP_abs-262",
            "text": [
                {
                    "index": "262-0",
                    "sentence": "Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories.",
                    "sentence_kor": "측면 범주 정서 분석(ACSA)은 주어진 측면 범주에 대한 문장의 감정 극성을 예측하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "262-1",
                    "sentence": "To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation.",
                    "sentence_kor": "문장의 특정 측면 범주에 대한 감정을 감지하기 위해 대부분의 이전 방법은 먼저 측면 범주에 대한 측면 범주별 문장 표현을 생성한 다음 표현을 기반으로 감정 극성을 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "262-2",
                    "sentence": "These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance.",
                    "sentence_kor": "이러한 방법은 문장에서 언급된 측면 범주의 정서가 문장에서 측면 범주를 나타내는 단어의 정서를 종합한 것이라는 사실을 무시하며, 이는 차선의 성능으로 이어진다.",
                    "tag": "3"
                },
                {
                    "index": "262-3",
                    "sentence": "In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category.",
                    "sentence_kor": "본 논문에서는 문장을 가방으로, 단어를 인스턴스로, 양상 범주를 나타내는 단어를 양상 범주의 핵심 인스턴스로 처리하는 AC-MIMLLN(Multi-Instance Multi-Label Learning Network for Aspect-Category 정서 분석)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "262-4",
                    "sentence": "Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments.",
                    "sentence_kor": "문장과 문장에 언급된 측면 범주가 주어지면 AC-MIMLLN은 먼저 인스턴스의 감정을 예측한 다음 측면 범주에 대한 핵심 인스턴스를 찾고, 마지막으로 주요 인스턴스 감정을 집계하여 측면 범주에 대한 문장의 정서를 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "262-5",
                    "sentence": "Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.",
                    "sentence_kor": "세 개의 공개 데이터 세트에 대한 실험 결과는 AC-MIMLLN의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "263",
            "abstractID": "EMNLP_abs-263",
            "text": [
                {
                    "index": "263-0",
                    "sentence": "Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention.",
                    "sentence_kor": "주어진 측면의 감정 극성을 예측하는 측면 기반 감정 분석은 광범위한 관심을 끌었다.",
                    "tag": "1"
                },
                {
                    "index": "263-1",
                    "sentence": "Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification.",
                    "sentence_kor": "이전의 주의 기반 모델은 분류를 위한 의견 특징을 추출하는 데 도움이 되는 측면 의미론을 사용하는 것을 강조한다.",
                    "tag": "1"
                },
                {
                    "index": "263-2",
                    "sentence": "However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans.",
                    "sentence_kor": "그러나 이러한 작업은 의견 범위를 전체적으로 포착할 수 없거나 가변 길이의 의견 범위를 포착할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "263-3",
                    "sentence": "In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs.",
                    "sentence_kor": "본 논문에서는 여러 선형 체인 CRF를 집계하여 깔끔하고 효과적인 구조화된 주의 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "263-4",
                    "sentence": "Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features.",
                    "sentence_kor": "이러한 설계를 통해 모델은 측면별 의견 범위를 추출한 다음 추출된 의견 특징을 이용하여 감정 극성을 평가할 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "263-5",
                    "sentence": "The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.",
                    "sentence_kor": "네 개의 데이터 세트에 대한 실험 결과는 제안된 모델의 효과를 입증하며, 우리의 분석은 우리의 모델이 측면별 의견 범위를 포착할 수 있음을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "264",
            "abstractID": "EMNLP_abs-264",
            "text": [
                {
                    "index": "264-0",
                    "sentence": "Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document.",
                    "sentence_kor": "감정 원인 쌍 추출(ECPE)은 문서에서 잠재적인 감정 쌍과 해당 원인을 추출하는 것을 목표로 하는 새로운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "264-1",
                    "sentence": "The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering.",
                    "sentence_kor": "기존 방법은 먼저 감정 추출을 수행하고 독립적으로 추출을 유발한 다음 감정 원인 짝짓기와 필터링을 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "264-2",
                    "sentence": "However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step.",
                    "sentence_kor": "그러나 위의 방법들은 원인과 그 원인에 의해 촉발되는 감정이 불가분의 관계에 있다는 사실을 무시하며, 감정을 명시하지 않고 원인을 추출하는 것은 병리적이어서 첫 번째 단계에서 위의 방법들의 수행을 크게 제한한다.",
                    "tag": "1"
                },
                {
                    "index": "264-3",
                    "sentence": "To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL).",
                    "sentence_kor": "이러한 단점을 해결하기 위해, 우리는 ECPE에 대한 두 가지 공동 프레임워크를 제안한다. 1) 지정된 감정 절(CMLL)에 해당하는 원인 절의 추출을 위한 다중 라벨 학습과 2) 지정된 원인 절(EMLL)에 해당하는 감정 절의 추출을 위한 다중 라벨 학습을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "264-4",
                    "sentence": "The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move.",
                    "sentence_kor": "다중 레이블 학습 창은 위치가 이동함에 따라 지정된 감정 절 또는 원인 절과 슬라이드에 집중됩니다.",
                    "tag": "3"
                },
                {
                    "index": "264-5",
                    "sentence": "Finally, CMLL and EMLL are integrated to obtain the final result.",
                    "sentence_kor": "마지막으로 CML과 EML을 통합하여 최종 결과를 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "264-6",
                    "sentence": "We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.",
                    "sentence_kor": "벤치마크 감정 원인 말뭉치에 대해 모델을 평가하며, 그 결과는 우리의 접근 방식이 ECPE 과제에서 비교된 모든 시스템 중에서 최고의 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "265",
            "abstractID": "EMNLP_abs-265",
            "text": [
                {
                    "index": "265-0",
                    "sentence": "As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years.",
                    "sentence_kor": "자연어 처리 커뮤니티의 중요한 연구 이슈로서, 다중 라벨 감정 감지는 지난 몇 년 동안 점점 더 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "265-1",
                    "sentence": "However, almost all existing studies focus on one modality (e.g., textual modality).",
                    "sentence_kor": "그러나 거의 모든 기존 연구는 하나의 양식(예: 텍스트 양식)에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "265-2",
                    "sentence": "In this paper, we focus on multi-label emotion detection in a multi-modal scenario.",
                    "sentence_kor": "본 논문에서 우리는 다중 모달 시나리오에서 다중 레이블 감정 감지에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "265-3",
                    "sentence": "In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence).",
                    "sentence_kor": "이 시나리오에서, 우리는 다른 라벨 간의 의존성(레이블 의존성)과 각 예측 라벨과 다른 양식 간의 의존성(양형 의존성)을 모두 고려해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "265-4",
                    "sentence": "Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection.",
                    "sentence_kor": "특히 다중 모달 다중 라벨 감정 감지에서 두 종류의 의존성을 효과적으로 모델링하기 위한 다중 모달 시퀀스 투 세트 접근방식을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "265-5",
                    "sentence": "The detailed evaluation demonstrates the effectiveness of our approach.",
                    "sentence_kor": "자세한 평가는 우리의 접근 방식의 효과를 입증한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "266",
            "abstractID": "EMNLP_abs-266",
            "text": [
                {
                    "index": "266-0",
                    "sentence": "Modeling content importance is an essential yet challenging task for summarization.",
                    "sentence_kor": "콘텐츠의 중요성을 모델링하는 것은 요약에 있어 필수적이지만 어려운 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "266-1",
                    "sentence": "Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance.",
                    "sentence_kor": "이전 연구는 대부분 중요성을 수량화할 때 의미론과 더 큰 맥락을 고려하지 않는 단어 수준의 예의를 추정하는 통계적 방법에 기초한다.",
                    "tag": "1"
                },
                {
                    "index": "266-2",
                    "sentence": "It is thus hard for these methods to generalize to semantic units of longer text spans.",
                    "sentence_kor": "따라서 이러한 방법은 텍스트 범위가 긴 의미 단위로 일반화하기가 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "266-3",
                    "sentence": "In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount.",
                    "sentence_kor": "본 연구에서는 사전 훈련된 언어 모델 위에 정보 이론을 적용하고 정보 양의 관점에서 중요성의 개념을 정의한다.",
                    "tag": "1"
                },
                {
                    "index": "266-4",
                    "sentence": "It considers both the semantics and context when evaluating the importance of each semantic unit.",
                    "sentence_kor": "그것은 각 의미 단위의 중요성을 평가할 때 의미론과 맥락 모두를 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "266-5",
                    "sentence": "With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences.",
                    "sentence_kor": "사전 훈련된 언어 모델의 도움으로, 그것은 다른 종류의 의미 단위 n그램 또는 문장으로 쉽게 일반화할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "266-6",
                    "sentence": "Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.",
                    "sentence_kor": "CNN/Daily Mail 및 New York Times 데이터 세트에 대한 실험은 우리의 방법이 F1 및 ROUGE 점수를 기반으로 한 이전 작업보다 콘텐츠의 중요성을 더 잘 모델링할 수 있다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "267",
            "abstractID": "EMNLP_abs-267",
            "text": [
                {
                    "index": "267-0",
                    "sentence": "Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task.",
                    "sentence_kor": "문서 요약 시스템의 평가는 요약 작업의 성공에 영향을 미치는 중요한 요소였다.",
                    "tag": "1"
                },
                {
                    "index": "267-1",
                    "sentence": "Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary.",
                    "sentence_kor": "ROUGE와 같은 이전 접근방식은 주로 평가 요약의 정보성을 고려하며 각 시험 요약에 대해 사람이 작성한 참조 자료를 요구한다.",
                    "tag": "1"
                },
                {
                    "index": "267-2",
                    "sentence": "In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning.",
                    "sentence_kor": "본 연구에서는 비지도 대조 학습을 통해 기준 요약 없이 요약 품질을 평가할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "267-3",
                    "sentence": "Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT.",
                    "sentence_kor": "특히, 우리는 BERT를 기반으로 언어적 특성과 의미적 정보성을 모두 포함하는 새로운 측정 기준을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "267-4",
                    "sentence": "To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss.",
                    "sentence_kor": "메트릭을 학습하기 위해 각 요약에 대해 요약 품질의 다른 측면에 대해 다른 유형의 마이너스 샘플을 구성하고 순위 손실로 모델을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "267-5",
                    "sentence": "Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries.",
                    "sentence_kor": "뉴스룸과 CNN/Daily Mail에 대한 실험은 우리의 새로운 평가 방법이 참조 요약이 없어도 다른 측정 기준을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "267-6",
                    "sentence": "Furthermore, we show that our method is general and transferable across datasets.",
                    "sentence_kor": "또한, 우리는 우리의 방법이 일반적이며 데이터셋 간에 전송 가능하다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "268",
            "abstractID": "EMNLP_abs-268",
            "text": [
                {
                    "index": "268-0",
                    "sentence": "Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations.",
                    "sentence_kor": "문장 수준 추출 텍스트 요약은 정보 구성 요소와 간결한 표현을 고수하는 네트워크 마이닝의 노드 분류 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "268-1",
                    "sentence": "There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods.",
                    "sentence_kor": "추출된 문장 사이에는 많은 중복 문구가 있지만, 일반적인 감독 방법으로 정확하게 모델링하기는 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "268-2",
                    "sentence": "Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences.",
                    "sentence_kor": "이전 문장 인코더, 특히 BERT는 소스 문장 간의 관계를 모델링하는 데 전문적이다.",
                    "tag": "1"
                },
                {
                    "index": "268-3",
                    "sentence": "While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences.",
                    "sentence_kor": "그러나 대상 선택된 요약의 중복을 고려할 능력은 없으며, 대상 문장의 레이블 사이에 고유한 종속성이 있다.",
                    "tag": "1"
                },
                {
                    "index": "268-4",
                    "sentence": "In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences.",
                    "sentence_kor": "본 논문에서 우리는 단어 및 문장을 포함한 다양한 수준의 정보를 모델링하고 문장 간의 중복성 의존성을 조명하는 HAHSum(텍스트 요약을 위한 계층적 주의 이종 그래프의 약자)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "268-5",
                    "sentence": "Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing.",
                    "sentence_kor": "우리의 접근 방식은 중복성 인식 그래프로 문장 표현을 반복적으로 조정하고 메시지 전달을 통해 레이블 의존성을 전달한다.",
                    "tag": "3"
                },
                {
                    "index": "268-6",
                    "sentence": "Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.",
                    "sentence_kor": "대규모 벤치마크 말뭉치(CNN/DM, NYT 및 NEWSROOM)에 대한 실험은 HAHSum이 획기적인 성능을 제공하고 이전의 추출 요약자를 능가한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "269",
            "abstractID": "EMNLP_abs-269",
            "text": [
                {
                    "index": "269-0",
                    "sentence": "We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization.",
                    "sentence_kor": "쿼리 중심의 다중 문서 요약을 용이하게 하기 위해 쿼리-클러스터 상호 작용을 모델링하는 문제를 고려합니다.",
                    "tag": "1"
                },
                {
                    "index": "269-1",
                    "sentence": "Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries.",
                    "sentence_kor": "교육 데이터가 부족하기 때문에 기존 작업은 쿼리 관련 요약을 조립하는 검색 스타일 방법에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "269-2",
                    "sentence": "We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central.",
                    "sentence_kor": "우리는 텍스트 세그먼트가 관련이 있는지, 답을 포함할 가능성이 있는지, 그리고 중심적인지를 추정하기 위해 점진적으로 더 정확한 모듈을 사용하는 거친 모델링 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "269-3",
                    "sentence": "The modules can be independently developed and leverage training data if available.",
                    "sentence_kor": "모듈은 독립적으로 개발될 수 있으며 가능한 경우 교육 데이터를 활용할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "269-4",
                    "sentence": "We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary.",
                    "sentence_kor": "우리는 질의에 대답할 가능성이 높고 요약에 포함되어야 하는 세그먼트를 식별하기 위해 질문 답변(다양한 자원이 존재하는 곳)의 원격 감독에 의존하는 훈련된 증거 추정기로 이 프레임워크의 인스턴스화를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "269-5",
                    "sentence": "Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.",
                    "sentence_kor": "우리의 프레임워크는 도메인 및 쿼리 유형(예: 긴 것과 짧은 것)에 걸쳐 견고하며 벤치마크 데이터 세트에서 강력한 비교 시스템을 능가한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "270",
            "abstractID": "EMNLP_abs-270",
            "text": [
                {
                    "index": "270-0",
                    "sentence": "Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem.",
                    "sentence_kor": "추상적인 문서 요약은 일반적으로 시퀀스 투 시퀀스(SEQ2SEQ) 학습 문제로 모델링됩니다.",
                    "tag": "1"
                },
                {
                    "index": "270-1",
                    "sentence": "Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging.",
                    "sentence_kor": "불행히도 대규모 SEQ2 교육제한된 감독 요약 데이터를 기반으로 한 SEQ 기반 요약 모델은 어려운 과제입니다.",
                    "tag": "1"
                },
                {
                    "index": "270-2",
                    "sentence": "This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text.",
                    "sentence_kor": "본 논문은 SEQ2를 사전 교육할 수 있는 세 가지 시퀀스 대 시퀀스 사전 교육(속기, STEP) 목표를 제시한다.레이블이 지정되지 않은 텍스트에 대한 SEQ 기반 추상 요약 모델.",
                    "tag": "2"
                },
                {
                    "index": "270-3",
                    "sentence": "The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document.",
                    "sentence_kor": "주요 아이디어는 문서에서 인위적으로 구성된 입력 텍스트가 주어지면 모델이 원래 문서를 복원하도록 사전 훈련된다는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "270-4",
                    "sentence": "These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task.",
                    "sentence_kor": "이러한 목표에는 추상적인 문서 요약 작업과 밀접한 관계가 있는 문장 재정렬, 다음 문장 생성 및 마스킹된 문서 생성이 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "270-5",
                    "sentence": "Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines.",
                    "sentence_kor": "두 가지 벤치마크 요약 데이터 세트(예: CNN/DailyMail 및 뉴욕 타임즈)에 대한 실험을 통해 세 가지 목표 모두 기준선에 따라 성능을 향상시킬 수 있음을 알 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "270-6",
                    "sentence": "Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.",
                    "sentence_kor": "대규모 데이터(160GB보다 큰)에 대해 사전 교육된 모델과 비교하여 사전 교육용 텍스트는 19GB에 불과하며, 이는 그 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "271",
            "abstractID": "EMNLP_abs-271",
            "text": [
                {
                    "index": "271-0",
                    "sentence": "Neural models have achieved remarkable success on relation extraction (RE) benchmarks.",
                    "sentence_kor": "신경 모델은 관계 추출(RE) 벤치마크에서 주목할 만한 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "271-1",
                    "sentence": "However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models.",
                    "sentence_kor": "그러나 텍스트의 어떤 정보가 의사결정을 위해 기존 RE 모델에 영향을 미치는지와 이러한 모델의 성능을 추가로 개선하는 방법에 대해서는 명확하게 이해하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "271-2",
                    "sentence": "To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names).",
                    "sentence_kor": "이를 위해 텍스트의 두 가지 주요 정보 출처, 즉 텍스트 컨텍스트와 엔티티 멘션(이름)의 효과를 경험적으로 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "271-3",
                    "sentence": "We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks.",
                    "sentence_kor": "우리는 (i) 맥락이 예측을 뒷받침하는 주요 원천이지만, RE 모델도 대부분 유형 정보인 엔티티 언급의 정보에 크게 의존하며, (ii) 기존 데이터셋은 엔티티 언급을 통해 얕은 휴리스틱을 유출하여 RE 벤치마크의 고성능에 기여할 수 있다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "271-4",
                    "sentence": "Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions.",
                    "sentence_kor": "분석을 바탕으로, 우리는 RE가 텍스트 컨텍스트와 유형 정보 모두에 대해 더 깊이 이해하도록 하면서, 실체에 대한 암기 또는 언급의 표면적 단서 사용을 피하기 위해 실체 마스킹 대비 사전 훈련 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "271-5",
                    "sentence": "We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios.",
                    "sentence_kor": "우리는 우리의 관점을 지원하기 위해 광범위한 실험을 수행하고 우리의 프레임워크가 다른 RE 시나리오에서 신경 모델의 효과와 견고성을 개선할 수 있다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "271-6",
                    "sentence": "All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.",
                    "sentence_kor": "모든 코드와 데이터 세트는 https://github.com/thunlp/RE-Context-or-Names에서 공개됩니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "272",
            "abstractID": "EMNLP_abs-272",
            "text": [
                {
                    "index": "272-0",
                    "sentence": "Open relation extraction is the task of extracting open-domain relation facts from natural language sentences.",
                    "sentence_kor": "개방형 관계 추출은 자연어 문장에서 개방형 도메인 관계 사실을 추출하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "272-1",
                    "sentence": "Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power.",
                    "sentence_kor": "기존 연구는 휴리스틱 또는 원거리 감독 주석을 활용하여 사전 정의된 관계에 대해 감독 분류기를 훈련시키거나 차별력이 적은 추가 가정을 가진 감독되지 않은 방법을 채택한다.",
                    "tag": "1"
                },
                {
                    "index": "272-2",
                    "sentence": "In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification.",
                    "sentence_kor": "본 연구에서 우리는 SelfORE라는 자체 감독 프레임워크를 제안한다. SelfORE는 문맥화된 관계형 특징에서 적응형 클러스터링을 위해 사전 훈련된 대규모 언어 모델을 활용하여 약한 자체 감독 신호를 활용하고 관계 분류에서 상황별 특징을 개선하여 자체 감독 신호를 부트스트랩한다.",
                    "tag": "2+3"
                },
                {
                    "index": "272-3",
                    "sentence": "Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.",
                    "sentence_kor": "세 데이터 세트에 대한 실험 결과는 경쟁 기준선과 비교할 때 개방형 도메인 관계 추출에 대한 SelfORE의 효과와 견고성을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "273",
            "abstractID": "EMNLP_abs-273",
            "text": [
                {
                    "index": "273-0",
                    "sentence": "Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results.",
                    "sentence_kor": "원격 감독(DS)은 문장 수준 관계 추출(RE)을 위한 자동 레이블 데이터를 생성하기 위해 널리 채택되어 큰 결과를 얻었다.",
                    "tag": "1"
                },
                {
                    "index": "273-1",
                    "sentence": "However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE.",
                    "sentence_kor": "그러나 DS에 의해 야기되는 불가피한 노이즈가 문서에 곱해져 RE의 성능을 크게 해칠 수 있기 때문에 DS의 기존 성공은 더 까다로운 문서 수준 관계 추출(DocRE)로 직접 이전될 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "273-2",
                    "sentence": "To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks.",
                    "sentence_kor": "이 문제를 완화하기 위해 여러 사전 교육 작업을 통해 노이즈가 많은 DS 데이터를 강조하는 DocRE용 새로운 사전 교육 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "273-3",
                    "sentence": "The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.",
                    "sentence_kor": "대규모 DocRE 벤치마크에 대한 실험 결과는 우리 모델이 노이즈가 많은 데이터에서 유용한 정보를 캡처하고 유망한 결과를 얻을 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "274",
            "abstractID": "EMNLP_abs-274",
            "text": [
                {
                    "index": "274-0",
                    "sentence": "Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work.",
                    "sentence_kor": "세 가지 평가 설정을 구별하려는 노력에도 불구하고(Bekoulis 등, 2018), 수많은 엔드 투 엔드 관계 추출(RE) 기사는 이전 작업과 비교할 수 없는 성능을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "274-1",
                    "sentence": "In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation.",
                    "sentence_kor": "이 논문에서, 우리는 먼저 발표된 논문에서 잘못된 비교의 몇 가지 패턴을 식별하고 그 확산을 피하기 위해 그것들을 설명한다.",
                    "tag": "2"
                },
                {
                    "index": "274-2",
                    "sentence": "We then propose a small empirical study to quantify the most common mistake’s impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05.",
                    "sentence_kor": "그런 다음 가장 일반적인 실수의 영향을 정량화하고 이를 평가하여 ACE05에서 최종 RE 성능을 약 5% 과대평가하는 소규모 경험적 연구를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "274-3",
                    "sentence": "We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER.",
                    "sentence_kor": "우리는 또한 두 가지 최근 개발, 즉 언어 모델 사전 교육(특히 BERT)과 스팬 레벨 NER의 사용에 대한 미답사를 연구할 기회를 잡았다.",
                    "tag": "3"
                },
                {
                    "index": "274-4",
                    "sentence": "This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics.",
                    "sentence_kor": "이 메타 분석은 평가 설정과 데이터 세트 통계 모두에 대한 보고서에서 엄격함의 필요성을 강조한다.",
                    "tag": "3"
                },
                {
                    "index": "274-5",
                    "sentence": "We finally call for unifying the evaluation setting in end-to-end RE.",
                    "sentence_kor": "마지막으로 우리는 엔드 투 엔드 RE에서 평가 설정의 통합을 요구한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "275",
            "abstractID": "EMNLP_abs-275",
            "text": [
                {
                    "index": "275-0",
                    "sentence": "The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior.",
                    "sentence_kor": "훈련 데이터를 수집하고 주석을 다는 프로세스는 올바른 일반화 동작을 학습하는 모델의 능력을 제한할 수 있는 분포 아티팩트를 도입할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "275-1",
                    "sentence": "We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process.",
                    "sentence_kor": "우리는 TACRED에 대해 훈련된 SOTA 관계 추출(RE) 모델의 고장 모드를 식별하는데, 이는 데이터 주석 프로세스의 한계 때문이라고 본다.",
                    "tag": "1"
                },
                {
                    "index": "275-2",
                    "sentence": "We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior.",
                    "sentence_kor": "우리는 이 동작을 벤치마킹하기 위해 자연적으로 발생하는 말뭉치의 예를 기반으로 도전 RE(CRE)라고 하는 도전 세트를 수집하고 주석을 달았다.",
                    "tag": "1"
                },
                {
                    "index": "275-3",
                    "sentence": "Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data.",
                    "sentence_kor": "4개의 최첨단 RE 모델을 사용한 우리의 실험은 그들이 실제로 도전 집합 데이터에 일반화되지 않는 얕은 휴리스틱을 채택했음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "275-4",
                    "sentence": "Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance.",
                    "sentence_kor": "또한 대체 질문 답변 모델링은 전반적인 TACRED 성능이 떨어졌음에도 불구하고 과제 세트의 SOTA 모델보다 훨씬 더 우수하다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "275-5",
                    "sentence": "By adding some of the challenge data as training examples, the performance of the model improves.",
                    "sentence_kor": "일부 과제 데이터를 교육 예제로 추가함으로써 모델의 성능이 향상됩니다.",
                    "tag": "1"
                },
                {
                    "index": "275-6",
                    "sentence": "Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.",
                    "sentence_kor": "마지막으로 이러한 동작을 완화하기 위해 RE 데이터 수집을 개선하는 방법에 대한 구체적인 제안을 제공한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "276",
            "abstractID": "EMNLP_abs-276",
            "text": [
                {
                    "index": "276-0",
                    "sentence": "Relation extraction (RE) aims to identify the semantic relations between named entities in text.",
                    "sentence_kor": "관계 추출(RE)은 텍스트에서 명명된 개체 간의 의미 관계를 식별하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "276-1",
                    "sentence": "Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document.",
                    "sentence_kor": "최근 몇 년 동안 문서 수준으로 상향 조정되었으며, 따라서 전체 문서에 걸쳐 실체와 언급이 복잡한 추론을 필요로 한다.",
                    "tag": "2"
                },
                {
                    "index": "276-2",
                    "sentence": "In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations.",
                    "sentence_kor": "본 논문에서 우리는 개체 전역 및 로컬 표현과 컨텍스트 관계 표현 측면에서 문서 정보를 인코딩하여 문서 수준 RE에 대한 새로운 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "276-3",
                    "sentence": "Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations.",
                    "sentence_kor": "엔티티 전역 표현은 문서에 있는 모든 엔티티의 의미 정보를 모델링하고, 엔티티 로컬 표현은 특정 엔티티에 대한 여러 언급의 상황별 정보를 집계하며, 상황 관계 표현은 다른 관계의 주제 정보를 인코딩한다.",
                    "tag": "3"
                },
                {
                    "index": "276-4",
                    "sentence": "Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE.",
                    "sentence_kor": "실험 결과는 우리 모델이 문서 수준 RE에 대한 두 개의 공개 데이터 세트에서 우수한 성능을 달성한다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "276-5",
                    "sentence": "It is particularly effective in extracting relations between entities of long distance and having multiple mentions.",
                    "sentence_kor": "특히 장거리 실체 간의 관계를 추출하고 여러 개의 언급을 하는 데 효과적이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "277",
            "abstractID": "EMNLP_abs-277",
            "text": [
                {
                    "index": "277-0",
                    "sentence": "The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task.",
                    "sentence_kor": "실체와 관계의 공동 추출을 다루기 위해 다중 작업 학습 접근법을 사용한다는 아이디어는 실체 인식 과제와 관계 분류 과제 사이의 관련성에 의해 동기가 부여된다.",
                    "tag": "1"
                },
                {
                    "index": "277-1",
                    "sentence": "Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction.",
                    "sentence_kor": "다중 작업 학습 기법을 사용하여 문제를 해결하는 기존 방법은 공유 네트워크를 통해 두 작업 간의 상호 작용을 학습하며, 여기서 공유 정보는 예측을 위해 작업별 네트워크로 전달된다.",
                    "tag": "1"
                },
                {
                    "index": "277-2",
                    "sentence": "However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks.",
                    "sentence_kor": "그러나 이러한 접근법은 모델이 개별 작업에 대한 성과를 개선하기 위해 두 작업 사이의 명시적 상호 작용을 학습하는 것을 방해한다.",
                    "tag": "1"
                },
                {
                    "index": "277-3",
                    "sentence": "As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification.",
                    "sentence_kor": "해결책으로, 우리는 상호작용을 동적으로 학습할 수 있고 분류를 위한 작업별 특징을 효과적으로 모델링할 수 있는 반복 상호 작용 네트워크라고 부르는 다중 작업 학습 모델을 설계한다.",
                    "tag": "1"
                },
                {
                    "index": "277-4",
                    "sentence": "Empirical studies on two real-world datasets confirm the superiority of the proposed model.",
                    "sentence_kor": "두 개의 실제 데이터 세트에 대한 경험적 연구를 통해 제안된 모델의 우수성을 확인할 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "278",
            "abstractID": "EMNLP_abs-278",
            "text": [
                {
                    "index": "278-0",
                    "sentence": "Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days.",
                    "sentence_kor": "관계적 사실(s,r,o)을 유효 기간(또는 시간 순간)과 연관시키는 시간적 지식 기반에 대한 연구는 초기 단계에 있다.",
                    "tag": "1"
                },
                {
                    "index": "278-1",
                    "sentence": "Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space.",
                    "sentence_kor": "우리의 연구는 누락된 엔티티(링크 예측)와 누락된 시간 간격(시간 예측)을 공동 시간 지식 기반 완료(TKBC) 작업으로 간주하고, 엔티티, 관계 및 시간이 모두 균일하고 호환 가능한 공간에 내장되는 새로운 TKBC 방법인 TIME플렉스를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "278-2",
                    "sentence": "TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks.",
                    "sentence_kor": "TIMEPLEX는 일부 사실/사건의 반복적 특성과 관계 쌍 간의 시간적 상호작용을 이용하여 두 예측 작업에서 모두 최첨단 결과를 산출한다.",
                    "tag": "3+4"
                },
                {
                    "index": "278-3",
                    "sentence": "We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms.",
                    "sentence_kor": "우리는 또한 기존 TKBC 모델이 불완전한 평가 메커니즘으로 인해 링크 예측 성능을 지나치게 과대평가한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "278-4",
                    "sentence": "In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.",
                    "sentence_kor": "이에 대해, 우리는 골드 인스턴스와 시스템 예측의 시간 간격의 부분적 중복으로 발생하는 미묘한 문제를 다루면서 링크 및 시간 예측 작업에 대한 개선된 TKBC 평가 프로토콜을 제안한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "279",
            "abstractID": "EMNLP_abs-279",
            "text": [
                {
                    "index": "279-0",
                    "sentence": "A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs.",
                    "sentence_kor": "최근의 최첨단 신경 개방 정보 추출(OpenIE) 시스템은 추출을 반복적으로 생성하므로 부분 출력을 반복적으로 인코딩해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "279-1",
                    "sentence": "This comes at a significant computational cost.",
                    "sentence_kor": "이 경우 상당한 계산 비용이 소요됩니다.",
                    "tag": "1"
                },
                {
                    "index": "279-2",
                    "sentence": "On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality.",
                    "sentence_kor": "한편, Open에 대한 시퀀스 라벨링 접근법IE는 훨씬 빠르지만 추출 품질은 더 나쁘다.",
                    "tag": "1"
                },
                {
                    "index": "279-3",
                    "sentence": "In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster.",
                    "sentence_kor": "본 논문에서, 우리는 개방에 대한 새로운 최첨단 기술을 확립하는 반복적인 라벨 기반 시스템을 제시함으로써 이러한 절충을 메운다.즉, 10배 더 빨리 추출합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "279-4",
                    "sentence": "This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task.",
                    "sentence_kor": "이것은 개방을 다루는 새로운 반복 그리드 라벨링(IGL) 아키텍처를 통해 달성된다.IE는 2-D 그리드 라벨링 작업이다.",
                    "tag": "3"
                },
                {
                    "index": "279-5",
                    "sentence": "We improve its performance further by applying coverage (soft) constraints on the grid at training time.",
                    "sentence_kor": "교육 시 그리드에 커버리지(소프트) 제약 조건을 적용하여 성능을 더욱 향상시킨다.",
                    "tag": "3"
                },
                {
                    "index": "279-6",
                    "sentence": "Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture.",
                    "sentence_kor": "또한, 최고의 오픈을 관찰할 때IE 시스템이 조정 구조 처리 시 흔들림, 우리의 오픈IE 시스템은 또한 동일한 인터넷 거버넌스 아키텍처로 구축된 새로운 조정 분석기를 통합한다.",
                    "tag": "5"
                },
                {
                    "index": "279-7",
                    "sentence": "This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers.",
                    "sentence_kor": "이 IGL 기반 조정 분석기는 Open에 도움이 됩니다.IE 시스템은 복잡한 조정 구조를 처리하는 동시에 이전 분석기기에 비해 F1이 12.3ppt 향상되는 조정 분석 작업에 대한 새로운 기술 상태를 확립한다.",
                    "tag": "3"
                },
                {
                    "index": "279-8",
                    "sentence": "Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.",
                    "sentence_kor": "우리 오픈IE 시스템 - 열기IE6 - F1에서 이전 시스템을 4ppt까지 능가하는 동시에 훨씬 빠릅니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "280",
            "abstractID": "EMNLP_abs-280",
            "text": [
                {
                    "index": "280-0",
                    "sentence": "Detecting public sentiment drift is a challenging task due to sentiment change over time.",
                    "sentence_kor": "시간이 지남에 따라 민심의 변화를 감지하는 것은 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "280-1",
                    "sentence": "Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data.",
                    "sentence_kor": "기존 방법은 먼저 과거 데이터를 사용하여 분류 모델을 구축한 후 모델이 새 데이터에 대해 훨씬 더 나쁜 성능을 발휘하는 경우 드리프트를 탐지합니다.",
                    "tag": "1"
                },
                {
                    "index": "280-2",
                    "sentence": "In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.",
                    "sentence_kor": "본 논문에서, 우리는 더 나은 분포 표현을 학습하기 위해 새로운 계층적 변동 자동 인코더(HVAE) 모델을 제안하고 과거 데이터와 새로운 데이터 사이의 분포 변화를 직접 평가하기 위한 새로운 드리프트 측정을 설계함으로써 분포 학습에 초점을 맞춘다.우리의 실험 결과는 제안된 모델이 기존의 세 가지 최첨단 방법보다 더 나은 결과를 달성한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "281",
            "abstractID": "EMNLP_abs-281",
            "text": [
                {
                    "index": "281-0",
                    "sentence": "Solving algebraic word problems has recently emerged as an important natural language processing task.",
                    "sentence_kor": "대수적 단어 문제 해결이 최근 중요한 자연어 처리 과제로 떠올랐다.",
                    "tag": "1"
                },
                {
                    "index": "281-1",
                    "sentence": "To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output.",
                    "sentence_kor": "대수적 단어 문제를 풀기 위해, 최근의 연구들은 'Op (operator/operand)' 토큰을 입출력 단위로 사용하여 해방정식을 생성하는 신경 모델을 제안했다.",
                    "tag": "1"
                },
                {
                    "index": "281-2",
                    "sentence": "However, such a neural model suffered two issues: expression fragmentation and operand-context separation.",
                    "sentence_kor": "그러나 그러한 신경 모델은 표현 단편화와 피연산자-상황 분리라는 두 가지 문제를 겪었다.",
                    "tag": "1"
                },
                {
                    "index": "281-3",
                    "sentence": "To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations.",
                    "sentence_kor": "이 두 가지 문제를 각각 해결하기 위해, 우리는 (1) '표현식' 토큰과 (2) 연산자-컨텍스트 포인터를 사용하는 순수 신경 모델인 EPT를 제안한다. Pointer Transformer)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "281-4",
                    "sentence": "The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS.",
                    "sentence_kor": "EPT 모델의 성능은 ALG514, DRAW-1K 및 MAWPS의 세 가지 데이터 세트에서 테스트된다.",
                    "tag": "3"
                },
                {
                    "index": "281-5",
                    "sentence": "Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS.",
                    "sentence_kor": "EPT 모델은 최첨단(SoTA) 모델과 비교하여 ALG514의 81.3%, DRAW-1K의 59.5%, MAWPS의 84.5% 등 세 데이터 세트 각각에서 비슷한 성능 정확도를 달성했다.",
                    "tag": "4"
                },
                {
                    "index": "281-6",
                    "sentence": "The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation.",
                    "sentence_kor": "본 논문의 기여는 두 가지로, (1) 표현 단편화와 피연산자-컨텍스트 분리를 다룰 수 있는 순수 신경 모델 EPT를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "281-7",
                    "sentence": "(2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.",
                    "sentence_kor": "(2) 수공예 기능을 사용하지 않는 완전 자동 EPT 모델은 수공예 기능을 사용하는 기존 모델과 유사한 성능을 제공하며 기존 순수 신경 모델보다 최대 40% 더 나은 성능을 달성한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "282",
            "abstractID": "EMNLP_abs-282",
            "text": [
                {
                    "index": "282-0",
                    "sentence": "A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs.",
                    "sentence_kor": "실제 자동 텍스트 수학 단어 문제(MWP) 해결사는 다양한 텍스트 MWP를 해결할 수 있어야 하지만 대부분의 기존 작업은 알려지지 않은 선형 MWP에만 초점을 맞추어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "282-1",
                    "sentence": "Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly.",
                    "sentence_kor": "여기서는 다양한 MWP의 방정식을 균일하게 표현하기 위한 첫 번째 시도를 하는 UET(Universal Expression Tree)라는 간단하지만 효율적인 방법을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "282-2",
                    "sentence": "Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation.",
                    "sentence_kor": "그런 다음 인코더-디코더 프레임워크를 기반으로 의미론적으로 정렬된 범용 트리 구조 솔버(SAU-Solver)를 제안하여 통합 모델에서 여러 유형의 MWP를 해결하여 UET 표현의 이점을 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "282-3",
                    "sentence": "Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols’ semantic meanings like human solving MWPs.",
                    "sentence_kor": "우리의 SAU-Solver는 인간 해결 MWP와 같이 생성된 기호의 의미에 따라 생성할 기호를 결정함으로써 보편적 표현 트리를 명시적으로 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "282-4",
                    "sentence": "Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information.",
                    "sentence_kor": "또한 SAU-Solver에는 상황별 정보와 일치하여 생성된 표현 트리의 의미적 제약과 합리성을 더욱 강화하기 위해 의미적으로 정렬된 새로운 하위 수준 정규화가 포함되어 있다.",
                    "tag": "1"
                },
                {
                    "index": "282-5",
                    "sentence": "Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs.",
                    "sentence_kor": "마지막으로, 해결사의 보편성을 검증하고 MWP의 연구 경계를 확장하기 위해 세 가지 유형의 MWP로 구성된 새로운 도전적인 HMWP 데이터 세트를 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "282-6",
                    "sentence": "Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.",
                    "sentence_kor": "여러 MWP 데이터 세트에 대한 실험 결과는 우리 모델이 보편적인 유형의 MWP를 해결할 수 있고 여러 최첨단 모델을 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "283",
            "abstractID": "EMNLP_abs-283",
            "text": [
                {
                    "index": "283-0",
                    "sentence": "Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community.",
                    "sentence_kor": "메시지 전달을 통해 그래프 노드 간의 관계를 포착하는 GNN(Graph Neural Networks)은 자연어 처리 커뮤니티에서 뜨거운 연구 방향이었다.",
                    "tag": "1"
                },
                {
                    "index": "283-1",
                    "sentence": "In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph.",
                    "sentence_kor": "본 논문에서는 말뭉치를 문서 관계 그래프로 나타내는 GNN 기반 신경 주제 모델인 GTM(Graph Topic Model)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "283-2",
                    "sentence": "Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences.",
                    "sentence_kor": "말뭉치의 문서와 단어는 그래프의 노드가 되며 문서와 단어의 동시 발생을 기반으로 연결된다.",
                    "tag": "3"
                },
                {
                    "index": "283-3",
                    "sentence": "By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution.",
                    "sentence_kor": "그래프 구조를 도입함으로써 문서 간의 관계가 공유 단어를 통해 설정되고 따라서 문서의 주제 표현은 그래프 컨볼루션(convolution)을 사용하여 인접 노드의 정보를 취합하여 강화된다.",
                    "tag": "3"
                },
                {
                    "index": "283-4",
                    "sentence": "Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.",
                    "sentence_kor": "세 개의 데이터 세트에 대한 광범위한 실험이 수행되었고 그 결과는 제안된 접근 방식의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "284",
            "abstractID": "EMNLP_abs-284",
            "text": [
                {
                    "index": "284-0",
                    "sentence": "One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients.",
                    "sentence_kor": "레시피 생성에서 가장 어려운 부분 중 하나는 입력 성분 간의 복잡한 제한을 처리하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "284-1",
                    "sentence": "Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible.",
                    "sentence_kor": "이전의 연구는 입력 정보를 독립적으로 처리하고 가능한 많은 정보를 포함하는 조리법을 생성하여 문제를 단순화합니다.",
                    "tag": "1"
                },
                {
                    "index": "284-2",
                    "sentence": "In this work, we propose a routing method to dive into the content selection under the internal restrictions.",
                    "sentence_kor": "본 연구에서는 내부 제한에 따라 콘텐츠 선택에 뛰어들 수 있는 라우팅 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "284-3",
                    "sentence": "The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences.",
                    "sentence_kor": "라우팅 강제 생성 모델(RGM)은 지정된 성분 및 사용자 기본 설정에 따라 적절한 조리법을 생성할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "284-4",
                    "sentence": "Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.",
                    "sentence_kor": "우리 모델은 BLEU, F1 및 인간 평가에서 크게 개선된 레시피 생성 작업에 대한 새로운 최첨단 결과를 산출한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "285",
            "abstractID": "EMNLP_abs-285",
            "text": [
                {
                    "index": "285-0",
                    "sentence": "Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ.",
                    "sentence_kor": "제2외국어로서의 많은 영어 학습자들은 거의 동음이의어에 가까운 단어(예: 작은 단어 대 작은 단어, 짧은 단어 대 짧은 단어)를 정확하게 사용하는 데 어려움을 겪으며, 종종 두 개의 거의 동의어에 가까운 용어가 어떻게 다른지 배우기 위해 예문을 찾는다.",
                    "tag": "1"
                },
                {
                    "index": "285-1",
                    "sentence": "Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways.",
                    "sentence_kor": "이전 연구에서는 문장을 추천하기 위해 수작업 점수를 사용했지만 거의 동의어에 가까운 모든 점수는 다양한 방식으로 차이가 나기 때문에 채택하기가 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "285-2",
                    "sentence": "We notice that the helpfulness of the learning material would reflect on the learners’ performance.",
                    "sentence_kor": "학습 자료의 유용성이 학습자의 성과에 반영된다는 것을 알 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "285-3",
                    "sentence": "Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent’s performance.",
                    "sentence_kor": "따라서, 우리는 학습자 행동을 모방하고 에이전트의 성과를 검사하여 좋은 학습 자료를 식별하기 위해 추론 기반 학습자 유사 에이전트를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "285-4",
                    "sentence": "To enable the agent to behave like a learner, we leverage entailment modeling’s capability of inferring answers from the provided materials.",
                    "sentence_kor": "에이전트가 학습자처럼 행동할 수 있도록 제공된 자료에서 답변을 추론하는 수반 모델링 기능을 활용합니다.",
                    "tag": "3"
                },
                {
                    "index": "285-5",
                    "sentence": "Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks.",
                    "sentence_kor": "실험 결과에 따르면 제안된 에이전트는 빈칸 채우기(FITB)와 좋은 예제 문장 선택 과제 모두에서 최상의 성능을 달성하기 위해 좋은 학습자 같은 행동을 갖추고 있다.",
                    "tag": "4"
                },
                {
                    "index": "285-6",
                    "sentence": "We further conduct a classroom user study with college ESL learners.",
                    "sentence_kor": "또한 대학 ESL 학습자와 함께 강의실 사용자 스터디를 실시합니다.",
                    "tag": "4"
                },
                {
                    "index": "285-7",
                    "sentence": "The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently.",
                    "sentence_kor": "사용자 연구의 결과는 제안된 에이전트가 학생들이 더 쉽고 효율적으로 배우는 데 도움이 되는 예제 문장을 찾을 수 있다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "285-8",
                    "sentence": "Compared to other models, the proposed agent improves the score of more than 17% of students after learning.",
                    "sentence_kor": "다른 모델에 비해 제안된 에이전트는 학습 후 학생의 17% 이상의 점수를 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "286",
            "abstractID": "EMNLP_abs-286",
            "text": [
                {
                    "index": "286-0",
                    "sentence": "As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention.",
                    "sentence_kor": "전자상거래가 번성함에 따라 고품질 온라인 광고 카피라이팅이 점점 더 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "286-1",
                    "sentence": "Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic.",
                    "sentence_kor": "광고(AD) 게시물에는 단일 제품에 대한 광고 카피라이팅과 달리 고객의 요구 사항을 충족하는 매력적인 주제가 포함되어 있으며 해당 주제에 포함된 여러 제품에 대한 설명 카피라이팅이 포함되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "286-2",
                    "sentence": "A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products.",
                    "sentence_kor": "좋은 광고 게시물은 각 제품의 특성을 강조할 수 있으므로 고객이 후보 제품 중에서 좋은 선택을 할 수 있도록 도와줍니다.",
                    "tag": "1"
                },
                {
                    "index": "286-3",
                    "sentence": "Hence, multi-product AD post generation is meaningful and important.",
                    "sentence_kor": "따라서, 다중 제품 AD 사후 생성은 의미 있고 중요합니다.",
                    "tag": "1"
                },
                {
                    "index": "286-4",
                    "sentence": "We propose a novel end-to-end model named S-MG Net to generate the AD post.",
                    "sentence_kor": "AD 포스트를 생성하기 위해 S-MG Net이라는 새로운 엔드 투 엔드 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "286-5",
                    "sentence": "Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network).",
                    "sentence_kor": "이러한 도전적인 실제 문제를 목표로 AD 사후 생성 작업을 (1) SelectNet(선택 네트워크)을 통해 제품 세트를 선택하는 두 가지 하위 프로세스로 분할했다.",
                    "tag": "1"
                },
                {
                    "index": "286-6",
                    "sentence": "(2) generate a post including selected products via the MGenNet (Multi-Generator Network).",
                    "sentence_kor": "(2) MGenNet(Multi-Generator Network)을 통해 선택된 제품을 포함한 게시물을 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "286-7",
                    "sentence": "Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products.",
                    "sentence_kor": "구체적으로 SelectNet은 먼저 포스트 주제와 대표적인 제품을 출력하기 위한 제품 간의 관계를 포착합니다.",
                    "tag": "1"
                },
                {
                    "index": "286-8",
                    "sentence": "Then, MGenNet generates the description copywriting of each product.",
                    "sentence_kor": "그런 다음 MGenNet은 각 제품의 설명 카피라이팅을 생성합니다.",
                    "tag": "1"
                },
                {
                    "index": "286-9",
                    "sentence": "Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.",
                    "sentence_kor": "대규모 실제 AD 포스트 데이터 세트에서 수행된 실험은 제안된 모델이 자동 메트릭과 인간 평가 측면에서 모두 인상적인 성능을 달성한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "287",
            "abstractID": "EMNLP_abs-287",
            "text": [
                {
                    "index": "287-0",
                    "sentence": "Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks.",
                    "sentence_kor": "문서 구조 추출은 완전 컨볼루션 네트워크를 사용하는 문서 이미지에 대한 의미론적 분할 작업으로 수행하는 최근 연구로 수십 년 동안 널리 연구되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "287-1",
                    "sentence": "Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms.",
                    "sentence_kor": "이러한 방법은 일반적으로 형태로 나타나는 밀집 영역의 구조를 명확히 하지 못하기 때문에 이미지 해상도에 의해 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "287-2",
                    "sentence": "To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures.",
                    "sentence_kor": "이를 완화하기 위해, 우리는 구조의 상대적 공간 배열을 활용하는 형태에 특정한 초점을 두고 텍스트를 사용한 구조 추출을 위한 새로운 시퀀스 투 시퀀스(Seq2Seq) 영감을 받은 프레임워크인 Form2Seq를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "287-3",
                    "sentence": "We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms.",
                    "sentence_kor": "우리는 두 가지 작업, 즉 1) 하위 수준 구성 요소(텍스트 블록 및 빈 채우기 가능 위젯)를 필드 캡션, 목록 항목 및 기타와 같은 10가지 유형으로 분류하고, 2) 하위 수준 요소를 텍스트 필드, ChoiceFields 및 ChoiceGroups와 같은 고차 구조로 그룹화하는 작업에 대해 논의한다.",
                    "tag": "3"
                },
                {
                    "index": "287-4",
                    "sentence": "To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task.",
                    "sentence_kor": "이를 달성하기 위해 구성 요소를 자연스러운 읽기 순서로 선형으로 배열하고, 구성 요소의 공간 및 텍스트 표현을 Seq2Seq 프레임워크에 공급하며, 최종 작업에 따라 각 요소의 예측을 순차적으로 출력한다.",
                    "tag": "3"
                },
                {
                    "index": "287-5",
                    "sentence": "We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation.",
                    "sentence_kor": "작업을 그룹화하기 위해 Seq2Seq를 수정하고 두 작업의 단계적 엔드 투 엔드 훈련을 통해 얻은 개선 사항과 별도로 교육을 논의한다.",
                    "tag": "3"
                },
                {
                    "index": "287-6",
                    "sentence": "Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines.",
                    "sentence_kor": "실험 결과는 분류 작업에서 90%의 정확도를 달성하고 위에서 논의한 그룹에서 각각 75.82, 86.01, 61.63의 F1을 달성하여 분할 기준을 능가하는 텍스트 기반 접근법의 효과를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "287-7",
                    "sentence": "Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.",
                    "sentence_kor": "또한 프레임워크가 ICDAR 2013 데이터 세트에서 테이블 구조 인식에 대한 결과 상태를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "288",
            "abstractID": "EMNLP_abs-288",
            "text": [
                {
                    "index": "288-0",
                    "sentence": "Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent.",
                    "sentence_kor": "많은 자연어 처리 작업과 마찬가지로 태국어 단어 세분화는 도메인에 따라 달라진다.",
                    "tag": "1"
                },
                {
                    "index": "288-1",
                    "sentence": "Researchers have been relying on transfer learning to adapt an existing model to a new domain.",
                    "sentence_kor": "연구자들은 기존 모델을 새로운 영역에 적응시키기 위해 전송 학습에 의존해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "288-2",
                    "sentence": "However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as “black boxes”.",
                    "sentence_kor": "그러나 이 접근법은 \"블랙박스\"라고도 알려진 모델의 입력 및 출력 계층과만 상호작용할 수 있는 경우에는 적용되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "288-3",
                    "sentence": "We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation.",
                    "sentence_kor": "우리는 이러한 블랙박스 한계를 해결하기 위해 스택형 앙상블 학습 패러다임에 기초한 필터 및 정제 솔루션을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "288-4",
                    "sentence": "We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning.",
                    "sentence_kor": "우리는 우리의 방법을 최첨단 모델 및 전이 학습과 비교한 광범위한 실험 연구를 수행했다.",
                    "tag": "3"
                },
                {
                    "index": "288-5",
                    "sentence": "Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.",
                    "sentence_kor": "실험 결과는 제안된 솔루션이 효과적인 도메인 적응 방법이며 전송 학습 방법과 유사한 성능을 가지고 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "289",
            "abstractID": "EMNLP_abs-289",
            "text": [
                {
                    "index": "289-0",
                    "sentence": "Can pretrained language models (PLMs) generate derivationally complex words?",
                    "sentence_kor": "사전 훈련된 언어 모델(PLM)이 파생적으로 복잡한 단어를 생성할 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "289-1",
                    "sentence": "We present the first study investigating this question, taking BERT as the example PLM.",
                    "sentence_kor": "우리는 BERT를 PLM의 예로 들면서 이 질문을 조사하는 첫 번째 연구를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "289-2",
                    "sentence": "We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning.",
                    "sentence_kor": "수정되지 않은 사전 훈련된 모델 사용부터 완전한 미세 조정에 이르기까지 다양한 설정에서 BERT의 파생 기능을 검토한다.",
                    "tag": "1"
                },
                {
                    "index": "289-3",
                    "sentence": "Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG).",
                    "sentence_kor": "우리의 최고 모델인 DagoBERT(파생 및 생성적으로 최적화된 BERT)는 파생 생성(DG)에서 이전 최첨단 기술을 확실히 능가한다.",
                    "tag": "1"
                },
                {
                    "index": "289-4",
                    "sentence": "Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.",
                    "sentence_kor": "또한, 우리의 실험은 입력 분할이 BERT의 파생 지식에 결정적으로 영향을 미친다는 것을 보여주며, 형태학적으로 정보에 입각한 단위 어휘를 사용할 경우 PLM의 성능이 더욱 향상될 수 있음을 시사한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "290",
            "abstractID": "EMNLP_abs-290",
            "text": [
                {
                    "index": "290-0",
                    "sentence": "Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model.",
                    "sentence_kor": "그리디 디코딩 알고리즘을 채택하여 이 작업은 중국어 단어 분할(CWS) 모델 자체를 더욱 강화하여 훨씬 더 빠르고 정확한 CWS 모델을 만드는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "290-1",
                    "sentence": "Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.",
                    "sentence_kor": "우리 모델은 그리디 분할을 위한 주의력만 쌓인 인코더와 충분히 가벼운 디코더와 부드러운 훈련을 위한 두 개의 고속도로 연결로 구성되어 있으며, 인코더는 새롭게 제안된 트랜스포머 변형, 가우스 마스크 방향(GD) 트랜스포머 및 바이아핀 주의력 득점자로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "290-2",
                    "sentence": "With the effective encoder design, our model only needs to take unigram features for scoring.",
                    "sentence_kor": "효과적인 인코더 설계로 우리 모델은 점수를 매기기 위해 유니그램 기능만 취하면 된다.",
                    "tag": "1"
                },
                {
                    "index": "290-3",
                    "sentence": "Our model is evaluated on SIGHAN Bakeoff benchmark datasets.",
                    "sentence_kor": "우리 모델은 SIGHAN Bakoff 벤치마크 데이터 세트에서 평가된다.",
                    "tag": "1"
                },
                {
                    "index": "290-4",
                    "sentence": "The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.",
                    "sentence_kor": "실험 결과는 제안된 모델이 가장 높은 분할 속도로 엄격한 폐쇄 테스트 설정 측면에서 강력한 기준선에 대해 새로운 최첨단 또는 유사한 성능을 달성한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "291",
            "abstractID": "EMNLP_abs-291",
            "text": [
                {
                    "index": "291-0",
                    "sentence": "Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity.",
                    "sentence_kor": "단어 수준 정보는 언어적 복잡성이 높기 때문에 특히 중국어의 경우 자연어 처리(NLP)에서 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "291-1",
                    "sentence": "Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks.",
                    "sentence_kor": "중국어 단어 분할(CWS)은 중국어 다운스트림 NLP 작업에 필수적인 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "291-2",
                    "sentence": "Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora.",
                    "sentence_kor": "기존 방법은 이미 대규모 주석이 달린 말뭉치에서 CWS의 경쟁력 있는 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "291-3",
                    "sentence": "However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words.",
                    "sentence_kor": "그러나 어휘 이탈(OOV) 단어가 많은 세그먼트화되지 않은 텍스트를 처리할 경우 방법의 정확도가 크게 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "291-4",
                    "sentence": "In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks.",
                    "sentence_kor": "또한 다운스트림 NLP 작업의 다양한 요구사항을 해결하기 위한 다양한 세분화 기준이 있다.",
                    "tag": "1"
                },
                {
                    "index": "291-5",
                    "sentence": "Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters.",
                    "sentence_kor": "서로 다른 기준을 저장하는 모델이 너무 많으면 전체 파라미터가 폭발적으로 증가합니다.",
                    "tag": "1"
                },
                {
                    "index": "291-6",
                    "sentence": "To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model.",
                    "sentence_kor": "이를 위해, 우리는 서로 다른 세분화 기준을 하나의 모델에 통합하기 위해 모든 매개변수를 공유하는 공동 다중 기준 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "291-7",
                    "sentence": "Besides, we utilize a transfer learning method to improve the performance of OOV words.",
                    "sentence_kor": "또한 OOV 단어의 성능을 향상시키기 위해 전이 학습 방법을 활용한다.",
                    "tag": "1"
                },
                {
                    "index": "291-8",
                    "sentence": "Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010).",
                    "sentence_kor": "우리가 제안한 방법은 여러 벤치마크 데이터 세트(예: Bakoff 2005, Bakoff 2008 및 SIGAN 2010)에 대한 포괄적인 실험을 설계하여 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "291-9",
                    "sentence": "Our method achieves the state-of-the-art performances on all datasets.",
                    "sentence_kor": "우리의 방법은 모든 데이터 세트에서 최첨단 성능을 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "291-10",
                    "sentence": "Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.",
                    "sentence_kor": "중요한 것은, 우리의 방법이 CWS 작업에 대한 경쟁력 있는 실행 가능성과 일반화 능력도 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "292",
            "abstractID": "EMNLP_abs-292",
            "text": [
                {
                    "index": "292-0",
                    "sentence": "Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language.",
                    "sentence_kor": "언어 간 의미 역할 라벨링(SRL)은 소스 언어의 자원을 활용하여 새로운 대상 언어에 대한 주석 또는 모델을 구성하는 데 필요한 노력을 최소화하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "292-1",
                    "sentence": "Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers.",
                    "sentence_kor": "최근의 접근 방식은 워드 얼라인먼트, 기계 번역 엔진 또는 파서나 태그거와 같은 사전 처리 도구에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "292-2",
                    "sentence": "We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus.",
                    "sentence_kor": "소스 언어의 주석과 병렬 말뭉치 형태의 원시 텍스트에 대한 액세스만 필요한 교차 언어 SRL 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "292-3",
                    "sentence": "The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings.",
                    "sentence_kor": "우리 모델의 백본은 의미 역할 압축기 및 다국어 단어 임베딩과 공동으로 훈련된 LSTM 기반 의미 역할 라벨러이다.",
                    "tag": "1"
                },
                {
                    "index": "292-4",
                    "sentence": "The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence.",
                    "sentence_kor": "압축기는 시맨틱 역할 라벨러의 출력에서 유용한 정보를 수집하여 노이즈가 많고 상충되는 증거를 필터링합니다.",
                    "tag": "1"
                },
                {
                    "index": "292-5",
                    "sentence": "It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language.",
                    "sentence_kor": "다국어 임베딩 공간에 살고 대상 언어의 의미적 역할을 예측하기 위한 직접적인 감독을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "292-6",
                    "sentence": "Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.",
                    "sentence_kor": "범용 제안 은행 및 수동으로 주석을 단 데이터 세트에 대한 결과는 우리의 방법이 감독 기능을 사용하는 시스템에 대해서도 매우 효과적이라는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "293",
            "abstractID": "EMNLP_abs-293",
            "text": [
                {
                    "index": "293-0",
                    "sentence": "We study the detection of propagandistic text fragments in news articles.",
                    "sentence_kor": "우리는 뉴스 기사에서 선전적인 텍스트 파편의 탐지를 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "293-1",
                    "sentence": "Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques.",
                    "sentence_kor": "훈련 데이터의 입력-출력 데이터 포인트에서 학습하는 대신, 세분화된 선전 기법에 대한 선언적 지식을 주입하는 접근방식을 도입한다.",
                    "tag": "1"
                },
                {
                    "index": "293-2",
                    "sentence": "Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language.",
                    "sentence_kor": "특히, 우리는 1차 논리와 자연어로 표현된 선언적 지식을 활용한다.",
                    "tag": "1"
                },
                {
                    "index": "293-3",
                    "sentence": "The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions.",
                    "sentence_kor": "전자는 명제 부울 식으로 훈련 과정을 정규화하는 데 사용되는 거친 예측과 세밀한 예측 사이의 논리적 일관성을 의미한다.",
                    "tag": "1"
                },
                {
                    "index": "293-4",
                    "sentence": "The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters.",
                    "sentence_kor": "후자는 모델 매개 변수를 정규화하기 위한 클래스 표현을 얻는 데 사용되는 각 선전 기법의 문자적 정의를 나타냅니다.",
                    "tag": "1"
                },
                {
                    "index": "293-5",
                    "sentence": "We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection.",
                    "sentence_kor": "우리는 세밀한 선전 탐지를 위해 수동으로 주석을 단 대규모 데이터 세트인 선전 기술 코퍼스에 대한 실험을 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "293-6",
                    "sentence": "Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.",
                    "sentence_kor": "실험에 따르면 우리 방법은 선언적 지식을 활용하여 모델이 보다 정확한 예측을 하는 데 도움이 될 수 있음을 입증하는 우수한 성능을 달성한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "294",
            "abstractID": "EMNLP_abs-294",
            "text": [
                {
                    "index": "294-0",
                    "sentence": "Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available.",
                    "sentence_kor": "SRL이 여러 언어에 대해 연구되고 있지만, 주요 개선 사항은 대부분 영어에 대한 것으로, 더 많은 자원을 이용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "294-1",
                    "sentence": "In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning.",
                    "sentence_kor": "실제로 기존의 다국어 SRL 데이터 세트는 상이한 주석 스타일을 포함하거나 다른 도메인에서 제공되어 다국어 학습의 일반화를 방해한다.",
                    "tag": "1"
                },
                {
                    "index": "294-2",
                    "sentence": "In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages.",
                    "sentence_kor": "본 연구에서는 4개 언어로 병렬인 SRL 말뭉치를 자동으로 구성하는 방법을 제안한다. 영어, 프랑스어, 독일어, 스페인어. 언어 간에 완전히 비교할 수 있는 통일된 술어 및 역할 주석을 사용합니다.",
                    "tag": "2"
                },
                {
                    "index": "294-3",
                    "sentence": "We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages.",
                    "sentence_kor": "우리는 영어 CoNLL-09 데이터 세트에 고품질 기계 변환을 적용하고 다국어 BERT를 사용하여 고품질 주석을 대상 언어에 투영한다.",
                    "tag": "3"
                },
                {
                    "index": "294-4",
                    "sentence": "We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline.",
                    "sentence_kor": "우리는 투영 품질을 측정하는 데 사용하는 인간 검증 테스트 세트를 포함하며, 투영이 강력한 기준선보다 밀도가 높고 정확하다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "294-5",
                    "sentence": "Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.",
                    "sentence_kor": "마지막으로, 우리는 다국어 SRL을 위한 새로운 말뭉치에서 다른 SOTA 모델을 훈련시켜 다국어 주석이 특히 약한 언어에 대한 성능을 향상시킨다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "295",
            "abstractID": "EMNLP_abs-295",
            "text": [
                {
                    "index": "295-0",
                    "sentence": "Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles.",
                    "sentence_kor": "SRL(Semantic Role Labeling)은 술어를 식별하고 의미적 역할을 가진 인수 범위를 식별하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "295-1",
                    "sentence": "Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax.",
                    "sentence_kor": "대부분의 의미론적 역할 형식주의가 구성 구문을 기반으로 구축되고 구문 구성 요소만 인수(예: FrameNet 및 PropBank)로 레이블이 지정될 수 있지만, 구문 인식 SRL에 대한 모든 최근 연구는 구문의 의존성 표현에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "295-2",
                    "sentence": "In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system.",
                    "sentence_kor": "반대로, 우리는 GCN(그래프 컨볼루션 네트워크)을 사용하여 구성 구조를 인코딩하고 SRL 시스템에 알리는 방법을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "295-3",
                    "sentence": "Nodes in our SpanGCN correspond to constituents.",
                    "sentence_kor": "SpanGCN의 노드는 구성 요소에 해당합니다.",
                    "tag": "1"
                },
                {
                    "index": "295-4",
                    "sentence": "The computation is done in 3 stages.",
                    "sentence_kor": "연산은 3단계로 이루어진다.",
                    "tag": "1"
                },
                {
                    "index": "295-5",
                    "sentence": "First, initial node representations are produced by ‘composing’ word representations of the first and last words in the constituent.",
                    "sentence_kor": "첫째, 초기 노드 표현은 구성 요소의 첫 번째 단어와 마지막 단어에 대한 단어 표현을 '합성'함으로써 생성된다.",
                    "tag": "1"
                },
                {
                    "index": "295-6",
                    "sentence": "Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations.",
                    "sentence_kor": "둘째, 구성 트리에 의존하는 그래프 컨볼루션이 수행되어 구문 정보에 입각한 구성 표현을 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "295-7",
                    "sentence": "Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier.",
                    "sentence_kor": "마지막으로 구성 표현은 SRL 분류기에 대한 입력으로 사용되는 단어 표현으로 다시 '분해'된다.",
                    "tag": "1"
                },
                {
                    "index": "295-8",
                    "sentence": "We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.",
                    "sentence_kor": "종속성 트리보다 GCN을 사용하는 모델을 포함한 대안과 비교하여 SpanGCN을 평가하고 표준 영어 SRL 벤치마크 CoNLL-2005, CoNLL-2012 및 FrameNet에 대한 효과를 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "296",
            "abstractID": "EMNLP_abs-296",
            "text": [
                {
                    "index": "296-0",
                    "sentence": "AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks.",
                    "sentence_kor": "AM 의존성 파싱은 여러 그래프 뱅크에서 높은 정확도로 신경 의미 파싱을 위한 언어학적으로 원칙적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "296-1",
                    "sentence": "It relies on a type system that models semantic valency but makes existing parsers slow.",
                    "sentence_kor": "그것은 시맨틱 밸런스를 모델링하지만 기존 파서를 느리게 만드는 유형 시스템에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "296-2",
                    "sentence": "We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.",
                    "sentence_kor": "정확성을 유지하거나 향상시키면서 올바른 유형을 보장하고 구문 분석 속도를 최대 3배까지 향상시키는 AM 종속성 구문 분석을 위한 전환 기반 구문 분석기에 대해 설명한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "297",
            "abstractID": "EMNLP_abs-297",
            "text": [
                {
                    "index": "297-0",
                    "sentence": "This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques.",
                    "sentence_kor": "이 논문은 클래스 레이블을 개별 기호 집합이 아니라 각 클래스에 연결된 단어 그래프를 일반적인 그래프 임베딩 기법을 사용하여 매핑하는 공간으로 표시함으로써 의도 분류를 개선할 수 있는 방법을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "297-1",
                    "sentence": "The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes.",
                    "sentence_kor": "역사전 작업에 사용된 이전 알고리즘에서 영감을 받은 이 접근법은 분류 알고리즘이 다른 클래스의 훈련 예제에서 일부 단어가 반복적으로 발생함으로써 제공되는 클래스 간 유사성을 고려할 수 있도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "297-2",
                    "sentence": "The classification is carried out by mapping text embeddings to the word graph embeddings of the classes.",
                    "sentence_kor": "분류는 텍스트 임베딩을 클래스의 워드 그래프 임베딩에 매핑하여 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "297-3",
                    "sentence": "Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved.",
                    "sentence_kor": "클래스 라벨 세트의 표현력 향상에만 초점을 맞추어, 민간 및 공공의도 분류 데이터 세트 모두에서 수행된 실험에서 OOS(Out of Scope)의 더 나은 검출이 달성되고 결과적으로 의도 분류의 전반적인 정확성도 향상된다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "297-4",
                    "sentence": "In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.",
                    "sentence_kor": "특히 최근 출시된 Larson 데이터 세트를 사용하면 이전의 최첨단 결과를 31% 포인트 이상 능가하는 약 9.9%의 오류가 OOOS 검출에 달성되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "298",
            "abstractID": "EMNLP_abs-298",
            "text": [
                {
                    "index": "298-0",
                    "sentence": "Language drift has been one of the major obstacles to train language models through interaction.",
                    "sentence_kor": "언어 표류는 상호작용을 통해 언어 모델을 훈련시키는 주요 장애물 중 하나였다.",
                    "tag": "1"
                },
                {
                    "index": "298-1",
                    "sentence": "When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language.",
                    "sentence_kor": "단어 기반의 대화 요원들이 임무를 완수하도록 훈련 받을 때, 그들은 자연어를 활용하기 보다는 그들의 언어를 발명하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "298-2",
                    "sentence": "In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).",
                    "sentence_kor": "최근 문헌에서, 두 가지 일반적인 방법은 부분적으로 이 현상에 대응한다. S2P(감독 셀프플레이) 및 SIL(시드 반복 학습)입니다.",
                    "tag": "1"
                },
                {
                    "index": "298-3",
                    "sentence": "While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring.",
                    "sentence_kor": "S2P가 드리프트에 대응하기 위해 상호 작용 및 감독 손실을 공동으로 훈련하는 동안, SIL은 언어 드리프트가 발생하는 것을 방지하기 위해 훈련 역학을 변경합니다.",
                    "tag": "1"
                },
                {
                    "index": "298-4",
                    "sentence": "In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus.",
                    "sentence_kor": "본 논문에서 우리는 먼저 이들의 각각의 약점, 즉 인간 말뭉치에서 평가할 때 후기 훈련이 붕괴되고 부정적인 가능성이 더 높다는 점을 강조한다.",
                    "tag": "1"
                },
                {
                    "index": "298-5",
                    "sentence": "Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses.",
                    "sentence_kor": "이러한 관찰을 고려하여, 우리는 각각의 약점을 최소화하기 위해 두 방법을 결합하기 위해 감독 시드 반복 학습(SSIL)을 도입한다.",
                    "tag": "1"
                },
                {
                    "index": "298-6",
                    "sentence": "We then show the effectiveness of in the language-drift translation game.",
                    "sentence_kor": "그런 다음 우리는 언어 이동 번역 게임에서 의 효과를 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "299",
            "abstractID": "EMNLP_abs-299",
            "text": [
                {
                    "index": "299-0",
                    "sentence": "The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots).",
                    "sentence_kor": "시간 효율적이고 신뢰할 수 있는 평가 방법의 부족은 전환 대화 시스템(챗 봇)의 개발을 방해하고 있다.",
                    "tag": "2"
                },
                {
                    "index": "299-1",
                    "sentence": "Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results.",
                    "sentence_kor": "인간이 채팅 봇과 대화해야 하는 평가는 시간과 비용이 많이 들고, 인간 심판에게 높은 인지적 요구를 하며, 낮은 품질의 결과를 산출하는 경향이 있다.",
                    "tag": "2"
                },
                {
                    "index": "299-2",
                    "sentence": "In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots.",
                    "sentence_kor": "본 연구에서는 인간-봇 대화를 봇 간의 대화로 대체하는 비용 효율적이고 강력한 평가 프레임워크인 Spot The Bot을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "299-3",
                    "sentence": "Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations).",
                    "sentence_kor": "그런 다음 인간 판단은 대화의 각 실체에 대해 그것이 인간이라고 생각하는지 아닌지에 대해서만 주석을 달 수 있다(이 대화에 인간이 참여한다고 가정한다.",
                    "tag": "2+3"
                },
                {
                    "index": "299-4",
                    "sentence": "These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans.",
                    "sentence_kor": "그런 다음 이러한 주석을 통해 인간의 대화 행동을 모방하는 능력에 관한 채팅 봇의 순위를 매길 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "299-5",
                    "sentence": "Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis.",
                    "sentence_kor": "모든 봇이 결국 그렇게 인식될 것으로 예상하기 때문에, 우리는 어떤 챗 봇이 인간과 유사한 상태를 가장 오래 유지할 수 있는지 측정하는 메트릭을 통합한다.생존 분석.",
                    "tag": "3"
                },
                {
                    "index": "299-6",
                    "sentence": "This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results.",
                    "sentence_kor": "이 메트릭은 봇의 성능을 특정 특성(예: 유창성 또는 민감성)과 상관시켜 해석 가능한 결과를 산출할 수 있는 능력을 가지고 있다.",
                    "tag": "4"
                },
                {
                    "index": "299-7",
                    "sentence": "The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle.",
                    "sentence_kor": "프레임 작업의 비교적 낮은 비용으로 평가 주기 동안 챗봇을 자주 평가할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "299-8",
                    "sentence": "We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work.",
                    "sentence_kor": "우리는 Spot The Bot을 세 개의 도메인에 적용하고, 몇 가지 최첨단 채팅 봇을 평가하고, 관련 작업을 비교함으로써 우리의 주장을 경험적으로 검증한다.",
                    "tag": "4"
                },
                {
                    "index": "299-9",
                    "sentence": "The framework is released asa ready-to-use tool.",
                    "sentence_kor": "프레임워크는 즉시 사용할 수 있는 툴로 출시된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "300",
            "abstractID": "EMNLP_abs-300",
            "text": [
                {
                    "index": "300-0",
                    "sentence": "How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors?",
                    "sentence_kor": "인간이 해로운 채팅 행동을 가르칠 위험 없이, 어떻게 우리는 인간의 피드백을 통해 더 나은 대화를 만들어 낼 수 있는 대화 모델을 훈련시킬 수 있을까?",
                    "tag": "1"
                },
                {
                    "index": "300-1",
                    "sentence": "We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL).",
                    "sentence_kor": "우리는 모델을 온라인으로 호스팅하는 것으로 시작하여 실시간 개방형 대화에서 사람의 피드백을 수집하며, 이를 통해 오프라인 강화 학습(RL)을 사용하여 모델을 교육하고 개선하는 데 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "300-2",
                    "sentence": "We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.",
                    "sentence_kor": "우리는 언어 유사성, 웃음 유도, 감정 등을 포함한 암묵적인 대화 단서를 식별하고 이를 여러 보상 기능에 포함시킨다.",
                    "tag": "1"
                },
                {
                    "index": "300-3",
                    "sentence": "A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward.",
                    "sentence_kor": "잘 알려진 과제는 오프라인 환경에서 RL 정책을 학습하는 것이 대개 탐색 능력 부족과 미래 보상의 지나치게 낙관적인 추정 경향으로 인해 실패한다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "300-4",
                    "sentence": "These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.",
                    "sentence_kor": "이러한 문제들은 언어 모델에 RL을 사용할 때 훨씬 더 어려워지는데, 언어 모델은 20,000개의 행동 어휘와 많은 가능한 보상 기능을 쉽게 가질 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "300-5",
                    "sentence": "We solve the challenge by developing a novel class of offline RL algorithms.",
                    "sentence_kor": "우리는 오프라인 RL 알고리즘의 새로운 클래스를 개발하여 과제를 해결한다.",
                    "tag": "1"
                },
                {
                    "index": "300-6",
                    "sentence": "These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.",
                    "sentence_kor": "이러한 알고리즘은 KL-컨트롤을 사용하여 사전 훈련된 사전 언어 모델과의 차이를 벌하고, 불확실성에 직면하여 알고리즘을 낙관적이 아닌 비관적으로 만드는 새로운 전략을 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "300-7",
                    "sentence": "We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches.",
                    "sentence_kor": "개방형 도메인 설정에서 80명의 사용자로부터 등급을 받아 결과 대화 상자 모델을 테스트한 결과 기존의 딥 오프라인 RL 접근법에 비해 크게 개선되었다.",
                    "tag": "1"
                },
                {
                    "index": "300-8",
                    "sentence": "The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.",
                    "sentence_kor": "새로운 오프라인 RL 방법은 인간 피드백의 정적 데이터 세트를 사용하여 기존의 생성 대화 상자 모델을 개선하는 데 사용할 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "301",
            "abstractID": "EMNLP_abs-301",
            "text": [
                {
                    "index": "301-0",
                    "sentence": "Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules.",
                    "sentence_kor": "언어 전반에 걸쳐, 명사(예: \"큰 빨간 개\")를 수정하는 여러 연속 형용사는 표시되지 않은 특정 순서 규칙을 따릅니다.",
                    "tag": "1"
                },
                {
                    "index": "301-1",
                    "sentence": "While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data.",
                    "sentence_kor": "설명 설명이 제시되었지만, 이 영역에서 수행된 연구의 대부분은 말뭉치 데이터보다는 원어민 화자의 직관적 판단에 주로 의존해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "301-2",
                    "sentence": "We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different.",
                    "sentence_kor": "우리는 훈련과 시험 언어가 다른 경우에도 24개 언어에 걸쳐 형용사를 정확하게 정렬할 수 있는 잠재 변수 모델의 형태로 다국어 형용사 순전히 말뭉치 중심의 첫 번째 모델을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "301-3",
                    "sentence": "We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.",
                    "sentence_kor": "우리는 보편적이고 언어 간, 계층적 형용사 순서 경향이 존재한다는 강력한 수렴 증거를 제공하기 위해 이 새로운 통계 모델을 활용한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "302",
            "abstractID": "EMNLP_abs-302",
            "text": [
                {
                    "index": "302-0",
                    "sentence": "Why do bilinguals switch languages within a sentence?",
                    "sentence_kor": "이중언어 사용자는 왜 문장 내에서 언어를 바꾸는가?",
                    "tag": "1"
                },
                {
                    "index": "302-1",
                    "sentence": "The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation.",
                    "sentence_kor": "현재의 관찰 연구는 2개 국어로 쓰여진 대화에서 단어 놀라움과 단어 엔트로피가 코드 전환을 예측하는지 여부를 묻는다.",
                    "tag": "1"
                },
                {
                    "index": "302-2",
                    "sentence": "We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese.",
                    "sentence_kor": "우리는 중국어로 다시 번역된 1476개의 깨끗한 코드 교환 문장으로 된 중국어-영어 텍스트의 새로운 데이터 세트를 설명하고 모델링한다.",
                    "tag": "1"
                },
                {
                    "index": "302-3",
                    "sentence": "The model includes known control variables together with word surprisal and word entropy.",
                    "sentence_kor": "이 모델에는 알려진 제어 변수와 함께 단어 놀라움 및 단어 엔트로피가 포함되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "302-4",
                    "sentence": "We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors.",
                    "sentence_kor": "우리는 엔트로피가 아닌 깜짝 놀랄만한 단어가 다른 잘 알려진 예측 변수보다 코드 전환을 설명하는 중요한 예측 변수라는 것을 발견했습니다.",
                    "tag": "1"
                },
                {
                    "index": "302-5",
                    "sentence": "We also found sentence length to be a significant predictor, which has been related to sentence complexity.",
                    "sentence_kor": "우리는 또한 문장 길이가 문장 복잡성과 관련된 중요한 예측 변수라는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "302-6",
                    "sentence": "We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language.",
                    "sentence_kor": "대체 언어의 억제를 위한 자원이 적기 때문에 코드 전환의 이유로 높은 인지 노력을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "302-7",
                    "sentence": "We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.",
                    "sentence_kor": "우리는 또한 이전의 발견들을 확증합니다. 하지만 이번에는 새로운 언어 쌍인 깜짝 살의 계산 모델을 사용하여 문어에 대해 그렇게 합니다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "303",
            "abstractID": "EMNLP_abs-303",
            "text": [
                {
                    "index": "303-0",
                    "sentence": "Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages.",
                    "sentence_kor": "신경 언어 모델은 다양한 수준의 정확도로 자연 언어의 문법적 특성을 학습합니다.",
                    "tag": "1"
                },
                {
                    "index": "303-1",
                    "sentence": "In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy.",
                    "sentence_kor": "본 연구에서는 언어 모델의 정확도에 체계적인 변동 원인이 있는지 여부를 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "303-2",
                    "sentence": "Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models.",
                    "sentence_kor": "주어-동사 일치와 반사적 아나포라에 초점을 맞추면, 우리는 특정 명사가 다른 명사들보다 체계적으로 더 잘 이해된다는 것을 발견하는데, 이것은 문법적인 작업과 다른 언어 모델에 걸쳐 강력한 효과이다.",
                    "tag": "3"
                },
                {
                    "index": "303-3",
                    "sentence": "Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks.",
                    "sentence_kor": "놀랍게도, 우리는 4차 규모의 말뭉치 빈도가 문법 작업에 대한 명사의 수행과 무관하다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "303-4",
                    "sentence": "Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data.",
                    "sentence_kor": "마지막으로, 우리는 새로운 명사의 문법적 특성이 다양한 유형의 훈련 데이터에서 학습될 수 있다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "303-5",
                    "sentence": "The results present a paradox: there should be less variation in grammatical performance than is actually observed.",
                    "sentence_kor": "결과는 역설적이다: 문법적 성과에 실제로 관찰된 것보다 변화가 적어야 한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "304",
            "abstractID": "EMNLP_abs-304",
            "text": [
                {
                    "index": "304-0",
                    "sentence": "It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD).",
                    "sentence_kor": "다국어 정보는 단일 언어 단어의 모호성 해소에 도움이 될 수 있다고 추측되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "304-1",
                    "sentence": "However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations.",
                    "sentence_kor": "그러나 기존 WSD 시스템은 다국어 정보를 거의 고려하지 않으며, 번역본을 생성하여 WSD를 개선하기 위한 효과적인 방법은 제안되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "304-2",
                    "sentence": "In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation.",
                    "sentence_kor": "본 논문에서는 기계 번역을 사용하여 기본 WSD 시스템의 성능을 향상시키는 새로운 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "304-3",
                    "sentence": "Since our approach is language independent, we perform WSD experiments on several languages.",
                    "sentence_kor": "우리의 접근 방식은 언어에 독립적이기 때문에, 우리는 여러 언어에 대해 WSD 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "304-4",
                    "sentence": "The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD.",
                    "sentence_kor": "그 결과는 우리의 방법이 WSD 시스템의 성능을 지속적으로 개선하고 영어와 다국어 WSD 모두에서 최첨단 결과를 얻을 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "304-5",
                    "sentence": "To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.",
                    "sentence_kor": "어휘 번역 정보의 사용을 용이하게 하기 위해, 우리는 BAB도 제안한다.ALIGN, BabelNet의 다국어 어휘 대응에 의해 안내되는 정밀한 BITXT 정렬 알고리즘.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "305",
            "abstractID": "EMNLP_abs-305",
            "text": [
                {
                    "index": "305-0",
                    "sentence": "One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics.",
                    "sentence_kor": "문맥화 모델의 가장 강력한 특징 중 하나는 문맥 내 단어에 대한 동적 임베딩으로, 문맥 인식 어휘 의미론에 대한 최첨단 표현으로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "305-1",
                    "sentence": "In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors.",
                    "sentence_kor": "본 논문에서는 정적 앵커를 통해 변환을 학습하여 이러한 표현을 향상시키는 후처리 기법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "305-2",
                    "sentence": "Our method requires only another pre-trained model and no labeled data is needed.",
                    "sentence_kor": "이 방법은 사전 교육된 다른 모델만 필요하며 라벨링된 데이터는 필요하지 않습니다.",
                    "tag": "3"
                },
                {
                    "index": "305-3",
                    "sentence": "We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context.",
                    "sentence_kor": "우리는 단어의 서로 다른 사용법과 문맥에 사용되는 다른 단어 모두에 걸쳐 의미의 맥락 변화를 테스트하는 벤치마크 작업의 범위에서 일관된 개선을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "305-4",
                    "sentence": "We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.",
                    "sentence_kor": "우리는 원래 상황별 표현이 상황별 모델과 정적 모델 모두의 또 다른 임베딩 공간에 의해 개선될 수 있지만, 계산 요구 사항이 낮은 정적 임베딩이 가장 많은 이득을 제공한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "306",
            "abstractID": "EMNLP_abs-306",
            "text": [
                {
                    "index": "306-0",
                    "sentence": "Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations.",
                    "sentence_kor": "단어 임베딩은 일반적으로 많은 개인의 텍스트를 포함하는 말뭉치에서 파생되므로 개별적으로 개인화된 표현보다는 범용 표현으로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "306-1",
                    "sentence": "While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users.",
                    "sentence_kor": "개인화된 임베딩은 언어 모델 성능 및 기타 언어 처리 작업을 개선하는 데 유용할 수 있지만, 종방향 데이터가 많은 사람에 대해서만 계산될 수 있으며, 이는 신규 사용자에게는 해당되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "306-2",
                    "sentence": "We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion).",
                    "sentence_kor": "우리는 사용자에 대한 전체 또는 부분 인구통계학적 정보(예: 성별, 나이, 위치, 종교)에서 합성하여 파생된 인구통계별 단어 표현을 사용하는 새로운 형태의 개인화된 단어 임베딩을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "306-3",
                    "sentence": "We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations.",
                    "sentence_kor": "우리는 결과적인 인구 통계 인식 단어 표현이 영어의 두 가지 작업인 언어 모델링과 단어 연관성에서 일반적인 단어 표현을 능가한다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "306-4",
                    "sentence": "We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.",
                    "sentence_kor": "우리는 사용 가능한 속성의 수와 속성의 상대적 효과 사이의 절충을 더 자세히 살펴보고 그것들을 사용할 때의 윤리적 영향에 대해 논의한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "307",
            "abstractID": "EMNLP_abs-307",
            "text": [
                {
                    "index": "307-0",
                    "sentence": "In politics, neologisms are frequently invented for partisan objectives.",
                    "sentence_kor": "정치에서 신조어는 종종 당파적 목적을 위해 발명된다.",
                    "tag": "1"
                },
                {
                    "index": "307-1",
                    "sentence": "For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations.",
                    "sentence_kor": "예를 들어, \"문서화되지 않은 노동자\"와 \"불법체류자\"는 같은 그룹의 사람들을 지칭하지만, 그들은 분명히 다른 함축적 의미를 지니고 있다.",
                    "tag": "1"
                },
                {
                    "index": "307-2",
                    "sentence": "Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists.",
                    "sentence_kor": "이와 같은 예는 전통적으로 참조 기반 의미 이론에 대한 도전을 제기했으며 철학자와 인지 과학자들 사이에 대안 이론(예: 2요소 의미론)의 수용을 증가시켰다.",
                    "tag": "1"
                },
                {
                    "index": "307-3",
                    "sentence": "In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation.",
                    "sentence_kor": "그러나 NLP에서 인기 있는 사전 교육 모델은 표시와 함축 모두를 하나의 얽힌 표현으로 인코딩한다.",
                    "tag": "1"
                },
                {
                    "index": "307-4",
                    "sentence": "In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations.",
                    "sentence_kor": "본 연구에서 우리는 사전 훈련된 표현을 독립적 함축적 함축적 함축적 표현으로 분해하는 적대적 함축적 함축적 함축적 함축적 함축적 표현을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "307-5",
                    "sentence": "For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., “immigrants” vs. “aliens”, “estate tax” vs. “death tax”) move closer to each other in denotation space while moving further apart in connotation space.",
                    "sentence_kor": "내재적 해석성을 위해, 우리는 같은 함축은 같지만 다른 함축적 의미를 가진 단어들을 보여준다(예: \"이민자\" 대). \"세금\", \"세금\" vs. 표시 공간에서는 서로 더 가까이 이동하면서 함축 공간에서는 더 멀리 이동합니다.",
                    "tag": "1"
                },
                {
                    "index": "307-6",
                    "sentence": "For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.",
                    "sentence_kor": "외부 애플리케이션의 경우, 우리는 분리된 표현으로 정보 검색 시스템을 훈련시키고 표시 벡터가 문서 순위의 관점 다양성을 개선한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "308",
            "abstractID": "EMNLP_abs-308",
            "text": [
                {
                    "index": "308-0",
                    "sentence": "Text summarization is one of the most challenging and interesting problems in NLP.",
                    "sentence_kor": "텍스트 요약은 NLP에서 가장 어렵고 흥미로운 문제 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "308-1",
                    "sentence": "Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated.",
                    "sentence_kor": "뉴스 보고서나 백과사전 기사와 같은 구조화된 텍스트를 요약하는 데 많은 주의를 기울였지만, 대부분의 중요한 정보가 서로 다른 화자의 다양한 발언에 흩어져 있는 인간-인간/기계 상호작용의 필수적인 부분인 대화를 요약하는 것은 상대적으로 조사가 덜 된 상태로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "308-2",
                    "sentence": "This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries.",
                    "sentence_kor": "이 연구는 먼저 다른 뷰에서 구조화되지 않은 일상 채팅의 대화 구조를 추출하여 대화를 표현한 다음 다중 뷰 디코더를 활용하여 대화 요약을 생성하는 다중 뷰 시퀀스 대 시퀀스 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "308-3",
                    "sentence": "Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment.",
                    "sentence_kor": "대규모 대화 요약 말뭉치에 대한 실험은 우리의 방법이 자동 평가와 인간 판단 모두를 통해 이전의 최첨단 모델을 크게 능가한다는 것을 보여주었다.",
                    "tag": "4"
                },
                {
                    "index": "308-4",
                    "sentence": "We also discussed specific challenges that current approaches faced with this task.",
                    "sentence_kor": "또한 현재의 접근 방식이 이 과제에 직면한 구체적인 과제에 대해서도 논의하였다.",
                    "tag": "4"
                },
                {
                    "index": "308-5",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.",
                    "sentence_kor": "우리는 https://github.com/GT-SALT/Multi-View-Seq2Seq에서 코드를 공개했습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "309",
            "abstractID": "EMNLP_abs-309",
            "text": [
                {
                    "index": "309-0",
                    "sentence": "Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product.",
                    "sentence_kor": "의견 요약은 제품의 사용자 리뷰와 같은 여러 문서에 표현된 주관적인 정보를 반영하는 텍스트의 자동 작성입니다.",
                    "tag": "1"
                },
                {
                    "index": "309-1",
                    "sentence": "The task is practically important and has attracted a lot of attention.",
                    "sentence_kor": "그 일은 실질적으로 중요하고 많은 관심을 끌었다.",
                    "tag": "1"
                },
                {
                    "index": "309-2",
                    "sentence": "However, due to the high cost of summary production, datasets large enough for training supervised models are lacking.",
                    "sentence_kor": "그러나 요약 제작 비용이 높기 때문에 감독 모델을 교육하기에 충분한 데이터 세트가 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "309-3",
                    "sentence": "Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way.",
                    "sentence_kor": "대신, 이 작업은 전통적으로 감독되지 않거나 약하게 감독되는 방식으로 텍스트 조각을 선택하는 방법을 배우는 추출 방법을 사용하여 접근해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "309-4",
                    "sentence": "Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion.",
                    "sentence_kor": "최근에는 잠재적으로 더 유창하고 상충되는 정보를 더 잘 반영할 수 있는 추상적 요약도 감독되지 않은 방식으로 작성될 수 있는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "309-5",
                    "sentence": "However, these models, not being exposed to actual summaries, fail to capture their essential properties.",
                    "sentence_kor": "그러나 이러한 모델은 실제 요약에 노출되지 않고 필수 속성을 캡처하지 못합니다.",
                    "tag": "1"
                },
                {
                    "index": "309-6",
                    "sentence": "In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation.",
                    "sentence_kor": "이 연구에서, 우리는 소수의 요약이라도 쓰기 스타일, 정보성, 유창성 및 정서 보존과 같은 모든 예상 속성을 가진 요약 텍스트를 부트스트랩으로 생성하기에 충분하다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "309-7",
                    "sentence": "We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product.",
                    "sentence_kor": "우리는 제품에 대한 다른 사용 가능한 리뷰가 주어졌을 때 새로운 제품 리뷰를 생성하기 위해 조건부 트랜스포머 언어 모델을 교육하는 것으로 시작한다.",
                    "tag": "1"
                },
                {
                    "index": "309-8",
                    "sentence": "The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort.",
                    "sentence_kor": "또한 모델은 요약과 직접 관련된 검토 속성에 따라 조정되며, 속성은 수동 작업 없이 검토에서 파생됩니다.",
                    "tag": "1"
                },
                {
                    "index": "309-9",
                    "sentence": "In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries.",
                    "sentence_kor": "두 번째 단계에서는 소수의 요약에서 속성 값을 예측하는 방법을 배우는 플러그인 모듈을 미세 조정한다.",
                    "tag": "1"
                },
                {
                    "index": "309-10",
                    "sentence": "This lets us switch the generator to the summarization mode.",
                    "sentence_kor": "이를 통해 제너레이터를 요약 모드로 전환할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "309-11",
                    "sentence": "We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.",
                    "sentence_kor": "우리는 아마존과 Yelp 데이터 세트에서 우리의 접근 방식이 자동 및 인간 평가에서 이전의 추출적이고 추상적인 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "310",
            "abstractID": "EMNLP_abs-310",
            "text": [
                {
                    "index": "310-0",
                    "sentence": "The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts.",
                    "sentence_kor": "문장을 융합하는 능력은 요약 시스템에 매우 매력적이다. 왜냐하면 그것은 간결한 추상화를 만드는 필수적인 단계이기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "310-1",
                    "sentence": "However, to date, summarizers can fail on fusing sentences.",
                    "sentence_kor": "그러나 현재까지 요약자는 문장을 융합하는 데 실패할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "310-2",
                    "sentence": "They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning.",
                    "sentence_kor": "그들은 융합으로 요약 문장을 거의 생성하지 못하거나 요약이 원래의 의미를 유지하지 못하게 하는 잘못된 융합을 생성하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "310-3",
                    "sentence": "In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences.",
                    "sentence_kor": "본 논문에서, 우리는 문장 간의 대응 지점에 대한 지식을 활용하여 문장을 융합하고 문장 융합을 수행하는 능력을 향상시키는 새로운 알고리즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "310-4",
                    "sentence": "Through extensive experiments, we investigate the effects of different design choices on Transformer’s performance.",
                    "sentence_kor": "광범위한 실험을 통해 다양한 설계 선택이 트랜스포머 성능에 미치는 영향을 조사한다.",
                    "tag": "4"
                },
                {
                    "index": "310-5",
                    "sentence": "Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.",
                    "sentence_kor": "우리의 연구 결과는 효과적인 문장 융합을 위해 문장 간의 대응 모델링 지점의 중요성을 강조한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "311",
            "abstractID": "EMNLP_abs-311",
            "text": [
                {
                    "index": "311-0",
                    "sentence": "We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers.",
                    "sentence_kor": "우리는 구조화된 트랜스포머인 HiBERT 및 확장 트랜스포머를 사용하여 추출 요약을 위한 인코더 중심의 단계적 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "311-1",
                    "sentence": "We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure.",
                    "sentence_kor": "우리는 이전에 생성된 요약을 구조화된 변압기에 보조 하위 구조로 주입하여 단계적 요약을 가능하게 한다.",
                    "tag": "3"
                },
                {
                    "index": "311-2",
                    "sentence": "Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks.",
                    "sentence_kor": "우리 모델은 긴 입력의 구조를 모델링하는 데 효율적일 뿐만 아니라 작업별 중복성 인식 모델링에 의존하지 않기 때문에 다양한 작업에 대한 범용 추출 콘텐츠 플래너가 된다.",
                    "tag": "3"
                },
                {
                    "index": "311-3",
                    "sentence": "When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering.",
                    "sentence_kor": "CNN/DailyMail 추출 요약에서 평가한 단계적 모델은 중복 인식 모델링 또는 문장 필터링 없이 Rouge 측면에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "311-4",
                    "sentence": "This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling.",
                    "sentence_kor": "이는 Rotowire 표 대 텍스트 생성에서도 마찬가지이며, 여기서 당사의 모델은 컨텐츠 선택, 계획 및 주문에 대해 이전에 보고된 메트릭을 능가하여 단계적 모델링의 강점을 강조합니다.",
                    "tag": "3"
                },
                {
                    "index": "311-5",
                    "sentence": "Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.",
                    "sentence_kor": "테스트하는 두 가지 구조화된 트랜스포머 중 단계적 확장 트랜스포머는 두 데이터셋 모두에서 최고의 성능을 제공하고 이러한 과제에 대한 새로운 표준을 설정합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "312",
            "abstractID": "EMNLP_abs-312",
            "text": [
                {
                    "index": "312-0",
                    "sentence": "We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia.",
                    "sentence_kor": "우리는 위키피디아에서 자동으로 추출된 교차 언어 정보 검색을 위한 대규모 이중 언어 및 다국어 데이터 세트 모음인 CLIMMatrix를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "312-1",
                    "sentence": "CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.",
                    "sentence_kor": "CLIMMatrix는 (1) 다른 언어의 관련 문서와 일치하는 한 언어의 질의 2개 국어 데이터 세트인 BI-139와 (2) 8개 언어로 공동으로 정렬된 질의와 문서의 다국어 데이터 세트인 MULTI-8로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "312-2",
                    "sentence": "In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date.",
                    "sentence_kor": "총 4900만 개의 고유 쿼리와 340억 개의 세 쌍둥이(쿼리, 문서, 레이블)를 채굴하여 현재까지 가장 크고 포괄적인 CLIR 데이터 세트로 만들었다.",
                    "tag": "3"
                },
                {
                    "index": "312-3",
                    "sentence": "This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url].",
                    "sentence_kor": "이 컬렉션은 엔드 투 엔드 신경 정보 검색 연구를 지원하기 위한 것이며 [url]에서 공개적으로 이용할 수 있다.",
                    "tag": "6"
                },
                {
                    "index": "312-4",
                    "sentence": "We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.",
                    "sentence_kor": "BI-139에 대한 기준 신경 모델 결과를 제공하고 단일 언어 검색 및 혼합 언어 검색 설정에서 Multi-8을 평가한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "313",
            "abstractID": "EMNLP_abs-313",
            "text": [
                {
                    "index": "313-0",
                    "sentence": "With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus.",
                    "sentence_kor": "중증급성호흡기증후군 코로나바이러스 2(SARS-CoV-2)를 둘러싼 전 세계적인 우려와 함께, 이 바이러스에 대한 과학 문헌이 빠르게 증가하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "313-1",
                    "sentence": "Clinicians, researchers, and policy-makers need to be able to search these articles effectively.",
                    "sentence_kor": "임상의, 연구원 및 정책 입안자는 이러한 문서를 효과적으로 검색할 수 있어야 합니다.",
                    "tag": "1"
                },
                {
                    "index": "313-2",
                    "sentence": "In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature.",
                    "sentence_kor": "본 연구에서는 COVID 관련 과학 문헌에 적응하는 제로샷 순위 알고리즘을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "313-3",
                    "sentence": "Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection.",
                    "sentence_kor": "우리의 접근 방식은 다른 컬렉션의 훈련 데이터를 의료 관련 쿼리까지 필터링하고, 과학 텍스트(SciBERT)에서 사전 훈련된 신경 재순위화 모델을 사용하며, 대상 문서 컬렉션을 필터링한다.",
                    "tag": "3"
                },
                {
                    "index": "313-4",
                    "sentence": "This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments.",
                    "sentence_kor": "이 접근방식은 TRC COVID 1라운드 리더보드의 제로샷 방법 중 최상위이며, 1라운드 및 2라운드 판정에서 평가할 때 P@5는 0.80, nDCG@10은 0.68을 나타낸다.",
                    "tag": "3"
                },
                {
                    "index": "313-5",
                    "sentence": "Despite not relying on TREC-COVID data, our method outperforms models that do.",
                    "sentence_kor": "TRC-COVID 데이터에 의존하지 않음에도 불구하고, 우리의 방법은 다음과 같은 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "313-6",
                    "sentence": "As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.",
                    "sentence_kor": "COVID-19 검색을 철저히 평가하는 첫 번째 검색 방법 중 하나로, 우리는 이것이 강력한 기준선이 되어 글로벌 위기에 도움이 되기를 바란다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "314",
            "abstractID": "EMNLP_abs-314",
            "text": [
                {
                    "index": "314-0",
                    "sentence": "Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval.",
                    "sentence_kor": "트랜스포머 기반 순위 모델의 최근 혁신은 정보 검색의 최첨단 기술을 발전시켰다.",
                    "tag": "1"
                },
                {
                    "index": "314-1",
                    "sentence": "However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process.",
                    "sentence_kor": "그러나 이러한 트랜스포머는 계산 비용이 많이 들고 불투명한 은닉 상태가 순위 지정 프로세스를 이해하기 어렵게 한다.",
                    "tag": "1"
                },
                {
                    "index": "314-2",
                    "sentence": "In this work, we modularize the Transformer ranker into separate modules for text representation and interaction.",
                    "sentence_kor": "이 작업에서는 트랜스포머 랭커를 텍스트 표현 및 상호 작용을 위한 별도의 모듈로 모듈화한다.",
                    "tag": "1"
                },
                {
                    "index": "314-3",
                    "sentence": "We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions.",
                    "sentence_kor": "우리는 이 설계가 오프라인 사전 계산된 표현과 가벼운 온라인 상호 작용을 사용하여 실질적으로 더 빠른 순위를 가능하게 하는 방법을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "314-4",
                    "sentence": "The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.",
                    "sentence_kor": "모듈식 설계는 또한 트랜스포머 랭커의 순위 프로세스를 해석하고 조명하기 쉽다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "315",
            "abstractID": "EMNLP_abs-315",
            "text": [
                {
                    "index": "315-0",
                    "sentence": "We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval.",
                    "sentence_kor": "우리는 임시 문서 검색 작업에 대한 딥 러닝 모델을 훈련하기 위해 약하게 감독되는 방법을 설명한다.",
                    "tag": "2"
                },
                {
                    "index": "315-1",
                    "sentence": "Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus.",
                    "sentence_kor": "우리의 방법은 말뭉치의 문서에서만 약한 감독을 사용하여 훈련되는 생성적이고 차별적인 모델에 기초한다.",
                    "tag": "3"
                },
                {
                    "index": "315-2",
                    "sentence": "We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers.",
                    "sentence_kor": "우리는 전통적인 정보 검색 방법으로 시작하여 두 명의 딥 러닝 재순위자가 뒤따르는 엔드 투 엔드 검색 시스템을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "315-3",
                    "sentence": "We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets.",
                    "sentence_kor": "우리는 COVID-19 관련 과학 문헌 데이터 세트와 두 개의 뉴스 데이터 세트 등 세 가지 데이터 세트에서 우리의 방법을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "315-4",
                    "sentence": "We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.",
                    "sentence_kor": "우리는 우리의 방법이 최첨단 방법을 능가한다는 것을 보여준다. 이것은 수동으로 데이터에 레이블을 붙이는 값비싼 과정을 필요로 하지 않는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "316",
            "abstractID": "EMNLP_abs-316",
            "text": [
                {
                    "index": "316-0",
                    "sentence": "We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models.",
                    "sentence_kor": "우리는 의미론 충돌, 의미론적으로는 관련이 없지만 NLP 모델에 의해 유사한 것으로 판단되는 텍스트를 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "316-1",
                    "sentence": "We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions.",
                    "sentence_kor": "우리는 의미론적 충돌 생성을 위한 경사도 기반 접근 방식을 개발하고 의역 식별, 문서 검색, 응답 제안 및 추출적 요약을 포함한 텍스트의 의미와 유사성을 분석하는 데 의존하는 많은 작업에 대한 최첨단 모델이 의미론적 충돌에 취약하다는 것을 입증한다.",
                    "tag": "1"
                },
                {
                    "index": "316-2",
                    "sentence": "For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3.",
                    "sentence_kor": "예를 들어 대상 쿼리가 주어졌을 때 관련 없는 문서에 조작된 충돌을 삽입하면 검색 순위가 1000에서 상위 3으로 이동할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "316-3",
                    "sentence": "We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations.",
                    "sentence_kor": "우리는 난해성 기반 필터링을 회피하는 의미론적 충돌을 생성하는 방법을 보여주고 다른 잠재적 완화에 대해 논의한다.",
                    "tag": "2+3"
                },
                {
                    "index": "316-4",
                    "sentence": "Our code is available at https://github.com/csong27/collision-bert.",
                    "sentence_kor": "우리의 코드는 https://github.com/csong27/collision-bert에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "317",
            "abstractID": "EMNLP_abs-317",
            "text": [
                {
                    "index": "317-0",
                    "sentence": "Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world.",
                    "sentence_kor": "예측 모델의 해석성은 실제 세계에서 채택이 증가함에 따라 점점 더 중요해지고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "317-1",
                    "sentence": "We present RuleNN, a neural network architecture for learning transparent models for sentence classification.",
                    "sentence_kor": "우리는 문장 분류를 위한 투명한 모델을 학습하기 위한 신경망 아키텍처인 RuleNN을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "317-2",
                    "sentence": "The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics.",
                    "sentence_kor": "모델은 잘 정의되고 인간이 이해할 수 있는 의미론을 가진 방언인 1차 논리로 표현된 규칙 형태이다.",
                    "tag": "1"
                },
                {
                    "index": "317-3",
                    "sentence": "More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding.",
                    "sentence_kor": "보다 정확히는 RuleNN은 얕은 자연어 이해를 사용하여 추출한 술어 위에 구축된 언어 표현(LE)을 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "317-4",
                    "sentence": "Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks.",
                    "sentence_kor": "우리의 실험 결과는 RuleNN이 통계적 관계 학습 및 기타 신경 기호 방법을 능가하며 블랙박스 반복 신경망과 유사한 성능을 발휘한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "317-5",
                    "sentence": "Our user studies confirm that the learned LEs are explainable and capture domain semantics.",
                    "sentence_kor": "우리의 사용자 연구는 학습된 LE가 설명 가능하고 도메인 의미를 포착할 수 있음을 확인한다.",
                    "tag": "1"
                },
                {
                    "index": "317-6",
                    "sentence": "Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.",
                    "sentence_kor": "또한 도메인 전문가가 LE를 수정하고 더 많은 도메인 지식을 주입할 수 있도록 하면 더 나은 성능의 모델을 인간-기계 공동 만들 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "318",
            "abstractID": "EMNLP_abs-318",
            "text": [
                {
                    "index": "318-0",
                    "sentence": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining.",
                    "sentence_kor": "사전 훈련된 언어 모델의 주목할 만한 성공은 이러한 모델들이 사전 훈련 동안 어떤 종류의 지식을 배우는지에 대한 연구에 동기를 부여했다.",
                    "tag": "1"
                },
                {
                    "index": "318-1",
                    "sentence": "Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts.",
                    "sentence_kor": "사각 채우기 문제(예: 클로즈 시험)로 작업을 재구성하는 것은 그러한 지식을 측정하기 위한 자연스러운 접근법이지만, 적절한 프롬프트 작성에 필요한 수동 노력과 추측 작업에 의해 사용이 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "318-2",
                    "sentence": "To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search.",
                    "sentence_kor": "이를 해결하기 위해, 우리는 구배 안내 검색을 기반으로 다양한 작업에 대한 프롬프트를 생성하는 자동화된 방법인 AutoPrompt를 개발한다.",
                    "tag": "1"
                },
                {
                    "index": "318-3",
                    "sentence": "Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models.",
                    "sentence_kor": "AutoPrompt를 사용하여 마스킹 언어 모델(MLM)은 추가 매개 변수나 미세 조정 없이 감정 분석과 자연어 추론을 수행할 수 있는 고유한 기능을 가지고 있으며, 때로는 최신 감독 모델과 동등한 성능을 달성하기도 한다.",
                    "tag": "1"
                },
                {
                    "index": "318-4",
                    "sentence": "We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models.",
                    "sentence_kor": "또한 우리의 프롬프트가 LAMA 벤치마크에서 수동으로 생성된 프롬프트보다 MLM으로부터 더 정확한 사실 지식을 이끌어내고 MLM이 감독된 관계 추출 모델보다 더 효과적으로 관계 추출기로 사용될 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "318-5",
                    "sentence": "These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.",
                    "sentence_kor": "이러한 결과는 자동으로 생성된 프롬프트가 기존 탐색 방법에 대한 실행 가능한 매개 변수 없는 대안이며 사전 훈련된 LM이 더 정교하고 능력 있게 되어 잠재적으로 미세 조정을 위한 대체물이 될 수 있음을 입증한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "319",
            "abstractID": "EMNLP_abs-319",
            "text": [
                {
                    "index": "319-0",
                    "sentence": "To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations.",
                    "sentence_kor": "해석 가능한 신경 텍스트 분류기를 구축하기 위해 대부분의 이전 작업은 본질적으로 해석 가능한 모델을 설계하거나 충실한 설명을 찾는 데 초점을 맞추었다.",
                    "tag": "1"
                },
                {
                    "index": "319-1",
                    "sentence": "A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training.",
                    "sentence_kor": "모델 해석성 개선에 대한 새로운 연구가 막 시작되었으며, 많은 기존 방법들은 훈련의 추가 입력으로 사전 정보 또는 인간 주석을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "319-2",
                    "sentence": "To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions.",
                    "sentence_kor": "이러한 한계를 해결하기 위해 작업별 중요 단어를 자동으로 학습하고 분류에 대한 관련 정보를 줄여 모델 예측의 해석성을 향상시키는 VMASK(Variative Word Mask) 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "319-3",
                    "sentence": "The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets.",
                    "sentence_kor": "제안된 방법은 7개의 벤치마크 텍스트 분류 데이터 세트에서 세 개의 신경 텍스트 분류기(CNN, LSTM 및 BERT)로 평가된다.",
                    "tag": "3"
                },
                {
                    "index": "319-4",
                    "sentence": "Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.",
                    "sentence_kor": "실험은 VMASK가 모델 예측 정확도와 해석성을 모두 향상시키는 효과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "320",
            "abstractID": "EMNLP_abs-320",
            "text": [
                {
                    "index": "320-0",
                    "sentence": "Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance.",
                    "sentence_kor": "현재의 최첨단 텍스트 생성기는 GPT-2와 같은 강력한 언어 모델을 기반으로 하여 인상적인 성능을 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "320-1",
                    "sentence": "However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling.",
                    "sentence_kor": "단, 퇴화 텍스트를 방지하기 위해서는 top-k 또는 핵 샘플링에서와 같이 온도 매개 변수 또는 임시 절단 기법을 통해 수정된 소프트맥스에서 샘플링해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "320-2",
                    "sentence": "This creates a mismatch between training and testing conditions.",
                    "sentence_kor": "이로 인해 교육 조건과 테스트 조건이 일치하지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "320-3",
                    "sentence": "In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch.",
                    "sentence_kor": "본 논문에서는 최근에 도입된 엔트맥스 변환을 사용하여 원래 희박한 언어 모델을 훈련하고 샘플을 추출하여 이러한 불일치를 방지한다.",
                    "tag": "2"
                },
                {
                    "index": "320-4",
                    "sentence": "The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text.",
                    "sentence_kor": "그 결과 유연성과 일관성, 반복 횟수 감소 및 인간 텍스트에 가까운 N그램 다양성 측면에서 유리한 성능을 가진 텍스트 생성기가 된다.",
                    "tag": "4"
                },
                {
                    "index": "320-5",
                    "sentence": "In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: 𝜖-perplexity, sparsemax score, and Jensen-Shannon divergence.",
                    "sentence_kor": "모델을 평가하기 위해 희박하거나 잘린 분포를 비교하기 위한 세 가지 새로운 메트릭, 즉 λ-퍼플렉시, 희소 최대 점수 및 젠센-섀논 발산을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "320-6",
                    "sentence": "Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.",
                    "sentence_kor": "인간이 평가한 스토리 완성 및 대화 생성 실험은 엔트맥스 샘플링이 보다 몰입적이고 일관된 스토리와 대화를 이끌어 낸다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "321",
            "abstractID": "EMNLP_abs-321",
            "text": [
                {
                    "index": "321-0",
                    "sentence": "We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline.",
                    "sentence_kor": "우리는 개요 조건 이야기 생성 과제를 제안한다. 개요가 이야기에 등장할 주요 인물과 사건을 설명하는 문구 집합으로 주어지면, 과제는 제공된 개요와 일치하는 일관된 이야기를 생성하는 것이다.",
                    "tag": "2+3"
                },
                {
                    "index": "321-1",
                    "sentence": "This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline.",
                    "sentence_kor": "이 작업은 입력이 플롯의 대략적인 스케치만 제공하므로 모델이 개요에 제공된 핵심 포인트를 서로 엮어 스토리를 생성해야 하기 때문에 어렵다.",
                    "tag": "3"
                },
                {
                    "index": "321-2",
                    "sentence": "This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story.",
                    "sentence_kor": "이를 위해서는 모델이 전체 스토리를 생성하는 동안 입력 개요에서 조건화하면서 잠재 플롯의 동적 상태를 추적해야 한다.",
                    "tag": "3"
                },
                {
                    "index": "321-3",
                    "sentence": "We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states.",
                    "sentence_kor": "우리는 동적 플롯 상태를 추적함으로써 윤곽을 일관된 이야기로 변환하는 방법을 배우는 신경 서술 모델인 플롯 머신스를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "321-4",
                    "sentence": "In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative.",
                    "sentence_kor": "또한, 우리는 모델이 이야기의 다른 부분에 해당하는 다른 쓰기 스타일을 배울 수 있도록 높은 수준의 담화 구조를 가진 PlotMachines를 풍부하게 한다.",
                    "tag": "3"
                },
                {
                    "index": "321-5",
                    "sentence": "Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.",
                    "sentence_kor": "세 개의 픽션 및 논픽션 데이터 세트에 대한 종합적인 실험은 GPT-2와 그로버와 같은 대규모 언어 모델이 인상적인 세대 성능에도 불구하고 주어진 개요에 대해 일관성 있는 내러티브를 생성하는 데 충분하지 않으며, 동적 플롯 상태 추적은 내러티브를 더 긴밀하게 구성하는데 중요하다는 것을 보여준다. 좀 더 일관된 플롯입니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "322",
            "abstractID": "EMNLP_abs-322",
            "text": [
                {
                    "index": "322-0",
                    "sentence": "Autoregressive language models are powerful and relatively easy to train.",
                    "sentence_kor": "자기 회귀 언어 모델은 강력하고 상대적으로 훈련하기 쉽습니다.",
                    "tag": "1"
                },
                {
                    "index": "322-1",
                    "sentence": "However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation.",
                    "sentence_kor": "그러나 이러한 모델은 대개 명시적 조건화 라벨 없이 훈련되며 생성 중 감정이나 주제와 같은 전역적 측면을 제어하는 쉬운 방법을 제공하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "322-2",
                    "sentence": "Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner.",
                    "sentence_kor": "Bowman & al. 2016은 시퀀스 투 시퀀스 아키텍처로 자연어에 VAE(Variational Autoencoder)를 적용했으며 잠재 벡터가 이러한 글로벌 특징을 감독되지 않은 방식으로 포착할 수 있었다고 주장했다.",
                    "tag": "1"
                },
                {
                    "index": "322-3",
                    "sentence": "We question this claim.",
                    "sentence_kor": "우리는 이 주장에 의문을 제기한다.",
                    "tag": "1"
                },
                {
                    "index": "322-4",
                    "sentence": "We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence.",
                    "sentence_kor": "우리는 문장 내 위치당 재구성 손실을 분해하여 잠재 정보로부터 어떤 단어가 가장 도움이 되는지 측정한다.",
                    "tag": "1+2"
                },
                {
                    "index": "322-5",
                    "sentence": "Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness.",
                    "sentence_kor": "이 방법을 사용하여, 우리는 VAE가 첫 번째 단어와 문장 길이를 암기하여 제한된 유용성의 지역적 특징을 생성하는 경향이 있다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "322-6",
                    "sentence": "To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining.",
                    "sentence_kor": "이를 완화하기 위해, 우리는 단어 가방 가정과 언어 모델 사전 훈련을 기반으로 하는 대체 아키텍처를 조사한다.",
                    "tag": "4"
                },
                {
                    "index": "322-7",
                    "sentence": "These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels.",
                    "sentence_kor": "이러한 변형은 더 전역적인 잠재 변수, 즉 주제 또는 감정 레이블을 더 예측하기 쉬운 잠재 변수를 학습합니다.",
                    "tag": "4"
                },
                {
                    "index": "322-8",
                    "sentence": "Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.",
                    "sentence_kor": "더욱이 재구성을 사용하면 암기력이 감소한다는 것을 관찰한다. 첫 번째 단어와 문장 길이가 기준선을 사용하는 것보다 더 정확하게 복구되지 않고 결과적으로 더 다양한 재구성을 산출한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "323",
            "abstractID": "EMNLP_abs-323",
            "text": [
                {
                    "index": "323-0",
                    "sentence": "Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion.",
                    "sentence_kor": "큰 언어 모델에서 생성된 긴 형식의 서술 텍스트는 인간 글쓰기의 유창한 가장을 관리하지만, 지역 문장 수준에서만 관리되며 구조나 전역적 응집력이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "323-1",
                    "sentence": "We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation.",
                    "sentence_kor": "우리는 이야기 생성의 많은 문제점들이 고품질의 콘텐츠 계획을 통해 해결될 수 있다고 확신하고 이야기 생성을 안내하기 위해 좋은 플롯 구조를 배우는 방법에 초점을 맞춘 시스템을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "323-2",
                    "sentence": "We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle’s Poetics.",
                    "sentence_kor": "우리는 줄거리 생성 언어 모델을 아리스토텔레스의 시학에서 자세히 설명한 좋은 이야기 쓰기의 측면을 각각 구현하는 모델들의 앙상블과 함께 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "323-3",
                    "sentence": "We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.",
                    "sentence_kor": "우리는 우리의 보다 원칙적인 줄거리 구조로 쓰여진 이야기들이 내용 계획이 없는 기준선이나 원칙 없는 방식으로 쓰여진 것보다 주어진 신속성과 더 높은 품질에 더 적합하다는 것을 발견한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "324",
            "abstractID": "EMNLP_abs-324",
            "text": [
                {
                    "index": "324-0",
                    "sentence": "Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary.",
                    "sentence_kor": "기존의 개방형 도메인 대화 생성 모델은 일반적으로 어휘의 교차 엔트로피 손실을 사용하여 훈련 세트의 골드 응답을 모방하도록 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "324-1",
                    "sentence": "However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt.",
                    "sentence_kor": "그러나 주어진 프롬프트에 대해 여러 개의 가능한 반응이 있기 때문에 좋은 반응은 금색 반응과 유사할 필요는 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "324-2",
                    "sentence": "In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses.",
                    "sentence_kor": "이 연구에서, 우리는 현재 모델이 프롬프트의 여러 의미론적으로 유사한 유효한 응답의 정보를 통합할 수 없어 일반적이고 비정보적인 응답이 생성된다고 가정한다.",
                    "tag": "2"
                },
                {
                    "index": "324-3",
                    "sentence": "To address this issue, we propose an alternative to the end-to-end classification on vocabulary.",
                    "sentence_kor": "이 문제를 해결하기 위해 어휘에 대한 엔드 투 엔드 분류에 대한 대안을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "324-4",
                    "sentence": "We learn the pair relationship between the prompts and responses as a regression task on a latent space instead.",
                    "sentence_kor": "대신 잠재 공간에 대한 회귀 작업으로 프롬프트와 응답 사이의 쌍 관계를 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "324-5",
                    "sentence": "In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space.",
                    "sentence_kor": "우리의 새로운 대화 상자 생성 모델에서, 의미론적으로 관련된 문장의 표현은 잠재 공간에서 서로 가깝다.",
                    "tag": "4"
                },
                {
                    "index": "324-6",
                    "sentence": "Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.",
                    "sentence_kor": "인간 평가는 연속된 공간에서 과제를 학습하면 관련성과 정보성을 모두 갖춘 응답을 생성할 수 있다는 것을 보여주었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "325",
            "abstractID": "EMNLP_abs-325",
            "text": [
                {
                    "index": "325-0",
                    "sentence": "Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness.",
                    "sentence_kor": "대화 참여자는 종종 대화 내의 실체나 상황을 반복적으로 언급하며, 이는 대화 응집력에 기여한다.",
                    "tag": "1"
                },
                {
                    "index": "325-1",
                    "sentence": "Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions.",
                    "sentence_kor": "후속 참고문헌은 대화자에 의해 축적된 공통점을 이용하므로 몇 가지 흥미로운 특성을 가지고 있다. 즉, 이전 언급에서 효과적이었던 표현은 더 짧고 재사용되는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "325-2",
                    "sentence": "In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue.",
                    "sentence_kor": "본 논문에서는 시각적 기반 대화에서 첫 번째 및 후속 참조의 생성을 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "325-3",
                    "sentence": "We propose a generation model that produces referring utterances grounded in both the visual and the conversational context.",
                    "sentence_kor": "시각적 및 대화적 맥락에 바탕을 둔 참조 발화를 생성하는 생성 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "325-4",
                    "sentence": "To assess the referring effectiveness of its output, we also implement a reference resolution system.",
                    "sentence_kor": "출력물의 참조 효과를 평가하기 위해 기준 해결 시스템도 구현합니다.",
                    "tag": "2"
                },
                {
                    "index": "325-5",
                    "sentence": "Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.",
                    "sentence_kor": "우리의 실험과 분석에 따르면 모델은 대화 맥락에 근거하지 않은 모델보다 더 낫고 효과적인 참조 발언을 생성하고 인간과 유사한 언어 패턴을 나타내는 후속 참조를 생성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "326",
            "abstractID": "EMNLP_abs-326",
            "text": [
                {
                    "index": "326-0",
                    "sentence": "Exploiting visual groundings for language understanding has recently been drawing much attention.",
                    "sentence_kor": "언어 이해를 위해 시각적인 배경을 이용하는 것이 최근 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "326-1",
                    "sentence": "In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings.",
                    "sentence_kor": "본 연구에서는 시각적으로 기초하는 문법 유도를 연구하고 레이블이 없는 텍스트와 시각적 기초에서 구성 분석기를 학습한다.",
                    "tag": "2"
                },
                {
                    "index": "326-2",
                    "sentence": "Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences.",
                    "sentence_kor": "이 작업에 대한 기존 작업(Shi 등, 2019)은 보강을 통해 파서를 최적화하고 이미지와 문장의 정렬에서만 학습 신호를 도출한다.",
                    "tag": "3"
                },
                {
                    "index": "326-3",
                    "sentence": "While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs).",
                    "sentence_kor": "이들의 모델은 전체적으로 비교적 정확하지만 오류 분포는 매우 고르지 못하여 특정 구성 요소 유형(예: 동사 구, VP에 대한 26.2% 회수)과 다른 구성 요소 유형(예: 명사 구, NP에 대한 79.6% 회수)의 성능이 높다.",
                    "tag": "3"
                },
                {
                    "index": "326-4",
                    "sentence": "This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy.",
                    "sentence_kor": "학습 신호가 구문 구문과 구배 추정의 모든 측면을 도출하기에 불충분할 가능성이 높기 때문에 이는 놀라운 일이 아니다.",
                    "tag": "4"
                },
                {
                    "index": "326-5",
                    "sentence": "We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning.",
                    "sentence_kor": "확률론적 문맥 없는 문법 모델의 확장을 사용하여 시각적으로 완전히 차별화 가능한 엔드 투 엔드 학습을 수행할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "326-6",
                    "sentence": "Additionally, this enables us to complement the image-text alignment loss with a language modeling objective.",
                    "sentence_kor": "또한 이를 통해 이미지-텍스트 정렬 손실을 언어 모델링 목표로 보완할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "326-7",
                    "sentence": "On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction.",
                    "sentence_kor": "MSCOCO 테스트 캡션에서 우리 모델은 근거 없는 버전을 능가하는 새로운 최첨단 기술을 확립하여 선거구 문법 유도에서 시각적 기초의 효과를 확인한다.",
                    "tag": "5"
                },
                {
                    "index": "326-8",
                    "sentence": "It also substantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs).",
                    "sentence_kor": "또한 더 많은 '추상적인' 범주(예: VP에 대한 +55.1% 리콜)에서 가장 크게 개선되어 이전의 접지 모델을 크게 능가합니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "327",
            "abstractID": "EMNLP_abs-327",
            "text": [
                {
                    "index": "327-0",
                    "sentence": "Training a supervised neural network classifier typically requires many annotated training samples.",
                    "sentence_kor": "감독된 신경망 분류기를 훈련하려면 일반적으로 주석이 달린 많은 훈련 샘플이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "327-1",
                    "sentence": "Collecting and annotating a large number of data points are costly and sometimes even infeasible.",
                    "sentence_kor": "많은 데이터 포인트를 수집하고 주석을 다는 것은 비용이 많이 들고 때로는 실행 불가능하기도 합니다.",
                    "tag": "1"
                },
                {
                    "index": "327-2",
                    "sentence": "Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information.",
                    "sentence_kor": "기존의 주석 프로세스는 낮은 대역폭의 인간-기계 통신 인터페이스, 즉 분류 레이블을 사용하며, 각각은 몇 개의 정보만 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "327-3",
                    "sentence": "We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning.",
                    "sentence_kor": "대비 자연어 설명을 활용하여 학습에서 데이터 효율성을 향상시키는 루프 내 전문가 훈련 프레임워크인 ALICE(Active Learning with Contrastive Descriptions)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "327-4",
                    "sentence": "AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts.",
                    "sentence_kor": "AL-ICE는 먼저 능동 학습을 사용하여 가장 유익한 레이블 클래스 쌍을 선택하여 전문가로부터 대조적인 자연어 설명을 유도하는 방법을 배운다.",
                    "tag": "3"
                },
                {
                    "index": "327-5",
                    "sentence": "Then it extracts knowledge from these explanations using a semantic parser.",
                    "sentence_kor": "그런 다음 의미 분석기를 사용하여 이러한 설명에서 지식을 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "327-6",
                    "sentence": "Finally, it incorporates the extracted knowledge through dynamically changing the learning model’s structure.",
                    "sentence_kor": "마지막으로, 학습 모델의 구조를 동적으로 변경하여 추출된 지식을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "327-7",
                    "sentence": "We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification.",
                    "sentence_kor": "우리는 조류 분류와 사회적 관계 분류라는 두 가지 시각적 인식 과제를 적용했다.",
                    "tag": "3"
                },
                {
                    "index": "327-8",
                    "sentence": "We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data.",
                    "sentence_kor": "대조적인 설명을 통합함으로써 당사의 모델은 40-100% 더 많은 교육 데이터로 훈련된 기준 모델을 능가한다는 것을 발견했다.",
                    "tag": "3+4"
                },
                {
                    "index": "327-9",
                    "sentence": "We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.",
                    "sentence_kor": "우리는 1expla 국가를 추가하면 13-30개의 레이블링된 교육 데이터 포인트를 추가하는 것과 유사한 성능 향상을 가져올 수 있다는 것을 발견했습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "328",
            "abstractID": "EMNLP_abs-328",
            "text": [
                {
                    "index": "328-0",
                    "sentence": "Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step.",
                    "sentence_kor": "반복 언어 기반 이미지 편집(ILBIE) 작업은 반복 지침에 따라 이미지를 단계별로 편집한다.",
                    "tag": "1"
                },
                {
                    "index": "328-1",
                    "sentence": "Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes.",
                    "sentence_kor": "ILBIE는 명령 기반 변경 전후에 대규모 이미지 예를 수집하는 것이 어렵기 때문에 데이터 부족은 중요한 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "328-2",
                    "sentence": "Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair.",
                    "sentence_kor": "그러나 인간은 익숙하지 않은 이미지-지시 쌍이 제시되더라도 여전히 이러한 편집 작업을 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "328-3",
                    "sentence": "Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already.",
                    "sentence_kor": "그러한 능력은 반실제적 사고, 즉 이미 일어난 사건들에 대한 가능한 대안을 생각하는 능력에서 비롯됩니다.",
                    "tag": "1"
                },
                {
                    "index": "328-4",
                    "sentence": "In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity.",
                    "sentence_kor": "본 논문에서, 우리는 데이터 부족을 극복하기 위해 반사실적 사고를 통합하는 자체 감독 반사실적 추론(SSCR) 프레임워크를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "328-5",
                    "sentence": "SSCR allows the model to consider out-of-distribution instructions paired with previous images.",
                    "sentence_kor": "SSCR을 통해 모델은 이전 이미지와 쌍으로 구성된 분포 이탈 명령을 고려할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "328-6",
                    "sentence": "With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario.",
                    "sentence_kor": "교차 작업 일관성(CTC)의 도움을 받아 이러한 반사실적 지침을 자체 감독 시나리오에서 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "328-7",
                    "sentence": "Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw).",
                    "sentence_kor": "광범위한 결과에 따르면 SSCR은 객체 정체성과 위치 측면에서 IBIE의 정확성을 개선하여 두 개의 IBLIE 데이터 세트(i-CLEVR 및 CoDraw)에 새로운 기술(SOTA)를 확립한다.",
                    "tag": "4"
                },
                {
                    "index": "328-8",
                    "sentence": "Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.",
                    "sentence_kor": "훈련 데이터의 50%만 있어도 SSCR은 전체 데이터 사용과 유사한 결과를 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "329",
            "abstractID": "EMNLP_abs-329",
            "text": [
                {
                    "index": "329-0",
                    "sentence": "It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer.",
                    "sentence_kor": "다국어 BERT(mBERT)는 고품질 다국어 표현을 제공하고 효과적인 제로샷 전송을 가능하게 하는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "329-1",
                    "sentence": "This is surprising given that mBERT does not use any crosslingual signal during training.",
                    "sentence_kor": "훈련 중에 mBERT가 교차 언어 신호를 사용하지 않는다는 점을 고려하면 이는 놀라운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "329-2",
                    "sentence": "While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure.",
                    "sentence_kor": "최근 문헌들이 이 현상을 연구해 왔지만, 다언어성의 이유는 여전히 다소 불분명하다.",
                    "tag": "1"
                },
                {
                    "index": "329-3",
                    "sentence": "We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual.",
                    "sentence_kor": "우리는 BERT의 구조적 특성과 BERT가 다국어가 되는 데 필요한 언어의 언어적 특성을 식별하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "329-4",
                    "sentence": "To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data.",
                    "sentence_kor": "빠른 실험을 위해 합성 데이터와 자연 데이터의 혼합에 대해 훈련된 소규모 BERT 모델을 통한 효율적인 설정을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "329-5",
                    "sentence": "Overall, we identify four architectural and two linguistic elements that influence multilinguality.",
                    "sentence_kor": "전반적으로, 우리는 다국어에 영향을 미치는 네 가지 건축적 요소와 두 가지 언어적 요소를 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "329-6",
                    "sentence": "Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment.",
                    "sentence_kor": "우리의 통찰력을 바탕으로 VecMap, 즉 비지도 임베딩 정렬을 사용하여 마스킹 전략을 수정하는 다국어 사전 훈련 설정을 실험한다.",
                    "tag": "3"
                },
                {
                    "index": "329-7",
                    "sentence": "Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.",
                    "sentence_kor": "3개 언어로 XNLI를 실험한 결과, 우리의 연구 결과가 소규모 설정에서 대규모 설정으로 전환되었음을 알 수 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "330",
            "abstractID": "EMNLP_abs-330",
            "text": [
                {
                    "index": "330-0",
                    "sentence": "Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages.",
                    "sentence_kor": "현대의 다국어 모델은 각 언어에 대한 이점(긍정적 전송)을 부여하기 위해 여러 언어의 연결된 텍스트에 대해 훈련되며, 가장 뚜렷한 이점은 저자원 언어에 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "330-1",
                    "sentence": "However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference.",
                    "sentence_kor": "그러나 최근 연구는 이 접근 방식이 고자원 언어에서 성능을 저하시킬 수 있다는 것을 보여주었다. 이는 부정적인 간섭으로 알려진 현상이다.",
                    "tag": "1"
                },
                {
                    "index": "330-2",
                    "sentence": "In this paper, we present the first systematic study of negative interference.",
                    "sentence_kor": "본 논문에서, 우리는 부정적인 간섭에 대한 첫 번째 체계적인 연구를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "330-3",
                    "sentence": "We show that, contrary to previous belief, negative interference also impacts low-resource languages.",
                    "sentence_kor": "우리는 이전의 믿음과는 달리, 부정적인 간섭이 저자원 언어에도 영향을 미친다는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "330-4",
                    "sentence": "While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference.",
                    "sentence_kor": "매개변수는 언어-범용 구조를 학습하기 위해 최대한 공유되지만, 언어-특정 매개변수가 다국어 모델에 존재하며 부정적인 간섭의 잠재적 원인이라는 것을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "330-5",
                    "sentence": "Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers’ generalization on all languages.",
                    "sentence_kor": "이러한 관찰에 의해 동기 부여되어, 우리는 또한 언어별 계층을 메타 매개 변수로 추가하고 모든 언어에 대한 공유 계층의 일반화를 명시적으로 개선하는 방식으로 훈련함으로써 언어 간 전송 가능성을 높이고 부정적인 간섭을 완화하는 메타 학습 알고리즘을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "330-6",
                    "sentence": "Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.",
                    "sentence_kor": "전반적으로, 우리의 결과는 부정적인 간섭이 이전에 알려진 것보다 더 흔하다는 것을 보여주며, 다국어 표현을 개선하기 위한 새로운 방향을 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "331",
            "abstractID": "EMNLP_abs-331",
            "text": [
                {
                    "index": "331-0",
                    "sentence": "Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space.",
                    "sentence_kor": "교차 언어 단어 임베딩(CWE) 알고리즘은 통일된 벡터 공간에서 여러 언어로 단어를 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "331-1",
                    "sentence": "Multi-Word Expressions (MWE) are common in every language.",
                    "sentence_kor": "MWE(Multi-Word Expression)는 모든 언어에서 공통적입니다.",
                    "tag": "1"
                },
                {
                    "index": "331-2",
                    "sentence": "When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs.",
                    "sentence_kor": "단어 임베딩을 훈련할 때, MWE의 각 구성 요소는 고유한 임베딩을 얻으므로 MWE는 CWE에 의해 번역되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "331-3",
                    "sentence": "We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings.",
                    "sentence_kor": "우리는 10개 언어로 MWE를 영어로 또는 영어로 단어 번역하기 위한 간단한 방법을 제안한다. 우리는 먼저 각 언어의 MWE 목록을 편집한 다음 단어 임베딩을 훈련하기 전에 MWE를 단일 토큰으로 토큰화한다.",
                    "tag": "2+3"
                },
                {
                    "index": "331-4",
                    "sentence": "CWEs are trained on a word-translation task using the dictionaries that only contain single words.",
                    "sentence_kor": "CWE는 단일 단어만 포함하는 사전을 사용하여 단어 번역 작업에 대해 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "331-5",
                    "sentence": "In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language.",
                    "sentence_kor": "MWE 번역을 평가하기 위해 다국어 WordNet에서 단일 토큰 단어와 MWE를 포함하는 이중 언어 단어 목록을 작성했으며, 가장 중요한 것은 다른 언어의 단일 단어에 해당하는 MWE를 포함한다는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "331-6",
                    "sentence": "We release these dictionaries to the research community.",
                    "sentence_kor": "우리는 이 사전들을 연구 단체에 배포한다.",
                    "tag": "3"
                },
                {
                    "index": "331-7",
                    "sentence": "We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE.",
                    "sentence_kor": "MWE를 단일 토큰으로 사전 토큰화하는 것이 MWE 개별 토큰의 임베딩 평균보다 더 나은 성능을 발휘한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "331-8",
                    "sentence": "We can translate MWEs at a top-10 precision of 30-60%.",
                    "sentence_kor": "우리는 MWE를 30-60%의 정밀도로 번역할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "331-9",
                    "sentence": "The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.",
                    "sentence_kor": "MWE의 토큰화는 훈련 말뭉치에서 단일 단어의 발생을 더 희박하게 만들지만, 우리는 그것이 단일 단어 번역에 부정적인 영향을 미치지 않는다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "332",
            "abstractID": "EMNLP_abs-332",
            "text": [
                {
                    "index": "332-0",
                    "sentence": "We propose a novel adapter layer formalism for adapting multilingual models.",
                    "sentence_kor": "다국어 모델을 채택하기 위한 새로운 어댑터 계층 형식주의를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "332-1",
                    "sentence": "They are more parameter-efficient than existing adapter layers while obtaining as good or better performance.",
                    "sentence_kor": "기존 어댑터 레이어보다 매개 변수 효율이 높으면서도 성능이 우수하거나 더 우수합니다.",
                    "tag": "4"
                },
                {
                    "index": "332-2",
                    "sentence": "The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs.",
                    "sentence_kor": "계층은 (2개 국어 어댑터와 반대로) 하나의 언어에 한정되어 있으며, 계층을 구성하고 보이지 않는 언어 쌍으로 일반화할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "332-3",
                    "sentence": "In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",
                    "sentence_kor": "이 제로샷 설정에서, 그들은 TED 강연에서 훈련된 강력한 20개 언어 다국어 트랜스포머 기준선에 비해 +2.77 BLEU 포인트의 중앙값을 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "333",
            "abstractID": "EMNLP_abs-333",
            "text": [
                {
                    "index": "333-0",
                    "sentence": "Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation.",
                    "sentence_kor": "다국어 BERT(mBERT), XLM-RoBERTa(XLR) 및 기타 비지도 다국어 인코더는 언어 간 표현을 효과적으로 학습할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "333-1",
                    "sentence": "Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations.",
                    "sentence_kor": "Europarl 또는 MultiUN과 같은 비트엑스트를 기반으로 한 명시적 정렬 목표는 이러한 표현을 더욱 개선하는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "333-2",
                    "sentence": "However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages.",
                    "sentence_kor": "그러나 단어 수준 정렬은 종종 차선책이며 이러한 bitxts는 많은 언어에서 사용할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "333-3",
                    "sentence": "In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection.",
                    "sentence_kor": "본 논문에서 우리는 이러한 신호를 더 잘 활용할 수 있는 새로운 대조 정렬 목표를 제안하고 이러한 이전 정렬 방법을 정렬 데이터의 노이즈 소스인 OPUS 컬렉션의 무작위로 샘플링된 100만 쌍 하위 집합에 적용할 수 있는지 여부를 검토한다.",
                    "tag": "2"
                },
                {
                    "index": "333-4",
                    "sentence": "Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks.",
                    "sentence_kor": "또한 단일 모델 실행으로 단일 데이터 세트에 대한 결과를 보고하는 대신, 4개의 데이터 세트와 작업에 대해 서로 다른 시드를 사용한 다중 실행의 평균 및 표준 도출을 보고한다.",
                    "tag": "2"
                },
                {
                    "index": "333-5",
                    "sentence": "Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework.",
                    "sentence_kor": "우리의 보다 광범위한 분석에 따르면 우리의 새로운 목표는 이전 작업을 능가하는 반면, 전반적으로 이러한 방법은 보다 강력한 평가 프레임워크로 성능을 개선하지 못한다.",
                    "tag": "4"
                },
                {
                    "index": "333-6",
                    "sentence": "Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training.",
                    "sentence_kor": "또한 더 나은 기본 모델을 사용함으로써 얻을 수 있는 이점은 정렬 훈련의 이점을 상쇄할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "333-7",
                    "sentence": "These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",
                    "sentence_kor": "이러한 부정적인 결과는 이러한 방법을 평가할 때 더 주의를 기울여야 하며 명시적 정렬 목표를 적용할 때 한계를 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "334",
            "abstractID": "EMNLP_abs-334",
            "text": [
                {
                    "index": "334-0",
                    "sentence": "Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance.",
                    "sentence_kor": "언어 모델링(예: mBERT, XLM-R)을 통해 사전 훈련된 대규모 다국어 변압기(MMT)는 NLP에서 제로샷 언어 전송을 위한 기본 패러다임이 되어 타의 추종을 불허하는 전송 성능을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "334-1",
                    "sentence": "Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages.",
                    "sentence_kor": "그러나 현재의 평가는 (a) 충분히 큰 사전 훈련 말뭉치를 가진 언어로 이전하고 (b) 가까운 언어들 사이에서 그 효과를 검증한다.",
                    "tag": "1"
                },
                {
                    "index": "334-2",
                    "sentence": "In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages.",
                    "sentence_kor": "본 연구에서는 MMT를 사용한 다운스트림 언어 전송의 한계를 분석하여 교차 언어 단어 임베딩과 마찬가지로 리소스 임대 시나리오와 원격 언어에서 훨씬 덜 효과적이라는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "334-3",
                    "sentence": "Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining.",
                    "sentence_kor": "세 가지 하위 수준 작업(POS 태그 지정, 종속성 구문 분석, NER)과 두 가지 상위 수준 작업(NLI, QA)을 포함하는 우리의 실험은 전송 성능과 소스 언어와 대상 언어 간의 언어 근접성뿐만 아니라 MMT 사전 교육에 사용되는 대상 언어 말뭉치의 크기를 경험적으로 상관시킨다.",
                    "tag": "3"
                },
                {
                    "index": "334-4",
                    "sentence": "Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.",
                    "sentence_kor": "가장 중요한 것은 저렴한 퓨샷 전송(즉, 일부 대상 언어 인스턴스에 대한 추가 미세 조정)이 전반적으로 놀라울 정도로 효과적이어서 제한 제로샷 조건을 넘어서는 더 많은 연구 노력을 보장한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "335",
            "abstractID": "EMNLP_abs-335",
            "text": [
                {
                    "index": "335-0",
                    "sentence": "Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource.",
                    "sentence_kor": "신경 기계 변환은 고자원 조건에서 인상적인 결과를 달성하지만 입력 도메인이 저자원일 경우 성능이 저하되는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "335-1",
                    "sentence": "The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity).",
                    "sentence_kor": "관심 영역별로 별도의 모델을 채택하는 표준 관행은 품질 관점(도메인 이동에 따른 유연성)과 비용 관점(추가된 유지보수 및 추론 복잡성) 모두에서 실제로 잘 확장되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "335-2",
                    "sentence": "In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage.",
                    "sentence_kor": "본 논문에서, 우리는 추론 시간이나 메모리 사용량을 늘리지 않고 여러 도메인을 번역할 수 있는 단일 다중 도메인 신경 기계 변환 모델을 훈련하기 위한 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "335-3",
                    "sentence": "We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines.",
                    "sentence_kor": "우리는 이 모델이 강력한 다중 도메인 기준선에 비해 고자원 도메인과 저자원 도메인 모두에서 변환을 개선할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "335-4",
                    "sentence": "In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",
                    "sentence_kor": "또한, 우리가 제안한 모델은 훈련 중에 도메인 레이블을 알 수 없을 뿐만 아니라 노이즈가 많은 데이터 조건에서도 강력할 때 효과적이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "336",
            "abstractID": "EMNLP_abs-336",
            "text": [
                {
                    "index": "336-0",
                    "sentence": "We present an easy and efficient method to extend existing sentence embedding models to new languages.",
                    "sentence_kor": "기존 문장 임베딩 모델을 새로운 언어로 확장하는 쉽고 효율적인 방법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "336-1",
                    "sentence": "This allows to create multilingual versions from previously monolingual models.",
                    "sentence_kor": "이를 통해 이전의 단일 언어 모델에서 다국어 버전을 만들 수 있습니다.",
                    "tag": "2"
                },
                {
                    "index": "336-2",
                    "sentence": "The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence.",
                    "sentence_kor": "훈련은 번역된 문장이 원래 문장과 동일한 벡터 공간의 위치에 매핑되어야 한다는 생각에 기초한다.",
                    "tag": "3"
                },
                {
                    "index": "336-3",
                    "sentence": "We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model.",
                    "sentence_kor": "원본(단일 언어) 모델을 사용하여 원본 언어에 대한 문장 임베딩을 생성한 다음 원래 모델을 모방하도록 번역된 문장에 대한 새로운 시스템을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "336-4",
                    "sentence": "Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower.",
                    "sentence_kor": "다국어 문장 임베딩 훈련을 위한 다른 방법과 비교할 때, 이 접근법은 다음과 같은 몇 가지 장점을 가지고 있다. 상대적으로 샘플이 적은 기존 모델을 새로운 언어로 쉽게 확장할 수 있고 벡터 공간에 원하는 속성을 보장하기가 쉬우며 훈련을 위한 하드웨어 요구 사항이 더 낮다.",
                    "tag": "4"
                },
                {
                    "index": "336-5",
                    "sentence": "We demonstrate the effectiveness of our approach for 50+ languages from various language families.",
                    "sentence_kor": "우리는 다양한 언어군의 50개 이상의 언어에 대한 접근 방식의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "336-6",
                    "sentence": "Code to extend sentence embeddings models to more than 400 languages is publicly available.",
                    "sentence_kor": "문장 임베딩 모델을 400개 이상의 언어로 확장하는 코드를 공개적으로 사용할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "337",
            "abstractID": "EMNLP_abs-337",
            "text": [
                {
                    "index": "337-0",
                    "sentence": "We propose an efficient batching strategy for variable-length decoding on GPU architectures.",
                    "sentence_kor": "GPU 아키텍처에서 가변 길이 디코딩을 위한 효율적인 배치 전략을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "337-1",
                    "sentence": "During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates.",
                    "sentence_kor": "후보들이 휴리스틱에 따라 종료되거나 폐기될 때, 우리의 스트리밍 접근법은 선택된 후보들의 하위 집합을 진행하기 전에 배치를 주기적으로 \"재충전\"한다.",
                    "tag": "3"
                },
                {
                    "index": "337-2",
                    "sentence": "We apply our method to variable-width beam search on a state-of-the-art machine translation model.",
                    "sentence_kor": "우리는 우리의 방법을 최첨단 기계 변환 모델의 가변 폭 빔 검색에 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "337-3",
                    "sentence": "Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines’ BLEU.",
                    "sentence_kor": "이 방법은 고정 너비 빔 검색 기준선에 비해 런타임을 최대 71%, 가변 너비 기준선에 비해 17% 감소시키는 동시에 기준선의 BLEU와 일치시킨다.",
                    "tag": "4"
                },
                {
                    "index": "337-4",
                    "sentence": "Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.",
                    "sentence_kor": "마지막으로, 실험은 우리의 방법이 시맨틱 및 구문 분석과 같은 다른 도메인에서 디코딩 속도를 높일 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "338",
            "abstractID": "EMNLP_abs-338",
            "text": [
                {
                    "index": "338-0",
                    "sentence": "State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications.",
                    "sentence_kor": "최첨단 다국어 모델은 모델이 추론 시 보게 될 모든 언어를 포함하는 어휘에 의존하지만, 이러한 어휘를 생성하는 표준 방법은 대규모 다국어 애플리케이션에 이상적이지 않다.",
                    "tag": "1"
                },
                {
                    "index": "338-1",
                    "sentence": "In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies.",
                    "sentence_kor": "본 연구에서는 자동으로 파생된 여러 언어 클러스터의 별도로 훈련된 어휘를 결합하여 언어 간 하위 단어 공유와 언어별 어휘 간의 균형을 맞추는 다국어 어휘 생성을 위한 새로운 절차를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "338-2",
                    "sentence": "Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.",
                    "sentence_kor": "우리의 실험은 모델이나 데이터의 크기를 늘리지 않고 주요 다국어 벤치마크 작업 TyDi QA(+2.9 F1), XNLI(+2.1%), WikiAnn NER(+2.8 F1) 및 어휘 부족률 8 감소 요인에 대한 언어 전반의 개선을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "339",
            "abstractID": "EMNLP_abs-339",
            "text": [
                {
                    "index": "339-0",
                    "sentence": "Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance.",
                    "sentence_kor": "전략적 지식 공유가 다운스트림 작업 성과를 향상시키는 것으로 나타났기 때문에 작업 간에 공유할 내용을 배우는 것이 매우 중요한 주제가 되었다.",
                    "tag": "1"
                },
                {
                    "index": "339-1",
                    "sentence": "This is particularly important for multilingual applications, as most languages in the world are under-resourced.",
                    "sentence_kor": "세계 대부분의 언어가 자원이 부족하기 때문에 다국어 애플리케이션에 특히 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "339-2",
                    "sentence": "Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English.",
                    "sentence_kor": "여기서는 영어를 제외한 언어에 대한 데이터가 거의 없거나 전혀 없는 경우 여러 언어로 동시에 교육 모델을 설정하는 것을 고려합니다.",
                    "tag": "2"
                },
                {
                    "index": "339-3",
                    "sentence": "We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first.",
                    "sentence_kor": "우리는 메타 학습을 사용하여 이 어려운 설정에 접근할 수 있다는 것을 보여준다. 소스 언어 모델을 훈련하는 것 외에도, 다른 모델은 첫 번째 모델에 가장 유익한 훈련 인스턴스를 선택하는 방법을 배운다.",
                    "tag": "2"
                },
                {
                    "index": "339-4",
                    "sentence": "We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering).",
                    "sentence_kor": "다양한 자연어 이해 작업에 대해 표준 지도, 제로샷 교차 언어 및 퓨샷 교차 언어 설정을 사용하여 실험한다(자연어 추론, 질문 답변).",
                    "tag": "3"
                },
                {
                    "index": "339-5",
                    "sentence": "Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages.",
                    "sentence_kor": "우리의 광범위한 실험 설정은 총 15개 언어에 대한 메타 학습의 일관된 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "339-6",
                    "sentence": "We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset).",
                    "sentence_kor": "제로샷 및 퓨샷 NLI(MultiNLI 및 XNLI)와 QA(MLQA 데이터 세트)에 대한 최첨단 기술을 개선하였다.",
                    "tag": "4"
                },
                {
                    "index": "339-7",
                    "sentence": "A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.",
                    "sentence_kor": "포괄적인 오류 분석은 언어 간의 유형적 특징의 상관관계가 메타 학습을 통해 학습된 매개변수 공유가 언제 유익한지를 부분적으로 설명할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "340",
            "abstractID": "EMNLP_abs-340",
            "text": [
                {
                    "index": "340-0",
                    "sentence": "We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification.",
                    "sentence_kor": "다국어 텍스트 분류를 위한 대규모 아마존 리뷰 모음인 다국어 아마존 리뷰 코퍼스(MARC)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "340-1",
                    "sentence": "The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019.",
                    "sentence_kor": "말뭉치는 2015년과 2019년 사이에 수집된 영어, 일본어, 독일어, 프랑스어, 스페인어 및 중국어로 된 리뷰를 포함하고 있다.",
                    "tag": "3"
                },
                {
                    "index": "340-2",
                    "sentence": "Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.)",
                    "sentence_kor": "데이터 세트의 각 레코드에는 리뷰 텍스트, 리뷰 제목, 스타 등급, 익명화된 리뷰어 ID, 익명화된 제품 ID 및 거친 제품 범주(예: '책', '첨부' 등)가 포함됩니다.",
                    "tag": "3"
                },
                {
                    "index": "340-3",
                    "sentence": "The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language.",
                    "sentence_kor": "말뭉치는 5개의 가능한 별 등급에 걸쳐 균형을 이루므로, 각 등급은 각 언어 리뷰의 20%를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "340-4",
                    "sentence": "For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively.",
                    "sentence_kor": "언어별로 교육, 개발, 시험장 리뷰가 각각 20만 개, 5000개, 5000개다.",
                    "tag": "3"
                },
                {
                    "index": "340-5",
                    "sentence": "We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data.",
                    "sentence_kor": "리뷰 데이터에 대한 다국어 BERT 모델을 미세 조정하여 지도 텍스트 분류 및 제로샷 언어 간 전송 학습에 대한 기준 결과를 보고한다.",
                    "tag": "3"
                },
                {
                    "index": "340-6",
                    "sentence": "We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.",
                    "sentence_kor": "우리는 MAE가 등급의 순서적 특성을 설명하기 때문에 이 작업에 대한 분류 정확도 대신 평균 절대 오차(MAE)를 사용할 것을 제안한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "341",
            "abstractID": "EMNLP_abs-341",
            "text": [
                {
                    "index": "341-0",
                    "sentence": "We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing.",
                    "sentence_kor": "우리는 담화 표현 구조 구문 분석에 대한 성능을 향상시키기 위해 문자 수준과 상황별 언어 모델 표현을 결합한다.",
                    "tag": "2+3"
                },
                {
                    "index": "341-1",
                    "sentence": "Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets.",
                    "sentence_kor": "문자 표현은 하나의 인코더에서 시퀀스 투 시퀀스 모델 또는 완전히 별개의 인코더로 쉽게 추가할 수 있으며, 다른 언어 모델, 언어 및 데이터 세트에 강력한 개선 기능을 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "341-2",
                    "sentence": "For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings.",
                    "sentence_kor": "영어의 경우, 이러한 향상은 언어 정보의 개별 소스를 추가하거나 상황에 맞지 않는 임베딩을 추가하는 것보다 더 크다.",
                    "tag": "3"
                },
                {
                    "index": "341-3",
                    "sentence": "A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.",
                    "sentence_kor": "의미 태그를 기반으로 한 새로운 분석 방법은 문자 수준 표현이 선택된 의미 현상의 하위 집합에 걸쳐 성능을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "342",
            "abstractID": "EMNLP_abs-342",
            "text": [
                {
                    "index": "342-0",
                    "sentence": "Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment.",
                    "sentence_kor": "질병의 지식은 징후와 증상, 진단과 치료와 같은 질병의 다양한 측면에 대한 정보를 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "342-1",
                    "sentence": "This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition.",
                    "sentence_kor": "이 질병 지식은 소비자 건강 질문 답변, 의료 언어 추론 및 질병 이름 인식을 포함한 많은 건강 관련 및 생물의학 작업에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "342-2",
                    "sentence": "While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects.",
                    "sentence_kor": "BERT와 같은 사전 훈련된 언어 모델은 텍스트에서 구문, 의미 및 세계 지식을 획득하는 데 성공했지만, 증상, 진단, 치료 및 기타 질병 측면의 지식과 같은 특정 정보에 의해 더욱 보완될 수 있다는 것을 발견했다.",
                    "tag": "2"
                },
                {
                    "index": "342-3",
                    "sentence": "Hence, we integrate BERT with disease knowledge for improving these important tasks.",
                    "sentence_kor": "따라서 이러한 중요한 작업을 개선하기 위해 BERT와 질병 지식을 통합한다.",
                    "tag": "2"
                },
                {
                    "index": "342-4",
                    "sentence": "Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT.",
                    "sentence_kor": "특히, 우리는 새로운 질병 지식 주입 훈련 절차를 제안하고 BERT, BioBERT, SciBERT, ClinicalB를 포함한 BERT 모델 제품군에 대해 평가한다.ERT, BlueBERT 및 Albert.",
                    "tag": "2"
                },
                {
                    "index": "342-5",
                    "sentence": "Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion.",
                    "sentence_kor": "세 가지 작업에 대한 실험은 이러한 모델이 거의 모든 경우에 강화될 수 있다는 것을 보여줌으로써 질병 지식 주입의 실행 가능성을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "342-6",
                    "sentence": "For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets.",
                    "sentence_kor": "예를 들어 소비자 건강 질문 답변에 대한 BioBERT의 정확도는 68.29%에서 72.09%로 향상되는 반면 새로운 SOTA 결과는 두 데이터 세트에서 관찰된다.",
                    "tag": "4"
                },
                {
                    "index": "342-7",
                    "sentence": "We make our data and code freely available.",
                    "sentence_kor": "우리는 데이터와 코드를 자유롭게 사용할 수 있도록 합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "343",
            "abstractID": "EMNLP_abs-343",
            "text": [
                {
                    "index": "343-0",
                    "sentence": "Natural language understanding involves reading between the lines with implicit background knowledge.",
                    "sentence_kor": "자연어 이해는 암묵적인 배경지식을 가지고 행간을 읽는 것을 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "343-1",
                    "sentence": "Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge.",
                    "sentence_kor": "현재 시스템은 세계 지식의 유일한 암묵적 출처로 사전 훈련된 언어 모델에 의존하거나, 추가적인 관련 지식을 통합하기 위해 외부 지식 기반(KB)에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "343-2",
                    "sentence": "We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks.",
                    "sentence_kor": "우리는 객관식 상식적인 과제에 대한 새로운 대안으로 자기 대화에 기초한 감독되지 않은 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "343-3",
                    "sentence": "Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as “what is the definition of...” to discover additional background knowledge.",
                    "sentence_kor": "조사 기반 발견 학습(Bruner, 1961년)에서 영감을 얻은 우리의 접근법은 추가적인 배경 지식을 발견하기 위해 \"...의 정의란 무엇인가\"와 같은 질문을 추구하는 많은 정보들로 언어 모델을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "343-4",
                    "sentence": "Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs.",
                    "sentence_kor": "경험적 결과는 셀프 토크 절차가 상식 벤치마크 6개 중 4개에서 제로샷 언어 모델 기준선의 성능을 크게 개선하고 외부 KB로부터 지식을 얻는 모델과 경쟁한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "343-5",
                    "sentence": "While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.",
                    "sentence_kor": "우리의 접근 방식이 여러 벤치마크의 성능을 향상시키지만, 정답으로 이어질 때조차 셀프 토크 유도 지식은 인간 심판들에 의해 항상 유용한 것으로 보여지는 것은 아니며, 상식적인 추론을 위한 사전 훈련된 언어 모델의 내부 작업에 대한 흥미로운 질문을 제기한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "344",
            "abstractID": "EMNLP_abs-344",
            "text": [
                {
                    "index": "344-0",
                    "sentence": "We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”).",
                    "sentence_kor": "우리는 절차적 사건 사이의 두 가지 유형의 관계에 대한 일련의 추론 작업을 제안한다. 목표 단계 관계(\"학습 포즈\"는 \"요가를 하는\" 더 큰 목표의 한 단계이다\")와 단계 단계 단계 시간 관계(\"요가 매트 구입\"은 일반적으로 \"학습 포즈\"보다 우선한다).",
                    "tag": "2"
                },
                {
                    "index": "344-1",
                    "sentence": "We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles.",
                    "sentence_kor": "우리는 교육 방법 기사 웹사이트인 wikiHow를 기반으로 이 두 가지 관계를 대상으로 한 데이터 세트를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "344-2",
                    "sentence": "Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance.",
                    "sentence_kor": "인간이 검증한 테스트 세트는 최첨단 변압기 모델의 성능과 인간 성능 사이의 격차가 약 10%에서 20%로 상식 추론을 위한 신뢰할 수 있는 벤치마크 역할을 한다.",
                    "tag": "3"
                },
                {
                    "index": "344-3",
                    "sentence": "Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.",
                    "sentence_kor": "자동으로 생성된 교육 세트를 통해 모델은 제로샷 및 퓨샷 설정에서 SWAG, 스닙스 및 스토리 클로즈 테스트에 대한 성능이 크게 향상되어 절차 이벤트에 대한 지식이 필요한 도메인 외 작업으로 효과적으로 전송할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "345",
            "abstractID": "EMNLP_abs-345",
            "text": [
                {
                    "index": "345-0",
                    "sentence": "Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts.",
                    "sentence_kor": "인간은 최소한의 경험으로부터 단어에 대한 구조적 특성을 배울 수 있고, 학습된 구문적 표현을 다른 문법적 맥락에서 균일하게 배치할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "345-1",
                    "sentence": "We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes.",
                    "sentence_kor": "우리는 영어로 이러한 행동을 재현하는 현대 신경 언어 모델의 능력을 평가하고 학습 성과에 대한 구조적 감독의 영향을 평가한다.",
                    "tag": "2"
                },
                {
                    "index": "345-2",
                    "sentence": "First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training.",
                    "sentence_kor": "첫째, 훈련 중에 적게는 두 번 보이는 토큰에 대한 모델의 구문 공칭 수와 구두 인수 구조 일반화를 조사하는 제어된 실험을 개발하여 퓨샷 학습 능력을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "345-3",
                    "sentence": "Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence).",
                    "sentence_kor": "둘째, 학습된 표현의 불변성 특성, 즉 기본 컨텍스트(예: 단순한 선언적 능동-음성 문장)에서 변환된 컨텍스트(예: 질문형 문장)로 구문 일반화를 전송하는 모델의 능력을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "345-4",
                    "sentence": "We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision.",
                    "sentence_kor": "동일한 데이터 세트에 대해 훈련된 4가지 모델, 즉 n그램 기준선, LSTM 및 명시적 구조적 감독을 통해 훈련된 2개의 LSTM 변수를 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "345-5",
                    "sentence": "We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model.",
                    "sentence_kor": "우리는 대부분의 경우 신경 모델이 훈련 중 단 두 가지 예에서 최소한의 노출 후에 적절한 구문 일반화를 유도할 수 있으며, 구조적으로 감독되는 두 모델이 LSTM 모델보다 더 정확하게 일반화된다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "345-6",
                    "sentence": "All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.",
                    "sentence_kor": "모든 신경 모델은 기본 컨텍스트에서 학습된 정보를 활용하여 변환된 컨텍스트에서 기대를 유도할 수 있으며, 이는 구문의 일부 불변 특성을 학습했음을 나타낸다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "346",
            "abstractID": "EMNLP_abs-346",
            "text": [
                {
                    "index": "346-0",
                    "sentence": "When speakers describe an image, they tend to look at objects before mentioning them.",
                    "sentence_kor": "화자들이 이미지를 묘사할 때, 그들은 그것들을 언급하기 전에 사물들을 보는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "346-1",
                    "sentence": "In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally.",
                    "sentence_kor": "본 논문에서 우리는 이미지 설명 생성 프로세스를 계산적으로 모델링하여 이러한 순차적 교차 모드 정렬을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "346-2",
                    "sentence": "We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production.",
                    "sentence_kor": "우리는 최첨단 이미지 캡션 시스템을 출발점으로 삼고 언어 제작 중에 기록된 인간의 시선 패턴의 정보를 이용하는 몇 가지 모델 변형을 개발한다.",
                    "tag": "2"
                },
                {
                    "index": "346-3",
                    "sentence": "In particular, we propose the first approach to image description generation where visual processing is modelled sequentially.",
                    "sentence_kor": "특히, 시각적 처리가 순차적으로 모델링되는 이미지 설명 생성에 대한 첫 번째 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "346-4",
                    "sentence": "Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production.",
                    "sentence_kor": "우리의 실험과 분석은 시선 중심 주의를 활용하여 더 나은 설명을 얻을 수 있고 시선 양식을 언어 생성과 일치시키는 다른 방법을 비교함으로써 인간의 인지 과정을 조명할 수 있다는 것을 확인시켜 준다.",
                    "tag": "4"
                },
                {
                    "index": "346-5",
                    "sentence": "We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.",
                    "sentence_kor": "시선 데이터를 순차적으로 처리하면 특히 시선이 전용 반복 구성요소로 인코딩될 때 화자에 의해 생성된 설명과 더 잘 정렬되고, 더 다양하고, 더 자연스러운 설명이 도출된다는 것을 알게 된다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "347",
            "abstractID": "EMNLP_abs-347",
            "text": [
                {
                    "index": "347-0",
                    "sentence": "When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language.",
                    "sentence_kor": "VAE(Variational Autoencoder)는 효과적으로 훈련될 경우 강력한 생성 모델이자 자연어에 대한 효과적인 표현 학습 프레임워크가 될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "347-1",
                    "sentence": "In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space).",
                    "sentence_kor": "본 논문에서는 최초의 대규모 언어 VAE 모델 Optimus(유니버설 공간의 사전 훈련된 모델링을 통해 문장 구성)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "347-2",
                    "sentence": "A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks.",
                    "sentence_kor": "문장을 위한 보편적인 잠재 임베딩 공간은 먼저 대규모 텍스트 말뭉치에서 사전 교육된 다음 다양한 언어 생성 및 이해 작업에 대해 미세 조정된다.",
                    "tag": "3"
                },
                {
                    "index": "347-3",
                    "sentence": "Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors.",
                    "sentence_kor": "GPT-2와 비교하여 Optimus는 잠재 벡터를 사용하여 추상 수준에서 유도 언어 생성을 가능하게 한다.",
                    "tag": "4"
                },
                {
                    "index": "347-4",
                    "sentence": "Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure.",
                    "sentence_kor": "BERT와 비교하여 Optimus는 부드러운 잠재 공간 구조로 인해 저자원 언어 이해 작업에서 더 잘 일반화할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "347-5",
                    "sentence": "Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus.",
                    "sentence_kor": "광범위한 언어 작업에 대한 광범위한 실험 결과는 Optimus의 효과를 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "347-6",
                    "sentence": "It achieves new state-of-the-art on VAE language modeling benchmarks.",
                    "sentence_kor": "VAE 언어 모델링 벤치마크에서 새로운 최첨단 기술을 달성합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "348",
            "abstractID": "EMNLP_abs-348",
            "text": [
                {
                    "index": "348-0",
                    "sentence": "There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books.",
                    "sentence_kor": "생물의학 텍스트에 대해 사전 훈련된 언어 모델이 Wikipedia 및 Books와 같은 일반 도메인 텍스트 말뭉치에 대해 훈련된 언어 모델보다 생물의학 도메인 벤치마크에서 더 잘 수행된다는 것을 보여주는 생물의학 영역별 언어 모델의 유입이 있었다.",
                    "tag": "1"
                },
                {
                    "index": "348-1",
                    "sentence": "Yet, most works do not study the factors affecting each domain language application deeply.",
                    "sentence_kor": "그러나 대부분의 연구는 각 도메인 언어 응용 프로그램에 영향을 미치는 요인을 깊이 연구하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "348-2",
                    "sentence": "Additionally, the study of model size on domain-specific models has been mostly missing.",
                    "sentence_kor": "또한 도메인별 모델에 대한 모델 크기 연구는 대부분 누락되었다.",
                    "tag": "1"
                },
                {
                    "index": "348-3",
                    "sentence": "We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer.",
                    "sentence_kor": "하위 단어 어휘 세트, 모델 크기, 사전 훈련 말뭉치 및 도메인 전송과 같이 도메인 언어 애플리케이션의 성능에 영향을 미칠 수 있는 몇 가지 요인을 경험적으로 연구하고 평가한다.",
                    "tag": "2"
                },
                {
                    "index": "348-4",
                    "sentence": "We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications.",
                    "sentence_kor": "우리는 더 큰 도메인 말뭉치에서 훈련된 더 큰 BioMegatron 모델을 통해 벤치마크에서 일관된 개선을 보여줌으로써 도메인 언어 모델 애플리케이션에 대한 이해에 기여한다.",
                    "tag": "4"
                },
                {
                    "index": "348-5",
                    "sentence": "We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction.",
                    "sentence_kor": "우리는 질문 답변, 명명된 개체 인식 및 관계 추출의 표준 생물의학 NLP 벤치마크에 대해 이전의 최첨단(SOTA)에 비해 눈에 띄게 개선되었음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "348-6",
                    "sentence": "Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].",
                    "sentence_kor": "실험을 재현하기 위한 코드 및 체크포인트는 [github.com/NVIDIA/NeMo]에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "349",
            "abstractID": "EMNLP_abs-349",
            "text": [
                {
                    "index": "349-0",
                    "sentence": "Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization.",
                    "sentence_kor": "문서 및 담화 세분화는 텍스트를 구성요소로 분할하는 것과 관련된 두 가지 기본적인 NLP 작업으로, 정보 검색 또는 텍스트 요약과 같은 다운스트림 작업에 일반적으로 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "349-1",
                    "sentence": "In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets.",
                    "sentence_kor": "본 연구에서는 세 가지 변압기 기반 아키텍처를 제안하고 세 가지 표준 데이터 세트에 대해 이전에 제안된 접근 방식과 포괄적인 비교를 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "349-2",
                    "sentence": "We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases.",
                    "sentence_kor": "우리는 특히 모든 경우에 오류율을 큰 폭으로 감소시키는 새로운 최첨단 기술을 확립한다.",
                    "tag": "2"
                },
                {
                    "index": "349-3",
                    "sentence": "We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.",
                    "sentence_kor": "우리는 모델 크기를 추가로 분석하여 우수한 성능을 유지하면서 훨씬 더 적은 매개 변수로 모델을 구축하여 실제 애플리케이션을 용이하게 할 수 있다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "350",
            "abstractID": "EMNLP_abs-350",
            "text": [
                {
                    "index": "350-0",
                    "sentence": "Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.",
                    "sentence_kor": "mBERT(Devlin et al., 2019) 및 XLM-RoBERTa(Conneau et al., 2020a)와 같은 다국어 사전 훈련된 트랜스포머가 효과적인 언어 간 제로샷 전송을 가능하게 하는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "350-1",
                    "sentence": "In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning.",
                    "sentence_kor": "본 논문에서 우리는 아랍어 NLP 및 영어-아랍어 제로샷 전송 학습을 위해 특별히 설계된 GigaBERT라고 하는 맞춤형 이중 언어 BERT를 사전 교육한다.",
                    "tag": "2"
                },
                {
                    "index": "350-2",
                    "sentence": "We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction.",
                    "sentence_kor": "우리는 GigaB를 공부한다.명명된 개체 인식, 음성 부분 태그 지정, 인수 역할 라벨링 및 관계 추출의 네 가지 IE 작업에 걸쳐 제로 쇼트 전송에 대한 ERT의 효과.",
                    "tag": "2"
                },
                {
                    "index": "350-3",
                    "sentence": "Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings.",
                    "sentence_kor": "우리의 최고 모델은 감독 및 제로샷 전송 설정 모두에서 mBERT, XLM-RoBERTa 및 AraBERT(Antoun 등, 2020)를 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "350-4",
                    "sentence": "We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.",
                    "sentence_kor": "사전 교육 받은 모델을 https://github.com/lanwuwei/GigaBERT에서 공개했습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "351",
            "abstractID": "EMNLP_abs-351",
            "text": [
                {
                    "index": "351-0",
                    "sentence": "In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation.",
                    "sentence_kor": "본 연구에서는 콘텐츠 조정을 위한 새로운 언어 사전 훈련 모델 TNT(Text Normalization 기반 Transformer 사전 훈련)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "351-1",
                    "sentence": "Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion.",
                    "sentence_kor": "마스킹 전략과 텍스트 정규화에 영감을 받아, TNT는 텍스트 조작에서 일반적으로 볼 수 있는 4가지 작동 유형(대체, 전치, 삭제 및 삽입)에서 텍스트를 재구성하기 위해 변압기를 훈련시켜 언어 표현을 학습하도록 개발되었다.",
                    "tag": "3"
                },
                {
                    "index": "351-2",
                    "sentence": "Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery.",
                    "sentence_kor": "또한, 정규화는 운영 유형과 토큰 레이블의 예측을 모두 포함하므로 TNT는 마스킹된 단어 복구라는 표준 작업보다 더 어려운 작업에서 학습할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "351-3",
                    "sentence": "As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task.",
                    "sentence_kor": "결과적으로, 실험은 TNT가 혐오 음성 분류 작업에서 강력한 기준선을 능가한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "351-4",
                    "sentence": "Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.",
                    "sentence_kor": "추가적인 텍스트 정규화 실험과 사례 연구는 TNT가 철자 오류 수정에 대한 새로운 잠재적 접근법이라는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "352",
            "abstractID": "EMNLP_abs-352",
            "text": [
                {
                    "index": "352-0",
                    "sentence": "Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text.",
                    "sentence_kor": "워드 임베딩 모델은 일반적으로 분포 가설을 통해 단어의 의미를 포착할 수 있지만 텍스트에 나타나는 숫자의 숫자 속성을 포착하지는 못한다.",
                    "tag": "1"
                },
                {
                    "index": "352-1",
                    "sentence": "This leads to problems with numerical reasoning involving tasks such as question answering.",
                    "sentence_kor": "이는 질문 답변과 같은 작업과 관련된 수치 추론 문제로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "352-2",
                    "sentence": "We propose a new methodology to assign and learn embeddings for numbers.",
                    "sentence_kor": "우리는 숫자에 대한 임베딩을 할당하고 학습하는 새로운 방법론을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "352-3",
                    "sentence": "Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line.",
                    "sentence_kor": "우리의 접근 방식은 코사인 유사성이 숫자 선의 실제 거리를 반영하도록 숫자에 대해 결정론적 독립적 내장(모델을 DICE라고 함)을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "352-4",
                    "sentence": "DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition.",
                    "sentence_kor": "DICE는 (i) 숫자 및 크기를 캡처하는 능력 평가, (ii) 목록 최대값, 디코딩 및 추가를 수행하는 두 가지 작업의 여러 예에서 광범위한 사전 훈련된 단어 임베딩 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "352-5",
                    "sentence": "We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction.",
                    "sentence_kor": "우리는 규모 예측 작업에 대한 접근 방식을 사용하여 숫자를 초기화함으로써 다운스트림 작업에서 이러한 임베딩의 유용성을 추가로 탐구한다.",
                    "tag": "3"
                },
                {
                    "index": "352-6",
                    "sentence": "We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.",
                    "sentence_kor": "또한 상황별 설정에서 모델 기반 숫자의 임베딩을 학습하기 위한 정규화 접근 방식을 도입한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "353",
            "abstractID": "EMNLP_abs-353",
            "text": [
                {
                    "index": "353-0",
                    "sentence": "We conduct a large scale empirical investigation of contextualized number prediction in running text.",
                    "sentence_kor": "실행 중인 텍스트에서 상황별 숫자 예측에 대한 대규모 경험적 조사를 수행한다.",
                    "tag": "2"
                },
                {
                    "index": "353-1",
                    "sentence": "Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value within a sentence.",
                    "sentence_kor": "특히 (1)가면 번호 예측 – 문장 내 누락된 숫자 값 예측 – (2)수치 이상 탐지 – 문장 내 잘못된 숫자 값 검출 – 두 가지 작업을 고려한다.",
                    "tag": "3"
                },
                {
                    "index": "353-2",
                    "sentence": "We experiment with novel combinations of contextual encoders and output distributions over the real number line.",
                    "sentence_kor": "우리는 실숫선에 대한 상황별 인코더와 출력 분포의 새로운 조합을 실험한다.",
                    "tag": "2"
                },
                {
                    "index": "353-3",
                    "sentence": "Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures.",
                    "sentence_kor": "특히, 잠재 변수를 통합하여 표현성을 추가하고 실행 중인 텍스트에서 숫자 값의 자연스러운 분포를 더 잘 맞추고 이를 반복 임대 및 변압기 기반 인코더 아키텍처와 결합하는 출력 분포 매개 변수화 제품군을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "353-4",
                    "sentence": "We evaluate these models on two numeric datasets in the financial and scientific domain.",
                    "sentence_kor": "재무 및 과학 영역의 두 숫자 데이터 세트에서 이러한 모델을 평가한다.",
                    "tag": "2"
                },
                {
                    "index": "353-5",
                    "sentence": "Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection.",
                    "sentence_kor": "우리의 연구 결과는 이산 잠재 변수를 통합하고 여러 모드를 허용하는 출력 분포가 모든 데이터 세트에서 단순한 흐름 기반 분포보다 성능이 뛰어나 더 정확한 수치 사전 결정 및 이상 탐지를 산출한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "353-6",
                    "sentence": "We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.",
                    "sentence_kor": "또한 우리의 모델이 텍스트 컨텍스트를 효과적으로 활용하고 범용 비지도 사전 훈련의 이점을 제공한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "354",
            "abstractID": "EMNLP_abs-354",
            "text": [
                {
                    "index": "354-0",
                    "sentence": "Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors.",
                    "sentence_kor": "미디어에서 폭력, 성적 또는 약물 남용 콘텐츠에 노출되면 어린이와 청소년의 유사한 행동 모방 의지가 증가한다.",
                    "tag": "1"
                },
                {
                    "index": "354-1",
                    "sentence": "Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive.",
                    "sentence_kor": "시청각 단서로부터 위험 행동의 묘사를 식별하는 계산 방법은 수정 비용이 엄청나게 많이 들 수 있는 사후 제작 영화에 대한 적용 가능성이 제한적이다.",
                    "tag": "1"
                },
                {
                    "index": "354-2",
                    "sentence": "To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production.",
                    "sentence_kor": "이러한 한계를 해결하기 위해 영화 대본의 언어 사용에 따라 콘텐츠 등급을 추정하는 모델을 제안하여 크리에이티브 프로덕션의 초기 단계에서 솔루션을 사용할 수 있도록 합니다.",
                    "tag": "2"
                },
                {
                    "index": "354-3",
                    "sentence": "Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character’s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach.",
                    "sentence_kor": "우리 모델은 캐릭터의 언어 사용의 의미와 정서 측면으로부터 더 나은 영화 표현을 배우고 다중 작업 접근법에 따른 위험 행동의 공존을 활용하기 위해 새로운 기법을 채택함으로써 최첨단 기술을 크게 향상시킨다.",
                    "tag": "3"
                },
                {
                    "index": "354-4",
                    "sentence": "Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.",
                    "sentence_kor": "또한, 우리는 이러한 행동들의 공동 묘사와 그렇지 않으면 영화 제작자들이 이해하지 못할 수 있는 섬세함에 대한 새로운 통찰력을 배우는데 이 접근법이 어떻게 유용할 수 있는지 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "355",
            "abstractID": "EMNLP_abs-355",
            "text": [
                {
                    "index": "355-0",
                    "sentence": "Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures.",
                    "sentence_kor": "형태학적으로 풍부한 언어는 파이프라인 아키텍처와 비교하여 형태학과 구문의 공동 처리에서 이익을 얻는 것으로 보인다.",
                    "tag": "1"
                },
                {
                    "index": "355-1",
                    "sentence": "We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit.",
                    "sentence_kor": "산스크리트어로 공동 형태학적 구문 분석 및 종속성 구문 분석을 위한 그래프 기반 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "355-2",
                    "sentence": "Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways.",
                    "sentence_kor": "여기서는 산스크리트어로 여러 구조화된 예측 작업에 대해 제안된 에너지 기반 모델 프레임워크(Krishna et al., 2020)를 간단하지만 중요한 두 가지 방법으로 확장한다.",
                    "tag": "3"
                },
                {
                    "index": "355-3",
                    "sentence": "First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference.",
                    "sentence_kor": "첫째, 프레임워크의 기본 입력 그래프 생성 방법이 멀티그래프를 생성하도록 수정되어 정확한 검색 추론을 사용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "355-4",
                    "sentence": "Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit.",
                    "sentence_kor": "둘째, 우리는 산스크리트어의 전통적인 문법 분석에 뿌리를 둔 언어학적인 동기 부여 접근법을 사용하여 입력 검색 공간을 제거한다.",
                    "tag": "3"
                },
                {
                    "index": "355-5",
                    "sentence": "Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers.",
                    "sentence_kor": "우리의 실험은 공동 모델의 형태학적 파싱이 독립형 형태학적 파서보다 우수하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "355-6",
                    "sentence": "We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.",
                    "sentence_kor": "우리는 최첨단 결과를 독립 실행형(금 형태학적 태그 포함) 및 관절 형태학적 구문 분석 설정에서 모두 형태학적 구문 분석 및 의존성 구문 분석으로 보고한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "356",
            "abstractID": "EMNLP_abs-356",
            "text": [
                {
                    "index": "356-0",
                    "sentence": "We propose a method for unsupervised parsing based on the linguistic notion of a constituency test.",
                    "sentence_kor": "구성 테스트의 언어적 개념을 기반으로 비지도 구문 분석 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "356-1",
                    "sentence": "One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical).",
                    "sentence_kor": "구성 테스트의 한 유형은 일부 변환(예: 범위를 대명사로 대체)을 통해 문장을 수정한 다음 결과를 판단(예: 문법적인지 확인)하는 것을 포함한다.",
                    "tag": "3"
                },
                {
                    "index": "356-2",
                    "sentence": "Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.",
                    "sentence_kor": "이 아이디어에 자극을 받아, 우리는 일련의 변환을 지정하고 비지도 신경 수용성 모델을 사용하여 문법적 결정을 함으로써 비지도 분석기를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "356-3",
                    "sentence": "To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score.",
                    "sentence_kor": "주어진 문장을 만들기 위해, 우리는 구성 요소 테스트 판단을 집계하여 각 범위에 점수를 매기고, 가장 높은 총점을 가진 이진 트리를 선택한다.",
                    "tag": "3"
                },
                {
                    "index": "356-4",
                    "sentence": "While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model.",
                    "sentence_kor": "이 접근 방식은 이미 현재 방법 범위에서 성능을 달성했지만, 추정 트리 개선과 문법성 모델 개선을 번갈아 수행하는 정제 절차를 통해 문법성 모델을 미세 조정함으로써 정확도를 더욱 향상시킨다.",
                    "tag": "3"
                },
                {
                    "index": "356-5",
                    "sentence": "The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.",
                    "sentence_kor": "정제된 모델은 Penn Treebank 테스트 세트에서 62.8 F1을 달성하여 이전에 가장 잘 발표된 결과보다 7.6점이 절대적으로 향상되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "357",
            "abstractID": "EMNLP_abs-357",
            "text": [
                {
                    "index": "357-0",
                    "sentence": "The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers.",
                    "sentence_kor": "의존성 트리와 신장 트리 사이의 연결은 그래프 기반 의존성 파서를 훈련하고 디코딩하기 위해 NLP 커뮤니티에 의해 이용된다.",
                    "tag": "1"
                },
                {
                    "index": "357-1",
                    "sentence": "However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree.",
                    "sentence_kor": "그러나, NLP 문헌은 두 구조 사이의 중요한 차이를 놓쳤다. 즉, 의존성 트리의 루트로부터 한 가장자리만 방출될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "357-2",
                    "sentence": "We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.",
                    "sentence_kor": "우리는 범용 의존성 트리뱅크의 여러 언어에 대한 최첨단 파서의 출력을 분석했다. 이러한 파서는 종종 제약 조건을 위반하는 트리에 낮은 확률을 할당해야 한다는 것을 배울 수 있지만, 훈련 세트의 크기가 줄어들수록 그렇게 할 수 있는 능력은 놀랄 것도 없다.",
                    "tag": "2"
                },
                {
                    "index": "357-3",
                    "sentence": "In fact, the worst constraint-violation rate we observe is 24%.",
                    "sentence_kor": "사실, 우리가 관찰하는 최악의 제약 위반률은 24%입니다.",
                    "tag": "4"
                },
                {
                    "index": "357-4",
                    "sentence": "Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime.",
                    "sentence_kor": "이전 연구에서는 디코딩 런타임에 n의 계수를 추가하는 제약 조건을 적용하기 위한 비효율적인 알고리즘을 제안했다.",
                    "tag": "4"
                },
                {
                    "index": "357-5",
                    "sentence": "We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.",
                    "sentence_kor": "우리는 Gabow와 Tarjan(1984)으로 인해 알고리즘을 의존성 파싱에 적응시켜 원래 런타임을 손상시키지 않고 제약 조건을 충족시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "358",
            "abstractID": "EMNLP_abs-358",
            "text": [
                {
                    "index": "358-0",
                    "sentence": "We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario.",
                    "sentence_kor": "우리는 실제로 낮은 자원 시나리오에서 음성 부분 태그 지정에 대해 완전히 감독되지 않은 언어 간 전송 접근 방식을 설명한다.",
                    "tag": "2"
                },
                {
                    "index": "358-1",
                    "sentence": "We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available.",
                    "sentence_kor": "대상 언어와 POS 태그를 사용할 수 있는 하나 이상의 소스 언어 사이의 병렬 변환에 액세스할 수 있다고 가정한다.",
                    "tag": "3"
                },
                {
                    "index": "358-2",
                    "sentence": "We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages.",
                    "sentence_kor": "우리는 실험에서 성경을 병렬 데이터로 사용한다: 작은 크기, 영역 밖, 그리고 많은 다양한 언어를 포함한다.",
                    "tag": "3"
                },
                {
                    "index": "358-3",
                    "sentence": "Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology.",
                    "sentence_kor": "우리의 접근 방식은 세 가지 방식으로 혁신한다. 1) 감독되지 않은 유형 및 토큰 제약의 모범 사례, 예측된 POS의 단어 정렬 신뢰도와 밀도, 2) 상황별 단어 임베딩, 부착 및 h를 사용하는 Bi-LSTM 아키텍처 등을 활용하는 교차 언어 주석 투영을 통해 훈련 인스턴스를 선택하는 강력한 접근 방식이다.전두엽적 갈색 군집, 3) 언어군 및 형태학적 유형학 측면에서 12개 다양한 언어에 대한 평가.",
                    "tag": "3"
                },
                {
                    "index": "358-4",
                    "sentence": "In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work.",
                    "sentence_kor": "제한적이고 영역 밖 병렬 데이터를 사용함에도 불구하고, 우리의 실험은 이전 작업에 비해 정확도가 크게 향상되었음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "358-5",
                    "sentence": "In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.",
                    "sentence_kor": "또한 투영 또는 출력 조합을 통해 다중 소스 정보를 사용하면 대부분의 대상 언어의 성능이 향상된다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "359",
            "abstractID": "EMNLP_abs-359",
            "text": [
                {
                    "index": "359-0",
                    "sentence": "The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*.",
                    "sentence_kor": "내부 내부 재귀 자동 인코더(DIORA; Drozdov 등. 2019)는 *레이블링된 훈련 데이터에 액세스하지 않고* 입력 문장에 구문 트리 구조를 유도하는 방법을 학습하는 자가 감독 신경 모델이다.",
                    "tag": "1"
                },
                {
                    "index": "359-1",
                    "sentence": "In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing.",
                    "sentence_kor": "본 논문에서 우리는 DIORA가 문장의 가능한 모든 이진 트리를 소프트 다이내믹 프로그램으로 완전히 인코딩하지만, 벡터 평균화 접근 방식은 국부적으로 그리디하며 상향식 차트 구문 분석에서 가장 높은 점수를 받는 구문 분석 트리를 계산할 때 오류를 복구할 수 없다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "359-2",
                    "sentence": "To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart.",
                    "sentence_kor": "이 문제를 해결하기 위해 차트의 각 셀에 하드 아르그맥스 연산과 빔을 사용하여 트리의 부드러운 가중치가 아닌 단일 트리를 인코딩하는 개선된 변형인 S-DIORA를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "359-3",
                    "sentence": "Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.",
                    "sentence_kor": "우리의 실험은 새로운 알고리즘으로 사전 훈련된 DIORA를 통해 미세 조정에 사용된 데이터에 따라 영어 WSJ 펜 트리뱅크에서 *비감독* 구성 구문 분석에서 최첨단 기술을 2.2-6% F1 향상시킨다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "360",
            "abstractID": "EMNLP_abs-360",
            "text": [
                {
                    "index": "360-0",
                    "sentence": "Text classification is a critical research topic with broad applications in natural language processing.",
                    "sentence_kor": "텍스트 분류는 자연어 처리에 광범위하게 응용되는 중요한 연구 주제이다.",
                    "tag": "1"
                },
                {
                    "index": "360-1",
                    "sentence": "Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task.",
                    "sentence_kor": "최근 그래프 신경망(GNN)은 연구 커뮤니티에서 점점 더 많은 관심을 받고 있으며 이 표준 작업에 대한 유망한 결과를 입증했다.",
                    "tag": "1"
                },
                {
                    "index": "360-2",
                    "sentence": "Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents.",
                    "sentence_kor": "성공에도 불구하고, (1) 단어 간의 고차 상호 작용을 포착할 수 없음, (2) 대규모 데이터 세트와 새 문서를 처리하는 데 비효율적이기 때문에 실제로 성능이 크게 저하될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "360-3",
                    "sentence": "To address those issues, in this paper, we propose a principled model – hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning.",
                    "sentence_kor": "이러한 문제를 해결하기 위해 본 논문에서 우리는 텍스트 표현 학습을 위한 계산 소비량이 적은 상태에서 더 많은 표현력을 얻을 수 있는 원칙적인 모델인 하이퍼그래프 주의 네트워크(HyperGAT)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "360-4",
                    "sentence": "Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.",
                    "sentence_kor": "다양한 벤치마크 데이터 세트에 대한 광범위한 실험은 텍스트 분류 작업에 대해 제안된 접근 방식의 효과를 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "361",
            "abstractID": "EMNLP_abs-361",
            "text": [
                {
                    "index": "361-0",
                    "sentence": "We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model.",
                    "sentence_kor": "우리는 언어 모델의 학습된 매개 변수에서 실체에 대한 선언적 지식을 포착하는 문제에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "361-1",
                    "sentence": "We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text.",
                    "sentence_kor": "텍스트에 언급된 엔티티의 고유한 메모리에 액세스할 수 있는 새로운 모델인 전문가로서의 엔터티(EaE)를 소개합니다.",
                    "tag": "2"
                },
                {
                    "index": "361-2",
                    "sentence": "Unlike previous efforts to integrate entity knowledge into sequence models, EaE’s entity representations are learned directly from text.",
                    "sentence_kor": "엔티티 지식을 시퀀스 모델에 통합하려는 이전의 노력과는 달리, EaE의 엔티티 표현은 텍스트에서 직접 학습된다.",
                    "tag": "3"
                },
                {
                    "index": "361-3",
                    "sentence": "We show that EaE’s learned representations capture sufficient knowledge to answer TriviaQA questions such as “Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?”, outperforming an encoder-generator Transformer model with 10x the parameters on this task.",
                    "sentence_kor": "우리는 EaE의 학습된 표현이 \"어느 박사인가\"와 같은 트리비아QA 질문에 답하기에 충분한 지식을 포착한다는 것을 보여준다. 로저 델가도, 앤서니 애인리, 에릭 로버츠에 의해 연기된 악역은 이 작업에서 10배의 매개 변수를 가진 인코더-제너레이터 트랜스포머 모델을 능가했다.",
                    "tag": "4"
                },
                {
                    "index": "361-4",
                    "sentence": "According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE’s performance.",
                    "sentence_kor": "라마 지식 조사에 따르면, EaE는 실체 지식의 외부 소스를 통합하는 이전의 접근법뿐만 아니라 유사한 크기의 Bert보다 더 사실적인 지식을 포함하고 있다.EaE는 매개 변수를 특정 엔티티와 연관시키기 때문에 추론 시 매개 변수의 일부만 액세스하면 되며, 우리는 EaE의 성능에 실체의 정확한 식별과 표현이 필수적이라는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "362",
            "abstractID": "EMNLP_abs-362",
            "text": [
                {
                    "index": "362-0",
                    "sentence": "Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising.",
                    "sentence_kor": "라벨 평활은 분류에서 과적합을 방지하고 라벨 노이즈 제거에 도움이 되는 효과적인 정규화 전략인 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "362-1",
                    "sentence": "However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs.",
                    "sentence_kor": "그러나 이러한 방법을 기계 번역과 같은 seq2seq 설정으로 직접 확장하는 것은 어려운 일이다. 이러한 문제의 큰 대상 출력 공간은 가능한 모든 출력에 레이블 평활을 적용하기 어렵게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "362-2",
                    "sentence": "Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence.",
                    "sentence_kor": "seq2seq 설정에 대한 대부분의 기존 접근 방식은 토큰 수준 평활을 수행하거나 대상 시퀀스에서 토큰을 임의로 대체하여 생성된 시퀀스를 평활합니다.",
                    "tag": "2"
                },
                {
                    "index": "362-3",
                    "sentence": "Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar.",
                    "sentence_kor": "이러한 작업과 달리 본 논문에서 우리는 대상 시퀀스와 충분한 n그램이 중복될 뿐만 아니라 의미론적으로 유사한 잘 형성된 관련 시퀀스를 부드럽게 하는 기법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "362-4",
                    "sentence": "Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.",
                    "sentence_kor": "우리의 방법은 서로 다른 데이터 세트의 최첨단 기술에 비해 일관되고 상당한 개선을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "363",
            "abstractID": "EMNLP_abs-363",
            "text": [
                {
                    "index": "363-0",
                    "sentence": "Most recent improvements in NLP come from changes to the neural network architectures modeling the text input.",
                    "sentence_kor": "NLP의 가장 최근의 개선은 텍스트 입력을 모델링하는 신경망 아키텍처의 변경에서 비롯된다.",
                    "tag": "1"
                },
                {
                    "index": "363-1",
                    "sentence": "Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging.",
                    "sentence_kor": "그러나 최첨단 모델은 종종 레이블 공간을 모델링하기 위한 간단한 접근법에 의존한다. 예를 들어 시퀀스 태깅의 빅램 조건부 랜덤 필드(CRF)가 그것이다.",
                    "tag": "1"
                },
                {
                    "index": "363-2",
                    "sentence": "More expressive graphical models are rarely used due to their prohibitive computational cost.",
                    "sentence_kor": "표현력이 뛰어난 그래픽 모델은 엄청난 계산 비용 때문에 거의 사용되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "363-3",
                    "sentence": "In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling.",
                    "sentence_kor": "본 연구에서는 Gibbs 샘플링을 기반으로 그래픽 모델과 신경망의 하이브리드를 효율적으로 훈련하고 디코딩하기 위한 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "363-4",
                    "sentence": "Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging.",
                    "sentence_kor": "우리의 접근 방식은 SampleRank(Wick et al., 2011)를 신경 모델에 자연스럽게 적용하는 것이며 시퀀스 태깅을 넘어 작업에 광범위하게 적용할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "363-5",
                    "sentence": "We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical.",
                    "sentence_kor": "우리는 명명된 개체 인식에 우리의 접근 방식을 적용하고 정확한 추론이 비현실적인 신경 스킵 체인 CRF 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "363-6",
                    "sentence": "The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03.",
                    "sentence_kor": "건너뛰기 체인 모델은 CoNLL-02/03의 3개 언어에 대한 강력한 기준선에 비해 개선된다.",
                    "tag": "2"
                },
                {
                    "index": "363-7",
                    "sentence": "We obtain new state-of-the-art results on Dutch.",
                    "sentence_kor": "우리는 네덜란드어로 새로운 최첨단 결과를 얻었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "364",
            "abstractID": "EMNLP_abs-364",
            "text": [
                {
                    "index": "364-0",
                    "sentence": "Text alignment finds application in tasks such as citation recommendation and plagiarism detection.",
                    "sentence_kor": "텍스트 정렬은 인용 권장 사항 및 표절 검색과 같은 작업에서 응용 프로그램을 찾습니다.",
                    "tag": "1"
                },
                {
                    "index": "364-1",
                    "sentence": "Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels.",
                    "sentence_kor": "기존 정렬 방법은 사전 정의된 단일 수준에서 작동하며 문장 및 문서 수준에서 텍스트를 정렬하는 방법을 배울 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "364-2",
                    "sentence": "We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document).",
                    "sentence_kor": "우리는 문서를 문서 간 주의 구성요소로 표현하기 위해 이전에 확립된 계층적 주의 인코더를 갖추어 서로 다른 수준(문서 대 문서 및 문장 대 문서)에 걸친 구조적 비교를 가능하게 하는 새로운 학습 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "364-3",
                    "sentence": "Our component is weakly supervised from document pairs and can align at multiple levels.",
                    "sentence_kor": "우리의 구성요소는 문서 쌍에서 약하게 감독되며 여러 수준에서 정렬할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "364-4",
                    "sentence": "Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.",
                    "sentence_kor": "인용 권고 및 표절 감지 작업에 대한 문서 대 문서 관계 및 문장 대 문서 관계를 예측한 우리의 평가는 우리의 접근 방식이 구조적 상관 관계를 알지 못하는 반복 및 변압기 상황화에 기초한 이전에 확립된 계층적 주의 인코더보다 우수하다는 것을 보여준다.서류 사이의 고민",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "365",
            "abstractID": "EMNLP_abs-365",
            "text": [
                {
                    "index": "365-0",
                    "sentence": "This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks.",
                    "sentence_kor": "본 논문은 사전 훈련된 언어 모델을 조사하여 어떤 모델이 본질적으로 작업 지향 대화 작업에 가장 유용한 표현을 수행하는지 알아낸다.",
                    "tag": "2"
                },
                {
                    "index": "365-1",
                    "sentence": "We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe.",
                    "sentence_kor": "우리는 감독 분류기 프로브와 감독되지 않은 상호 정보 프로브라는 두 가지 측면에서 문제에 접근한다.",
                    "tag": "3"
                },
                {
                    "index": "365-2",
                    "sentence": "We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way.",
                    "sentence_kor": "주석이 달린 레이블이 있는 고정 사전 교육 언어 모델 위에 있는 분류자 프로브로서 피드 포워드 레이어를 감독 방식으로 미세 조정한다.",
                    "tag": "3"
                },
                {
                    "index": "365-3",
                    "sentence": "Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering.",
                    "sentence_kor": "한편, 우리는 실제 클러스터링과 표현 클러스터링 사이의 상호 의존성을 평가하기 위해 감독되지 않은 상호 정보 시도를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "365-4",
                    "sentence": "The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.",
                    "sentence_kor": "이 경험적 논문의 목표는 1) 특히 감독되지 않은 상호 정보 측면의 탐색 기법 조사, 2) 대화 연구 커뮤니티에 사전 훈련된 언어 모델 선택 지침 제공, 3) 성공의 열쇠가 될 수 있는 대화 적용을 위한 사전 훈련 요소에 대한 통찰력을 찾는 것이다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "366",
            "abstractID": "EMNLP_abs-366",
            "text": [
                {
                    "index": "366-0",
                    "sentence": "Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks.",
                    "sentence_kor": "목표 지향 대화 시스템의 맥락에서 자연어 이해(NLU)에는 일반적으로 의도 분류 및 슬롯 라벨링 작업이 포함된다.",
                    "tag": "1"
                },
                {
                    "index": "366-1",
                    "sentence": "Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors.",
                    "sentence_kor": "NLU 시스템을 새로운 언어로 확장하는 기존 방법은 슬롯 레이블 투영을 소스로부터 번역된 발화까지 포함한 기계 번역을 사용하므로 투영 오류에 민감하다.",
                    "tag": "1"
                },
                {
                    "index": "366-2",
                    "sentence": "In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer.",
                    "sentence_kor": "본 연구에서는 언어 간 전송을 위해 대상 슬롯 레이블을 공동으로 정렬하고 예측하는 방법을 배우는 새로운 종단 간 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "366-3",
                    "sentence": "We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus.",
                    "sentence_kor": "MultiAT를 소개합니다.다국어 ATIS 말뭉치를 4개 언어군에 걸쳐 9개 언어로 확장하고 말뭉치를 사용하여 우리의 방법을 평가하는 새로운 다국어 NLU 말뭉치인 IS++.",
                    "tag": "2"
                },
                {
                    "index": "366-4",
                    "sentence": "Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time.",
                    "sentence_kor": "결과에 따르면 우리의 방법은 대부분의 언어에서 고속 정렬을 사용하는 간단한 레이블 투영 방법을 능가하며, 교육 시간의 절반만으로 더 복잡하고 최첨단 투영 방법에 비해 경쟁력 있는 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "366-5",
                    "sentence": "We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.",
                    "sentence_kor": "MultiAT를 출시언어 간 NLU에 대한 향후 연구를 계속하기 위해 IS++ 말뭉치.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "367",
            "abstractID": "EMNLP_abs-367",
            "text": [
                {
                    "index": "367-0",
                    "sentence": "Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill.",
                    "sentence_kor": "의도 감지는 목표 지향 대화 시스템의 핵심 요소 중 하나이며, OOS(Out of Scope) 요소를 감지하는 것도 실질적으로 중요한 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "367-1",
                    "sentence": "Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging.",
                    "sentence_kor": "퓨샷 학습은 데이터 부족을 완화하기 위해 많은 관심을 끌고 있지만, OO 탐지 작업은 더욱 어려워지고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "367-2",
                    "sentence": "In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention.",
                    "sentence_kor": "본 논문에서, 우리는 자기 관심이 깊은 단순하지만 효과적인 접근 방식, 차별적인 가장 가까운 이웃 분류를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "367-3",
                    "sentence": "Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input.",
                    "sentence_kor": "소프트맥스 분류기와 달리 BERT 스타일 쌍 인코딩을 활용하여 사용자 입력에 가장 적합한 훈련 예를 추정하는 이진 분류기를 훈련한다.",
                    "tag": "3"
                },
                {
                    "index": "367-4",
                    "sentence": "We propose to boost the discriminative ability by transferring a natural language inference (NLI) model.",
                    "sentence_kor": "우리는 자연어 추론(NLI) 모델을 전송하여 차별적 능력을 향상시킬 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "367-5",
                    "sentence": "Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches.",
                    "sentence_kor": "대규모 다중 도메인 의도 감지 작업에 대한 광범위한 실험은 우리의 방법이 RoBERTA 기반 분류기 및 임베딩 기반 가장 가까운 이웃 접근법보다 더 안정적이고 정확한 도메인 내 및 OOS 감지 정확도를 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "367-6",
                    "sentence": "More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.",
                    "sentence_kor": "더욱 주목할 만한 것은 NLI 전송을 통해 10샷 모델이 50샷 또는 심지어 풀샷 분류기로 경쟁적으로 수행할 수 있는 반면, 더 빠른 임베딩 검색 모델을 활용하여 추론 시간을 일정하게 유지할 수 있다는 점이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "368",
            "abstractID": "EMNLP_abs-368",
            "text": [
                {
                    "index": "368-0",
                    "sentence": "The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking.",
                    "sentence_kor": "대화법(DA)의 개념은 서로 다른 업무 지향 대화 영역에 걸쳐 보편적이다. 즉, \"요청\" 행위는 식당 예약이든 항공편 예약이든 간에 동일한 발언자의 의도를 수반한다.",
                    "tag": "1"
                },
                {
                    "index": "368-1",
                    "sentence": "However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain.",
                    "sentence_kor": "그러나 한 도메인에서 훈련된 DA 태그는 다른 도메인으로 잘 일반화되지 않으므로 대상 도메인에서 주석이 달린 대량의 데이터가 필요한 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "368-2",
                    "sentence": "In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data.",
                    "sentence_kor": "이 작업에서는 레이블이 지정되지 않은 데이터만 있는 원하는 대상 도메인에 DA 태거를 더 잘 적용하는 방법을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "368-3",
                    "sentence": "We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model.",
                    "sentence_kor": "BERT 모델에서 사전 훈련된 마스크 토큰을 활용하여 텍스트 입력을 늘리는 제어 가능한 메커니즘인 MaskAugment를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "368-4",
                    "sentence": "Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers.",
                    "sentence_kor": "일관성 정규화에 영감을 받아 MaskAugment를 사용하여 DA 태그의 도메인 적응을 검토하기 위해 감독되지 않은 교사-학생 학습 체계를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "368-5",
                    "sentence": "Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.",
                    "sentence_kor": "시뮬레이션 대화(GSIM) 및 스키마 유도 대화(SGD) 데이터 세트에 대한 광범위한 실험은 MaskAugment가 DA 태그 지정에 대한 교차 도메인 일반화를 개선하는 데 유용하다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "369",
            "abstractID": "EMNLP_abs-369",
            "text": [
                {
                    "index": "369-0",
                    "sentence": "Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.).",
                    "sentence_kor": "작업 지향 시맨틱 파싱은 가상 도우미의 중요한 구성 요소로, 사용자의 의도(설정 알림, 음악 재생 등)를 이해하는 역할을 합니다.",
                    "tag": "1"
                },
                {
                    "index": "369-1",
                    "sentence": "Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).",
                    "sentence_kor": "딥 러닝의 최근 발전으로 더 복잡한 쿼리를 성공적으로 구문 분석할 수 있는 몇 가지 접근법이 가능했지만(Gupta 등, 2018; Rongali 등,2020), 이러한 모델에는 새로운 도메인에 대한 쿼리를 구문 분석하기 위해 주석이 달린 많은 훈련 데이터가 필요하다(예: 상기, 음악).",
                    "tag": "1"
                },
                {
                    "index": "369-2",
                    "sentence": "In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction.",
                    "sentence_kor": "본 논문에서, 우리는 작업 지향 의미 파서를 저자원 영역에 적응시키는 데 초점을 맞추고, 10배 데이터 감소로 지도 신경 모델을 능가하는 새로운 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "369-3",
                    "sentence": "In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques.",
                    "sentence_kor": "특히, 우리는 저자원 영역 적응을 위한 두 가지 기본 요소인 표현 학습과 훈련 기법을 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "369-4",
                    "sentence": "Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work.",
                    "sentence_kor": "우리의 표현 학습은 BART(Lewis 등, 2019)를 사용하여 이전 작업에 사용된 인코더 전용 사전 훈련된 표현을 능가하는 모델을 초기화한다.",
                    "tag": "3"
                },
                {
                    "index": "369-5",
                    "sentence": "Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains.",
                    "sentence_kor": "또한, 우리는 저자원 영역에 대한 일반화를 개선하기 위해 최적화 기반 메타 학습(Finn 등, 2017)으로 훈련한다.",
                    "tag": "3"
                },
                {
                    "index": "369-6",
                    "sentence": "This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.",
                    "sentence_kor": "이 접근 방식은 대중에게 공개하는 새로 수집된 다중 도메인 작업 지향 의미 구문 분석 데이터 세트에 대한 실험에서 모든 기본 방법을 크게 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "370",
            "abstractID": "EMNLP_abs-370",
            "text": [
                {
                    "index": "370-0",
                    "sentence": "We introduce a new task of rephrasing for a more natural virtual assistant.",
                    "sentence_kor": "우리는 보다 자연스러운 가상 비서를 위한 새로운 작업을 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "370-1",
                    "sentence": "Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine.",
                    "sentence_kor": "현재 가상 도우미는 의도 슬롯 태그 지정 패러다임에서 작업하며 슬롯 값은 그대로 실행 엔진에 전달됩니다.",
                    "tag": "1"
                },
                {
                    "index": "370-2",
                    "sentence": "However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user.",
                    "sentence_kor": "그러나 사용자가 제공한 쿼리를 반복하거나 다른 사용자에게 전송하기 전에 변경해야 할 때 메시징과 같은 일부 시나리오에서는 이 설정이 실패합니다.",
                    "tag": "1"
                },
                {
                    "index": "370-3",
                    "sentence": "For example, for queries like ‘ask my wife if she can pick up the kids’ or ‘remind me to take my pills’, we need to rephrase the content to ‘can you pick up the kids’ and ‘take your pills’.",
                    "sentence_kor": "예를 들어, '아내에게 아이를 데리러 가도 되는지 물어보라'나 '내 약을 먹게 해달라'와 같은 질문의 경우, 내용을 '아이들을 데리러 올 수 있는가'와 '당신의 약을 먹어라'로 바꿀 필요가 있다.",
                    "tag": "1"
                },
                {
                    "index": "370-4",
                    "sentence": "In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query.",
                    "sentence_kor": "본 논문에서, 우리는 메시징을 사용 사례로 재지정하는 문제를 연구하고 3000쌍의 원본 쿼리와 재지정 쿼리로 구성된 데이터 세트를 발표한다.",
                    "tag": "2"
                },
                {
                    "index": "370-5",
                    "sentence": "We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it.",
                    "sentence_kor": "사전 훈련된 변압기 기반 마스킹 언어 모델인 BART가 작업의 강력한 기준선임을 보여주고 여기에 복사 포인터 및 복사 손실을 추가하여 개선 사항을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "370-6",
                    "sentence": "We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model",
                    "sentence_kor": "우리는 BART 기반 및 LSTM 기반 seq2seq 모델의 서로 다른 절충을 분석하고 최상의 실용 모델로 증류 LSTM 기반 seq2seq를 제안한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "371",
            "abstractID": "EMNLP_abs-371",
            "text": [
                {
                    "index": "371-0",
                    "sentence": "Sentence simplification aims to make sentences easier to read and understand.",
                    "sentence_kor": "문장 단순화는 문장을 읽고 이해하기 쉽게 만드는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "371-1",
                    "sentence": "Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.",
                    "sentence_kor": "최근의 접근법은 종종 영어로만 존재하는 대량의 병렬 데이터에 대해 훈련된 인코더-디코더 모델에서 유망한 결과를 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "371-2",
                    "sentence": "We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks.",
                    "sentence_kor": "여러 언어와 작업에 걸쳐 일반화하면서 단순화 지식을 영어에서 다른 언어(병렬 단순화 말뭉치가 존재하지 않는)로 이전하는 제로샷 모델링 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "371-3",
                    "sentence": "A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification).",
                    "sentence_kor": "공유 변압기 인코더는 위에 작업별 인코더 레이어의 조합이 추가되어 언어 불가지론적 표현을 구성한다(예: 변환 및 단순화를 위한).",
                    "tag": "3"
                },
                {
                    "index": "371-4",
                    "sentence": "Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.",
                    "sentence_kor": "인간과 자동 측정 기준을 모두 사용한 경험적 결과는 우리의 접근 방식이 비지도 및 피벗 기반 방법보다 더 나은 단순화를 산출한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "372",
            "abstractID": "EMNLP_abs-372",
            "text": [
                {
                    "index": "372-0",
                    "sentence": "Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.",
                    "sentence_kor": "기술의 도움으로 사람들은 지리적, 문화적, 언어적 장벽을 넘어 점점 더 소통할 수 있게 되었다.",
                    "tag": "1"
                },
                {
                    "index": "372-1",
                    "sentence": "This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances.",
                    "sentence_kor": "이러한 능력은 또한 대화자가 점점 더 다양한 상황에 맞게 통신 접근 방식을 조정해야 하기 때문에 새로운 도전을 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "372-2",
                    "sentence": "In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance.",
                    "sentence_kor": "본 연구에서, 우리는 사람들이 그들의 언어를 특정한 의사소통 환경에 적응하도록 자동적으로 도와주는 첫 번째 단계를 밟는다.",
                    "tag": "2"
                },
                {
                    "index": "372-3",
                    "sentence": "As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance.",
                    "sentence_kor": "사례 연구로서, 우리는 실용적 의도를 정확하게 전달하는 데 초점을 맞추고 주어진 의사소통 상황에서 의도된 수준의 공손함을 달성하는 의역법을 제안하기 위한 방법론을 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "372-4",
                    "sentence": "We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.",
                    "sentence_kor": "우리는 두 가지 현실적인 의사소통 시나리오에서 우리의 방법을 평가함으로써 이 접근법의 실현 가능성을 입증하고, 두 경우 모두 화자의 의도와 청자의 인식 사이의 불일치의 가능성을 줄일 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "373",
            "abstractID": "EMNLP_abs-373",
            "text": [
                {
                    "index": "373-0",
                    "sentence": "NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input.",
                    "sentence_kor": "NLP 모델은 견고성 문제로 어려움을 겪고 있는 것으로 나타났다. 즉, 모델의 예측은 입력에 대한 작은 섭동 하에서 쉽게 변경될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "373-1",
                    "sentence": "In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels.",
                    "sentence_kor": "본 연구에서는 입력 텍스트가 주어진 경우 작업 레이블에 불변하는 것으로 알려진 제어 가능한 속성을 통해 대립 텍스트를 생성하는 제어된 적대적 텍스트 생성(CAT-Gen) 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "373-2",
                    "sentence": "For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews.",
                    "sentence_kor": "예를 들어, 제품 리뷰에 대한 감정 분류 모델을 공격하기 위해 제품 범주를 관리 가능한 속성으로 사용하여 리뷰의 감정을 변경하지 않을 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "373-3",
                    "sentence": "Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches.",
                    "sentence_kor": "실제 NLP 데이터 세트에 대한 실험은 우리의 방법이 기존의 많은 적대적 텍스트 생성 접근법에 비해 더 다양하고 유창한 적대적 텍스트를 생성할 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "373-4",
                    "sentence": "We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.",
                    "sentence_kor": "우리는 또한 적대적 훈련을 통해 모델을 개선하기 위해 생성된 적대적 사례를 사용하며, 생성된 공격이 모델 재교육 및 다른 모델 아키텍처에 대해 더 강력하다는 것을 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "374",
            "abstractID": "EMNLP_abs-374",
            "text": [
                {
                    "index": "374-0",
                    "sentence": "We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts.",
                    "sentence_kor": "입력 텍스트와 출력 텍스트 간의 중복 정도가 높은 자연어 처리(NLP) 작업을 위한 시퀀스 편집에 대한 개방형 어휘 접근법인 Seq2Edits를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "374-1",
                    "sentence": "In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged.",
                    "sentence_kor": "이 접근 방식에서 각 시퀀스 간 변환은 편집 작업의 시퀀스로 표현되며, 여기서 각 작업은 전체 소스 범위를 대상 토큰으로 대체하거나 변경하지 않은 상태로 유지합니다.",
                    "tag": "3"
                },
                {
                    "index": "374-2",
                    "sentence": "We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board.",
                    "sentence_kor": "우리는 다섯 가지 NLP 작업(텍스트 정규화, 문장 융합, 문장 분할 & 다시쓰기, 텍스트 단순화 및 문법 오류 수정)에 대해 우리의 방법을 평가하고 전반적으로 경쟁력 있는 결과를 보고한다.",
                    "tag": "4"
                },
                {
                    "index": "374-3",
                    "sentence": "For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens.",
                    "sentence_kor": "문법적 오류 수정의 경우 추론 시간은 대상 토큰 수가 아닌 편집 횟수에 따라 달라지기 때문에 전체 시퀀스 모델에 비해 추론 속도가 최대 5.2배 빨라진다.",
                    "tag": "5"
                },
                {
                    "index": "374-4",
                    "sentence": "For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.",
                    "sentence_kor": "텍스트 정규화, 문장 융합 및 문법 오류 수정의 경우, 우리의 접근 방식은 각 편집 작업을 사람이 읽을 수 있는 태그와 연결함으로써 설명 가능성을 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "375",
            "abstractID": "EMNLP_abs-375",
            "text": [
                {
                    "index": "375-0",
                    "sentence": "We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.",
                    "sentence_kor": "우리는 의미 표현에서 자연어 생성을 수행할 때 신경 시퀀스 투 시퀀스 모델이 세분화된 제어 가능성을 나타내는 정도를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "375-1",
                    "sentence": "Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness.",
                    "sentence_kor": "두 개의 작업 지향 대화 생성 벤치마크를 사용하여 네 가지 입력 선형화 전략이 제어 가능성과 충실도에 미치는 영향을 체계적으로 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "375-2",
                    "sentence": "Additionally, we evaluate how a phrase-based data augmentation method can improve performance.",
                    "sentence_kor": "또한 구문 기반 데이터 확대 방법이 성능을 어떻게 개선할 수 있는지 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "375-3",
                    "sentence": "We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.",
                    "sentence_kor": "우리는 훈련 중에 입력 시퀀스를 적절하게 정렬하면 처음부터 훈련하거나 더 큰 사전 훈련 모델을 미세 조정할 때 모두 매우 제어 가능한 생성으로 이어진다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "375-4",
                    "sentence": "Data augmentation further improves control on difficult, randomly generated utterance plans.",
                    "sentence_kor": "데이터 확대는 무작위로 생성된 어려운 발화 계획에 대한 제어를 더욱 개선한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "376",
            "abstractID": "EMNLP_abs-376",
            "text": [
                {
                    "index": "376-0",
                    "sentence": "We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks.",
                    "sentence_kor": "우리는 빈칸을 동적으로 만들고 채워 시퀀스를 생성하는 모델인 BLM(Blank Language Model)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "376-1",
                    "sentence": "The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks.",
                    "sentence_kor": "공백은 시퀀스의 어떤 부분을 확장할지 제어하므로 BLM은 다양한 텍스트 편집 및 다시 쓰기 작업에 이상적입니다.",
                    "tag": "3"
                },
                {
                    "index": "376-2",
                    "sentence": "The model can start from a single blank or partially completed text with blanks at specified locations.",
                    "sentence_kor": "모델은 지정된 위치에 공백이 있는 단일 텍스트 또는 부분적으로 완료된 텍스트에서 시작할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "376-3",
                    "sentence": "It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill.",
                    "sentence_kor": "빈칸에 넣을 단어와 빈칸 삽입 여부를 반복적으로 결정하며, 빈칸이 없을 때 생성을 중지합니다.",
                    "tag": "3"
                },
                {
                    "index": "376-4",
                    "sentence": "BLM can be efficiently trained using a lower bound of the marginal data likelihood.",
                    "sentence_kor": "BLM은 한계 데이터 우도의 하한을 사용하여 효율적으로 훈련될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "376-5",
                    "sentence": "On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency.",
                    "sentence_kor": "누락된 텍스트 조각을 채우는 작업에서 BLM은 정확도와 유창성 측면에서 다른 모든 기준선을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "376-6",
                    "sentence": "Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.",
                    "sentence_kor": "스타일 전송 및 손상된 고대 텍스트 복원에 대한 실험은 광범위한 응용 분야에서 이 프레임워크의 가능성을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "377",
            "abstractID": "EMNLP_abs-377",
            "text": [
                {
                    "index": "377-0",
                    "sentence": "We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models.",
                    "sentence_kor": "신경 시퀀스 투 시퀀스(seq2seq) 모델을 사용하여 의미론적으로 다양한 문장을 생성하는 새로운 방법인 COD3S를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "377-1",
                    "sentence": "Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks.",
                    "sentence_kor": "입력을 조건으로 하는 seq2seqs는 일반적으로 의미론 및 구문론적으로 균질한 문장 집합을 생성하므로 일대다 시퀀스 생성 작업에서 성능이 떨어진다.",
                    "tag": "3"
                },
                {
                    "index": "377-2",
                    "sentence": "Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity.",
                    "sentence_kor": "우리의 2단계 접근방식은 해밍 거리가 의미론적 텍스트 유사성에 대한 인간의 판단과 높은 상관관계를 갖는 LSH(지역 민감 해시) 기반 의미론적 문장 코드에 대한 생성을 조절하여 출력 다양성을 개선한다.",
                    "tag": "3"
                },
                {
                    "index": "377-3",
                    "sentence": "Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects.",
                    "sentence_kor": "일반적으로 적용할 수 있지만, 우리는 명제의 그럴듯한 원인이나 결과를 예측하는 작업인 인과 생성에 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "377-4",
                    "sentence": "We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.",
                    "sentence_kor": "우리는 자동 및 인간 평가를 통해 우리의 방법을 사용하여 생성된 응답이 작업 성과를 저하시키지 않고 다양성을 개선한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "378",
            "abstractID": "EMNLP_abs-378",
            "text": [
                {
                    "index": "378-0",
                    "sentence": "Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation.",
                    "sentence_kor": "언어의 서술적 문법을 만드는 것은 언어 문서화와 보존에 필수적인 단계이다.",
                    "tag": "1"
                },
                {
                    "index": "378-1",
                    "sentence": "However, at the same time it is a tedious, time-consuming task.",
                    "sentence_kor": "그러나 동시에 그것은 지루하고 시간이 많이 걸리는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "378-2",
                    "sentence": "In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format.",
                    "sentence_kor": "본 논문에서 우리는 간결하고 인간적이며 기계가 읽을 수 있는 형식으로 원시 텍스트에서 첫 번째 패스 문법 규격을 추출하기 위한 자동화된 프레임워크를 고안하여 이 프로세스를 자동화하기 위한 단계를 밟는다.",
                    "tag": "2"
                },
                {
                    "index": "378-3",
                    "sentence": "We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages.",
                    "sentence_kor": "우리는 세계 많은 언어의 문법 중심에서 형태통사적 현상인 합의를 설명하는 규칙을 추출하는 데 초점을 맞추고 있다.",
                    "tag": "3"
                },
                {
                    "index": "378-4",
                    "sentence": "We apply our framework to all languages included in the Universal Dependencies project, with promising results.",
                    "sentence_kor": "우리는 범용 종속성 프로젝트에 포함된 모든 언어에 우리의 프레임워크를 적용하여 유망한 결과를 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "378-5",
                    "sentence": "Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data.",
                    "sentence_kor": "관심 언어의 전문가 주석이 없어도 언어 간 전송을 사용하여 프레임워크는 대량의 골드 표준 주석이 달린 데이터로 작성된 규격과 거의 동등한 문법 규격을 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "378-6",
                    "sentence": "We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%.",
                    "sentence_kor": "우리는 평균 78%의 정확도를 가진 우리의 프레임워크가 생성하는 규칙에 대한 인간 전문가 평가를 통해 이 결과를 확인한다.",
                    "tag": "4"
                },
                {
                    "index": "378-7",
                    "sentence": "We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/",
                    "sentence_kor": "우리는 추출된 규칙을 보여주는 인터페이스를 https://neulab.github.io/lase/에서 공개한다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "379",
            "abstractID": "EMNLP_abs-379",
            "text": [
                {
                    "index": "379-0",
                    "sentence": "An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech.",
                    "sentence_kor": "덜 문서화된 언어의 언어 분석의 중간 단계는 자연 언어에서 증명되는 굴절형을 찾고 구성하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "379-1",
                    "sentence": "From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to complete inflectional paradigm tables.",
                    "sentence_kor": "이 자료로부터 언어학자들은 언어의 굴절 패턴에 대한 가설을 테스트하고 굴절 패러다임 표를 완성하기 위해 보이지 않는 굴절 단어 형태를 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "379-2",
                    "sentence": "To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs).",
                    "sentence_kor": "데이터 언어학자가 선형 간 광택 텍스트(IGT)를 수동으로 작성하도록 하기 위해 많은 시간을 할애합니다.",
                    "tag": "1"
                },
                {
                    "index": "379-3",
                    "sentence": "We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P).",
                    "sentence_kor": "우리는 이 프로세스의 속도를 높이고 자연어 처리 시스템에 대한 새로운 형태학적 자원인 IGT-파라딤(IGT2P)을 자동으로 생성하는 새로운 작업을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "379-4",
                    "sentence": "IGT2P generates entire morphological paradigms from IGT input.",
                    "sentence_kor": "IGT2P는 IGT 입력으로부터 전체 형태학적 패러다임을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "379-5",
                    "sentence": "We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language.",
                    "sentence_kor": "우리는 기존의 형태학적 재선택 모델이 언어에 따라 21%에서 64%의 정확도로 과제를 해결할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "379-6",
                    "sentence": "We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.",
                    "sentence_kor": "또한 (i) 언어 전문가가 잡음이 많은 IGT 데이터를 청소하는 데 몇 시간만 할애하면 성능이 최대 21% 포인트 향상되며, 일반적으로 NLP 형태학적 재선택 입력의 필수적인 부분으로 간주되는 POS 태그는 여기서 고려한 모델의 정확도에 영향을 미치지 않는다는 것을 발견했다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "380",
            "abstractID": "EMNLP_abs-380",
            "text": [
                {
                    "index": "380-0",
                    "sentence": "Empathy is critical to successful mental health support.",
                    "sentence_kor": "감정이입은 성공적인 정신 건강 지원에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "380-1",
                    "sentence": "Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts.",
                    "sentence_kor": "공감 측정은 주로 동기식 대면 설정에서 발생했으며 비동기식 텍스트 기반 컨텍스트로 변환되지 않을 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "380-2",
                    "sentence": "Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial.",
                    "sentence_kor": "수백만 명의 사람들이 정신 건강 지원을 위해 텍스트 기반 플랫폼을 사용하기 때문에, 이러한 맥락에서 공감을 이해하는 것은 매우 중요합니다.",
                    "tag": "1"
                },
                {
                    "index": "380-3",
                    "sentence": "In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms.",
                    "sentence_kor": "본 연구에서는 온라인 정신 건강 플랫폼에서 감정이입이 어떻게 표현되는지를 이해하기 위한 전산 접근법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "380-4",
                    "sentence": "We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations.",
                    "sentence_kor": "우리는 텍스트 기반 대화에서 공감의 의사소통을 특성화하기 위해 이론적으로 기반을 둔 새로운 통합 프레임워크를 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "380-5",
                    "sentence": "We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales).",
                    "sentence_kor": "이 공감 프레임워크를 사용하여 주석이 달린 10k(포스트, 응답) 쌍의 말뭉치를 주석(합리)에 대한 뒷받침 증거와 수집하고 공유한다.",
                    "tag": "3"
                },
                {
                    "index": "380-6",
                    "sentence": "We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions.",
                    "sentence_kor": "우리는 대화에서 공감을 식별하고 그 예측의 기초가 되는 합리성을 추출하기 위한 다중 작업 RoBERTa 기반 바이인코더 모델을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "380-7",
                    "sentence": "Experiments demonstrate that our approach can effectively identify empathic conversations.",
                    "sentence_kor": "실험은 우리의 접근 방식이 공감하는 대화를 효과적으로 식별할 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "380-8",
                    "sentence": "We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.",
                    "sentence_kor": "우리는 또한 235k개의 정신 건강 상호작용을 분석하기 위해 이 모델을 적용하고 사용자가 시간이 지남에 따라 공감 훈련을 위한 기회와 피드백을 스스로 배우지 않는다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "381",
            "abstractID": "EMNLP_abs-381",
            "text": [
                {
                    "index": "381-0",
                    "sentence": "Emotions and their evolution play a central role in creating a captivating story.",
                    "sentence_kor": "감정과 그들의 진화는 매혹적인 이야기를 만드는 데 중심적인 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "381-1",
                    "sentence": "In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling.",
                    "sentence_kor": "이 논문에서 우리는 신경 스토리텔링에서 주인공의 감정 궤적을 모델링하는 첫 번째 연구를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "381-2",
                    "sentence": "We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist.",
                    "sentence_kor": "우리는 주어진 이야기 제목과 주인공에게 원하는 감정 호를 고수하는 이야기를 만들어내는 방법을 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "381-3",
                    "sentence": "Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models.",
                    "sentence_kor": "우리의 모델에는 감정 감독(EmoSup)과 두 개의 감정 강화(EmoRL) 모델이 있다.",
                    "tag": "3"
                },
                {
                    "index": "381-4",
                    "sentence": "The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning.",
                    "sentence_kor": "EmoRL 모델은 강화 학습을 통해 스토리 생성 과정을 정규화하기 위해 고안된 특별한 보상을 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "381-5",
                    "sentence": "Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.",
                    "sentence_kor": "우리의 자동 및 수동 평가는 이러한 모델이 이야기의 질을 저하시키지 않고 기본 방법에 비해 원하는 감정 호를 따르는 이야기를 훨씬 더 잘 만들어 낸다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "382",
            "abstractID": "EMNLP_abs-382",
            "text": [
                {
                    "index": "382-0",
                    "sentence": "Intimacy is a fundamental aspect of how we relate to others in social settings.",
                    "sentence_kor": "친밀감은 우리가 사회적 환경에서 다른 사람들과 관계를 맺는 방법의 근본적인 측면이다.",
                    "tag": "1"
                },
                {
                    "index": "382-1",
                    "sentence": "Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing).",
                    "sentence_kor": "언어는 주제와 다른 미묘한 단서(언어적 회피와 욕설 등)를 통해 친밀감의 사회적 정보를 암호화한다.",
                    "tag": "1"
                },
                {
                    "index": "382-2",
                    "sentence": "Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87).",
                    "sentence_kor": "여기서는 질문의 친밀도 수준을 정확하게 예측하기 위한 데이터 세트와 딥 러닝 모델을 사용하여 언어의 친밀도 표현을 연구하기 위한 새로운 계산 프레임워크를 소개한다(Pearson r = 0.87).",
                    "tag": "2"
                },
                {
                    "index": "382-3",
                    "sentence": "Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings.",
                    "sentence_kor": "소셜 미디어, 책 및 영화에 걸쳐 80.5백만 개의 질문 데이터 세트를 분석함으로써, 우리는 개인들이 그들의 친밀감을 사회적 환경에 맞추기 위해 그들의 언어로 대인관계 실용적인 움직임을 사용한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "382-4",
                    "sentence": "Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology.",
                    "sentence_kor": "그리고 나서, 세 연구에서, 우리는 개인이 어떻게 그들의 친밀감을 성별, 사회적 거리, 그리고 청중들에 대한 사회적 규범에 일치하도록 조절하는지, 각각은 사회 심리학 연구의 주요 발견들을 검증한다.",
                    "tag": "4"
                },
                {
                    "index": "382-5",
                    "sentence": "Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.",
                    "sentence_kor": "우리의 연구는 친밀감이 언어의 만연하고 영향력 있는 사회적 차원이라는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "383",
            "abstractID": "EMNLP_abs-383",
            "text": [
                {
                    "index": "383-0",
                    "sentence": "Subevents elaborate an event and widely exist in event descriptions.",
                    "sentence_kor": "하위 이벤트는 이벤트를 상세히 설명하고 이벤트 설명에 널리 존재합니다.",
                    "tag": "1"
                },
                {
                    "index": "383-1",
                    "sentence": "Subevent knowledge is useful for discourse analysis and event-centric applications.",
                    "sentence_kor": "하위 사건 지식은 담화 분석과 사건 중심 적용에 유용하다.",
                    "tag": "1"
                },
                {
                    "index": "383-2",
                    "sentence": "Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base.",
                    "sentence_kor": "하위 이벤트 지식의 부족을 인정하여 텍스트에서 하위 이벤트 관계 튜플을 추출하고 최초의 대규모 하위 이벤트 지식 기반을 구축하는 약하게 감독되는 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "383-3",
                    "sentence": "We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents.",
                    "sentence_kor": "먼저 1) 하위 이벤트가 부모 이벤트에 의해 일시적으로 포함되고 2) 상위 이벤트의 정의를 사용하여 하위 이벤트 식별을 추가로 안내할 수 있다는 두 가지 관측치를 이용하여 하위 이벤트 관계를 가질 가능성이 있는 초기 이벤트 쌍 세트를 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "383-4",
                    "sentence": "Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs.",
                    "sentence_kor": "그런 다음 초기 시드 하위 이벤트 쌍을 사용하여 풍부한 약한 감독을 수집하여 BERT를 사용하여 상황별 분류기를 교육하고 분류기를 적용하여 새로운 하위 이벤트 쌍을 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "383-5",
                    "sentence": "The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types.",
                    "sentence_kor": "평가 결과 획득된 하위 이벤트 튜플(239K)은 고품질(90.1% 정확도)이며 광범위한 이벤트 유형을 포괄하는 것으로 나타났다.",
                    "tag": "4"
                },
                {
                    "index": "383-6",
                    "sentence": "The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.",
                    "sentence_kor": "습득한 하위 사건 지식은 담화 분석과 사건-사건 관계의 범위를 식별하는 데 유용한 것으로 나타났다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "384",
            "abstractID": "EMNLP_abs-384",
            "text": [
                {
                    "index": "384-0",
                    "sentence": "We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model.",
                    "sentence_kor": "우리는 바이오메디컬 이벤트 추출을 공동 종단 간 신경 정보 추출 모델인 시퀀스 라벨링(BeeSL)으로 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "384-1",
                    "sentence": "BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning.",
                    "sentence_kor": "BeeSL은 다중 라벨 인식 인코딩 전략을 활용하고 다중 작업 학습을 통해 중간 작업을 공동으로 모델링하면서 작업을 시퀀스 라벨링으로 재작성한다.",
                    "tag": "3"
                },
                {
                    "index": "384-2",
                    "sentence": "BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools.",
                    "sentence_kor": "BeeSL은 빠르고 정확하며 엔드 투 엔드이며 현재 방법과 달리 외부 지식 기반이나 사전 처리 도구가 필요하지 않습니다.",
                    "tag": "3"
                },
                {
                    "index": "384-3",
                    "sentence": "BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task.",
                    "sentence_kor": "BeeSL은 60.22% F1에 도달하는 1.57% 절대 F1 점수로 Genia 2011 벤치마크에서 현재 최고의 시스템(Li 등, 2019)을 능가하여 과제에 대한 새로운 최첨단 기술을 확립한다.",
                    "tag": "3"
                },
                {
                    "index": "384-4",
                    "sentence": "Importantly, we also provide first results on biomedical event extraction without gold entity information.",
                    "sentence_kor": "중요한 것은, 우리는 또한 금 실체 정보가 없는 생물의학적 사건 추출에 대한 첫 번째 결과를 제공한다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "384-5",
                    "sentence": "Empirical results show that BeeSL’s speed and accuracy makes it a viable approach for large-scale real-world scenarios.",
                    "sentence_kor": "경험적 결과에 따르면 벌은SL의 속도와 정확성은 대규모 실제 시나리오에 적합한 접근 방식이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "385",
            "abstractID": "EMNLP_abs-385",
            "text": [
                {
                    "index": "385-0",
                    "sentence": "Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots.",
                    "sentence_kor": "사용자의 직업, 취미, 좋아하는 음식, 여행 선호도에 대한 개인적인 지식은 추천자나 챗봇과 같은 개인화된 인공지능에게 귀중한 자산이다.",
                    "tag": "1"
                },
                {
                    "index": "385-1",
                    "sentence": "Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts.",
                    "sentence_kor": "Reddit과 같은 소셜 미디어에서의 대화는 개인적인 사실을 추론하기 위한 풍부한 데이터 원천이다.",
                    "tag": "1"
                },
                {
                    "index": "385-2",
                    "sentence": "Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples.",
                    "sentence_kor": "이전 연구에서는 이러한 지식을 추출하기 위해 감독 방법을 개발했지만, 이러한 접근법은 라벨이 부착된 충분한 훈련 샘플로 속성 값을 넘어 일반화할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "385-3",
                    "sentence": "This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training.",
                    "sentence_kor": "본 논문은 훈련 중에 결코 볼 수 없었던 속성 값을 예측하기 위해 키워드 추출 및 문서 검색을 창의적으로 활용하는 제로샷 학습 방법인 CHARM을 고안함으로써 이러한 한계를 극복했다.",
                    "tag": "1"
                },
                {
                    "index": "385-4",
                    "sentence": "Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.",
                    "sentence_kor": "Reddit의 대규모 데이터 세트를 사용한 실험은 직업 및 취미와 같은 개방형 속성에 대한 CHARM의 실행 가능성을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "386",
            "abstractID": "EMNLP_abs-386",
            "text": [
                {
                    "index": "386-0",
                    "sentence": "Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance.",
                    "sentence_kor": "이벤트 감지(ED)에 대한 최근 연구는 구문 의존성 그래프를 그래프 컨볼루션 신경망(GCN)에 사용하여 최첨단 성능을 달성할 수 있다는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "386-1",
                    "sentence": "However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction.",
                    "sentence_kor": "그러나 이러한 그래프 기반 모델에서 숨겨진 벡터의 계산은 트리거 후보 단어와 무관하므로 이벤트 예측을 위한 트리거 후보와 무관한 정보를 남길 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "386-2",
                    "sentence": "In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance.",
                    "sentence_kor": "또한 ED의 현재 모델은 종속성 트리를 통해 얻을 수 있는 단어의 전반적인 상황별 중요도 점수를 활용하여 성능을 향상시키지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "386-3",
                    "sentence": "In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate.",
                    "sentence_kor": "본 연구에서는 트리거 후보 정보를 기반으로 ED용 GCN 모델의 숨겨진 벡터에서 노이즈 정보를 필터링하는 새로운 게이트 메커니즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "386-4",
                    "sentence": "We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.",
                    "sentence_kor": "또한 게이트의 맥락적 다양성과 ED의 그래프와 모델에 대한 중요도 점수 일관성을 달성하기 위한 새로운 메커니즘을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "386-5",
                    "sentence": "The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.",
                    "sentence_kor": "실험은 제안된 모델이 두 개의 ED 데이터 세트에서 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "387",
            "abstractID": "EMNLP_abs-387",
            "text": [
                {
                    "index": "387-0",
                    "sentence": "In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations.",
                    "sentence_kor": "본 논문에서, 우리는 신경 구조와 일시적 관계를 예측하여 이벤트를 정렬하기 위한 일련의 훈련 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "387-1",
                    "sentence": "Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them.",
                    "sentence_kor": "우리가 제안한 모델은 텍스트 범위 내의 한 쌍의 사건을 입력으로 수신하고 이들 사이의 시간적 관계(전, 후, 동일, 모호)를 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "387-2",
                    "sentence": "Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques.",
                    "sentence_kor": "이 작업의 핵심 과제가 주석이 달린 데이터의 부족이라는 점을 감안할 때, 우리 모델은 사전 훈련된 표현(예: RoBERTA, BERT 또는 ELMo), 전송 및 다중 작업 학습(보완 데이터 세트를 활용하여) 및 자체 훈련 기술에 의존한다.",
                    "tag": "3"
                },
                {
                    "index": "387-3",
                    "sentence": "Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.",
                    "sentence_kor": "영어 문서의 MATRES 데이터 세트에 대한 실험은 이 작업에 대한 새로운 최첨단 기술을 확립한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "388",
            "abstractID": "EMNLP_abs-388",
            "text": [
                {
                    "index": "388-0",
                    "sentence": "We propose an end-to-end approach for synthetic QA data generation.",
                    "sentence_kor": "우리는 합성 QA 데이터 생성을 위한 엔드 투 엔드 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "388-1",
                    "sentence": "Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions.",
                    "sentence_kor": "우리 모델은 답과 질문을 모두 생성하도록 종단 간 훈련을 받은 단일 변압기 기반 인코더-디코더 네트워크로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "388-2",
                    "sentence": "In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token.",
                    "sentence_kor": "간단히 말해서, 우리는 인코더에 통로를 공급하고 디코더에게 질문 및 답변 토큰을 생성하도록 요청한다.",
                    "tag": "3"
                },
                {
                    "index": "388-3",
                    "sentence": "The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model.",
                    "sentence_kor": "생성 프로세스에서 생성된 우도는 필터링 점수로 사용되므로 별도의 필터링 모델이 필요하지 않습니다.",
                    "tag": "3"
                },
                {
                    "index": "388-4",
                    "sentence": "Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation.",
                    "sentence_kor": "우리의 발전기는 최대우도 추정을 사용하여 사전 훈련된 LM을 미세 조정함으로써 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "388-5",
                    "sentence": "The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.",
                    "sentence_kor": "실험 결과는 현재 최첨단 방법을 능가하는 QA 모델의 도메인 적응이 크게 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "389",
            "abstractID": "EMNLP_abs-389",
            "text": [
                {
                    "index": "389-0",
                    "sentence": "Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain.",
                    "sentence_kor": "전송 학습 기법은 상당히 많은 양의 고품질 주석이 달린 데이터를 얻기 어려운 NLP 작업에 특히 유용하다.",
                    "tag": "1"
                },
                {
                    "index": "389-1",
                    "sentence": "Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks.",
                    "sentence_kor": "현재 접근 방식은 다운스트림 작업으로 미세 조정하기 전에 도메인 내 텍스트에서 사전 훈련된 언어 모델(LM)을 직접 조정한다.",
                    "tag": "1"
                },
                {
                    "index": "389-2",
                    "sentence": "We show that extending the vocabulary of the LM with domain-specific terms leads to further gains.",
                    "sentence_kor": "우리는 도메인별 용어를 사용하여 LM의 어휘를 확장하면 추가 이득으로 이어진다는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "389-3",
                    "sentence": "To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks.",
                    "sentence_kor": "보다 큰 효과를 위해 레이블이 지정되지 않은 데이터의 구조를 활용하여 LM이 다운스트림 작업으로 전송하는 데 도움이 되는 보조 합성 작업을 만든다.",
                    "tag": "2+3"
                },
                {
                    "index": "389-4",
                    "sentence": "We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.",
                    "sentence_kor": "사전 교육을 받은 로버타 대형 LM에 이러한 접근 방식을 점진적으로 적용하고 IT 영역의 세 가지 작업에서 상당한 성능 향상을 보여 줍니다. 추출적 독해력, 문서 순위 및 중복 질문 탐지.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "390",
            "abstractID": "EMNLP_abs-390",
            "text": [
                {
                    "index": "390-0",
                    "sentence": "Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams.",
                    "sentence_kor": "교과서 질문 답변은 기계 이해와 시각 질문 답변의 교차점에 있는 복잡한 작업으로, 텍스트와 다이어그램의 다중 모드 정보에 대한 추론을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "390-1",
                    "sentence": "For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails.",
                    "sentence_kor": "이 논문은 처음으로 변압기 언어 모델의 잠재력과 상향식 및 하향식 주의력을 활용하여 이 과제가 수반하는 언어 및 시각적 이해 과제를 해결한다.",
                    "tag": "2"
                },
                {
                    "index": "390-2",
                    "sentence": "Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling.",
                    "sentence_kor": "언어 시각 변압기를 처음부터 교육하는 대신 사전 훈련된 변압기, 미세 조정 및 앙상블링에 의존합니다.",
                    "tag": "3"
                },
                {
                    "index": "390-3",
                    "sentence": "We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options.",
                    "sentence_kor": "우리는 상향식 및 하향식 주의를 추가하여 다이어그램 구성 요소와 그 관계에 해당하는 관심 영역을 식별하여 각 질문과 답변 옵션에 대한 관련 시각적 정보의 선택을 개선한다.",
                    "tag": "3"
                },
                {
                    "index": "390-4",
                    "sentence": "Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions.",
                    "sentence_kor": "우리의 시스템 ISAAQ는 모든 TQA 질문 유형에서 81.36%, 71.11%, 참/거짓, 텍스트 전용 및 다이어그램 객관식 질문에 대한 55.12%의 정확도로 전례 없는 성공을 보고했다.",
                    "tag": "4"
                },
                {
                    "index": "390-5",
                    "sentence": "ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.",
                    "sentence_kor": "ISAAQ는 또한 광범위한 적용 가능성을 입증하여 다른 까다로운 데이터셋에서 최첨단 결과를 얻습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "391",
            "abstractID": "EMNLP_abs-391",
            "text": [
                {
                    "index": "391-0",
                    "sentence": "We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining.",
                    "sentence_kor": "우리는 직무에 구애받지 않는 다중 모드 사전 훈련을 통해 추론 레이블의 감독 없이 자연어 추론 문제를 해결할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "391-1",
                    "sentence": "Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled.",
                    "sentence_kor": "다중 모드 자기 지도 학습에 대한 최근의 연구도 언어 및 시각적 맥락을 나타내지만, 다른 양식에 대한 인코더는 결합되어 있다.",
                    "tag": "1"
                },
                {
                    "index": "391-2",
                    "sentence": "Thus they cannot incorporate visual information when encoding plain text alone.",
                    "sentence_kor": "따라서 일반 텍스트를 단독으로 인코딩할 때 시각적 정보를 통합할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "391-3",
                    "sentence": "In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network.",
                    "sentence_kor": "본 논문에서 우리는 다중 모드 정렬 대조 분리 학습(MACD) 네트워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "391-4",
                    "sentence": "MACD forces the decoupled text encoder to represent the visual information via contrastive learning.",
                    "sentence_kor": "MACD는 분리된 텍스트 인코더가 대조 학습을 통해 시각적 정보를 표현하도록 강제한다.",
                    "tag": "3"
                },
                {
                    "index": "391-5",
                    "sentence": "Therefore, it embeds visual knowledge even for plain text inference.",
                    "sentence_kor": "따라서 일반 텍스트 추론에도 시각적 지식이 포함되어 있다.",
                    "tag": "3"
                },
                {
                    "index": "391-6",
                    "sentence": "We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B).",
                    "sentence_kor": "일반 텍스트 추론 데이터 세트(예: SNLI 및 STS-B)에 대한 포괄적인 실험을 수행했다.",
                    "tag": "3"
                },
                {
                    "index": "391-7",
                    "sentence": "The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.",
                    "sentence_kor": "비지도 MACD는 STS-B에서 완전히 감독된 BiLSTM 및 BiLSTM+ELMO를 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "392",
            "abstractID": "EMNLP_abs-392",
            "text": [
                {
                    "index": "392-0",
                    "sentence": "In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.",
                    "sentence_kor": "본 논문에서 우리는 소리 없이 입으로 말하는 단어가 근육 자극을 포착하는 전자 촬영(EMG) 센서 측정을 기반으로 하는 청각적 음성으로 변환되는 디지털 음성 음성 말하기 과제를 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "392-1",
                    "sentence": "While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech.",
                    "sentence_kor": "이전 연구는 음성 음성 음성 중에 수집된 EMG의 음성 합성 모델을 훈련시키는 데 초점을 맞추었지만, 우리는 소리 없이 관절화된 음성 동안 수집된 EMG에서 훈련하는 첫 번째 사람이다.",
                    "tag": "1"
                },
                {
                    "index": "392-2",
                    "sentence": "We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.",
                    "sentence_kor": "오디오 대상을 음성 신호에서 음성 신호로 전송하여 소리 없는 EMG에 대한 교육 방법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "392-3",
                    "sentence": "Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another.",
                    "sentence_kor": "우리의 방법은 음성화된 데이터만 훈련하는 기준선에 비해 소리 없는 EMG에서 생성된 오디오의 지능성을 크게 향상시켜 한 데이터 조건에서 전사 단어 오류율을 64%에서 4%로, 다른 조건에서는 88%에서 68%로 감소시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "392-4",
                    "sentence": "To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.",
                    "sentence_kor": "이 작업에 대한 추가 개발을 촉진하기 위해, 우리는 소리 없는 음성 얼굴 EMG 측정의 새로운 데이터 세트를 공유한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "393",
            "abstractID": "EMNLP_abs-393",
            "text": [
                {
                    "index": "393-0",
                    "sentence": "Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors.",
                    "sentence_kor": "적들은 재정적 이득이나 모델 오류를 이용하기 위해 블랙박스 NLP 시스템을 훔치거나 공격하려고 할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "393-1",
                    "sentence": "One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly.",
                    "sentence_kor": "특히 관심을 끄는 한 가지 설정은 기계 번역(MT)입니다. 여기서 모델은 상업적 가치가 높고 오류가 발생할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "393-2",
                    "sentence": "We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats.",
                    "sentence_kor": "우리는 블랙박스 MT 시스템의 가능한 활용을 조사하고 그러한 위협에 대한 예비 방어를 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "393-3",
                    "sentence": "We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs.",
                    "sentence_kor": "먼저 MT 시스템을 단일 언어 문장과 출력물을 모방하기 위한 훈련 모델로 쿼리하여 도난당할 수 있음을 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "393-4",
                    "sentence": "Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models.",
                    "sentence_kor": "시뮬레이션 실험을 사용하여, 우리는 모방 모델이 대상 모델과 다른 입력 데이터 또는 아키텍처를 가지고 있는 경우에도 MT 모델 도용이 가능하다는 것을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "393-5",
                    "sentence": "Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs.",
                    "sentence_kor": "이러한 아이디어를 적용하여, 우리는 고자원 및 저자원 언어 쌍 모두에서 세 개의 생산 MT 시스템의 0.6 BLEU 내에 도달하는 모방 모델을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "393-6",
                    "sentence": "We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems.",
                    "sentence_kor": "그런 다음 모방 모델의 유사성을 활용하여 생산 시스템에 적대적인 예를 전달합니다.",
                    "tag": "3"
                },
                {
                    "index": "393-7",
                    "sentence": "We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs.",
                    "sentence_kor": "우리는 의미적으로 잘못된 변환, 손실된 내용 및 저속한 모델 출력을 초래하는 입력을 노출하는 그레이디언트 기반 공격을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "393-8",
                    "sentence": "To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models.",
                    "sentence_kor": "이러한 취약성을 완화하기 위해 모방 모델의 최적화를 잘못 지시하기 위해 변환 출력을 수정하는 방어 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "393-9",
                    "sentence": "This defense degrades the adversary’s BLEU score and attack success rate at some cost in the defender’s BLEU and inference speed.",
                    "sentence_kor": "이 수비는 수비수의 BLEU와 추론 속도에서 어느 정도의 비용으로 상대방의 BLEU 점수와 공격 성공률을 저하시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "394",
            "abstractID": "EMNLP_abs-394",
            "text": [
                {
                    "index": "394-0",
                    "sentence": "Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language.",
                    "sentence_kor": "그들의 경험적 성공에도 불구하고, 신경망은 여전히 자연 언어의 구성적 측면을 포착하는 데 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "394-1",
                    "sentence": "This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems.",
                    "sentence_kor": "본 연구는 시퀀스 투 시퀀스 문제에 대한 신경 모델의 구성 동작을 장려하기 위한 간단한 데이터 확대 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "394-2",
                    "sentence": "Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set.",
                    "sentence_kor": "우리의 접근 방식인 SeqMix는 훈련 세트의 입력/출력 시퀀스를 부드럽게 결합하여 새로운 합성 예를 만든다.",
                    "tag": "3"
                },
                {
                    "index": "394-3",
                    "sentence": "We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective.",
                    "sentence_kor": "우리는 이 접근 방식을 SwitchOut 및 워드 드롭아웃과 같은 기존 기법에 연결하고 이러한 기법이 모두 기본적으로 단일 목표의 변형에 가깝다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "394-4",
                    "sentence": "SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines.",
                    "sentence_kor": "SeqMix는 강력한 Transformer 기준선에 비해 5개의 서로 다른 변환 데이터 세트에서 약 1.0 BLEU를 지속적으로 개선한다.",
                    "tag": "4"
                },
                {
                    "index": "394-5",
                    "sentence": "On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.",
                    "sentence_kor": "SCAN 및 의미 파싱과 같은 강력한 구성 일반화가 필요한 작업에 대해서도 SeqMix는 추가적인 개선을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "395",
            "abstractID": "EMNLP_abs-395",
            "text": [
                {
                    "index": "395-0",
                    "sentence": "Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition.",
                    "sentence_kor": "다양한 작업에 대한 강력한 성능에도 불구하고, 최대 가능성으로 훈련된 신경 시퀀스 모델은 길이 편향 및 퇴화 반복과 같은 문제를 나타내는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "395-1",
                    "sentence": "We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms.",
                    "sentence_kor": "우리는 공통 디코딩 알고리즘을 사용할 때 반복 언어 모델에서 무한 길이 시퀀스를 수신하는 관련 문제를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "395-2",
                    "sentence": "To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model.",
                    "sentence_kor": "이 문제를 분석하기 위해 먼저 디코딩 알고리즘의 불일치를 정의하는데, 이는 알고리즘이 모델에서 확률이 0인 무한 길이 시퀀스를 생성할 수 있음을 의미한다.",
                    "tag": "2+3"
                },
                {
                    "index": "395-3",
                    "sentence": "We prove that commonly used incomplete decoding algorithms – greedy search, beam search, top-k sampling, and nucleus sampling – are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length.",
                    "sentence_kor": "반복 언어 모델이 유한 길이의 시퀀스를 생성하도록 훈련되었음에도 불구하고 일반적으로 사용되는 불완전한 디코딩 알고리즘(그리디 검색, 빔 검색, top-k 샘플링 및 핵 샘플링)이 일관되지 않음을 증명한다.",
                    "tag": "4"
                },
                {
                    "index": "395-4",
                    "sentence": "Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model.",
                    "sentence_kor": "이러한 통찰력을 바탕으로 불일치를 해결하는 두 가지 해결책, 즉 탑-k 및 핵 샘플링의 일관된 변형과 자체 종료 반복 언어 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "395-5",
                    "sentence": "Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.",
                    "sentence_kor": "경험적 결과는 불일치가 실제로 발생하고 제안된 방법이 불일치를 방지한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "396",
            "abstractID": "EMNLP_abs-396",
            "text": [
                {
                    "index": "396-0",
                    "sentence": "Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation.",
                    "sentence_kor": "자연어 처리의 많은 작업에는 시퀀스 라벨링, 의미적 역할 라벨링, 구문 분석 및 기계 번역과 같은 구조화된 출력의 예측이 포함된다.",
                    "tag": "1"
                },
                {
                    "index": "396-1",
                    "sentence": "Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic.",
                    "sentence_kor": "연구자들은 이러한 문제에 심층 표현 학습을 점점 더 적용하고 있지만, 이러한 접근법의 구조적 요소는 대개 매우 단순하다.",
                    "tag": "1"
                },
                {
                    "index": "396-2",
                    "sentence": "In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence.",
                    "sentence_kor": "본 연구에서, 우리는 전체 라벨 시퀀스를 고려하는 몇 가지를 포함하여 시퀀스 라벨의 레이블 간의 복잡한 의존성을 포착하기 위한 몇 가지 고차 에너지 용어를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "396-3",
                    "sentence": "We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks.",
                    "sentence_kor": "우리는 이러한 에너지 용어에 대해 컨볼루션, 반복 및 자기 주의 네트워크에서 도출한 신경 매개 변수화를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "396-4",
                    "sentence": "We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models.",
                    "sentence_kor": "우리는 에너지 기반 추론 네트워크 학습 프레임워크(Tu 및 Gimpel, 2018)를 사용하여 훈련과 추론의 어려움을 처리한다.",
                    "tag": "3"
                },
                {
                    "index": "396-5",
                    "sentence": "We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.",
                    "sentence_kor": "우리는 이 접근 방식이 간단한 로컬 분류기와 디코딩 속도는 같으면서 네 가지 시퀀스 라벨링 작업에서 다양한 고차 에너지 용어를 사용하여 상당한 개선을 달성한다는 것을 경험적으로 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "396-6",
                    "sentence": "We also find high-order energies to help in noisy data conditions.",
                    "sentence_kor": "또한 노이즈가 많은 데이터 조건에서 도움이 되는 고차 에너지를 찾는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "397",
            "abstractID": "EMNLP_abs-397",
            "text": [
                {
                    "index": "397-0",
                    "sentence": "Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy.",
                    "sentence_kor": "현대의 신경망은 교차 엔트로피와 같은 적절한 점수 기능으로 훈련되었을 때에도 항상 잘 보정된 예측을 생성하지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "397-1",
                    "sentence": "In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs.",
                    "sentence_kor": "분류 설정에서 모델 출력을 보정하기 위해 보류 데이터 세트와 함께 등방성 회귀 또는 온도 스케일링과 같은 간단한 방법을 사용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "397-2",
                    "sentence": "However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available.",
                    "sentence_kor": "그러나 이러한 방법을 구조화된 예측으로 확장하는 것이 항상 간단하거나 효과적인 것은 아니며, 또한 보류 교정 세트를 항상 사용할 수 있는 것은 아니다.",
                    "tag": "1"
                },
                {
                    "index": "397-3",
                    "sentence": "In this paper, we study ensemble distillation as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles.",
                    "sentence_kor": "본 논문에서 우리는 앙상블의 엄청난 추론 시간 비용을 피하면서 잘 보정된 구조화된 예측 모델을 생성하기 위한 일반적인 프레임워크로서 앙상블 증류를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "397-4",
                    "sentence": "We validate this framework on two tasks: named-entity recognition and machine translation.",
                    "sentence_kor": "우리는 명명된 엔터티 인식과 기계 번역이라는 두 가지 작업에서 이 프레임워크를 검증한다.",
                    "tag": "2"
                },
                {
                    "index": "397-5",
                    "sentence": "We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.",
                    "sentence_kor": "두 작업 모두에서 앙상블 증류는 앙상블의 성능 및 교정 이점을 상당 부분 유지하고 때로는 개선하는 모델을 생산하지만 시험 시간 동안 단일 모델만 필요로 한다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "398",
            "abstractID": "EMNLP_abs-398",
            "text": [
                {
                    "index": "398-0",
                    "sentence": "Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment.",
                    "sentence_kor": "측면 수준 정서 분석은 논평에서 측면 또는 대상의 정서 극성을 인식하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "398-1",
                    "sentence": "Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task.",
                    "sentence_kor": "최근 이 작업에 대해 언어 의존성 트리를 기반으로 한 그래프 컨볼루션 네트워크가 연구되었다.",
                    "tag": "1"
                },
                {
                    "index": "398-2",
                    "sentence": "However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory.",
                    "sentence_kor": "그러나 상용 제품 의견 또는 트윗의 의존성 구문 분석 정확도는 만족스럽지 못할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "398-3",
                    "sentence": "To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs.",
                    "sentence_kor": "이 문제를 해결하기 위해 언어 의존성 트리를 자동으로 유도된 측면별 그래프와 연결한다.",
                    "tag": "2"
                },
                {
                    "index": "398-4",
                    "sentence": "We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks.",
                    "sentence_kor": "우리는 자기 주의 네트워크를 통해 학습되는 단어 의존성 그래프와 잠재 그래프의 정보를 동적으로 결합하는 게이팅 메커니즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "398-5",
                    "sentence": "Our model can complement supervised syntactic features with latent semantic dependencies.",
                    "sentence_kor": "우리 모델은 잠재적인 의미 의존성으로 지도된 구문적 특징을 보완할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "398-6",
                    "sentence": "Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.",
                    "sentence_kor": "5개의 벤치마크에 대한 실험 결과는 우리가 제안한 잠재 모델의 효과를 보여주며 잠재 그래프를 사용하지 않는 모델보다 훨씬 더 나은 결과를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "399",
            "abstractID": "EMNLP_abs-399",
            "text": [
                {
                    "index": "399-0",
                    "sentence": "Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events.",
                    "sentence_kor": "이전의 연구는 정서적 극성을 사건과 연관시킬 필요성을 인식했으며 정서적 사건을 식별하기 위한 몇 가지 기술과 어휘 자원을 생산했다.",
                    "tag": "1"
                },
                {
                    "index": "399-1",
                    "sentence": "Our research introduces new classification models to assign affective polarity to event phrases.",
                    "sentence_kor": "우리의 연구는 사건 구문에 정서적 극성을 부여하기 위한 새로운 분류 모델을 도입한다.",
                    "tag": "2"
                },
                {
                    "index": "399-2",
                    "sentence": "First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base.",
                    "sentence_kor": "첫째, 우리는 정서적 이벤트 분류를 위한 BERT 기반 모델을 제시하고 분류자가 대규모 정서적 이벤트 지식 기반보다 실질적으로 더 나은 성능을 달성함을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "399-3",
                    "sentence": "Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data.",
                    "sentence_kor": "둘째, 레이블이 없는 데이터로 분류기를 반복적으로 개선하는 담화 강화 자가 훈련 방법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "399-4",
                    "sentence": "The key idea is to exploit event phrases that occur with a coreferent sentiment expression.",
                    "sentence_kor": "핵심 아이디어는 핵심 감정 표현과 함께 발생하는 이벤트 문구를 활용하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "399-5",
                    "sentence": "The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier’s predictions and the polarities of the event’s coreferent sentiment expressions.",
                    "sentence_kor": "담화 강화 자체 훈련 알고리즘은 분류자의 예측과 이벤트 핵심 감정 표현식의 극성 모두를 기반으로 새로운 이벤트 구문에 반복적으로 레이블을 붙인다.",
                    "tag": "3"
                },
                {
                    "index": "399-6",
                    "sentence": "Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.",
                    "sentence_kor": "우리의 결과는 담화 강화 자가 훈련이 정서적 사건 분류에 대한 기억력과 정밀도 모두를 더욱 향상시킨다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "400",
            "abstractID": "EMNLP_abs-400",
            "text": [
                {
                    "index": "400-0",
                    "sentence": "Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables.",
                    "sentence_kor": "딥 러닝은 다양한 NLP 작업에서 상당한 성공을 거두었지만, 대부분의 딥 러닝 모델에는 다른 유형의 변수들 사이의 복잡한 인과 관계를 모델링할 수 있는 명시적 도메인 지식을 인코딩하는 능력이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "400-1",
                    "sentence": "On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process.",
                    "sentence_kor": "반면에, 논리 규칙은 훈련 과정을 안내하기 위해 인과 관계를 나타내는 간단한 표현을 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "400-2",
                    "sentence": "Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT).",
                    "sentence_kor": "논리 프로그램은 만족 절(MaxSAT)의 수를 최대화하여 논리 변수에 대한 진실 할당을 찾는 것을 목표로 하는 만족성 문제로 캐스팅될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "400-3",
                    "sentence": "We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework.",
                    "sentence_kor": "우리는 논리 추론 프로세스를 모델링하고 공동 프레임워크에서 심층 신경 네트워크와 그래픽 모델을 연결하는 가중 버전의 MaxSAT를 원활하게 통합하기 위해 MaxSAT 의미론을 채택한다.",
                    "tag": "2"
                },
                {
                    "index": "400-4",
                    "sentence": "The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent.",
                    "sentence_kor": "공동 모델은 잘못된 예측을 수정하기 위해 가중 MaxSAT 계층에 딥 러닝 출력을 공급하고 종단 간 경사 하강법을 통해 훈련할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "400-5",
                    "sentence": "Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.",
                    "sentence_kor": "우리가 제안한 모델은 측면 기반 의견 추출 작업에 대한 높은 수준의 특징 학습, 지식 추론 및 구조화된 학습의 이점을 관찰 가능한 성능 이득과 연관시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "401",
            "abstractID": "EMNLP_abs-401",
            "text": [
                {
                    "index": "401-0",
                    "sentence": "This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies.",
                    "sentence_kor": "이 논문은 영화의 시놉스와 리뷰를 사용하여 주제와 스타일과 같은 특성을 유추함으로써 이야기를 특징짓는 문제를 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "401-1",
                    "sentence": "We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events).",
                    "sentence_kor": "영화 시놉스의 다중 레이블 데이터 세트와 이야기의 다양한 속성(예: 장르, 이벤트 유형)을 나타내는 태그 세트를 실험한다.",
                    "tag": "2"
                },
                {
                    "index": "401-2",
                    "sentence": "Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses.",
                    "sentence_kor": "우리가 제안한 다중 뷰 모델은 계층적 주의를 사용하여 시놉스와 리뷰를 인코딩하고 시놉스만 사용하는 방법에 대한 개선을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "401-3",
                    "sentence": "Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision.",
                    "sentence_kor": "마지막으로, 우리는 이러한 모델을 활용하여 직접 감독 없이 리뷰에서 보완적인 스토리 특성 세트를 추출할 수 있는 방법을 시연한다.",
                    "tag": "4"
                },
                {
                    "index": "401-4",
                    "sentence": "We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.",
                    "sentence_kor": "데이터 세트와 소스 코드를 https://ritual.uh.edu/multiview-tag-2020에서 공개적으로 사용할 수 있도록 했다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "402",
            "abstractID": "EMNLP_abs-402",
            "text": [
                {
                    "index": "402-0",
                    "sentence": "Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English.",
                    "sentence_kor": "굴절변화는 싱가포르 구어체 영어와 아프리카계 미국인 방언 영어와 같은 월드 잉글리시스의 공통적인 특징이다.",
                    "tag": "1"
                },
                {
                    "index": "402-1",
                    "sentence": "Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust.",
                    "sentence_kor": "인간 독자의 이해는 대개 비표준 굴절로 인해 손상되지 않지만, 현재 NLP 시스템은 아직 강력하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "402-2",
                    "sentence": "We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols.",
                    "sentence_kor": "우리는 문법 정보를 특수 기호로 다시 주입하기 전에 굴절된 단어를 기본 형식으로 줄여 영어 텍스트를 토큰화하는 방법인 BITE(Base-Infection Encoding)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "402-3",
                    "sentence": "Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data.",
                    "sentence_kor": "인코딩을 사용하여 다운스트림 작업에 대해 사전 훈련된 NLP 모델을 미세 조정하면 클린 데이터의 성능을 유지하면서 굴절 적에 대해 방어할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "402-4",
                    "sentence": "Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE.",
                    "sentence_kor": "BITE를 사용하는 모델은 명시적 교육 및 변환 모델이 없는 비표준 굴절형이 있는 방언으로 더 잘 일반화되며 BITE로 훈련될 때 더 빨리 수렴된다.",
                    "tag": "3"
                },
                {
                    "index": "402-5",
                    "sentence": "Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers.",
                    "sentence_kor": "마지막으로, 인코딩이 인기 있는 데이터 기반 하위 단어 토큰라이저의 어휘 효율성을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "402-6",
                    "sentence": "Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.",
                    "sentence_kor": "어휘 효율성을 정량적으로 평가하기 위한 선행 작업이 없었기 때문에, 우리는 그렇게 하기 위한 메트릭스를 제안한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "403",
            "abstractID": "EMNLP_abs-403",
            "text": [
                {
                    "index": "403-0",
                    "sentence": "A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories.",
                    "sentence_kor": "문법적 성별 체계는 어휘를 상대적으로 고정된 소수의 문법 범주로 나눈다.",
                    "tag": "1"
                },
                {
                    "index": "403-1",
                    "sentence": "How similar are these gender systems across languages?",
                    "sentence_kor": "이러한 성 체계는 언어들 간에 얼마나 유사한가?",
                    "tag": "1"
                },
                {
                    "index": "403-2",
                    "sentence": "To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation.",
                    "sentence_kor": "유사성을 정량화하기 위해 성별 시스템을 확장 정의하여 언어의 성별 시스템과 클러스터 평가 간의 비교 문제를 줄인다.",
                    "tag": "2+3"
                },
                {
                    "index": "403-3",
                    "sentence": "We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems.",
                    "sentence_kor": "우리는 공동체 탐지 분야(Driver and Kroeber, 1932; Cattell, 1945)에서 클러스터 평가를 위한 통계 도구의 풍부한 목록을 빌려 성별 시스템 간의 유사성을 측정하기 위한 새로운 정보 이론적 메트릭스를 만들 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "403-4",
                    "sentence": "We first validate our metrics, then use them to measure gender system similarity in 20 languages.",
                    "sentence_kor": "먼저 측정 기준을 확인한 다음 20개 언어로 성별 시스템 유사성을 측정하는 데 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "403-5",
                    "sentence": "We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages.",
                    "sentence_kor": "그런 다음 우리는 우리의 성별 체계 유사성만으로도 언어들 사이의 역사적 관계를 재구성하기에 충분한지 묻는다.",
                    "tag": "3"
                },
                {
                    "index": "403-6",
                    "sentence": "Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages.",
                    "sentence_kor": "이를 위해, 우리는 현존하는 인도유럽어족에 대한 계통발생학적 나무를 유도하는 역사언어학에서 유래된, 그러나 가시 있는 문제에 대해 계통발생학적 예측을 한다.",
                    "tag": "3"
                },
                {
                    "index": "403-7",
                    "sentence": "Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.",
                    "sentence_kor": "특히, 우리 계통수의 같은 가지에 있는 언어들은 눈에 띄게 유사한 반면, 분리된 가지에 있는 언어들은 우연에 지나지 않는다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "404",
            "abstractID": "EMNLP_abs-404",
            "text": [
                {
                    "index": "404-0",
                    "sentence": "The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models.",
                    "sentence_kor": "중국어 단어 분할(CWS) 시스템의 성능은 심층 신경망의 빠른 개발, 특히 대규모 사전 교육 모델의 성공적인 사용과 함께 점차 안정기에 도달했다.",
                    "tag": "1"
                },
                {
                    "index": "404-1",
                    "sentence": "In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task.",
                    "sentence_kor": "본 논문에서 우리는 우리가 성취한 것을 검토하고 CWS 과제에서 남은 것을 재고한다.",
                    "tag": "2"
                },
                {
                    "index": "404-2",
                    "sentence": "Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning.",
                    "sentence_kor": "방법론적으로, 우리는 기존 CWS 시스템에 대해 세분화된 평가를 제안한다. 이 평가는 (데이터셋 내 설정에서) 기존 모델의 장단점을 진단할 수 있을 뿐만 아니라 다중 기준 학습을 수행할 때 서로 다른 기준 간의 불일치를 정량화하고 부정적인 전송 문제를 완화할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "404-3",
                    "sentence": "Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research.",
                    "sentence_kor": "전략적으로 본 논문에서 새로운 모델을 제안하는 것을 목표로 하지 않음에도 불구하고, 철저한 분석뿐만 아니라 8개 모델과 7개 데이터 세트에 대한 포괄적인 실험은 향후 연구를 위한 유망한 방향을 찾을 수 있을 것이다.",
                    "tag": "5"
                },
                {
                    "index": "404-4",
                    "sentence": "We make all codes publicly available and release an interface that can quickly evaluate and diagnose user’s models: https://github.com/neulab/InterpretEval",
                    "sentence_kor": "우리는 모든 코드를 공개하고 사용자 모델을 신속하게 평가하고 진단할 수 있는 인터페이스를 출시한다. https://github.com/neulab/InterpretEval",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "405",
            "abstractID": "EMNLP_abs-405",
            "text": [
                {
                    "index": "405-0",
                    "sentence": "We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary.",
                    "sentence_kor": "우리는 중국어 텍스트를 발음 사전 없이 만다린어로 발음하는 것을 배우는 프로그램을 시연한다.",
                    "tag": "2"
                },
                {
                    "index": "405-1",
                    "sentence": "From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations.",
                    "sentence_kor": "한자 음절과 한자 음절의 평행하지 않은 흐름에서, 그것은 문자와 발음 사이의 다대다 매핑을 형성한다.",
                    "tag": "3"
                },
                {
                    "index": "405-2",
                    "sentence": "Using unsupervised methods, the program effectively deciphers writing into speech.",
                    "sentence_kor": "이 프로그램은 감독되지 않은 방법을 사용하여 효과적으로 작문을 음성으로 해독한다.",
                    "tag": "3+4"
                },
                {
                    "index": "405-3",
                    "sentence": "Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.",
                    "sentence_kor": "토큰 수준의 문자 대 기호 정확도는 89%로 이전 작업의 22% 정확도를 크게 웃돈다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "406",
            "abstractID": "EMNLP_abs-406",
            "text": [
                {
                    "index": "406-0",
                    "sentence": "Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion.",
                    "sentence_kor": "지식 그래프(KG) 완료를 위한 효과적이고 해석 가능한 방법을 모색하기 위해 멀티홉 추론이 최근 널리 연구되었다.",
                    "tag": "1"
                },
                {
                    "index": "406-1",
                    "sentence": "Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning.",
                    "sentence_kor": "대부분의 이전 추론 방법은 개체 사이에 충분한 경로를 가진 밀집 KG에 대해 설계되었지만 추론을 위한 희박한 경로만 포함하는 희소 KG에서는 잘 작동하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "406-2",
                    "sentence": "On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths.",
                    "sentence_kor": "한편, 희소 KG에는 정보가 적게 들어 있어 모델이 올바른 경로를 선택하기가 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "406-3",
                    "sentence": "On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult.",
                    "sentence_kor": "반면에, 대상 실체에 대한 증거 경로가 없다는 점도 추론 과정을 어렵게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "406-4",
                    "sentence": "To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs.",
                    "sentence_kor": "이러한 문제를 해결하기 위해, 우리는 새로운 동적 기대 및 완료 전략을 적용하여 희소 KG에 대한 다중 홉 추론 모델을 제안한다. (1) 기대 전략은 임베딩 기반 모델의 잠재 예측을 활용하여 희소 KG에 대한 잠재적 경로 검색을 더 많이 수행하도록 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "406-5",
                    "sentence": "(2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs.",
                    "sentence_kor": "(2) 예상 정보를 기반으로 완료 전략은 경로 검색 중 추가 동작으로 에지를 동적으로 추가함으로써 KG의 희소성 문제를 더욱 완화한다.",
                    "tag": "3"
                },
                {
                    "index": "406-6",
                    "sentence": "The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines.",
                    "sentence_kor": "Freebase, NELL 및 Wikidata에서 샘플링된 5개 데이터 세트에 대한 실험 결과는 우리의 방법이 최첨단 기준선을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "406-7",
                    "sentence": "Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.",
                    "sentence_kor": "코드와 데이터 세트는 https://github.com/THU-KEG/DacKGR에서 얻을 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "407",
            "abstractID": "EMNLP_abs-407",
            "text": [
                {
                    "index": "407-0",
                    "sentence": "Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations.",
                    "sentence_kor": "엔티티 정렬, 엔티티 유형 추론 및 기타 관련 작업을 통해 지식 그래프(KG)에 대한 연관성을 포착하면 포괄적인 지식 표현과 함께 NLP 애플리케이션에 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "407-1",
                    "sentence": "Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs.",
                    "sentence_kor": "유클리드 임베딩에 구축된 최근 관련 방법은 KG의 계층 구조 및 다양한 척도로 인해 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "407-2",
                    "sentence": "They also depend on high embedding dimensions to realize enough expressiveness.",
                    "sentence_kor": "또한 충분한 표현성을 실현하기 위해 높은 임베딩 차원에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "407-3",
                    "sentence": "Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association.",
                    "sentence_kor": "다르게, 우리는 지식 연결을 위해 저차원 쌍곡 임베딩을 사용하여 탐구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "407-4",
                    "sentence": "We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation.",
                    "sentence_kor": "우리는 KG 임베딩을 위한 쌍곡선 관계 그래프 신경망을 제안하고 쌍곡선 변환과 지식 연관성을 포착한다.",
                    "tag": "2+3"
                },
                {
                    "index": "407-5",
                    "sentence": "Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.",
                    "sentence_kor": "개체 정렬 및 유형 추론에 대한 광범위한 실험은 우리 방법의 효과와 효율성을 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "408",
            "abstractID": "EMNLP_abs-408",
            "text": [
                {
                    "index": "408-0",
                    "sentence": "Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding.",
                    "sentence_kor": "이벤트 시간 관계 추출은 정보 추출에 중요한 작업이며 자연어 이해에 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "408-1",
                    "sentence": "Prior systems leverage deep learning and pre-trained language models to improve the performance of the task.",
                    "sentence_kor": "이전 시스템은 딥러닝 및 사전 훈련된 언어 모델을 활용하여 작업 성능을 개선합니다.",
                    "tag": "1"
                },
                {
                    "index": "408-2",
                    "sentence": "However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data.",
                    "sentence_kor": "그러나 이러한 시스템은 종종 두 가지 단점을 겪는다. 1) 신경 모델에 기초한 최대 사후(MAP) 추론을 수행할 때, 이전 시스템은 절대적으로 정확한 것으로 가정되는 구조화된 지식만 사용했다. 즉, 하드 제약 조건, 2) 제한된 양의 데이터로 훈련할 때 지배적인 시간 관계에 대한 편향된 예측.",
                    "tag": "1"
                },
                {
                    "index": "408-3",
                    "sentence": "To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge.",
                    "sentence_kor": "이러한 문제를 해결하기 위해 확률적 영역 지식에 의해 구성된 분포 제약 조건으로 심층 신경망을 강화하는 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "408-4",
                    "sentence": "We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks.",
                    "sentence_kor": "우리는 라그랑지안 완화를 통해 제한된 추론 문제를 해결하고 이를 엔드 투 엔드 이벤트 시간 관계 추출 작업에 적용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "408-5",
                    "sentence": "Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.",
                    "sentence_kor": "실험 결과는 우리의 프레임워크가 뉴스 및 임상 영역에서 널리 사용되는 두 데이터 세트에서 강력한 통계적 중요성을 지닌 기준선 신경망 모델을 개선할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "409",
            "abstractID": "EMNLP_abs-409",
            "text": [
                {
                    "index": "409-0",
                    "sentence": "Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task.",
                    "sentence_kor": "시간 지식 그래프(TKG)에서 누락된 사실을 추론하는 것은 근본적이고 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "409-1",
                    "sentence": "Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations.",
                    "sentence_kor": "이전 연구에서는 정적 지식 그래프 방법을 보강하여 시간에 따른 표현을 활용함으로써 이 문제에 접근했다.",
                    "tag": "1"
                },
                {
                    "index": "409-2",
                    "sentence": "However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions.",
                    "sentence_kor": "그러나 이러한 방법은 예측을 강화하기 위해 최근 시간 단계의 다중 홉 구조 정보와 시간적 사실을 명시적으로 활용하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "409-3",
                    "sentence": "Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs.",
                    "sentence_kor": "또한 이전 연구에서는 TKG에서 개체 분포의 시간적 희소성과 가변성을 명시적으로 다루지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "409-4",
                    "sentence": "We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques.",
                    "sentence_kor": "그래프 신경 네트워크, 시간 역학 모델, 데이터 귀속 및 주파수 기반 게이트 기술을 결합하여 이러한 과제를 해결하기 위한 TeMP(Temporal Message Passing) 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "409-5",
                    "sentence": "Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks.",
                    "sentence_kor": "표준 TKG 작업에 대한 실험 결과, 우리의 접근 방식이 이전 최신 기술에 비해 상당한 이득을 제공하여 세 가지 표준 벤치마크에서 Hits@10의 평균 상대적인 개선을 달성하였다.",
                    "tag": "4"
                },
                {
                    "index": "409-6",
                    "sentence": "Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.",
                    "sentence_kor": "또한 분석 결과 TKG 데이터 세트 내 및 전체에 걸쳐 중요한 변동 원인이 드러났으며, 특정 설정에서 이전 기술 수준을 능가하는 단순하지만 강력한 기준선을 몇 가지 소개한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "410",
            "abstractID": "EMNLP_abs-410",
            "text": [
                {
                    "index": "410-0",
                    "sentence": "Transformers have proved effective in many NLP tasks.",
                    "sentence_kor": "변압기는 많은 NLP 작업에서 효과적이라는 것이 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "410-1",
                    "sentence": "However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively).",
                    "sentence_kor": "그러나 이들의 훈련에는 첨단 최적화기 및 학습 속도 스케줄러를 신중하게 설계하는 것과 관련된 사소한 노력이 필요하다(예: 기존 SGD는 트랜스포머를 효과적으로 훈련시키지 못한다).",
                    "tag": "1"
                },
                {
                    "index": "410-2",
                    "sentence": "Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives.",
                    "sentence_kor": "여기서 우리의 목표는 경험적 및 이론적 관점 모두에서 트랜스포머 훈련을 복잡하게 만드는 것을 이해하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "410-3",
                    "sentence": "Our analysis reveals that unbalanced gradients are not the root cause of the instability of training.",
                    "sentence_kor": "분석 결과 불균형 그레이디언트가 훈련 불안정성의 근본 원인이 아니다.",
                    "tag": "3"
                },
                {
                    "index": "410-4",
                    "sentence": "Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output.",
                    "sentence_kor": "대신, 우리는 교육에 실질적으로 영향을 미치는 증폭 효과를 식별한다. 다층 트랜스포머 모델의 각 계층에 대해 잔여 분기에 대한 의존도가 높으면 작은 매개 변수 섭동이 증폭되고(예: 매개 변수 업데이트) 모델 출력에 상당한 장애가 발생하기 때문에 교육이 불안정해진다.",
                    "tag": "3"
                },
                {
                    "index": "410-5",
                    "sentence": "Yet we observe that a light dependency limits the model potential and leads to inferior trained models.",
                    "sentence_kor": "그러나 우리는 빛 의존성이 모델 잠재력을 제한하고 열등한 훈련 모델을 초래한다는 것을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "410-6",
                    "sentence": "Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage.",
                    "sentence_kor": "분석에서 영감을 받아 초기 단계의 훈련을 안정화시키고 후반 단계에서 모든 잠재력을 발휘하기 위해 Admin(Adaptive Model 초기화)을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "410-7",
                    "sentence": "Extensive experiments show that Admin is more stable, converges faster, and leads to better performance",
                    "sentence_kor": "광범위한 실험 결과, 관리자가 더 안정적이고, 더 빠르게 수렴되며, 더 나은 성능으로 이어짐",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "411",
            "abstractID": "EMNLP_abs-411",
            "text": [
                {
                    "index": "411-0",
                    "sentence": "In this work, we present an empirical study of generation order for machine translation.",
                    "sentence_kor": "본 연구에서는 기계 번역을 위한 생성 순서에 대한 경험적 연구를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "411-1",
                    "sentence": "Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies.",
                    "sentence_kor": "삽입 기반 모델링의 최근 발전을 바탕으로, 우리는 먼저 임의의 오라클 생성 정책을 따르도록 모델을 교육할 수 있는 소프트 오더 보상 프레임워크를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "411-2",
                    "sentence": "We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders.",
                    "sentence_kor": "그런 다음 이 프레임워크를 사용하여 정보가 없는 주문, 위치 기반 주문, 빈도 기반 주문, 내용 기반 주문 및 모델 기반 주문을 포함한 다양한 생성 주문을 탐색한다.",
                    "tag": "3"
                },
                {
                    "index": "411-3",
                    "sentence": "Curiously, we find that for the WMT’14 English → German and WMT’18 English → Chinese translation tasks, order does not have a substantial impact on output quality.",
                    "sentence_kor": "흥미롭게도 WMT'14 영어 → 독일어 및 WMT'18 영어 → 중국어 번역 작업의 경우 순서가 출력 품질에 큰 영향을 미치지 않는다.",
                    "tag": "4"
                },
                {
                    "index": "411-4",
                    "sentence": "Moreover, for English → German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.",
                    "sentence_kor": "또한 영어 → 독일어의 경우, 알파벳 순서와 최단 우선 순서와 같은 비동조적인 순서가 표준 트랜스포머의 성능과 일치할 수 있다는 것을 발견했는데, 이는 높은 성능을 달성하기 위해 기존의 왼쪽에서 오른쪽 세대가 필요하지 않을 수 있음을 시사한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "412",
            "abstractID": "EMNLP_abs-412",
            "text": [
                {
                    "index": "412-0",
                    "sentence": "Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation.",
                    "sentence_kor": "조건부 마스크 언어 모델(CMLM) 교육은 기계 번역과 같은 비 자기 회귀 및 반 자기 회귀 시퀀스 생성 작업에서 성공적인 것으로 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "412-1",
                    "sentence": "Given a trained CMLM, however, it is not clear what the best inference strategy is.",
                    "sentence_kor": "그러나 훈련된 CMLM을 고려할 때 최상의 추론 전략이 무엇인지 명확하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "412-2",
                    "sentence": "We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective.",
                    "sentence_kor": "우리는 부분 시퀀스의 조건부 확률의 인수분해로 마스킹 추론을 공식화하고, 이것이 성능에 해를 끼치지 않는다는 것을 보여주고, 이 관점에 의해 동기화된 많은 간단한 휴리스틱을 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "412-3",
                    "sentence": "We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks.",
                    "sentence_kor": "우리는 표준 \"마스크 예측\" 알고리즘보다 유리한 임계값 전략을 식별하고 기계 번역 작업에 대한 동작 분석을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "413",
            "abstractID": "EMNLP_abs-413",
            "text": [
                {
                    "index": "413-0",
                    "sentence": "Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer.",
                    "sentence_kor": "모호성은 개방형 도메인 질문 답변에 내재되어 있다. 특히 새로운 주제를 탐구할 때 모호하지 않은 단일 답변을 가진 질문을 하는 것은 어려울 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "413-1",
                    "sentence": "In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity.",
                    "sentence_kor": "본 논문에서 우리는 모든 그럴듯한 답을 찾은 다음 모호성을 해결하기 위해 각 질문에 대한 질문을 다시 쓰는 새로운 개방형 도메인 질문 답변 작업인 AmbigQA를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "413-2",
                    "sentence": "To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.",
                    "sentence_kor": "이 작업을 연구하기 위해 기존 개방형 도메인 QA 벤치마크인 NQ-open의 14,042개 질문을 포함하는 데이터 세트인 AmbigNQ를 구성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "413-3",
                    "sentence": "We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references.",
                    "sentence_kor": "우리는 NQ-open 질문의 절반 이상이 사건 및 개체 참조와 같은 모호성의 다양한 출처와 함께 모호하다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "413-4",
                    "sentence": "We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort.",
                    "sentence_kor": "우리는 또한 Ambig의 강력한 기준선 모델을 제시한다.QA는 NQ-오픈을 통합하는 약하게 감독되는 학습의 이점을 보여주며, 우리의 새로운 과제와 데이터가 향후 상당한 연구 노력을 지원할 것임을 강력히 시사한다.",
                    "tag": "4+5"
                },
                {
                    "index": "413-5",
                    "sentence": "Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.",
                    "sentence_kor": "데이터와 기준선은 https://nlp.cs.washington.edu/ambigqa에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "414",
            "abstractID": "EMNLP_abs-414",
            "text": [
                {
                    "index": "414-0",
                    "sentence": "In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks.",
                    "sentence_kor": "본 논문에서 우리는 기계 판독 이해(MRC), 질문 생성 및 자연어 추론 작업에 대한 제어 가능한 재작성 기반 질문 데이터 확대(CRQDA)라는 새로운 데이터 확대 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "414-1",
                    "sentence": "We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples.",
                    "sentence_kor": "우리는 질문 데이터 확대 작업을 상황 관련, 고품질 및 다양한 질문 데이터 샘플을 생성하기 위해 제한된 질문 다시 쓰기 문제로 취급한다.",
                    "tag": "2+3"
                },
                {
                    "index": "414-2",
                    "sentence": "CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space.",
                    "sentence_kor": "CRQDA는 트랜스포머 자동 인코더를 사용하여 원래 이산 질문을 연속 임베딩 공간에 매핑한다.",
                    "tag": "3"
                },
                {
                    "index": "414-3",
                    "sentence": "It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization.",
                    "sentence_kor": "그런 다음 사전 훈련된 MRC 모델을 사용하여 그라데이션 기반 최적화를 통해 질문 표현을 반복적으로 수정한다.",
                    "tag": "3"
                },
                {
                    "index": "414-4",
                    "sentence": "Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data.",
                    "sentence_kor": "마지막으로, 수정된 질문 표현은 추가 질문 데이터 역할을 하는 이산 공간에 다시 매핑된다.",
                    "tag": "3"
                },
                {
                    "index": "414-5",
                    "sentence": "Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.",
                    "sentence_kor": "SQuAD 2.0, SQuAD 1.1 질문 생성 및 QNLI 작업에 대한 종합적인 실험은 CRQDA의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "415",
            "abstractID": "EMNLP_abs-415",
            "text": [
                {
                    "index": "415-0",
                    "sentence": "Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB).",
                    "sentence_kor": "복잡한 질의응답(CQA)은 지식 기반(KB)에서 복잡한 자연어 질문에 답하는 것을 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "415-1",
                    "sentence": "However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level.",
                    "sentence_kor": "그러나 기존의 신경 프로그램 유도(NPI) 접근방식은 질문이 다른 유형을 가질 때, 본질적으로 다른 특성(예: 난이도)을 가질 때 불균등한 성능을 보인다.",
                    "tag": "1"
                },
                {
                    "index": "415-2",
                    "sentence": "This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions.",
                    "sentence_kor": "본 논문은 질문의 잠재적 분포 편향을 해결하기 위해 CQA의 프로그램 유도에 대한 메타 강화 학습 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "415-3",
                    "sentence": "Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data.",
                    "sentence_kor": "우리의 방법은 메타 학습 프로그래머를 교육 데이터에서 검색된 가장 유사한 질문에 빠르고 효과적으로 적응시킨다.",
                    "tag": "4"
                },
                {
                    "index": "415-4",
                    "sentence": "The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set.",
                    "sentence_kor": "그런 다음 메타 학습 정책은 지원 세트의 유사한 질문에 대한 시험 궤적과 보상을 활용하여 좋은 프로그래밍 정책을 학습하는 데 사용된다.",
                    "tag": "4"
                },
                {
                    "index": "415-5",
                    "sentence": "Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set.",
                    "sentence_kor": "우리의 방법은 각 지원 세트에서 검색된 상위 5개 질문에 대해 5개의 시험 궤적만 사용하고 훈련 세트의 1%에서만 구성된 작업에 대한 메타 훈련을 사용하면서 CQA 데이터 세트에서 최첨단 성능을 달성한다(Saha 등, 2018).",
                    "tag": "5"
                },
                {
                    "index": "415-6",
                    "sentence": "We have released our code at https://github.com/DevinJake/MRL-CQA.",
                    "sentence_kor": "https://github.com/DevinJake/MRL-CQA에서 코드를 공개했습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "416",
            "abstractID": "EMNLP_abs-416",
            "text": [
                {
                    "index": "416-0",
                    "sentence": "Offensive content is pervasive in social media and a reason for concern to companies and government organizations.",
                    "sentence_kor": "소셜 미디어에는 유해한 콘텐츠가 만연해 있으며, 기업과 정부 기관에는 우려의 이유가 되고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "416-1",
                    "sentence": "Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression).",
                    "sentence_kor": "최근 그러한 내용의 다양한 형태(예: 증오 발언, 사이버 불링, 사이버 공격)를 탐지하는 여러 연구들이 발표되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "416-2",
                    "sentence": "The clear majority of these studies deal with English partially because most annotated datasets available contain English data.",
                    "sentence_kor": "이러한 연구의 분명한 대다수는 부분적으로 영어를 다루는데, 이는 사용 가능한 주석이 달린 대부분의 데이터셋에 영어 데이터가 포함되어 있기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "416-3",
                    "sentence": "In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources.",
                    "sentence_kor": "본 논문에서, 우리는 교차 언어 맥락 단어 임베딩을 적용하고 더 적은 자원을 가진 언어로 예측을 하기 위해 이전 학습을 적용하여 이용할 수 있는 영어 데이터를 활용한다.",
                    "tag": "2"
                },
                {
                    "index": "416-4",
                    "sentence": "We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish.",
                    "sentence_kor": "우리는 벵골어, 힌디어 및 스페인어의 비교 데이터에 대한 예측을 예측하고 벵골어의 경우 0.8415 F1 매크로, 힌디어의 경우 0.8568 F1 매크로, 스페인어의 경우 0.7513 F1 매크로의 결과를 보고한다.",
                    "tag": "2"
                },
                {
                    "index": "416-5",
                    "sentence": "Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.",
                    "sentence_kor": "마지막으로, 우리의 접근 방식이 이 세 가지 언어에 대해 최근 공유된 작업에 제출된 최상의 시스템과 비교하여 이 작업에 대한 교차 언어 맥락 임베딩 및 전송 학습의 견고성을 확인한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "417",
            "abstractID": "EMNLP_abs-417",
            "text": [
                {
                    "index": "417-0",
                    "sentence": "We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.",
                    "sentence_kor": "디코딩 격자를 구성하고 신경 언어 모델로 격자를 검색하여 어려운 단어 기반 대체 코드를 해결한다.",
                    "tag": "2"
                },
                {
                    "index": "417-1",
                    "sentence": "We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.",
                    "sentence_kor": "우리는 1700년대 후반과 1800년대 초반에 미국 의회도서관에서 입수한 제임스 윌킨슨 육군 장군과 스페인 왕실의 요원들 사이에 교환된 일련의 암호화된 편지에 우리의 방법을 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "417-2",
                    "sentence": "We are able to decipher 75.1% of the cipher-word tokens correctly.",
                    "sentence_kor": "우리는 암호어 토큰의 75.1%를 정확하게 해독할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "418",
            "abstractID": "EMNLP_abs-418",
            "text": [
                {
                    "index": "418-0",
                    "sentence": "Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties.",
                    "sentence_kor": "방언의 예측은 광범위한 적용과 함께 중요한 언어 처리 작업이지만, 기존의 작업은 대부분 거친 다양성에 한정되어 있다.",
                    "tag": "1"
                },
                {
                    "index": "418-1",
                    "sentence": "Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message.",
                    "sentence_kor": "지리 위치 연구에서 영감을 받아, 우리는 마이크로 다이얼 식별(MDI)이라는 새로운 과제를 제안하고 단일의 짧은 메시지가 주어진 세분화된 다양성(도시와 같은 작은)을 예측할 수 있는 놀라운 능력을 가진 새로운 언어 모델인 MARBERT를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "418-2",
                    "sentence": "For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models.",
                    "sentence_kor": "모델링을 위해 공간적, 언어적으로 동기화된 다양한 다중 작업 학습 모델을 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "418-3",
                    "sentence": "To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks.",
                    "sentence_kor": "모델의 유용성을 보여주기 위해 작업에 적합한 아랍어 마이크로 부분(저자원)의 새로운 대규모 데이터 세트를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "418-4",
                    "sentence": "MARBERT predicts micro-dialects with 9.9% F1, 76 better than a majority class baseline.",
                    "sentence_kor": "MARBERT는 다수 등급 기준선보다 76 나은 9.9% F1로 마이크로 다이얼을 예측한다.",
                    "tag": "4"
                },
                {
                    "index": "418-5",
                    "sentence": "Our new language model also establishes new state-of-the-art on several external tasks.",
                    "sentence_kor": "우리의 새로운 언어 모델은 또한 몇 가지 외부 작업에 대한 새로운 최첨단 기술을 확립한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "419",
            "abstractID": "EMNLP_abs-419",
            "text": [
                {
                    "index": "419-0",
                    "sentence": "In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent.",
                    "sentence_kor": "본 논문에서는 도메인 내 병렬 말뭉치가 부족하거나 존재하지 않는 낮은 리소스를 가진 도메인별 변환에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "419-1",
                    "sentence": "One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method.",
                    "sentence_kor": "이 사례에 대한 한 가지 공통적이고 효과적인 전략은 역번역 방법을 사용하여 도메인 내 단일 언어 데이터를 활용하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "419-2",
                    "sentence": "However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation.",
                    "sentence_kor": "그러나 합성 병렬 데이터는 불완전한 도메인 외부 시스템에서 생성되어 도메인 적응의 성능이 저하되기 때문에 노이즈가 매우 크다.",
                    "tag": "1"
                },
                {
                    "index": "419-3",
                    "sentence": "To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 합성 이중 언어 데이터의 변환을 개선하기 위해 도메인 복구(DR) 모델을 도입하는 새로운 반복 도메인 리페어링 역번역 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "419-4",
                    "sentence": "To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly.",
                    "sentence_kor": "이를 위해 단일 언어 문장을 왕복 번역하여 DR 모델 교육에 해당하는 데이터를 구성한 다음 쌍으로 구성된 DR 및 NMT 모델을 공동으로 최적화하는 통합 교육 프레임워크를 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "419-5",
                    "sentence": "Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.",
                    "sentence_kor": "특정 도메인 간 및 일반 도메인에서 특정 도메인에 NMT 모델을 적응시키는 실험은 우리가 제안한 접근 방식의 효과를 입증하며, 적응되지 않은 모델 및 역번역 모델에 비해 평균 15.79 및 4.47 BLEU 개선을 달성한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "420",
            "abstractID": "EMNLP_abs-420",
            "text": [
                {
                    "index": "420-0",
                    "sentence": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance.",
                    "sentence_kor": "역번역은 단일 언어 데이터를 신경 기계 번역(NMT)에 활용하는 효과적인 방법임이 입증되었으며 역번역을 반복적으로 수행하면 모델 성능을 더욱 향상시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "420-1",
                    "sentence": "Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain.",
                    "sentence_kor": "결과 합성 데이터가 고품질이고 대상 도메인을 반영해야 하기 때문에 역번역할 단일 언어 데이터를 선택하는 것이 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "420-2",
                    "sentence": "To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text.",
                    "sentence_kor": "이 두 가지 목표를 달성하기 위해 데이터 선택과 가중치 전략이 제안되었으며, 일반적인 관행은 대상 영역에 가깝지만 평균 일반 도메인 텍스트와 다른 표본을 선택하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "420-3",
                    "sentence": "In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models.",
                    "sentence_kor": "본 논문에서, 우리는 일반적으로 사용되는 이 접근법에 대한 통찰력을 제공하고 반복 역번역 모델에 적용되는 동적 커리큘럼 학습 전략에 일반화한다.",
                    "tag": "2"
                },
                {
                    "index": "420-4",
                    "sentence": "In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration.",
                    "sentence_kor": "또한 문장의 현재 품질과 이전 반복에 대한 개선 모두에 기초한 가중치 전략을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "420-5",
                    "sentence": "We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs.",
                    "sentence_kor": "도메인 적응, 저자원 및 고자원 MT 설정과 두 언어 쌍에 대한 모델을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "420-6",
                    "sentence": "Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",
                    "sentence_kor": "실험 결과는 우리의 방법이 경쟁 기준선에 비해 최대 1.8 BLEU 점수를 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "421",
            "abstractID": "EMNLP_abs-421",
            "text": [
                {
                    "index": "421-0",
                    "sentence": "We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language.",
                    "sentence_kor": "우리는 신경 기계 번역(NMT) 시스템을 활용하여 새로운 언어의 의미 분석기를 현지화하는 툴킷인 시맨틱 파서 로컬라이저(SPL)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "421-1",
                    "sentence": "Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.",
                    "sentence_kor": "우리의 방법론은 (1) 공공 웹 사이트에서 스크랩된 로컬 엔터티로 기계 번역 데이터 세트를 증강하여 대상 언어로 자동으로 훈련 데이터를 생성하고 (2) 인간 번역 문장의 몇 샷 증가를 추가하고 새로운 XLMR-LSTM 의미 분석기를 훈련하며 (3) 인간 번역을 사용하여 큐레이션된 자연 발화에 대한 모델을 테스트하는 것이다.비틀다",
                    "tag": "3"
                },
                {
                    "index": "421-2",
                    "sentence": "We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains.",
                    "sentence_kor": "Schema2의 현재 기능을 확장하여 접근 방식의 효과를 평가합니다.오픈 웹의 QA(영어 질문 답변) 시스템인 QA는 레스토랑 및 호텔 도메인에 대해 10개의 새로운 언어를 제공합니다.",
                    "tag": "3"
                },
                {
                    "index": "421-3",
                    "sentence": "Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set.",
                    "sentence_kor": "우리 모델은 호텔 도메인의 경우 61%에서 69%, 레스토랑 도메인의 경우 64%에서 78% 사이의 전체 테스트 정확도를 달성하는데, 이는 골드 잉글리쉬 데이터에 대해 훈련된 영어 파서와 검증 세트의 몇 가지 예에서 얻은 69%에서 80%에 비해 유리하다.",
                    "tag": "4"
                },
                {
                    "index": "421-4",
                    "sentence": "We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested.",
                    "sentence_kor": "우리는 우리의 접근 방식이 호텔의 경우 30%, 테스트된 언어의 하위 집합에 대해 지역화된 온톨로지를 가진 레스토랑의 경우 40% 이상 이전 최첨단 방법론을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "421-5",
                    "sentence": "Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.",
                    "sentence_kor": "당사의 방법론을 통해 소프트웨어 개발자는 기계 변환을 활용하여 새로운 도메인에 대한 QA 시스템에 24시간 이내에 새로운 언어 기능을 추가할 수 있습니다.",
                    "tag": "5"
                },
                {
                    "index": "421-6",
                    "sentence": "Our code is released open-source.",
                    "sentence_kor": "우리의 코드는 오픈소스이다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "422",
            "abstractID": "EMNLP_abs-422",
            "text": [
                {
                    "index": "422-0",
                    "sentence": "Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages.",
                    "sentence_kor": "교차 언어 단어 임베딩은 언어 간에 지식을 전달한다. 고자원 언어로 훈련된 모델은 저자원 언어로 예측할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "422-1",
                    "sentence": "We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem.",
                    "sentence_kor": "우리는 주어진 분류 문제에 대해 언어 간 단어 임베딩을 신속하게 개선하기 위한 대화형 시스템 CLIME을 도입한다.",
                    "tag": "2"
                },
                {
                    "index": "422-2",
                    "sentence": "First, CLIME ranks words by their salience to the downstream task.",
                    "sentence_kor": "첫째, CLIME은 다운스트림 작업에 대한 경지에 따라 단어의 순위를 매긴다.",
                    "tag": "3"
                },
                {
                    "index": "422-3",
                    "sentence": "Then, users mark similarity between keywords and their nearest neighbors in the embedding space.",
                    "sentence_kor": "그런 다음 사용자는 내장 공간에서 키워드와 가장 가까운 이웃 간의 유사성을 표시합니다.",
                    "tag": "3"
                },
                {
                    "index": "422-4",
                    "sentence": "Finally, CLIME updates the embeddings using the annotations.",
                    "sentence_kor": "마지막으로 CLIME은 주석을 사용하여 임베딩을 업데이트합니다.",
                    "tag": "3"
                },
                {
                    "index": "422-5",
                    "sentence": "We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur.",
                    "sentence_kor": "우리는 CLIME을 네 가지 저자원 언어로 건강 관련 텍스트를 식별하는 데 평가한다. 일로카노, 신할레세, 티그리냐, 위구르.",
                    "tag": "3"
                },
                {
                    "index": "422-6",
                    "sentence": "Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings.",
                    "sentence_kor": "CLIME에 의해 정제된 임베딩은 더 미묘한 단어 의미를 포착하고 원래 임베딩보다 높은 테스트 정확도를 갖는다.",
                    "tag": "4"
                },
                {
                    "index": "422-7",
                    "sentence": "CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.",
                    "sentence_kor": "CLIME은 종종 능동적 학습 기준보다 더 빨리 정확도를 향상시키며 능동적 학습과 쉽게 결합하여 결과를 개선할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "423",
            "abstractID": "EMNLP_abs-423",
            "text": [
                {
                    "index": "423-0",
                    "sentence": "We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring.",
                    "sentence_kor": "우리는 후보 생성과 후보 재점수에 문장 순서 정보를 통합하는 간단한 문서 정렬 방법을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "423-1",
                    "sentence": "Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task.",
                    "sentence_kor": "우리의 방법은 WMT16 문서 정렬 공유 작업에 대해 이전에 발표된 최상의 결과와 비교하여 61%의 상대적 오류 감소를 가져온다.",
                    "tag": "3+4"
                },
                {
                    "index": "423-2",
                    "sentence": "Our method improves downstream MT performance on web-scraped Sinhala–English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release.",
                    "sentence_kor": "이 방법은 ParaCrawl의 웹 스크래핑된 Sinhala-영문 문서에서 다운스트림 MT 성능을 향상시켜 최신 ParaCraw 릴리스에 사용된 문서 정렬 방법을 능가한다.",
                    "tag": "3+4"
                },
                {
                    "index": "423-3",
                    "sentence": "It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.",
                    "sentence_kor": "또한 동일한 다국어 임베딩을 사용하는 유사한 말뭉치 방법을 능가하여 최종 목표가 문장 수준 bitext일지라도 문장 순서를 이용하는 것이 유익하다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "424",
            "abstractID": "EMNLP_abs-424",
            "text": [
                {
                    "index": "424-0",
                    "sentence": "In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks.",
                    "sentence_kor": "본 논문에서는 다국어 및 이중 언어 코퍼라를 사용하여 대규모 언어 간 사전 교육 모델을 교육하고 다양한 언어 간 작업에서 성능을 평가하기 위한 새로운 벤치마크 데이터 세트인 XGLUE를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "424-1",
                    "sentence": "Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages.",
                    "sentence_kor": "영어로 라벨이 부착되어 있고 자연어 이해 과제만 포함하는 GLUE(Wang et al.,2019)와 비교하여, XGLUE는 (1) 언어 간 사전 훈련을 위한 크기가 다른 두 개의 말뭉치를 제공하며, (2) 자연어 이해와 생성 시나리오를 모두 다루는 11개의 다양한 과제를 제공한다. 각 작업에 대해 레이블이 지정된 데이터를 여러 언어로 제공합니다.",
                    "tag": "3"
                },
                {
                    "index": "424-2",
                    "sentence": "We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline.",
                    "sentence_kor": "우리는 XGLUE에서 강력한 기준선으로 평가되는 이해와 생성 작업을 모두 다루기 위해 언어 간 사전 교육 모델 유니코더(Huang et al., 2019)를 확장한다.",
                    "tag": "3"
                },
                {
                    "index": "424-3",
                    "sentence": "We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.",
                    "sentence_kor": "또한 비교를 위해 다국어 BERT, XLM 및 XLM-R의 기본 버전(12계층)을 평가한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "425",
            "abstractID": "EMNLP_abs-425",
            "text": [
                {
                    "index": "425-0",
                    "sentence": "The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches.",
                    "sentence_kor": "선형 체인 조건부 랜덤 필드(CRF) 모델은 가장 널리 사용되는 신경 시퀀스 레이블링 접근법 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "425-1",
                    "sentence": "Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model.",
                    "sentence_kor": "전진 및 비터비 알고리즘과 같은 정확한 확률론적 추론 알고리즘은 일반적으로 CRF 모델의 훈련 및 예측 단계에 적용된다.",
                    "tag": "1"
                },
                {
                    "index": "425-2",
                    "sentence": "However, these algorithms require sequential computation that makes parallelization impossible.",
                    "sentence_kor": "그러나 이러한 알고리즘은 병렬화를 불가능하게 만드는 순차적 계산을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "425-3",
                    "sentence": "In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model.",
                    "sentence_kor": "본 논문에서는 CRF 모델에 병렬 가능한 근사 변동 추론 알고리즘을 사용할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "425-4",
                    "sentence": "Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction.",
                    "sentence_kor": "이 알고리즘을 기반으로, 우리는 더 빠른 훈련과 예측을 위해 병렬화가 가능한 엔드 투 엔드 네트워크를 형성하기 위해 신경 CRF 모델의 인코더와 연결할 수 있는 대략적인 추론 네트워크를 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "425-5",
                    "sentence": "The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.",
                    "sentence_kor": "경험적 결과에 따르면 제안된 접근 방식은 기존의 CRF 접근 방식에 비해 긴 문장으로 디코딩 속도가 12.7배 향상되고 경쟁 정확도가 향상되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "426",
            "abstractID": "EMNLP_abs-426",
            "text": [
                {
                    "index": "426-0",
                    "sentence": "Named Entity Recognition (NER) is a fundamental task in natural language processing.",
                    "sentence_kor": "명명된 개체 인식(NER)은 자연어 처리의 기본 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "426-1",
                    "sentence": "In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures.",
                    "sentence_kor": "중첩 구조를 가진 실체를 식별하기 위해, 최근 기존의 시퀀스 라벨링 접근법 또는 지시된 하이퍼그래프 구조를 기반으로 많은 정교한 방법이 개발되었다.",
                    "tag": "1"
                },
                {
                    "index": "426-2",
                    "sentence": "Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity.",
                    "sentence_kor": "이러한 방법은 성공적이긴 하지만 내포된 구조에 대한 표현력과 모델 복잡성 간의 균형을 잘 맞추지 못하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "426-3",
                    "sentence": "To address this issue, we present a novel nested NER model named HIT.",
                    "sentence_kor": "이 문제를 해결하기 위해 HIT라는 새로운 중첩 NER 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "426-4",
                    "sentence": "Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary.",
                    "sentence_kor": "우리가 제안한 HIT 모델은 (1) 명시적 경계 토큰과 (2) 경계 내 토큰 간의 긴밀한 내부 연결을 포함하여 (내포된) 명명된 엔티티와 관련된 두 가지 핵심 속성을 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "426-5",
                    "sentence": "Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary.",
                    "sentence_kor": "특히 (1) 경계 토큰을 탐지하기 위한 다중 헤드 자기 주의 메커니즘 및 바이-아핀 분류기를 기반으로 머리-꼬리 검출기를 설계하고, (2) 경계 내 내부 토큰 연결을 특성화하기 위한 기존의 시퀀스 라벨링 접근 방식을 기반으로 토큰 상호 작용 태그거를 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "426-6",
                    "sentence": "Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.",
                    "sentence_kor": "세 개의 공개 NER 데이터 세트에 대한 실험은 제안된 HIT가 최첨단 성능을 달성한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "427",
            "abstractID": "EMNLP_abs-427",
            "text": [
                {
                    "index": "427-0",
                    "sentence": "Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task.",
                    "sentence_kor": "슈퍼태깅은 일반적으로 조합 범주 문법(CCG) 구문 분석에 중요한 작업으로 간주되며, 여기서 상황별 정보의 효과적인 모델링은 이 작업에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "427-1",
                    "sentence": "However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM).",
                    "sentence_kor": "그러나 기존 연구는 강력한 인코더(예: bi-LSTM)를 적용하는 것을 제외하고 상황별 특징을 활용하기 위한 노력을 제한했다.",
                    "tag": "1"
                },
                {
                    "index": "427-2",
                    "sentence": "In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information.",
                    "sentence_kor": "본 논문에서 우리는 상황별 정보를 활용하는 새로운 솔루션을 통해 신경 CCG 슈퍼태그를 강화하기 위해 세심한 그래프 컨볼루션 네트워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "427-3",
                    "sentence": "Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly.",
                    "sentence_kor": "특히, 우리는 어휘에서 추출한 청크(n그램)에서 그래프를 작성하고 그래프에 주의를 기울여 청크 내 및 청크 전체의 컨텍스트에서 다른 단어 쌍에 가중치가 부여되고 그에 따라 슈퍼태깅이 용이하도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "427-4",
                    "sentence": "The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing.",
                    "sentence_kor": "CCG뱅크에서 수행된 실험은 우리의 접근 방식이 슈퍼태깅과 구문 분석 측면에서 이전의 모든 연구보다 우수하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "427-5",
                    "sentence": "Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.",
                    "sentence_kor": "추가 분석은 CCG 슈퍼태깅을 강화하기 위해 단어 쌍으로부터 차별적으로 학습하는 접근 방식의 각 구성요소의 효과를 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "428",
            "abstractID": "EMNLP_abs-428",
            "text": [
                {
                    "index": "428-0",
                    "sentence": "Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization.",
                    "sentence_kor": "데이터 확대 기법은 일반화를 용이하게 할 때 머신러닝 성능을 개선하기 위해 널리 사용되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "428-1",
                    "sentence": "In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences.",
                    "sentence_kor": "본 연구에서는 선형화된 라벨 문장에 대해 훈련된 언어 모델을 사용하여 저자원 태깅 작업에 대한 고품질 합성 데이터를 생성하는 새로운 증강 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "428-2",
                    "sentence": "Our method is applicable to both supervised and semi-supervised settings.",
                    "sentence_kor": "이 방법은 감독 및 준감독 설정에 모두 적용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "428-3",
                    "sentence": "For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks.",
                    "sentence_kor": "감독 설정을 위해 명명된 개체 인식(NER), 음성 일부(POS) 태그 및 종단 간 대상 기반 감정 분석(E2E-TBSA) 작업에 대한 광범위한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "428-4",
                    "sentence": "For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base.",
                    "sentence_kor": "준감독 설정의 경우, 라벨이 부착되지 않은 데이터와 라벨이 부착되지 않은 데이터 및 지식 기반 조건 하에서 NER 작업에 대한 방법을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "428-5",
                    "sentence": "The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.",
                    "sentence_kor": "결과는 특히 주어진 골드 교육 데이터가 적을 때 우리의 방법이 지속적으로 기준선을 능가할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "429",
            "abstractID": "EMNLP_abs-429",
            "text": [
                {
                    "index": "429-0",
                    "sentence": "With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits.",
                    "sentence_kor": "자연어 처리 작업의 모델이 확산됨에 따라 모델과 모델의 상대적 장점 간의 차이를 이해하는 것은 더욱 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "429-1",
                    "sentence": "Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices.",
                    "sentence_kor": "정확도, BLEU 또는 F1과 같은 전체론적 메트릭 간의 차이를 단순히 살펴본다고 해서 특정 방법이 다르게 수행되는 이유 또는 방법 및 다양한 데이터셋이 모델 설계 선택에 미치는 영향을 알 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "429-2",
                    "sentence": "In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task.",
                    "sentence_kor": "본 논문에서 우리는 명명된 개체 인식(NER) 과제에 대한 해석 가능한 평가를 위한 일반적인 방법론을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "429-3",
                    "sentence": "The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems.",
                    "sentence_kor": "제안된 평가 방법을 통해 모델과 데이터 세트의 차이와 상호 작용을 해석하여 현재 시스템의 강점과 약점을 식별할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "429-4",
                    "sentence": "By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval",
                    "sentence_kor": "분석 도구를 사용할 수 있게 함으로써 향후 연구자들이 유사한 분석을 쉽게 실행하고 이 분야의 발전을 도모할 수 있도록 합니다. https://github.com/neulab/InterpretEval",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "430",
            "abstractID": "EMNLP_abs-430",
            "text": [
                {
                    "index": "430-0",
                    "sentence": "Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit.",
                    "sentence_kor": "파일 이름, 앨범 이름 또는 스케줄 제목과 같은 개방형 어휘 슬롯은 신경 기반 슬롯 채우기 모델의 성능을 크게 저하시킵니다. 이러한 슬롯 슬롯은 사실상 무제한 집합에서 값을 취할 수 있고 의미 제한이나 길이 제한이 없기 때문입니다.",
                    "tag": "1"
                },
                {
                    "index": "430-1",
                    "sentence": "In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context.",
                    "sentence_kor": "본 논문에서는 개방형 어휘 슬롯 단어에 내재된 로컬 의미론을 글로벌 컨텍스트에서 명시적으로 분리하는 강력한 적대적 모델 무관 슬롯 채우기 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "430-2",
                    "sentence": "We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence.",
                    "sentence_kor": "우리는 얽힌 문맥적 의미론에서 벗어나 전체 문장 수준의 전체적인 문맥에 더 초점을 맞추는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "430-3",
                    "sentence": "Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.",
                    "sentence_kor": "두 개의 공개 데이터 세트에 대한 실험 결과, 우리의 방법은 정상 슬롯의 성능을 저하시키지 않고 모든 오픈 어휘 슬롯에서 통계적으로 유의한 마진을 보이며 다른 방법을 지속적으로 능가하는 것으로 나타났다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "431",
            "abstractID": "EMNLP_abs-431",
            "text": [
                {
                    "index": "431-0",
                    "sentence": "Text autoencoders are commonly used for conditional generation tasks such as style transfer.",
                    "sentence_kor": "텍스트 자동 인코더는 일반적으로 스타일 전송과 같은 조건부 생성 작업에 사용됩니다.",
                    "tag": "1"
                },
                {
                    "index": "431-1",
                    "sentence": "We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding (Emb2Emb).",
                    "sentence_kor": "사전 훈련된 자동 인코더를 사용할 수 있고 자동 인코더의 내장 공간 내에서 매핑 학습만 필요로 하는 플러그 앤 플레이 방법을 제안한다. 임베딩 대 임베딩(Emb2Emb) 훈련.",
                    "tag": "2+3"
                },
                {
                    "index": "431-2",
                    "sentence": "This reduces the need for labeled training data for the task and makes the training procedure more efficient.",
                    "sentence_kor": "이를 통해 작업에 대한 레이블링된 교육 데이터의 필요성을 줄이고 교육 절차를 보다 효율적으로 만들 수 있습니다.",
                    "tag": "2"
                },
                {
                    "index": "431-3",
                    "sentence": "Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors.",
                    "sentence_kor": "이 방법의 성공에 중요한 것은 자동 인코더의 매니폴드에 매핑된 임베딩을 유지하기 위한 손실 기간과 오프셋 벡터를 학습하여 매니폴드를 탐색하도록 훈련된 매핑이다.",
                    "tag": "3"
                },
                {
                    "index": "431-4",
                    "sentence": "Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.",
                    "sentence_kor": "시퀀스 투 시퀀스 감독 유무에 대한 스타일 전송 작업에 대한 평가에 따르면 우리의 방법은 최대 4배 더 빠르면서도 강력한 기준선보다 더 우수하거나 그에 필적할 만한 성능을 발휘한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "432",
            "abstractID": "EMNLP_abs-432",
            "text": [
                {
                    "index": "432-0",
                    "sentence": "Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns.",
                    "sentence_kor": "대비 추정을 사용하여 지식 그래프에서 실체와 관계에 대한 저차원 표현을 학습하는 것은 연결 패턴을 추론하는 확장 가능하고 효과적인 방법을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "432-1",
                    "sentence": "A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data.",
                    "sentence_kor": "대조적 학습 접근법의 중요한 측면은 삽입 모델이 차별적 표현을 학습하고 관찰된 데이터의 중요한 특성을 찾도록 하는 하드 마이너스 샘플을 생성하는 손상 분포의 선택이다.",
                    "tag": "1+2"
                },
                {
                    "index": "432-2",
                    "sentence": "While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives.",
                    "sentence_kor": "이전의 방법들은 너무 단순한 손상 분포, 즉, 정보가 없는 부정 분포를 쉽게 생성하거나 까다로운 최적화 체계를 가진 정교한 적대적 분포를 채택하지만, 차선의 부정 분포를 초래하는 알려진 그래프 구조를 명시적으로 통합하지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "432-3",
                    "sentence": "In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood.",
                    "sentence_kor": "본 논문에서, 우리는 노드의 k-hop 주변에서 음의 샘플을 선택하여 풍부한 그래프 구조를 활용하는 저렴한 음의 샘플링 전략인 구조 인식 음의 샘플링(SANS)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "432-4",
                    "sentence": "Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.",
                    "sentence_kor": "경험적으로, 우리는 SANS가 의미론적으로 의미 있는 부정적인 것을 발견하며 SOTA 접근법과 경쟁적이지만 추가적인 매개 변수나 어려운 적대적 최적화가 필요하지 않음을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "433",
            "abstractID": "EMNLP_abs-433",
            "text": [
                {
                    "index": "433-0",
                    "sentence": "We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering).",
                    "sentence_kor": "우리는 언어 모델을 특정 대상 과제(예: 질문 답변)에 효과적으로 적응할 수 있도록 자체 감독 사전 훈련을 위해 주어진 텍스트의 도메인 및 작업 적응 마스크를 자동으로 생성하는 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "433-1",
                    "sentence": "Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts.",
                    "sentence_kor": "특히, 우리는 마스킹 정책을 학습하는 새로운 강화 학습 기반 프레임워크를 제시한다. 즉, 대상 언어 모델의 추가 사전 교육을 위해 생성된 마스크를 사용하는 것이 보이지 않는 텍스트에 대한 작업 성능을 향상시키는 데 도움이 된다.",
                    "tag": "3"
                },
                {
                    "index": "433-2",
                    "sentence": "We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text.",
                    "sentence_kor": "우리는 강화 학습을 위해 엔트로피 정규화 및 경험 재생과 함께 오프 폴리시 액터 크리틱을 사용하고 주어진 텍스트에서 단어의 상대적 중요성을 고려할 수 있는 트랜스포머 기반 정책 네트워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "433-3",
                    "sentence": "We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.",
                    "sentence_kor": "BERT 및 DistilB를 사용하여 여러 질문 답변 및 텍스트 분류 데이터 세트에서 NMG(Neural Mask Generator)를 검증한다.언어 모델로서 ERT는 최적의 적응형 마스킹을 자동으로 학습하여 규칙 기반 마스킹 전략을 능가한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "434",
            "abstractID": "EMNLP_abs-434",
            "text": [
                {
                    "index": "434-0",
                    "sentence": "The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.",
                    "sentence_kor": "자연어 생성 작업에 대한 자기 회귀 모델의 성능은 깊고 자기 주의적인 아키텍처의 채택으로 인해 크게 향상되었다.",
                    "tag": "1"
                },
                {
                    "index": "434-1",
                    "sentence": "However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.",
                    "sentence_kor": "그러나 이러한 이득은 추론 속도를 방해하여 최첨단 모델을 실제의 시간에 민감한 환경에서 배치하는 데 번거로움을 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "434-2",
                    "sentence": "We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.",
                    "sentence_kor": "우리는 지식 증류에 대한 모방 학습 관점에 의해 주도되는 자기 회귀 모델을 위한 압축 기술을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "434-3",
                    "sentence": "The algorithm is designed to address the exposure bias problem.",
                    "sentence_kor": "이 알고리즘은 노출 바이어스 문제를 해결하도록 설계되었습니다.",
                    "tag": "1+2"
                },
                {
                    "index": "434-4",
                    "sentence": "On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.",
                    "sentence_kor": "번역 및 요약과 같은 원형 언어 생성 작업에서 우리의 방법은 시퀀스 수준의 지식 증류와 같은 다른 증류 알고리즘을 지속적으로 능가한다.",
                    "tag": "3+4"
                },
                {
                    "index": "434-5",
                    "sentence": "Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.",
                    "sentence_kor": "이 방법으로 훈련된 학생 모델은 처음부터 학습한 학생 모델보다 1.4에서 4.8 BLEU/ROUGE 점수를 얻는 동시에 교사 모델에 비해 추론 속도를 최대 14배 향상시킨다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "435",
            "abstractID": "EMNLP_abs-435",
            "text": [
                {
                    "index": "435-0",
                    "sentence": "Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models.",
                    "sentence_kor": "입력에 악의 없어 보이는 수정을 수행하는 자연어 처리 시스템에 대한 적대적 공격은 대상 모델에 임의적인 실수를 유발할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "435-1",
                    "sentence": "Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models.",
                    "sentence_kor": "큰 우려를 제기했지만, 그러한 적대적 공격은 NLP 모델의 견고성을 추정하는 데 활용될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "435-2",
                    "sentence": "Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable.",
                    "sentence_kor": "연속 데이터 영역(예: 이미지)의 적대적 예제 생성과 비교하여 텍스트 공간은 분리되고 구별할 수 없기 때문에 원래 의미를 보존하는 적대적 텍스트를 생성하는 것은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "435-3",
                    "sentence": "To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks.",
                    "sentence_kor": "이러한 과제를 처리하기 위해 광범위한 NLP 작업에 적용할 수 있는 목표 제어 가능한 적대적 공격 프레임워크 T3를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "435-4",
                    "sentence": "In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation.",
                    "sentence_kor": "특히, 우리는 이산 텍스트 데이터를 연속적인 표현 공간에 내장하기 위한 트리 기반 자동 인코더를 제안하며, 여기서 적대적 동요를 최적화한다.",
                    "tag": "3"
                },
                {
                    "index": "435-5",
                    "sentence": "A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level.",
                    "sentence_kor": "그런 다음 새로운 트리 기반 디코더를 적용하여 생성된 텍스트의 구문 정확성을 정규화하고 문장(T3(Sent) 또는 워드(T3(Word) 수준에서 이를 조작한다.",
                    "tag": "3"
                },
                {
                    "index": "435-6",
                    "sentence": "We consider two most representative NLP tasks: sentiment analysis and question answering (QA).",
                    "sentence_kor": "우리는 가장 대표적인 두 가지 NLP 작업인 감정 분석 및 질문 답변(QA)을 고려한다.",
                    "tag": "3"
                },
                {
                    "index": "435-7",
                    "sentence": "Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human.",
                    "sentence_kor": "광범위한 실험 결과와 인간 연구에 따르면 T3에서 생성된 적대적 텍스트는 인간을 오도하지 않고 표적 오답을 출력하기 위해 NLP 모델을 성공적으로 조작할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "435-8",
                    "sentence": "Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice.",
                    "sentence_kor": "또한 생성된 적대적 텍스트는 전송성이 높아 실제로 블랙박스 공격을 가능하게 한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "435-9",
                    "sentence": "Our work sheds light on an effective and general way to examine the robustness of NLP models.",
                    "sentence_kor": "우리의 연구는 NLP 모델의 견고성을 조사하기 위한 효과적이고 일반적인 방법을 조명한다.",
                    "tag": "3"
                },
                {
                    "index": "435-10",
                    "sentence": "Our code is publicly available at https://github.com/AI-secure/T3/.",
                    "sentence_kor": "우리의 코드는 https://github.com/AI-secure/T3/에서 공개적으로 이용할 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "436",
            "abstractID": "EMNLP_abs-436",
            "text": [
                {
                    "index": "436-0",
                    "sentence": "Large language models have recently achieved state of the art performance across a wide variety of natural language tasks.",
                    "sentence_kor": "대규모 언어 모델은 최근 다양한 자연어 작업에서 최첨단 성능을 달성하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "436-1",
                    "sentence": "Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large?",
                    "sentence_kor": "한편, 이러한 모델의 크기와 대기 시간이 크게 증가하여 사용 비용이 많이 들고 언어 모델이 커야 하는가라는 흥미로운 의문이 제기되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "436-2",
                    "sentence": "We study this question through the lens of model compression.",
                    "sentence_kor": "우리는 모델 압축의 렌즈를 통해 이 문제를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "436-3",
                    "sentence": "We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training.",
                    "sentence_kor": "낮은 순위 인수분해를 사용하여 각 가중치 매트릭스를 매개 변수화하고 훈련 중에 1순위 구성 요소를 적응적으로 제거하여 일반적이고 구조화된 가지치기 접근법을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "436-4",
                    "sentence": "On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference.",
                    "sentence_kor": "언어 모델링 작업에서 우리의 구조화된 접근 방식은 다양한 압축 수준에서 다른 구조화되지 않고 블록 구조화된 가지치기 기준선을 능가하는 동시에 훈련과 추론 중에 상당한 속도 향상을 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "436-5",
                    "sentence": "We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.",
                    "sentence_kor": "우리는 또한 우리의 방법이 대규모 언어 모델에서 적응형 단어 임베딩의 가지치기 및 여러 다운스트림 미세 조정 분류 벤치마크에서 BERT 모델을 가지치기하는 데 적용될 수 있음을 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "437",
            "abstractID": "EMNLP_abs-437",
            "text": [
                {
                    "index": "437-0",
                    "sentence": "Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest.",
                    "sentence_kor": "최근 연구는 대상 작업의 영역에서 광범위한 적용 범위 상황별 임베딩 모델의 적응의 중요성을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "437-1",
                    "sentence": "Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens.",
                    "sentence_kor": "훈련 신호가 무작위로 마스킹된 토큰의 작은 비율에서 나오기 때문에 현재 자체 감독 적응 방법은 간단하다.",
                    "tag": "1"
                },
                {
                    "index": "437-2",
                    "sentence": "In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed.",
                    "sentence_kor": "본 논문에서, 우리는 신중한 마스킹 전략이 필요한 곳에 자체 감독을 할당함으로써 도메인에 대한 마스킹 언어 모델(MLM)의 지식 격차를 보다 효과적으로 메울 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "437-3",
                    "sentence": "Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM.",
                    "sentence_kor": "또한, 기본 MLM에 의해 재구성하기 어려운 토큰을 적대적으로 마스킹하여 효과적인 훈련 전략을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "437-4",
                    "sentence": "The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming.",
                    "sentence_kor": "적대적 목표는 토큰 하위 집합에 대한 까다로운 조합 최적화 문제로 이어지며, 우리는 이를 다양한 하한 및 동적 프로그래밍으로 완화를 통해 효율적으로 해결한다.",
                    "tag": "3"
                },
                {
                    "index": "437-5",
                    "sentence": "On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.",
                    "sentence_kor": "명명된 개체 인식을 포함하는 6개의 비지도 도메인 적응 작업에서 우리의 방법은 랜덤 마스킹 전략을 크게 능가하고 +1.64 F1 점수까지 향상된다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "438",
            "abstractID": "EMNLP_abs-438",
            "text": [
                {
                    "index": "438-0",
                    "sentence": "Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model.",
                    "sentence_kor": "현대의 텍스트 분류 모델은 모델에 의해 잘못 분류되는 인간이 알 수 없는 원본 텍스트의 교란된 버전인 적대적인 예에 취약하다.",
                    "tag": "1"
                },
                {
                    "index": "438-1",
                    "sentence": "Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples.",
                    "sentence_kor": "NLP의 최근 연구는 규칙 기반 동의어 대체 전략을 사용하여 적대적인 예를 생성한다.",
                    "tag": "1"
                },
                {
                    "index": "438-2",
                    "sentence": "These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans.",
                    "sentence_kor": "이러한 전략은 상황에 맞지 않고 비정상적으로 복잡한 토큰 교체로 이어질 수 있으며, 이는 사람에 의해 쉽게 식별될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "438-3",
                    "sentence": "We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model.",
                    "sentence_kor": "BERT 마스킹 언어 모델의 상황별 동요를 사용하여 적대적 예를 생성하기 위한 블랙박스 공격인 BAE를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "438-4",
                    "sentence": "BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens.",
                    "sentence_kor": "BAE는 텍스트의 일부를 마스킹하고 BERT-MLM을 활용하여 마스킹된 토큰에 대한 대안을 생성하여 원래 텍스트에 토큰을 대체하고 삽입합니다.",
                    "tag": "3"
                },
                {
                    "index": "438-5",
                    "sentence": "Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.",
                    "sentence_kor": "자동 및 인간 평가를 통해 BAE가 이전 작업에 비해 문법성과 의미적 일관성이 향상된 적대적 사례를 생성하는 것 외에도 더 강력한 공격을 수행한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "439",
            "abstractID": "EMNLP_abs-439",
            "text": [
                {
                    "index": "439-0",
                    "sentence": "Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks.",
                    "sentence_kor": "사전 훈련된 대규모 변압기 기반 언어 모델은 광범위한 NLP 작업에서 인상적인 결과를 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "439-1",
                    "sentence": "In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model.",
                    "sentence_kor": "지난 몇 년 동안 지식 증류(KD)는 계산 비용이 많이 드는 모델을 자원 효율적인 경량 모델로 압축하는 대중적인 패러다임이 되었다.",
                    "tag": "1"
                },
                {
                    "index": "439-2",
                    "sentence": "However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues.",
                    "sentence_kor": "그러나 대부분의 KD 알고리즘, 특히 NLP의 경우 개인 정보 보호 문제로 인해 사용할 수 없는 원래 교육 데이터 세트의 접근성에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "439-3",
                    "sentence": "To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT).",
                    "sentence_kor": "이 문제를 해결하기 위해 대규모 변압기 기반 모델(예: BERT) 압축용으로 설계된 적대적 자체 감독 데이터 무증류(AS-DFD)라는 새로운 2단계 무증류 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "439-4",
                    "sentence": "To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge.",
                    "sentence_kor": "이산 공간에서 텍스트 생성을 피하기 위해 교사의 숨겨진 지식에서 유사 임베딩을 조작하는 플러그 앤 플레이 임베딩 추측 방법을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "439-5",
                    "sentence": "Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner.",
                    "sentence_kor": "한편, 학생의 능력을 정량화하기 위한 자체 감독 모듈을 사용하여, 우리는 적대적 훈련 방식으로 의사 임베딩의 어려움을 적응시킨다.",
                    "tag": "2+3"
                },
                {
                    "index": "439-6",
                    "sentence": "To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks.",
                    "sentence_kor": "우리가 아는 한, 우리의 프레임워크는 NLP 작업을 위해 설계된 최초의 데이터 없는 증류 프레임워크이다.",
                    "tag": "1"
                },
                {
                    "index": "439-7",
                    "sentence": "We verify the effectiveness of our method on several text classification datasets.",
                    "sentence_kor": "우리는 여러 텍스트 분류 데이터 세트에 대한 방법의 효과를 검증한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "440",
            "abstractID": "EMNLP_abs-440",
            "text": [
                {
                    "index": "440-0",
                    "sentence": "Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods.",
                    "sentence_kor": "경사도 기반 방법으로 적대적 샘플을 생성하기 어렵기 때문에 이산 데이터(예: 텍스트)에 대한 적대적 공격은 연속 데이터(예: 이미지)보다 훨씬 더 어려운 것으로 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "440-1",
                    "sentence": "Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency.",
                    "sentence_kor": "텍스트에 대한 현재 성공적인 공격 방법은 일반적으로 문자 또는 단어 수준에서 경험적 대체 전략을 채택하며, 의미적 일관성과 언어 유창성을 유지하면서 가능한 대체 조합의 방대한 공간에서 최적의 솔루션을 찾는 것은 여전히 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "440-2",
                    "sentence": "In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT.",
                    "sentence_kor": "본 논문에서, 우리는 BERT에 예시된 사전 훈련된 마스킹 언어 모델을 사용하여 적대적 샘플을 생성하는 고품질의 효과적인 방법인 BERT-Attack을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "440-3",
                    "sentence": "We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly.",
                    "sentence_kor": "우리는 BERT를 다운스트림 작업에서 미세 조정된 모델 및 기타 심층 신경 모델과 비교하여 대상 모델이 잘못 예측하도록 성공적으로 오도할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "440-4",
                    "sentence": "Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved.",
                    "sentence_kor": "우리의 방법은 성공률과 섭동 비율 모두에서 최첨단 공격 전략을 능가하는 반면 생성된 적대적 샘플은 유창하고 의미론적으로 보존된다.",
                    "tag": "3+4"
                },
                {
                    "index": "440-5",
                    "sentence": "Also, the cost of calculation is low, thus possible for large-scale generations.",
                    "sentence_kor": "또한, 계산 비용이 낮기 때문에 대규모 발전도 가능합니다.",
                    "tag": "2"
                },
                {
                    "index": "440-6",
                    "sentence": "The code is available at https://github.com/LinyangLee/BERT-Attack.",
                    "sentence_kor": "코드는 https://github.com/LinyangLee/BERT-Attack에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "441",
            "abstractID": "EMNLP_abs-441",
            "text": [
                {
                    "index": "441-0",
                    "sentence": "Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data.",
                    "sentence_kor": "자연어 처리에 대한 사전 훈련은 피해자 모델에 대한 질의 액세스만 가진 상대방이 해당 데이터에 대한 피해자의 라벨과 짝을 이루는 횡설수설한 입력 데이터로 훈련함으로써 피해자의 로컬 사본을 재구성하는 것을 더 쉽게 한다.",
                    "tag": "1"
                },
                {
                    "index": "441-1",
                    "sentence": "We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual.",
                    "sentence_kor": "우리는 이 추출 프로세스가 피해자가 단일 언어를 사용하는 동안 사전 훈련된 다국어 모델에서 초기화된 로컬 복사본으로 확장된다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "441-2",
                    "sentence": "The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages.",
                    "sentence_kor": "추출된 모델은 단일 언어 사용 피해자로부터 작업을 학습하지만, 여러 다른 언어로 작업자보다 훨씬 더 잘 일반화된다.",
                    "tag": "1"
                },
                {
                    "index": "441-3",
                    "sentence": "This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task.",
                    "sentence_kor": "이 작업은 대상 작업에 대한 언어로 된 올바른 형식의 입력을 다국어 추출 모델에 표시하지 않고 수행됩니다.",
                    "tag": "1"
                },
                {
                    "index": "441-4",
                    "sentence": "We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.",
                    "sentence_kor": "또한 몇 가지 실제 사례가 성능을 크게 향상시킬 수 있음을 입증하고 이러한 결과가 이러한 추출 방법이 어떻게 성공하는지를 분석한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "442",
            "abstractID": "EMNLP_abs-442",
            "text": [
                {
                    "index": "442-0",
                    "sentence": "We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora.",
                    "sentence_kor": "우리는 큰 텍스트 말뭉치의 도움을 받아 단어 사이에 is-a 관계가 존재하는지 여부(x,y)와 같은 하이퍼니미 탐지를 다룬다.",
                    "tag": "1"
                },
                {
                    "index": "442-1",
                    "sentence": "Most conventional approaches to this task have been categorized to be either pattern-based or distributional.",
                    "sentence_kor": "이 작업에 대한 대부분의 기존 접근법은 패턴 기반 또는 분포 방식으로 분류되었다.",
                    "tag": "1"
                },
                {
                    "index": "442-2",
                    "sentence": "Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved.",
                    "sentence_kor": "최근 연구에 따르면 대규모 Hearst 쌍을 추출하여 공급하고 보이지 않는 (x,y) 쌍의 희소성을 완화하면 패턴 기반 쌍이 우수하다.",
                    "tag": "1"
                },
                {
                    "index": "442-3",
                    "sentence": "However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern.",
                    "sentence_kor": "그러나 x 또는 y가 어떤 패턴에도 관여하지 않는 특정 희소성 사례에서는 유효하지 않게 된다.",
                    "tag": "1"
                },
                {
                    "index": "442-4",
                    "sentence": "For the first time, this paper quantifies the non-negligible existence of those specific cases.",
                    "sentence_kor": "본 논문은 처음으로 그러한 특정 사례의 무시할 수 없는 존재를 수량화한다.",
                    "tag": "1+2"
                },
                {
                    "index": "442-5",
                    "sentence": "We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases.",
                    "sentence_kor": "또한 분포 방법이 그러한 경우 패턴 기반 방법을 보완하는 데 이상적이라는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "442-6",
                    "sentence": "We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer.",
                    "sentence_kor": "우리는 패턴 기반 및 분산 모델이 각각 선호하는 경우에 원활하게 협력하는 보완 프레임워크를 고안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "442-7",
                    "sentence": "On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.",
                    "sentence_kor": "여러 벤치마크 데이터 세트에서 프레임워크는 경쟁력 있고 설명 가능한 개선 사항을 보여줍니다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "443",
            "abstractID": "EMNLP_abs-443",
            "text": [
                {
                    "index": "443-0",
                    "sentence": "This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth).",
                    "sentence_kor": "본 논문은 속성(생년월일)을 가진 위키피디아 범주(20세기 남성 작가) 내에서 수정자 구성 요소(20세기)에 자동으로 주석을 달 수 있는 개방형 도메인 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "443-1",
                    "sentence": "The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories.",
                    "sentence_kor": "이 주석들은 범주의 기본 의미를 정의하는 데 있어 구성 요소의 역할에 대한 의미론적으로 고정된 이해를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "443-2",
                    "sentence": "In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method.",
                    "sentence_kor": "위키피디아 범주의 평가 세트에 대한 실험에서, 제안된 방법은 구성 수정자를 이전 방법의 단순한 문자열이 아닌 의미론적으로 고정된 속성으로 주석을 달았다.",
                    "tag": "3"
                },
                {
                    "index": "443-3",
                    "sentence": "It does so at a better trade-off between precision and recall.",
                    "sentence_kor": "그것은 정확성과 회수력 사이의 더 나은 절충을 통해 그렇게 한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "444",
            "abstractID": "EMNLP_abs-444",
            "text": [
                {
                    "index": "444-0",
                    "sentence": "Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques.",
                    "sentence_kor": "상황별 임베딩은 다른 감각 표현 기법과 비교하여 WSD(Word Sense Disambigization) 작업에 압도적으로 효과적인 것으로 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "444-1",
                    "sentence": "However, these embeddings fail to embed sense knowledge in semantic networks.",
                    "sentence_kor": "그러나 이러한 임베딩은 의미론적 네트워크에 감지 지식을 내장하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "444-2",
                    "sentence": "In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses.",
                    "sentence_kor": "본 논문에서, 우리는 증강된 워드넷 글로스에서 기본 감각 임베딩을 얻은 후 감각 임베딩 향상과 WSD를 다시 구현하는 시도 메커니즘을 모두 위해 감각 관계를 활용하는 SREF(Synset Relation-Enhanced Framework)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "444-3",
                    "sentence": "Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure.",
                    "sentence_kor": "모든 단어와 어휘 샘플 데이터 세트에 대한 실험은 제안된 시스템이 이전의 지식 기반 시스템을 최소 5.5 F1 측정으로 물리치고 새로운 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "444-4",
                    "sentence": "When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.",
                    "sentence_kor": "이 시스템은 SemCor에서 학습한 감지 임베딩을 활용할 때, 20% SemCor 데이터만으로 이전의 모든 감독 시스템을 능가한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "445",
            "abstractID": "EMNLP_abs-445",
            "text": [
                {
                    "index": "445-0",
                    "sentence": "News headline generation aims to produce a short sentence to attract readers to read the news.",
                    "sentence_kor": "뉴스 헤드라인 세대는 독자들이 뉴스를 읽도록 유도하기 위해 짧은 문장을 만드는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "445-1",
                    "sentence": "One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines.",
                    "sentence_kor": "하나의 뉴스 기사에는 종종 서로 다른 사용자가 관심을 갖는 여러 개의 주요 문구가 포함되어 있어 자연스럽게 여러 개의 합리적인 헤드라인을 포함할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "445-2",
                    "sentence": "However, most existing methods focus on the single headline generation.",
                    "sentence_kor": "그러나 대부분의 기존 방법은 단일 헤드라인 생성에 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "445-3",
                    "sentence": "In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines.",
                    "sentence_kor": "본 논문에서, 우리는 사용자 관심의 핵심 문구를 사용하여 여러 개의 헤드라인을 생성할 것을 제안한다. 주요 아이디어는 뉴스에 대해 사용자에게 먼저 여러 개의 주요 문구를 생성한 다음 여러 개의 키프레이즈 관련 헤드라인을 생성하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "445-4",
                    "sentence": "We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines.",
                    "sentence_kor": "우리는 (a) 키 프레이즈, (b) 키 프레이즈 필터링 기사 및 (c) 키 프레이즈 관련 고품질 다양한 헤드라인을 생성하기 위한 원본 기사의 세 가지 소스를 입력으로 사용하는 멀티 소스 트랜스포머 디코더를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "445-5",
                    "sentence": "Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>.",
                    "sentence_kor": "또한, 우리는 뉴스 기사에서 관심 있는 주요 문구를 발굴하고 180,000개 이상의 <뉴스 기사, 헤드라인, 키 프레이즈> 정렬 3배를 포함하는 최초의 대규모 키 프레이즈 인식 뉴스 헤드라인 말뭉치를 구축할 수 있는 간단하고 효과적인 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "445-6",
                    "sentence": "Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.",
                    "sentence_kor": "실제 데이터 세트에 대한 광범위한 실험 비교는 제안된 방법이 품질과 다양성 측면에서 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "446",
            "abstractID": "EMNLP_abs-446",
            "text": [
                {
                    "index": "446-0",
                    "sentence": "Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods.",
                    "sentence_kor": "신경 추상 요약 시스템은 자체 감독 방법으로 사전 훈련된 대규모 데이터 세트와 모델의 가용성 덕분에 유망한 진전을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "446-1",
                    "sentence": "However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge.",
                    "sentence_kor": "그러나 추상적 요약 시스템을 위해 생성된 요약의 사실적 일관성을 보장하는 것은 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "446-2",
                    "sentence": "We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries.",
                    "sentence_kor": "생성된 요약에서 사실 오류를 식별하고 수정하여 이 문제를 해결하기 위해 편집 후 수정기 모듈을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "446-3",
                    "sentence": "The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries.",
                    "sentence_kor": "신경 교정기 모델은 참조 요약에 일련의 휴리스틱 변환을 적용하여 생성된 인공 예제에 대해 사전 교육된다.",
                    "tag": "3"
                },
                {
                    "index": "446-4",
                    "sentence": "These transformations are inspired by the error analysis of state-of-the-art summarization model outputs.",
                    "sentence_kor": "이러한 변환은 최첨단 요약 모델 출력의 오류 분석에서 영감을 얻었다.",
                    "tag": "3"
                },
                {
                    "index": "446-5",
                    "sentence": "Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset.",
                    "sentence_kor": "실험 결과에 따르면 우리 모델은 다른 신경 요약 모델에서 생성된 요약의 사실 오류를 수정할 수 있으며 CNN/DailyMail 데이터 세트의 사실 일관성 평가에서 이전 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "446-6",
                    "sentence": "We also find that transferring from artificial error correction to downstream settings is still very challenging.",
                    "sentence_kor": "또한 인위적인 오류 수정에서 다운스트림 설정으로 전환하는 것이 여전히 매우 어렵다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "447",
            "abstractID": "EMNLP_abs-447",
            "text": [
                {
                    "index": "447-0",
                    "sentence": "Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE.",
                    "sentence_kor": "압축 요약 시스템은 일반적으로 통사 규칙의 시드 세트에 의존하여 어떤 상황에서 범위 삭제가 허용되는지를 결정한 다음 ROUGE를 최적화하여 실제로 적용할 압축을 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "447-1",
                    "sentence": "In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience.",
                    "sentence_kor": "본 연구에서는 후보 범위에 대한 이러한 명시적 구문적 제약을 완화하고 대신 무엇을 삭제할 것인가에 대한 결정을 두 가지 데이터 중심 기준인 타당성과 민감성에 맡길 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "447-2",
                    "sentence": "Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary.",
                    "sentence_kor": "한 범위를 삭제하는 것은 문장의 문법성과 사실성을 유지하고 요약에서 중요한 정보를 제거하는 경우 명확하다.",
                    "tag": "3"
                },
                {
                    "index": "447-3",
                    "sentence": "Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied.",
                    "sentence_kor": "이들 각각은 사전 훈련된 트랜스포머 모델로 판단되며, 타당성과 돌출성이 없는 삭제만 적용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "447-4",
                    "sentence": "When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions.",
                    "sentence_kor": "간단한 추출 압축 파이프라인에 통합될 때, 우리의 방법은 벤치마크 데이터 세트에서 강력한 도메인 내 결과를 달성하며, 인간 평가에 따르면 신뢰성 모델은 일반적으로 문법 및 사실 삭제를 위해 선택된다.",
                    "tag": "3+4"
                },
                {
                    "index": "447-5",
                    "sentence": "Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.",
                    "sentence_kor": "또한, 우리의 접근 방식의 유연성으로 교차 도메인을 일반화할 수 있으며, 우리는 새로운 도메인의 500개 샘플에 대해서만 미세 조정된 시스템이 강력한 도메인 내 추출 모델과 일치하거나 초과할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "448",
            "abstractID": "EMNLP_abs-448",
            "text": [
                {
                    "index": "448-0",
                    "sentence": "Amongst the best means to summarize is highlighting.",
                    "sentence_kor": "요약할 수 있는 가장 좋은 방법 중 하나는 강조하기입니다.",
                    "tag": "1"
                },
                {
                    "index": "448-1",
                    "sentence": "In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text.",
                    "sentence_kor": "본 논문에서, 우리는 독자들이 많은 양의 텍스트를 더 쉽게 걸러낼 수 있도록 원본 문서에 겹쳐질 요약 강조 표시를 생성하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "448-2",
                    "sentence": "The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short.",
                    "sentence_kor": "메소드를 사용하면 요약자가 원래 의미를 왜곡하지 않도록 요약을 컨텍스트에서 이해할 수 있으며 추상 요약자는 대개 이에 미치지 못합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "448-3",
                    "sentence": "In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion.",
                    "sentence_kor": "특히 혼동을 피하기 위해 스스로 이해할 수 있는 자급자족적인 하이라이트를 제작하는 새로운 방법을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "448-4",
                    "sentence": "Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights.",
                    "sentence_kor": "우리의 방법은 결정론적 포인트 프로세스와 심층 상황별 표현을 결합하여 요약 하이라이트를 형성하기 위해 중요하거나 중복되지 않는 최적의 하위 문장 세그먼트 집합을 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "448-5",
                    "sentence": "To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets.",
                    "sentence_kor": "우리 방법의 유연성과 모델링 성능을 입증하기 위해 요약 데이터 세트에 대한 광범위한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "448-6",
                    "sentence": "Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.",
                    "sentence_kor": "우리의 분석은 강조가 미래 요약을 향한 연구의 유망한 길이라는 증거를 제공한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "449",
            "abstractID": "EMNLP_abs-449",
            "text": [
                {
                    "index": "449-0",
                    "sentence": "Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect.",
                    "sentence_kor": "문서와 대상 측면(예: 관심 주제)이 주어지면 측면 기반 추상 요약은 측면에 관한 요약을 생성하려고 시도한다.",
                    "tag": "1"
                },
                {
                    "index": "449-1",
                    "sentence": "Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics.",
                    "sentence_kor": "이전 연구는 일반적으로 미리 정의된 작은 측면 세트를 가정하고 다른 다양한 주제에 대한 요약에는 미치지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "449-2",
                    "sentence": "In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice.",
                    "sentence_kor": "본 연구에서는 문서와 관련된 임의의 측면을 요약하여 실제 작업의 적용을 크게 확대한다.",
                    "tag": "1+2"
                },
                {
                    "index": "449-3",
                    "sentence": "Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia.",
                    "sentence_kor": "감독 데이터의 부족으로 인해, 우리는 ConceptNet과 Wikipedia와 같은 풍부한 외부 지식 소스를 통합하는 새로운 약한 감독 구성 방법과 측면 모델링 계획을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "449-4",
                    "sentence": "Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.",
                    "sentence_kor": "실험에 따르면 우리의 접근 방식은 사전 정의되거나 임의적인 측면에서 실제 문서와 합성 문서를 모두 요약할 때 성능을 향상시킨다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "450",
            "abstractID": "EMNLP_abs-450",
            "text": [
                {
                    "index": "450-0",
                    "sentence": "In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph.",
                    "sentence_kor": "본 논문에서, 우리는 전체 문단에 대한 일관성 모델링을 강화하기 위해 문장 간의 더 나은 의존 관계를 포착하기 위해 BERT를 활용하여 새로운 BERT 강화 관계형 문장 주문 네트워크(BRSON이라 함)를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "450-1",
                    "sentence": "In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences.",
                    "sentence_kor": "특히, 우리는 BERT를 활용하여 문장 간의 깊은 의미론적 연결과 상대적인 순서를 이용하는 심층 관계형 모듈(DRM)을 사용하여 상대 순서 정보를 포인터 네트워크에 통합함으로써 새로운 관계형 포인터 디코더(RPD)를 개발한다.이를 통해 문장 간의 지역적 및 전역적 의존성을 강화할 수 있습니다.",
                    "tag": "2+3"
                },
                {
                    "index": "450-2",
                    "sentence": "Extensive evaluations are conducted on six public datasets.",
                    "sentence_kor": "6개의 공개 데이터 세트에 대해 광범위한 평가가 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "450-3",
                    "sentence": "The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.",
                    "sentence_kor": "실험 결과는 BRSON의 효과와 가능성을 입증하여 최첨단 기술에 비해 큰 폭으로 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "451",
            "abstractID": "EMNLP_abs-451",
            "text": [
                {
                    "index": "451-0",
                    "sentence": "Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently.",
                    "sentence_kor": "매일 온라인 상에서 엄청난 양의 문자 대화가 이루어지며, 이 곳에서 동시에 여러 개의 대화가 이루어진다.",
                    "tag": "1"
                },
                {
                    "index": "451-1",
                    "sentence": "Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages.",
                    "sentence_kor": "인터리브 대화는 진행 중인 토론을 따라갈 뿐만 아니라 동시 메시지에서 관련 정보를 추출하는 데에도 어려움을 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "451-2",
                    "sentence": "Conversation disentanglement aims to separate intermingled messages into detached conversations.",
                    "sentence_kor": "대화 분리는 섞이는 메시지를 분리된 대화로 분리하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "451-3",
                    "sentence": "However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability.",
                    "sentence_kor": "그러나 기존의 분리 방법은 대부분 데이터 세트마다 다른 수작업 기능에 의존하므로 일반화와 적응성을 방해한다.",
                    "tag": "3"
                },
                {
                    "index": "451-4",
                    "sentence": "In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering.",
                    "sentence_kor": "본 연구에서는 시간이 많이 걸리는 도메인별 기능 엔지니어링을 피하는 대화 분리를 위한 엔드 투 엔드 온라인 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "451-5",
                    "sentence": "We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion.",
                    "sentence_kor": "우리는 타임스탬프, 스피커 및 메시지 텍스트로 구성된 전체 발화를 포함하는 새로운 방법을 설계하고 분리를 포인팅 문제로 모델링하는 동시에 엔드 투 엔드 방식으로 인터터넌스 상호 작용을 효과적으로 캡처하는 맞춤형 주의 메커니즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "451-6",
                    "sentence": "We also introduce a joint-learning objective to better capture contextual information.",
                    "sentence_kor": "우리는 또한 상황별 정보를 더 잘 포착하기 위한 공동 학습 목표를 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "451-7",
                    "sentence": "Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.",
                    "sentence_kor": "Ubuntu IRC 데이터 세트에 대한 우리의 실험은 우리의 방법이 링크 및 변환 예측 작업 모두에서 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "452",
            "abstractID": "EMNLP_abs-452",
            "text": [
                {
                    "index": "452-0",
                    "sentence": "In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases.",
                    "sentence_kor": "본 논문에서 우리는 정의 모델링 작업을 다루는데, 여기서 목표는 단어와 구문의 정의를 생성하는 방법을 배우는 것이다.",
                    "tag": "1+2"
                },
                {
                    "index": "452-1",
                    "sentence": "Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way.",
                    "sentence_kor": "이 작업에 대한 기존 접근법은 직접적이 아닌 암묵적 방법으로 분포적 의미론과 어휘적 의미론을 결합하는 차별적 접근 방식이다.",
                    "tag": "3"
                },
                {
                    "index": "452-2",
                    "sentence": "To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition.",
                    "sentence_kor": "이 문제를 해결하기 위해 우리는 상황에 사용되는 문맥과 그 정의 사이의 근본적인 관계를 명시적으로 모델링하기 위해 연속 잠재 변수를 도입하는 작업에 대한 생성 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "452-3",
                    "sentence": "We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance.",
                    "sentence_kor": "추정을 위해 변이 추론에 의존하며 성능 향상을 위해 상황에 맞는 단어 임베딩을 활용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "452-4",
                    "sentence": "Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, “Cambridge” and the first non-English corpus “Robert”, which we release to complement our empirical study.",
                    "sentence_kor": "우리의 접근 방식은 두 개의 새로운 데이터 세트인 \"Cambridge\"와 첫 번째 영어 이외의 말뭉치 \"Robert\"를 추가하여 기존의 네 가지 까다로운 벤치마크에서 평가된다.",
                    "tag": "3"
                },
                {
                    "index": "452-5",
                    "sentence": "Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.",
                    "sentence_kor": "VCDM(Variative Contextual Definition Modeler)은 자동 및 인간 평가 지표 측면에서 최첨단 성능을 달성하여 접근 방식의 효과를 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "453",
            "abstractID": "EMNLP_abs-453",
            "text": [
                {
                    "index": "453-0",
                    "sentence": "More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT.",
                    "sentence_kor": "더 최근에 명명된 기업인식은 BERT와 같은 사전 교육 접근법의 도움을 받아 큰 진전을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "453-1",
                    "sentence": "However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge.",
                    "sentence_kor": "그러나 현재의 사전 교육 기법은 명명된 엔티티 관련 지식을 무시하고 일반 표현을 학습하기 위한 LAN-Guage 모델링 목표를 구축하는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "453-2",
                    "sentence": "To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models.",
                    "sentence_kor": "이를 위해, 우리는 거칠고 미세하게 자동으로 채굴된 실체 지식을 사전 훈련된 모델로 분류하기 위한 NER 고유의 사전 훈련 프레임워크를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "453-3",
                    "sentence": "Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities.",
                    "sentence_kor": "구체적으로, 우리는 먼저 일반 유형 엔터티로 간주될 수 있는 Wikipedia 앵커로 모델을 교육하여 전체 범위 식별 작업을 통해 모델을 예열한다.",
                    "tag": "3"
                },
                {
                    "index": "453-4",
                    "sentence": "Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities.",
                    "sentence_kor": "그런 다음 가젯터 기반 원격 감독 전략을 활용하여 모델을 거친 유형별로 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "453-5",
                    "sentence": "Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks.",
                    "sentence_kor": "마지막으로, 클러스터링을 통해 세분화된 명명 지식을 캐내기 위해 자체 감독 보조 작업을 고안한다.세 개의 공개 NER 데이터 세트에 대한 경험적 연구는 우리의 프레임워크가 사전 훈련된 여러 베이스라인에 대해 상당한 개선을 달성하여 세 가지 벤치마크에서 새로운 최첨단 성능을 확립한다고 설명한다.",
                    "tag": "2+3"
                },
                {
                    "index": "453-6",
                    "sentence": "Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.",
                    "sentence_kor": "또한, 우리의 프레임워크가 인간 라벨이 부착된 훈련 데이터를 사용하지 않고 유망한 재설팅을 얻어 라벨 수가 적고 자원이 적은 시나리오에서 그 효과를 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "454",
            "abstractID": "EMNLP_abs-454",
            "text": [
                {
                    "index": "454-0",
                    "sentence": "Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs.",
                    "sentence_kor": "엔티티 얼라인먼트(EA)는 다양한 KG의 등가 엔티티를 연결하여 풍부한 콘텐츠의 통합 지식 그래프(KG)를 구축하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "454-1",
                    "sentence": "GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples.",
                    "sentence_kor": "GNN 기반 EA 방법은 관계 3배로 정의된 KG 구조를 모델링하여 유망한 성능을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "454-2",
                    "sentence": "However, attribute triples can also provide crucial alignment signal but have not been well explored yet.",
                    "sentence_kor": "그러나 속성 트리플은 중요한 정렬 신호도 제공할 수 있지만 아직 잘 탐구되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "454-3",
                    "sentence": "In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently.",
                    "sentence_kor": "본 논문에서, 우리는 다양한 유형의 속성 3배를 효율적으로 모델링하기 위해 귀속 값 인코더를 활용하고 KG를 하위 그래프로 분할할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "454-4",
                    "sentence": "Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets.",
                    "sentence_kor": "또한 기존 EA 데이터 세트의 이름 편중 때문에 현재 EA 방법의 성능이 과대 평가되고 있다.",
                    "tag": "3"
                },
                {
                    "index": "454-5",
                    "sentence": "To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set.",
                    "sentence_kor": "객관적인 평가를 위해 이름이 매우 다른 등가 엔티티 쌍을 테스트 세트로 선택하는 하드 실험 설정을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "454-6",
                    "sentence": "Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets.",
                    "sentence_kor": "일반 설정과 하드 설정 모두에서, 우리의 방법은 언어 간 및 단일 언어 데이터 세트의 12개 기준선에 비해 크게 향상되었다(DBP15k의 평균 Hits@1).",
                    "tag": "4"
                },
                {
                    "index": "454-7",
                    "sentence": "Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method.",
                    "sentence_kor": "다양한 하위 그래프에 대한 절제 연구와 속성 유형에 대한 사례 연구는 우리 방법의 효과를 추가로 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "454-8",
                    "sentence": "Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.",
                    "sentence_kor": "소스 코드와 데이터는 https://github.com/thunlp/explore-and-evaluate에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "455",
            "abstractID": "EMNLP_abs-455",
            "text": [
                {
                    "index": "455-0",
                    "sentence": "We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference.",
                    "sentence_kor": "우리는 가장 가까운 이웃 학습과 구조화된 추론을 기반으로 한 간단한 퓨샷 명명 개체 인식(NER) 시스템을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "455-1",
                    "sentence": "Our system uses a supervised NER model trained on the source domain, as a feature extractor.",
                    "sentence_kor": "우리 시스템은 소스 도메인에서 훈련된 감독 NER 모델을 기능 추출기로 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "455-2",
                    "sentence": "Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches.",
                    "sentence_kor": "여러 테스트 도메인에서 이 기능 공간에서 가장 가까운 이웃 분류기가 표준 메타 학습 접근 방식보다 훨씬 효과적이라는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "455-3",
                    "sentence": "We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training.",
                    "sentence_kor": "또한 값비싼 CRF 교육 없이 엔티티 태그 간의 레이블 종속성을 캡처할 수 있는 저렴하지만 효과적인 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "455-4",
                    "sentence": "We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.",
                    "sentence_kor": "구조화된 디코딩을 가장 가까운 이웃 학습과 결합하는 방법이 표준 퓨샷 NER 평가 작업에서 최첨단 성능을 달성하여 이전 메타 학습 기반 시스템에 비해 F1 점수를 6%에서 16% 절대 점수 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "456",
            "abstractID": "EMNLP_abs-456",
            "text": [
                {
                    "index": "456-0",
                    "sentence": "Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation.",
                    "sentence_kor": "엔티티 이름의 구조화된 표현은 엔티티 표준화 및 변형 생성과 같은 많은 엔티티 관련 작업에 유용합니다.",
                    "tag": "1"
                },
                {
                    "index": "456-1",
                    "sentence": "Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging.",
                    "sentence_kor": "맥락과 외부 지식 없이 실체 이름의 암묵적 구조화된 표현을 배우는 것은 특히 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "456-2",
                    "sentence": "In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem.",
                    "sentence_kor": "본 논문에서 우리는 이 문제를 해결하기 위해 적극적인 학습과 약한 감독을 결합한 새로운 학습 프레임워크를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "456-3",
                    "sentence": "Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.",
                    "sentence_kor": "우리의 실험 평가에 따르면 이 프레임워크는 12개 정도의 라벨링된 예에서 고품질 모델을 학습할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "457",
            "abstractID": "EMNLP_abs-457",
            "text": [
                {
                    "index": "457-0",
                    "sentence": "Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER.",
                    "sentence_kor": "중국어 사전 교육을 받은 문자 수준의 BERT는 사전 정보가 부족하여 중국어 NER에 대한 효과를 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "457-1",
                    "sentence": "To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method.",
                    "sentence_kor": "중국어 NER을 위한 사전 교육 LM에 어휘를 통합하기 위해 준감독 실체가 강화된 BERT 사전 교육 방법을 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "457-2",
                    "sentence": "In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method.",
                    "sentence_kor": "특히, 우리는 먼저 새로운 단어 검색 방법을 사용하여 관련 원시 텍스트에서 독립체 어휘를 추출한다.",
                    "tag": "1"
                },
                {
                    "index": "457-3",
                    "sentence": "We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations.",
                    "sentence_kor": "그런 다음 Char-Entity-Transformer를 사용하여 BERT에 엔티티 정보를 통합하며, 문자 및 엔티티 표현의 조합을 사용하여 셀프 어텐션을 강화한다.",
                    "tag": "1"
                },
                {
                    "index": "457-4",
                    "sentence": "In addition, an entity classification task helps inject the entity information into model parameters in pre-training.",
                    "sentence_kor": "또한 엔터티 분류 작업은 사전 교육에서 엔터티 정보를 모델 매개변수에 주입하는 데 도움이 됩니다.",
                    "tag": "1"
                },
                {
                    "index": "457-5",
                    "sentence": "The pre-trained models are used for NER fine-tuning.",
                    "sentence_kor": "사전 훈련된 모델은 NER 미세 조정에 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "457-6",
                    "sentence": "Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.",
                    "sentence_kor": "뉴스 데이터 세트와 NER에 대해 주석이 달린 두 개의 데이터 세트에 대한 실험은 우리의 방법이 매우 효과적이고 최상의 결과를 달성한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "458",
            "abstractID": "EMNLP_abs-458",
            "text": [
                {
                    "index": "458-0",
                    "sentence": "This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off.",
                    "sentence_kor": "본 논문은 개념적으로 단순하고 확장 가능하며 매우 효과적인 BERT 기반 엔티티 링크 모델과 그 정확도-속도 균형에 대한 광범위한 평가를 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "458-1",
                    "sentence": "We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description.",
                    "sentence_kor": "각 엔터티는 짧은 텍스트 설명으로만 정의되는 2단계 제로샷 링크 알고리즘을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "458-2",
                    "sentence": "The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions.",
                    "sentence_kor": "첫 번째 단계는 언급 컨텍스트와 엔티티 설명을 독립적으로 내장하는 바이 인코더에 의해 정의된 밀집 공간에서 검색한다.",
                    "tag": "3"
                },
                {
                    "index": "458-3",
                    "sentence": "Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text.",
                    "sentence_kor": "그런 다음 각 후보자는 언급 텍스트와 엔티티 텍스트를 연결하는 교차 인코더를 사용하여 다시 순위가 매겨집니다.",
                    "tag": "3"
                },
                {
                    "index": "458-4",
                    "sentence": "Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables).",
                    "sentence_kor": "실험은 이 접근방식이 비교적 단순함에도 불구하고 최근의 제로샷 벤치마크(6점 절대 이득)와 더 확립된 논제로샷 평가(예: TACBP-2010)에서 최첨단임을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "458-5",
                    "sentence": "We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation.",
                    "sentence_kor": "또한 바이인코더 링크는 가장 가까운 이웃 검색(예: 2밀리초 내에 590만 후보와의 링크)과 매우 빠르며, 더 비싼 크로스 인코더의 정확도 이득의 상당 부분이 지식 증류를 통해 바이인코더로 전송될 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "458-6",
                    "sentence": "Our code and models are available at https://github.com/facebookresearch/BLINK.",
                    "sentence_kor": "코드 및 모델은 https://github.com/facebookresearch/BLINK에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "459",
            "abstractID": "EMNLP_abs-459",
            "text": [
                {
                    "index": "459-0",
                    "sentence": "We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass.",
                    "sentence_kor": "우리는 질문에 대한 빠른 엔드 투 엔드 엔티티 링크 모델인 ELQ를 제시한다. ELQ는 비엔코더를 사용하여 하나의 패스로 멘션 감지 및 링크를 공동으로 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "459-1",
                    "sentence": "Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively.",
                    "sentence_kor": "질문당 여러 엔티티를 다루는 확장된 주석으로 WebQSP와 GraphQuestions에서 평가된 ELQ는 각각 +12.7%와 +19.6% F1의 큰 차이로 이전 기술 수준을 능가한다.",
                    "tag": "3+4"
                },
                {
                    "index": "459-2",
                    "sentence": "With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems.",
                    "sentence_kor": "매우 빠른 추론 시간(단일 CPU의 1.57 예제/s)으로 ELQ는 다운스트림 질문 응답 시스템에 유용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "459-3",
                    "sentence": "In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.",
                    "sentence_kor": "개념 증명 실험에서 ELQ를 사용하면 GraphRetriver의 다운스트림 QA 성능이 크게 향상된다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "460",
            "abstractID": "EMNLP_abs-460",
            "text": [
                {
                    "index": "460-0",
                    "sentence": "Entity representations are useful in natural language tasks involving entities.",
                    "sentence_kor": "엔티티 표현은 엔티티를 포함하는 자연어 작업에서 유용합니다.",
                    "tag": "1"
                },
                {
                    "index": "460-1",
                    "sentence": "In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer.",
                    "sentence_kor": "본 논문에서, 우리는 양방향 변압기를 기반으로 하는 단어와 실체의 사전 훈련된 문맥화된 새로운 표현을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "460-2",
                    "sentence": "The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.",
                    "sentence_kor": "제안된 모델은 주어진 텍스트의 단어와 엔터티를 독립적인 토큰으로 처리하고, 문맥화된 표현을 출력한다.",
                    "tag": "3"
                },
                {
                    "index": "460-3",
                    "sentence": "Our model is trained using a new pretraining task based on the masked language model of BERT.",
                    "sentence_kor": "우리 모델은 BERT의 마스킹 언어 모델에 기초한 새로운 사전 훈련 작업을 사용하여 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "460-4",
                    "sentence": "The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.",
                    "sentence_kor": "이 작업에는 위키피디아에서 검색된 대규모 개체 주석 말뭉치에서 무작위로 마스킹된 단어와 실체를 예측하는 작업이 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "460-5",
                    "sentence": "We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.",
                    "sentence_kor": "우리는 또한 변압기의 자기 주의 메커니즘의 확장인 실체 인식 자기 주의 메커니즘을 제안하고 주의 점수를 계산할 때 토큰(단어 또는 실체)의 유형을 고려한다.",
                    "tag": "3"
                },
                {
                    "index": "460-6",
                    "sentence": "The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.",
                    "sentence_kor": "제안된 모델은 광범위한 엔티티 관련 작업에서 인상적인 경험적 성과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "460-7",
                    "sentence": "In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).",
                    "sentence_kor": "특히, 5개의 잘 알려진 데이터셋에서 최첨단 결과를 얻습니다. 엔티티(엔티 타이핑), TACRED(관계 분류), CoNLL-2003(명칭 엔티티 인식), ReCoRD(클로즈 스타일 질문 답변) 및 SQuAD 1.1(추출적 질문 답변)을 엽니다.",
                    "tag": "4"
                },
                {
                    "index": "460-8",
                    "sentence": "Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.",
                    "sentence_kor": "소스 코드와 사전 검증된 표현은 https://github.com/studio-ousia/luke에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "461",
            "abstractID": "EMNLP_abs-461",
            "text": [
                {
                    "index": "461-0",
                    "sentence": "Literary tropes, from poetry to stories, are at the crux of human imagination and communication.",
                    "sentence_kor": "시에서 이야기에 이르기까지 문학은 인간의 상상력과 의사소통의 핵심에 있다.",
                    "tag": "1"
                },
                {
                    "index": "461-1",
                    "sentence": "Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations.",
                    "sentence_kor": "직유와 같은 비유적인 언어는 독자들에게 새로운 통찰력과 영감을 주기 위해 평범한 표현을 넘어선다.",
                    "tag": "1"
                },
                {
                    "index": "461-2",
                    "sentence": "In this paper, we tackle the problem of simile generation.",
                    "sentence_kor": "본 논문에서 우리는 직유 생성 문제를 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "461-3",
                    "sentence": "Generating a simile requires proper understanding for effective mapping of properties between two concepts.",
                    "sentence_kor": "직유를 생성하려면 두 개념 간의 속성을 효과적으로 매핑하기 위한 적절한 이해가 필요합니다.",
                    "tag": "1"
                },
                {
                    "index": "461-4",
                    "sentence": "To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge.",
                    "sentence_kor": "이를 위해 먼저 구조화된 상식 지식을 사용하여 Reddit에서 수집된 많은 직유를 문자 그대로 변환하여 병렬 말뭉치를 자동으로 구성하는 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "461-5",
                    "sentence": "We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence.",
                    "sentence_kor": "그런 다음 리터럴-시밀리 쌍에서 사전 훈련된 시퀀스 모델인 BART(Lewis et al 2019)를 미세 조정하여 리터럴 문장이 주어진 새로운 직유를 생성할 수 있도록 할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "461-6",
                    "sentence": "Experiments show that our approach generates 88% novel similes that do not share properties with the training data.",
                    "sentence_kor": "실험에 따르면 우리의 접근 방식은 훈련 데이터와 속성을 공유하지 않는 88%의 새로운 직유를 생성한다.",
                    "tag": "4"
                },
                {
                    "index": "461-7",
                    "sentence": "Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise.",
                    "sentence_kor": "독립적인 문자 그대로의 진술 집합에 대한 인간 평가는 우리 모델이 쌍으로 비교할 때 두 명의 문학 전문가보다 직유를 37% 더 잘 생성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "461-8",
                    "sentence": "We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.",
                    "sentence_kor": "또한 기계 생성 스토리에서 문자 그대로의 문장을 최고의 모델에서 직유로 대체하는 것이 어떻게 외향성을 향상시키고 인간 판사의 수용성을 높이는지 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "462",
            "abstractID": "EMNLP_abs-462",
            "text": [
                {
                    "index": "462-0",
                    "sentence": "Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints.",
                    "sentence_kor": "기존 언어 모델은 처음부터 작성에 탁월하지만, 많은 실제 시나리오에서는 일련의 제약 조건에 맞게 기존 문서를 다시 작성해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "462-1",
                    "sentence": "Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently.",
                    "sentence_kor": "문장 수준 재작성이 상당히 잘 연구되었지만 전체 문서를 일관성 있게 다시 작성해야 하는 과제를 해결하는 작업은 거의 없었다.",
                    "tag": "1"
                },
                {
                    "index": "462-2",
                    "sentence": "In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint.",
                    "sentence_kor": "본 연구에서, 우리는 문서 수준의 표적 콘텐츠 전송 과제를 소개하고 레시피 영역에서 그것을 다룬다. 레시피는 문서로, 식이 제한(채식주의자 또는 유제품이 없는 것과 같은)은 표적 제약조건으로 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "462-3",
                    "sentence": "We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs.",
                    "sentence_kor": "우리는 생성 사전 훈련된 언어 모델(GPT-2)을 기반으로 이 작업에 대한 새로운 모델을 제안하고 대략 정렬된 많은 레시피 쌍을 훈련시킨다.",
                    "tag": "2+3"
                },
                {
                    "index": "462-4",
                    "sentence": "Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document.",
                    "sentence_kor": "자동 및 인간 평가 모두 우리 모델이 원래 문서에 가깝게 유지하면서 제약 조건을 준수하는 일관되고 다양한 재작성을 생성하여 기존 방법을 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "462-5",
                    "sentence": "Finally, we analyze our model’s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.",
                    "sentence_kor": "마지막으로, 우리는 언어 생성을 문체적이라기 보다는 실질적인 제약조건에 더 적응시키는 목표를 향한 진행 상황을 평가하기 위해 모델의 재작성을 분석한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "463",
            "abstractID": "EMNLP_abs-463",
            "text": [
                {
                    "index": "463-0",
                    "sentence": "Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language.",
                    "sentence_kor": "구글 어시스턴트, 아마존 알렉사, 애플 시리와 같은 가상 도우미는 사용자들이 자연어를 사용하여 웹 상의 수많은 서비스와 API와 상호작용할 수 있게 해준다.",
                    "tag": "1"
                },
                {
                    "index": "463-1",
                    "sentence": "In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs.",
                    "sentence_kor": "본 연구에서는 다수의 API에서 단일 도메인 독립 모델을 사용하여 자연어 생성(NLG)을 위한 두 가지 방법을 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "463-2",
                    "sentence": "First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language.",
                    "sentence_kor": "첫째, API를 자연어로 설명하는 스키마의 생성을 조건화하는 스키마 안내 방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "463-3",
                    "sentence": "Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API.",
                    "sentence_kor": "두 번째 방법은 API의 의미를 전달하기 위해 슬롯 수에서 선형적으로 증가하는 소수의 템플릿 사용을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "463-4",
                    "sentence": "To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance.",
                    "sentence_kor": "임의 슬롯 조합에 대한 발화를 생성하기 위해 몇 가지 간단한 템플릿을 먼저 연결하여 의미론적으로는 올바르지만 일관성이 없고 문법적으로 맞지 않는 발화를 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "463-5",
                    "sentence": "A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text.",
                    "sentence_kor": "사전 훈련된 언어 모델은 일관되고 자연스럽게 들리는 텍스트로 다시 쓰기 위해 이후에 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "463-6",
                    "sentence": "Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.",
                    "sentence_kor": "자동 메트릭과 인간 평가를 통해, 우리는 우리의 방법이 강력한 기준선에 비해 개선되고, 도메인 외부 입력에 강하며, 샘플 효율성이 향상되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "464",
            "abstractID": "EMNLP_abs-464",
            "text": [
                {
                    "index": "464-0",
                    "sentence": "Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph).",
                    "sentence_kor": "다양한 NLP 작업에서 문맥화된 언어 모델이 최근 성공했음에도 불구하고, 언어 모델 자체는 긴 다중 문장 문서(예: 단락)의 텍스트 일관성을 포착할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "464-1",
                    "sentence": "Humans often make structural decisions on what and how to say about before making utterances.",
                    "sentence_kor": "인간은 종종 발언하기 전에 무엇을 어떻게 말해야 하는지에 대해 구조적인 결정을 내린다.",
                    "tag": "1"
                },
                {
                    "index": "464-2",
                    "sentence": "Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process.",
                    "sentence_kor": "이와 같은 높은 수준의 결정과 일관성 있는 구조화 텍스트를 통해 표면 실현을 유도하는 것을 본질적으로 계획 프로세스라고 한다.",
                    "tag": "1"
                },
                {
                    "index": "464-3",
                    "sentence": "Where can the model learn such high-level coherence?",
                    "sentence_kor": "모델은 어디에서 이러한 높은 수준의 일관성을 배울 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "464-4",
                    "sentence": "A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on.",
                    "sentence_kor": "단락 자체는 문장 순서, 주제 키워드, 수사적 구조 등과 같이 본 연구에서 자기 감독이라고 불리는 다양한 형태의 유도 일관성 신호를 포함하고 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "464-5",
                    "sentence": "Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph.",
                    "sentence_kor": "이에 동기가 부여된 이 연구는 단락에서 마스킹된 문장을 예측하는 새로운 단락 완성 과제 PARCOM을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "464-6",
                    "sentence": "However, the task suffers from predicting and selecting appropriate topical content with respect to the given context.",
                    "sentence_kor": "그러나 이 과제는 주어진 맥락과 관련하여 적절한 주제 내용을 예측하고 선택하는 데 어려움을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "464-7",
                    "sentence": "To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content.",
                    "sentence_kor": "이를 해결하기 위해 먼저 무엇을 말할지 예측한 다음 예측된 내용을 사용하여 사전 훈련된 언어 모델(표면 실현)을 안내하는 자체 감독 텍스트 플래너 SSPlanner를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "464-8",
                    "sentence": "SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation.",
                    "sentence_kor": "SSPlanner는 자동 및 인간 평가 모두에서 단락 완료 작업에서 기준선 생성 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "464-9",
                    "sentence": "We also find that a combination of noun and verb types of keywords is the most effective for content selection.",
                    "sentence_kor": "우리는 또한 키워드의 명사와 동사의 조합이 내용 선택에 가장 효과적이라는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "464-10",
                    "sentence": "As more number of content keywords are provided, overall generation quality also increases.",
                    "sentence_kor": "더 많은 콘텐츠 키워드가 제공될수록 전반적인 생성 품질도 향상됩니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "465",
            "abstractID": "EMNLP_abs-465",
            "text": [
                {
                    "index": "465-0",
                    "sentence": "Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains.",
                    "sentence_kor": "공감적 대화 모델은 수많은 영역에서 사용자 만족도와 작업 결과를 향상시키는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "465-1",
                    "sentence": "In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy.",
                    "sentence_kor": "심리학에서, 페르소나는 성격과 높은 상관관계가 있는 것으로 나타났고, 이것은 다시 감정이입에 영향을 미친다.",
                    "tag": "1"
                },
                {
                    "index": "465-2",
                    "sentence": "In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations.",
                    "sentence_kor": "게다가, 우리의 경험적 분석은 또한 페르소나가 공감하는 대화에 중요한 역할을 한다는 것을 암시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "465-3",
                    "sentence": "To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding.",
                    "sentence_kor": "이를 위해 페르소나 기반 공감 대화에 대한 새로운 과제를 제안하고 페르소나가 공감 반응에 미치는 영향에 대한 첫 번째 경험적 연구를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "465-4",
                    "sentence": "Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations.",
                    "sentence_kor": "특히, 우리는 먼저 개인 기반 공감 대화를 위한 새로운 대규모 다중 도메인 데이터 세트를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "465-5",
                    "sentence": "We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset.",
                    "sentence_kor": "그런 다음 데이터 세트에서 최첨단 성능을 얻는 효율적인 BERT 기반 응답 선택 모델인 CoBERT를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "465-6",
                    "sentence": "Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding.",
                    "sentence_kor": "마지막으로, 우리는 감정이입 반응에 대한 페르소나의 영향을 조사하기 위해 광범위한 실험을 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "465-7",
                    "sentence": "Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.",
                    "sentence_kor": "특히, 우리의 결과는 CoBERT가 비공감적 대화보다 공감적 대화에 대해 훈련되었을 때 공감적 반응을 더 향상시켜 인간 대화에서 페르소나와 공감 사이의 경험적 연결을 확립한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "466",
            "abstractID": "EMNLP_abs-466",
            "text": [
                {
                    "index": "466-0",
                    "sentence": "The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets.",
                    "sentence_kor": "마켓샌드마켓에 따르면 대화 도우미(챗봇)의 세계 시장 규모는 2024년까지 94억달러로 성장할 것으로 예상된다.",
                    "tag": "1"
                },
                {
                    "index": "466-1",
                    "sentence": "Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users.",
                    "sentence_kor": "챗봇의 광범위한 사용에도 불구하고, 챗봇을 통한 개인 정보의 유출은 사용자들에게 심각한 사생활 문제를 야기한다.",
                    "tag": "1"
                },
                {
                    "index": "466-2",
                    "sentence": "In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants.",
                    "sentence_kor": "본 연구에서는 대화 도우미가 탐지한 의심스러운 문장을 사용자에게 경고하여 개인정보를 보호할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "466-3",
                    "sentence": "The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation.",
                    "sentence_kor": "탐지 작업은 정렬 최적화 문제로 공식화되고 평가를 위해 새로운 데이터 세트 PERSONA-LEAKage가 수집된다.",
                    "tag": "3"
                },
                {
                    "index": "466-4",
                    "sentence": "In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems.",
                    "sentence_kor": "본 논문에서, 우리는 두 가지 새로운 제한된 정렬 모델을 제안한다. 또한, 우리는 최근 제안된 개인화된 채팅 대화 시스템의 동작에 대한 분석을 지속적으로 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "466-5",
                    "sentence": "The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model.",
                    "sentence_kor": "경험적 결과에 따르면 이러한 시스템은 널리 사용되는 Seq2Seq 모델과 언어 모델보다 개인정보 공개로 인해 더 많은 어려움을 겪고 있다.",
                    "tag": "4"
                },
                {
                    "index": "466-6",
                    "sentence": "In those cases, a significant number of information leaking utterances can be detected by our models with high precision.",
                    "sentence_kor": "이러한 경우 상당수의 정보 누출 발언을 당사의 모델에서 높은 정밀도로 감지할 수 있습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "467",
            "abstractID": "EMNLP_abs-467",
            "text": [
                {
                    "index": "467-0",
                    "sentence": "While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario.",
                    "sentence_kor": "다자간 다자간 대화 참여자는 동시에 여러 대화 주제를 수행하지만, 기존의 대응 선택 방법은 주로 양당 단일 전환 시나리오에 초점을 맞춰 개발된다.",
                    "tag": "1"
                },
                {
                    "index": "467-1",
                    "sentence": "Hence, the prolongation and transition of conversation topics are ignored by current methods.",
                    "sentence_kor": "따라서, 대화의 주제가 연장되고 전환되는 것은 현재의 방법에서는 무시된다.",
                    "tag": "1"
                },
                {
                    "index": "467-2",
                    "sentence": "In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context.",
                    "sentence_kor": "이 작업에서는 응답과 관련 대화 컨텍스트 사이의 주제를 일치시키기 위해 응답 선택을 동적 주제 추적 작업으로 정의한다.",
                    "tag": "1+2"
                },
                {
                    "index": "467-3",
                    "sentence": "With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection.",
                    "sentence_kor": "이 새로운 공식으로, 우리는 동적 주제 분리 및 응답 선택을 수행하기 위해 한 번에 두 개의 발화만으로 사전 훈련된 대형 모델을 통해 효율적인 인코딩을 지원하는 새로운 멀티태스킹 학습 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "467-4",
                    "sentence": "We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning.",
                    "sentence_kor": "또한 Topic-BERT는 주제 정보를 자체 지도 학습과 함께 BERT에 내장하기 위한 필수 사전 훈련 단계를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "467-5",
                    "sentence": "Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.",
                    "sentence_kor": "DSTC-8 Ubuntu IRC 데이터 세트에 대한 실험 결과는 응답 선택 및 주제 분리 작업에서 기존 방법을 상당한 차이로 능가하는 최첨단 결과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "468",
            "abstractID": "EMNLP_abs-468",
            "text": [
                {
                    "index": "468-0",
                    "sentence": "Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario.",
                    "sentence_kor": "인간 대화는 시나리오 기반이며, 일반적으로 적절한 대응은 특정 시나리오에 수반되는 잠재 컨텍스트 지식과 관련이 있다.",
                    "tag": "1"
                },
                {
                    "index": "468-1",
                    "sentence": "To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge.",
                    "sentence_kor": "보다 의미 있고 상황에 맞는 응답을 가능하게 하기 위해, 우리는 시나리오 관점에서 대화 이력과 미래 대화를 모두 고려하여 시나리오 지식을 암묵적으로 재구성하는 생성 대화 시스템을 개선할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "468-2",
                    "sentence": "More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference.",
                    "sentence_kor": "더욱 중요한 것은, 대화 시나리오는 미래 대화에 접근할 수 없는 기존의 대화 모델을 시나리오 기반 대화 모델에서 계층적 감독 신호에 포함된 시나리오 지식을 전달하여 효과적으로 정규화함으로써 모방 학습 프레임워크를 사용하여 더욱 내면화된다는 것이다. 실제 추론에 있어서 미래의 대화는 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "468-3",
                    "sentence": "Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.",
                    "sentence_kor": "광범위한 평가에 따르면 우리의 접근 방식이 다양성과 관련성에 대한 최첨단 기준을 크게 능가하고 시나리오별 지식을 표현한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "469",
            "abstractID": "EMNLP_abs-469",
            "text": [
                {
                    "index": "469-0",
                    "sentence": "Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed.",
                    "sentence_kor": "폐쇄된 도메인에서 인간과 심층 채팅을 수행할 수 있는 것은 오픈 도메인 챗봇을 청구하기 전에 전제 조건이다.",
                    "tag": "1"
                },
                {
                    "index": "469-1",
                    "sentence": "In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots.",
                    "sentence_kor": "이 작업에서는 영화 도메인을 자세히 살펴보고 영화 도메인 챗봇의 한계를 뛰어넘기 위해 세분화된 주석을 가진 대규모 고품질 말뭉치를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "469-2",
                    "sentence": "We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval.",
                    "sentence_kor": "의도 예측 및 지식 검색과 같은 모든 하위 작업을 조정하는 통합되고 쉽게 확장 가능한 신경 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "469-3",
                    "sentence": "The model is first pretrained on the huge general-domain data, then finetuned on our corpus.",
                    "sentence_kor": "이 모델은 처음에 거대한 일반 도메인 데이터에 대해 사전 교육을 받은 다음 말뭉치에서 미세 조정된다.",
                    "tag": "3"
                },
                {
                    "index": "469-4",
                    "sentence": "We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules.",
                    "sentence_kor": "고품질 데이터에 대해 훈련된 이 간단한 신경 접근법이 복잡한 규칙에 대응하는 상용 시스템을 능가할 수 있다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "469-5",
                    "sentence": "On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones.",
                    "sentence_kor": "정적 테스트와 대화형 테스트 모두에서 시스템에 의해 생성된 응답은 사람이 작성한 테스트에 매우 잘 참여하고 민감함을 나타냅니다.",
                    "tag": "4"
                },
                {
                    "index": "469-6",
                    "sentence": "We further analyze the limits of our work and point out potential directions for future work",
                    "sentence_kor": "우리는 우리 일의 한계를 더 분석하고 미래의 작업에 대한 잠재적인 방향을 지적합니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "470",
            "abstractID": "EMNLP_abs-470",
            "text": [
                {
                    "index": "470-0",
                    "sentence": "Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved.",
                    "sentence_kor": "최근 몇 년 동안 관찰된 기업 상호 참조 결의에 대한 상당한 진전에도 불구하고, 무엇이 개선되었는지에 대한 이해가 전반적으로 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "470-1",
                    "sentence": "We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.",
                    "sentence_kor": "우리는 일반 NLP 청중에게 기술 상태에 대한 더 나은 이해와 향후 연구 방향을 제공하는 것을 목표로 최첨단 해결사의 경험적 분석을 제시한다.",
                    "tag": "1+2"
                }
            ]
        },
        {
            "absNo": "471",
            "abstractID": "EMNLP_abs-471",
            "text": [
                {
                    "index": "471-0",
                    "sentence": "For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance.",
                    "sentence_kor": "다중 턴 대화 재작성의 경우 대화 컨텍스트에서 언어 지식을 효과적으로 모델링하고 소음을 이용하는 능력은 성능을 향상시키는 데 필수적이다.",
                    "tag": "1"
                },
                {
                    "index": "471-1",
                    "sentence": "Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words.",
                    "sentence_kor": "기존의 주의깊은 모델들은 사전 초점 없이 모든 단어들을 돌보기 때문에, 일부 불필요한 단어들에 부정확하게 집중하게 됩니다.",
                    "tag": "1"
                },
                {
                    "index": "471-2",
                    "sentence": "In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model.",
                    "sentence_kor": "본 논문에서 우리는 누가 누구에게 무엇을 했는지에 대한 핵심 의미 정보를 강조하는 의미론적 역할 라벨링(SRL)을 사용하여 리라이터 모델에 대한 추가 지침을 제공할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "471-3",
                    "sentence": "Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.",
                    "sentence_kor": "실험에 따르면 이 정보는 이미 이전의 최첨단 시스템을 능가하는 RoBERTa 기반 모델을 크게 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "472",
            "abstractID": "EMNLP_abs-472",
            "text": [
                {
                    "index": "472-0",
                    "sentence": "Quotations are crucial for successful explanations and persuasions in interpersonal communications.",
                    "sentence_kor": "인용문은 대인관계 의사소통에서 성공적인 설명과 설득에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "472-1",
                    "sentence": "However, finding what to quote in a conversation is challenging for both humans and machines.",
                    "sentence_kor": "그러나 대화에서 인용할 내용을 찾는 것은 인간과 기계 모두에게 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "472-2",
                    "sentence": "This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context.",
                    "sentence_kor": "본 연구는 온라인 대화에서 자동 인용문 생성을 연구하고 인용문이 주어진 문맥에 맞는지 여부에 언어 일관성이 어떻게 영향을 미치는지 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "472-3",
                    "sentence": "Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn’s existing contents.",
                    "sentence_kor": "여기서는 잠재 주제, 대화 내역과의 상호작용 및 쿼리 턴의 기존 내용에 대한 일관성 측면에서 인용문의 맥락적 일관성을 포착한다.",
                    "tag": "2"
                },
                {
                    "index": "472-4",
                    "sentence": "Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation.",
                    "sentence_kor": "또한, 언어 생성을 통해 인용문으로 컨텍스트를 계속하기 위해 인코더-디코더 신경 프레임워크를 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "472-5",
                    "sentence": "Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models.",
                    "sentence_kor": "영어와 중국어로 된 두 개의 대규모 데이터 세트에 대한 실험 결과는 우리의 인용 생성 모델이 최첨단 모델을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "472-6",
                    "sentence": "Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.",
                    "sentence_kor": "추가 분석에 따르면 주제, 상호 작용 및 질의 일관성이 모두 온라인 대화에서 인용하는 방법을 배우는 데 도움이 됩니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "473",
            "abstractID": "EMNLP_abs-473",
            "text": [
                {
                    "index": "473-0",
                    "sentence": "Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction.",
                    "sentence_kor": "기존 연구들은 법률 기사를 외부 지식으로 사용하면 법률적 판단 예측의 성과를 향상시킬 수 있다는 것을 증명했다.",
                    "tag": "1"
                },
                {
                    "index": "473-1",
                    "sentence": "However, they do not fully use law article information and most of the current work is only for single label samples.",
                    "sentence_kor": "그러나 법률 문서 정보를 완전히 사용하지 않으며 현재 작업의 대부분은 단일 레이블 샘플에만 사용됩니다.",
                    "tag": "1"
                },
                {
                    "index": "473-2",
                    "sentence": "In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples.",
                    "sentence_kor": "본 논문에서 우리는 법 조항 정보를 최대한 활용할 수 있고 다중 레이블 샘플에 사용할 수 있는 법 조항 요소 인식 다중 표현 모델(LEMM)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "473-3",
                    "sentence": "The model uses the labeled elements of law articles to extract fact description features from multiple angles.",
                    "sentence_kor": "이 모델은 법률 기사의 라벨링된 요소를 사용하여 여러 각도에서 사실 설명 특징을 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "473-4",
                    "sentence": "It generates multiple representations of a fact for classification.",
                    "sentence_kor": "분류를 위해 사실에 대한 여러 표현을 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "473-5",
                    "sentence": "Every label has a law-aware fact representation to encode more information.",
                    "sentence_kor": "모든 라벨에는 더 많은 정보를 인코딩할 수 있는 법률 인식 사실 표현이 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "473-6",
                    "sentence": "To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations.",
                    "sentence_kor": "법률 문서 간의 의존성을 포착하기 위해, 모델은 또한 여러 표현 사이에 자기 주의 메커니즘을 도입한다.",
                    "tag": "4"
                },
                {
                    "index": "473-7",
                    "sentence": "Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.",
                    "sentence_kor": "TopJudge와 같은 기준 모델과 비교하여 이 모델은 5.84%, 매크로 F1은 6.42%, 마이크로 F1은 4.28%의 정확도를 개선합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "474",
            "abstractID": "EMNLP_abs-474",
            "text": [
                {
                    "index": "474-0",
                    "sentence": "Knowledge graph reasoning is a critical task in natural language processing.",
                    "sentence_kor": "지식 그래프 추론은 자연어 처리에서 중요한 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "474-1",
                    "sentence": "The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp.",
                    "sentence_kor": "이 작업은 각 사실이 타임스탬프와 관련된 시간적 지식 그래프에서 더욱 어려워진다.",
                    "tag": "1"
                },
                {
                    "index": "474-2",
                    "sentence": "Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future.",
                    "sentence_kor": "대부분의 기존 방법들은 과거 타임스탬프에서의 추론에 초점을 맞추고 미래에 일어날 사실들을 예측할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "474-3",
                    "sentence": "This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions.",
                    "sentence_kor": "본 논문은 미래 상호작용을 예측하기 위한 새로운 자기 회귀 아키텍처인 반복 이벤트 네트워크(RE-Net)를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "474-4",
                    "sentence": "The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs.",
                    "sentence_kor": "사실(사건)의 발생은 과거 지식 그래프의 시간 순서에 따라 조건화된 확률 분포로 모델링된다.",
                    "tag": "3"
                },
                {
                    "index": "474-5",
                    "sentence": "Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp.",
                    "sentence_kor": "특히, RE-Net은 반복 이벤트 인코더를 사용하여 과거의 사실을 인코딩하고 이웃 집계기를 사용하여 동일한 타임스탬프의 사실 연결을 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "474-6",
                    "sentence": "Future facts can then be inferred in a sequential manner based on the two modules.",
                    "sentence_kor": "그런 다음 두 모듈에 기초하여 순차적인 방법으로 미래 사실을 추론할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "474-7",
                    "sentence": "We evaluate our proposed method via link prediction at future times on five public datasets.",
                    "sentence_kor": "향후 5개의 공개 데이터 세트에 대한 링크 예측을 통해 제안된 방법을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "474-8",
                    "sentence": "Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.",
                    "sentence_kor": "광범위한 실험을 통해 특히 향후 타임스탬프에 대한 다단계 추론에 대한 RE-Net의 강점을 입증하고 5개 데이터 세트 모두에서 최첨단 성능을 달성한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "475",
            "abstractID": "EMNLP_abs-475",
            "text": [
                {
                    "index": "475-0",
                    "sentence": "The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences.",
                    "sentence_kor": "문장 분류의 핵심 문제는 문장의 의미론적 의미를 이해하기 위해 다중 스케일 N그램 특징을 추출하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "475-1",
                    "sentence": "Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets.",
                    "sentence_kor": "대부분의 기존 모델은 CNN과 RNN 모델을 쌓아 이 문제를 해결하는데, 이는 상대적으로 제한된 데이터 세트로 인해 기능 중복성과 과적합으로 쉽게 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "475-2",
                    "sentence": "In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features.",
                    "sentence_kor": "본 논문에서, 우리는 효과적인 매개 변수와 우수한 일반화 능력을 가지고 있을 뿐만 아니라 멀티스케일 n그램 기능을 고려하는 다중 스케일 직교 InDependent LSTM(Multi-Scale InDependent LSTM)이라는 간단하지만 효과적인 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "475-3",
                    "sentence": "We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices.",
                    "sentence_kor": "LSTM의 숨겨진 상태를 독립적으로 업데이트되는 여러 개의 작은 숨김 상태로 분리하고 반복 행렬에 직교 제약 조건을 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "475-4",
                    "sentence": "We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features.",
                    "sentence_kor": "그런 다음 이 구조물에 멀티스케일 N그램 기능을 추출하기 위한 다양한 크기의 슬라이딩 윈도우를 장착한다.",
                    "tag": "3"
                },
                {
                    "index": "475-5",
                    "sentence": "Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets.",
                    "sentence_kor": "광범위한 실험을 통해 당사의 모델이 8개의 벤치마크 데이터 세트에서 최첨단 기준선에 비해 더 우수하거나 경쟁력 있는 성능을 달성한다는 것을 입증했다.",
                    "tag": "4"
                },
                {
                    "index": "475-6",
                    "sentence": "We also combine our model with BERT to further boost the generalization performance.",
                    "sentence_kor": "또한 BERT와 모델을 결합하여 일반화 성능을 더욱 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "476",
            "abstractID": "EMNLP_abs-476",
            "text": [
                {
                    "index": "476-0",
                    "sentence": "The data imbalance problem is a crucial issue for the multi-label text classification.",
                    "sentence_kor": "데이터 불균형 문제는 다중 레이블 텍스트 분류에 중요한 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "476-1",
                    "sentence": "Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data.",
                    "sentence_kor": "일부 기존 연구는 바닐라 교차 엔트로피 손실 대신 불균형 손실 목표를 제안하여 이를 해결하지만, 극도로 불균형한 데이터의 경우 성능은 제한적이다.",
                    "tag": "1"
                },
                {
                    "index": "476-2",
                    "sentence": "We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories.",
                    "sentence_kor": "우리는 헤드 범주에 일반 네트워크를 적용하고 테일 범주에 퓨샷 기술을 적용하는 하이브리드 솔루션을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "476-3",
                    "sentence": "We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN.",
                    "sentence_kor": "우리는 HSCNN 훈련을 위한 특정 샘플링 방법인 샴 구조의 범주별 유사성인 단일 및 샴 네트워크를 기반으로 하는 다중 작업 아키텍처와 같은 추가적인 기술적 속성을 가진 하이브리드 샴 컨볼루션 신경망을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "476-4",
                    "sentence": "The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.",
                    "sentence_kor": "두 개의 벤치마크 데이터 세트와 세 개의 손실 목표를 사용한 결과는 우리의 방법이 꼬리 또는 전체 범주에서 다양한 손실 목표를 가진 단일 네트워크의 성능을 향상시킬 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "477",
            "abstractID": "EMNLP_abs-477",
            "text": [
                {
                    "index": "477-0",
                    "sentence": "This paper proposes a pre-training based automated Chinese essay scoring method.",
                    "sentence_kor": "본 논문은 사전 교육 기반의 자동화된 중국어 에세이 채점 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "477-1",
                    "sentence": "The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning.",
                    "sentence_kor": "이 방법에는 약하게 감독되는 사전 훈련, 감독되는 교차 프롬프트 미세 조정 및 감독되는 대상 프롬프트 미세 조정의 세 가지 구성 요소가 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "477-2",
                    "sentence": "An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision.",
                    "sentence_kor": "에세이 채점자는 우선 다양한 주제를 다루는 대규모 에세이 데이터 세트에 대한 사전 교육을 받는다. 즉, 좋은 것과 나쁜 것, 즉, 약한 관리로 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "477-3",
                    "sentence": "The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision.",
                    "sentence_kor": "사전 훈련을 받은 에세이 채점자는 목표 프롬프트와 점수 범위가 같고 추가적인 감독을 제공하는 기존 프롬프트에서 이전에 평가된 에스에 더 세밀하게 조정될 것이다.",
                    "tag": "3+4"
                },
                {
                    "index": "477-4",
                    "sentence": "At last, the scorer is fine-tuned on the target-prompt training data.",
                    "sentence_kor": "마침내, 득점자는 목표 추진 훈련 데이터를 미세 조정한다.",
                    "tag": "3"
                },
                {
                    "index": "477-5",
                    "sentence": "The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..",
                    "sentence_kor": "네 가지 프롬프트에 대한 평가는 이 방법이 효과와 영역 적응 능력 면에서 최첨단 신경 에세이 점수를 향상시킬 수 있다는 것을 보여주며, 심층 분석에서도 그 한계를 드러낸다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "478",
            "abstractID": "EMNLP_abs-478",
            "text": [
                {
                    "index": "478-0",
                    "sentence": "Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions.",
                    "sentence_kor": "질문 중심 요약은 근원 문서를 요약하여 사실적이지 않은 질문에 대한 간결하지만 유익한 답변을 도출하는 효과적인 접근방식으로 최근에 연구되었다.",
                    "tag": "1"
                },
                {
                    "index": "478-1",
                    "sentence": "In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries.",
                    "sentence_kor": "본 연구에서는 다중 홉 추론을 질문 중심 요약에 통합하고 생성된 요약에 대한 정당성을 제공하기 위해 새로운 질문 중심 추상 요약 방법인 MSG를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "478-2",
                    "sentence": "Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer.",
                    "sentence_kor": "특히, 우리는 인간과 유사한 다중 홉 추론 모듈을 통해 질문과의 관련성과 다른 문장 간의 상호 관계를 공동으로 모델링한다. 이 모듈은 요약된 답변을 정당화하기 위해 중요한 문장을 캡처한다.",
                    "tag": "3"
                },
                {
                    "index": "478-3",
                    "sentence": "A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives.",
                    "sentence_kor": "다중 뷰 커버리지 메커니즘이 있는 게이트 선택적 포인터 생성기 네트워크는 다양한 관점에서 다양한 정보를 통합하도록 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "478-4",
                    "sentence": "Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.",
                    "sentence_kor": "실험 결과에 따르면 제안된 방법은 WikiHow와 PubMedQA라는 두 가지 비사실 QA 데이터 세트에서 최첨단 방법을 지속적으로 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "479",
            "abstractID": "EMNLP_abs-479",
            "text": [
                {
                    "index": "479-0",
                    "sentence": "We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation.",
                    "sentence_kor": "우리는 배경 단락에 기술된 원인과 결과를 이해하고 지식을 새로운 상황에 적용해야 하는 상황의 단락 효과에 대한 추론 작업에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "479-1",
                    "sentence": "Existing works ignore the complicated reasoning process and solve it with a one-step “black box” model.",
                    "sentence_kor": "기존 작품들은 복잡한 추론 과정을 무시하고 1단계 블랙박스 모델로 해결한다.",
                    "tag": "1"
                },
                {
                    "index": "479-2",
                    "sentence": "Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules.",
                    "sentence_kor": "인간의 인지 프로세스에서 영감을 받아, 본 논문에서 우리는 신경망 모듈을 사용하여 추론 프로세스의 각 단계를 명시적으로 모델링하는 이 작업에 대한 순차적 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "479-3",
                    "sentence": "In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model.",
                    "sentence_kor": "특히, 5개의 추론 모듈은 엔드투엔드 방식으로 설계되고 학습되어 더 해석 가능한 모델로 이어진다.",
                    "tag": "3"
                },
                {
                    "index": "479-4",
                    "sentence": "Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.",
                    "sentence_kor": "ROPS 데이터 세트에 대한 실험 결과는 제안된 접근 방식의 효과와 설명 가능성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "480",
            "abstractID": "EMNLP_abs-480",
            "text": [
                {
                    "index": "480-0",
                    "sentence": "Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation.",
                    "sentence_kor": "덧셈, 뺄셈, 정렬 및 셈과 같은 텍스트에 대한 수치적 추론은 자연어 이해와 산술 계산이 모두 필요하기 때문에 어려운 기계 독해 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "480-1",
                    "sentence": "To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph.",
                    "sentence_kor": "이 과제를 해결하기 위해, 우리는 이러한 추론에 필요한 문맥과 질문에 대한 이질적인 그래프 표현을 제안하고, 이 컨텍스트 그래프에 대한 다단계 수치 추론을 유도하는 질문 지향 그래프 주의 네트워크를 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "480-2",
                    "sentence": "Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.",
                    "sentence_kor": "딥 러닝과 그래프 추론을 결합한 이 모델은 DROP와 같은 벤치마크 데이터 세트에서 놀라운 결과를 달성한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "481",
            "abstractID": "EMNLP_abs-481",
            "text": [
                {
                    "index": "481-0",
                    "sentence": "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method.",
                    "sentence_kor": "개방형 도메인 질문 답변은 TF-IDF 또는 BM25와 같은 전통적인 희소 벡터 공간 모델이 사실상의 방법인 후보 컨텍스트를 선택하기 위해 효율적인 통과 검색에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "481-1",
                    "sentence": "In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.",
                    "sentence_kor": "이 연구에서, 우리는 검색이 조밀한 표현만으로 실질적으로 구현될 수 있다는 것을 보여준다. 여기서 임베딩은 간단한 이중 인코더 프레임워크에 의해 소수의 질문과 구절에서 학습된다.",
                    "tag": "1"
                },
                {
                    "index": "481-2",
                    "sentence": "When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
                    "sentence_kor": "광범위한 오픈 도메인 QA 데이터 세트에서 평가했을 때, 당사의 고밀도 검색기는 상위 20개 통과 검색 정확도 측면에서 강력한 Lucene-BM25 시스템을 9-19% 크게 능가하며, 엔드 투 엔드 QA 시스템이 다중 오픈 도메인 QA 벤치마크에서 새로운 최첨단 시스템을 구축할 수 있도록 돕는다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "482",
            "abstractID": "EMNLP_abs-482",
            "text": [
                {
                    "index": "482-0",
                    "sentence": "There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text.",
                    "sentence_kor": "텍스트에 언급된 실체 간의 관계에 대해 체계적으로 추론할 수 있는 텍스트 기반 관계 추론 시스템 개발에 대한 관심이 증가하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "482-1",
                    "sentence": "However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text.",
                    "sentence_kor": "그러나, 관계적 추론을 위한 NLP 모델과 텍스트의 기본 기호 표현에 액세스할 수 있는 그래프 신경망(GNN)을 기반으로 하는 모델 사이에는 상당한 성능 차이가 남아있다.",
                    "tag": "1"
                },
                {
                    "index": "482-2",
                    "sentence": "In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance.",
                    "sentence_kor": "본 연구에서는 GNN의 구조화된 지식을 성능을 향상시키기 위해 다양한 NLP 모델로 증류할 수 있는 방법을 조사한다.",
                    "tag": "1+2"
                },
                {
                    "index": "482-3",
                    "sentence": "We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation.",
                    "sentence_kor": "먼저 구조화된 입력을 사용하여 추론 작업에 대해 GNN을 사전 교육한 다음 지식 증류를 통해 해당 지식을 NLP 모델(예: LSTM)에 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "482-4",
                    "sentence": "To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN.",
                    "sentence_kor": "교차 모달 지식 전달의 어려움을 극복하기 위해, 우리는 또한 NLP 모델과 GNN의 잠재적 표현을 정렬하기 위해 대조적인 학습 기반 모듈을 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "482-5",
                    "sentence": "We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.",
                    "sentence_kor": "우리는 CLUTR 벤치마크에서 13개의 서로 다른 귀납적 추론 데이터 세트에서 두 개의 최첨단 NLP 모델로 우리의 접근 방식을 테스트하고 상당한 개선을 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "483",
            "abstractID": "EMNLP_abs-483",
            "text": [
                {
                    "index": "483-0",
                    "sentence": "The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure?",
                    "sentence_kor": "ELMo 및 BERT와 같이 사전 교육을 받은 상황별 인코더의 성공은 이러한 모델이 학습하는 것에 큰 관심을 가져왔다. 즉, 명시적 감독 없이 의미 있는 언어 구조의 개념을 인코딩하는 것을 배우는가?",
                    "tag": "1"
                },
                {
                    "index": "483-1",
                    "sentence": "If so, how is this structure encoded?",
                    "sentence_kor": "그렇다면 이 구조는 어떻게 암호화되는가?",
                    "tag": "1"
                },
                {
                    "index": "483-2",
                    "sentence": "To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs.",
                    "sentence_kor": "이를 조사하기 위해 프로브 입력의 잠재적 범주 분류(또는 온톨로지)를 유도하는 분류기 기반 프로빙의 수정인 잠재 하위 클래스 학습(LSL)을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "483-3",
                    "sentence": "Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form.",
                    "sentence_kor": "LSL은 세분화된 금 라벨에 접근하지 않고 해석 가능하고 수량화할 수 있는 형태의 입력 표현에서 새로운 구조를 추출한다.",
                    "tag": "3+4"
                },
                {
                    "index": "483-4",
                    "sentence": "In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments.",
                    "sentence_kor": "실험에서 우리는 ELMo의 인격 개념과 핵심 논거에 대한 세분화된 의미적 역할에 대한 선호와 같은 새로운 존재론적 차이와 같은 익숙한 범주의 강력한 증거를 발견한다.",
                    "tag": "3+4"
                },
                {
                    "index": "483-5",
                    "sentence": "Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.",
                    "sentence_kor": "우리의 결과는 이전 방법으로는 접근할 수 없는 기존 주석으로부터의 이탈을 포함하여 사전 검증된 인코더의 새로운 구조에 대한 고유한 새로운 증거를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "484",
            "abstractID": "EMNLP_abs-484",
            "text": [
                {
                    "index": "484-0",
                    "sentence": "While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied.",
                    "sentence_kor": "사전 훈련된 언어 모델(LM)의 행동을 철저히 조사했지만, 사전 훈련 중에 일어난 일은 거의 연구되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "484-1",
                    "sentence": "We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model.",
                    "sentence_kor": "따라서 우리는 무작위로 초기화된 매개 변수 집합에서 사전 훈련된 언어 모델의 발생학이라고 부르는 전체 언어 모델까지의 발달 과정을 조사한다.",
                    "tag": "1+2"
                },
                {
                    "index": "484-2",
                    "sentence": "Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining.",
                    "sentence_kor": "우리의 결과는 ALBERT가 사전 훈련 중에 다른 학습 속도로 다른 음성 부분의 토큰을 재구성하고 예측하는 방법을 배운다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "484-3",
                    "sentence": "We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance.",
                    "sentence_kor": "우리는 또한 언어 지식과 세계 지식은 사전 훈련이 진행됨에 따라 일반적으로 개선되지 않으며, 다운스트림 작업의 수행도 개선되지 않는다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "484-4",
                    "sentence": "These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge.",
                    "sentence_kor": "이러한 연구 결과는 사전 교육 중에 사전 교육된 모델에 대한 지식이 다르며 사전 교육 단계가 더 많다고 해서 모델이 더 포괄적인 지식을 제공하는 것은 아니라는 것을 시사한다.",
                    "tag": "4+5"
                },
                {
                    "index": "484-5",
                    "sentence": "We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.",
                    "sentence_kor": "https://github.com/d223302/albert-embryology에서 결과를 재현하기 위해 소스 코드와 사전 검증된 모델을 제공한다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "485",
            "abstractID": "EMNLP_abs-485",
            "text": [
                {
                    "index": "485-0",
                    "sentence": "We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models.",
                    "sentence_kor": "우리는 신경 언어 모델에서 문법 구조의 인코딩을 분석하는 방법으로 전이 학습을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "485-1",
                    "sentence": "We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language.",
                    "sentence_kor": "우리는 LSTM이 자연어에 사용할 수 있는 일반화 가능한 구조적 특징을 유도하는 데이터의 종류를 평가하기 위해 비언어적 데이터에 대해 LSTM을 훈련시키고 자연어에 대한 성능을 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "485-2",
                    "sentence": "We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary.",
                    "sentence_kor": "우리는 잠재 구조(MIDI 음악 또는 Java 코드)를 가진 비언어적 데이터에 대한 훈련이 표면 형태나 어휘가 겹치지 않음에도 불구하고 자연어에 대한 테스트 성능을 향상시킨다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "485-3",
                    "sentence": "To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion.",
                    "sentence_kor": "모델이 이러한 개선을 유도하기 위해 인코딩할 수 있는 추상 구조의 종류를 정확히 파악하기 위해, 우리는 두 개의 인공 괄호 언어, 즉 계층적 재귀 구조를 가진 언어와 쌍으로 구성된 토큰은 있지만 재귀는 없는 컨트롤을 사용하여 유사한 실험을 실행한다.",
                    "tag": "3"
                },
                {
                    "index": "485-4",
                    "sentence": "Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language.",
                    "sentence_kor": "놀랍게도, 이러한 인공 언어 중 하나에 대한 모델을 훈련하는 것은 자연 언어에서 시험할 때 동일한 실질적인 이득을 이끈다.",
                    "tag": "4"
                },
                {
                    "index": "485-5",
                    "sentence": "Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties.",
                    "sentence_kor": "어휘 중복을 제어하는 자연어 간 전송에 대한 추가 실험은 시험 언어에서 제로샷 성능이 훈련 언어와 유형론적 구문 유사성과 높은 상관관계가 있음을 보여주며, 사전 훈련에 의해 유도된 표현이 언어 간 구문 특성에 해당한다는 것을 시사한다.",
                    "tag": "4+5"
                },
                {
                    "index": "485-6",
                    "sentence": "Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.",
                    "sentence_kor": "우리의 결과는 신경 모델이 추상적인 통사적 구조를 나타내는 방법과 또한 자연어 습득을 허용하는 구조적 귀납적 편견의 종류에 대한 통찰력을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "486",
            "abstractID": "EMNLP_abs-486",
            "text": [
                {
                    "index": "486-0",
                    "sentence": "Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models.",
                    "sentence_kor": "사전 훈련된 언어 모델(LM)은 훈련 말뭉치에서 비롯된 편견을 다운스트림 모델로 영구화할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "486-1",
                    "sentence": "We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump).",
                    "sentence_kor": "우리는 주어진 이름의 표현과 관련된 아티팩트(예: Donald)에 초점을 맞춘다. 이 아티팩트는 말뭉치에 따라 다음 토큰 예측(예: Trump)에서 알 수 있듯이 특정 엔티티와 연관될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "486-2",
                    "sentence": "While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts.",
                    "sentence_kor": "일부 상황에서는 유용하지만, 접지는 불충분하거나 부적절한 컨텍스트에서도 발생합니다.",
                    "tag": "1"
                },
                {
                    "index": "486-3",
                    "sentence": "For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment.",
                    "sentence_kor": "예를 들어, 'Donald is a'에 대해 생성된 엔딩은 다른 이름들의 엔딩과 상당히 다르며, 종종 평균 이상의 부정적인 감정을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "486-4",
                    "sentence": "We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers.",
                    "sentence_kor": "이름 섭동이 모델 답변을 변경하는 판독 이해 프로브를 사용하여 다운스트림 작업에 대한 잠재적 영향을 입증한다.",
                    "tag": "2"
                },
                {
                    "index": "486-5",
                    "sentence": "As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.",
                    "sentence_kor": "긍정적인 측면으로, 우리의 실험은 다른 말뭉치에 대한 추가 사전 훈련이 이러한 편견을 완화시킬 수 있다는 것을 암시한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "487",
            "abstractID": "EMNLP_abs-487",
            "text": [
                {
                    "index": "487-0",
                    "sentence": "We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas).",
                    "sentence_kor": "기존 의미 파서를 새로운 환경(예: 새로운 데이터베이스 스키마)에 적응하기 위해 제로쇼트 실행 가능 의미 구문 분석(GAZP)을 위한 접지 적응을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "487-1",
                    "sentence": "GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g.",
                    "sentence_kor": "GAZP는 정방향 의미 분석기와 역방향 발화 생성기를 결합하여 데이터를 합성한다(예.",
                    "tag": "3"
                },
                {
                    "index": "487-2",
                    "sentence": "utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser.",
                    "sentence_kor": "새 환경에서 발언 및 SQL 쿼리)를 선택한 다음 구문 분석기를 적용할 주기 일치 예제를 선택합니다.",
                    "tag": "3"
                },
                {
                    "index": "487-3",
                    "sentence": "Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution.",
                    "sentence_kor": "일반적으로 훈련 환경에서 검증되지 않은 예제를 합성하는 데이터 확대와 달리, GAZP는 실행을 통해 입출력 일관성이 검증되는 새로운 환경에서 예제를 합성한다.",
                    "tag": "3"
                },
                {
                    "index": "487-4",
                    "sentence": "On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser.",
                    "sentence_kor": "Spider, Sparc 및 CoSQL 제로샷 의미 구문 분석 작업에서 GAZP는 기준 파서의 논리적 형식과 실행 정확도를 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "487-5",
                    "sentence": "Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.",
                    "sentence_kor": "분석 결과, GAZP는 교육 환경에서 데이터 보강을 능가하고, GAZP 동기화 데이터의 양에 따라 성능이 향상되며, 주기 일관성이 성공적인 적응의 핵심이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "488",
            "abstractID": "EMNLP_abs-488",
            "text": [
                {
                    "index": "488-0",
                    "sentence": "Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks.",
                    "sentence_kor": "널리 성공한 애플리케이션에도 불구하고 부트스트래핑 및 미세 조정 시맨틱 파서는 여전히 비용이 많이 드는 데이터 주석 및 개인 정보 위험과 같은 과제를 안고 있는 지루한 프로세스입니다.",
                    "tag": "1"
                },
                {
                    "index": "488-1",
                    "sentence": "In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users.",
                    "sentence_kor": "본 논문에서, 우리는 사용자로부터 직접 의미 파서를 학습하기 위한 대안적인 인간-더-루프 방법론을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "488-2",
                    "sentence": "A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain.",
                    "sentence_kor": "의미 분석기는 불확실성을 성찰하고 불확실할 경우 사용자 시연을 요청해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "488-3",
                    "sentence": "In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions.",
                    "sentence_kor": "그렇게 함으로써 사용자 행동을 모방하고, 결국에는 사용자 질문을 해석하는 데 있어서 사용자만큼 잘 될 수 있다는 희망과 함께 계속해서 자율적으로 개선될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "488-4",
                    "sentence": "To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011).",
                    "sentence_kor": "데모의 희소성과 싸우기 위해, 우리는 입증된 상태와 확신 있는 예측을 혼합하여 새로운 데이터 세트를 반복적으로 수집하고 데이터 세트 집계 방식으로 의미 파서를 재교육하는 새로운 주석 효율적인 모방 학습 알고리즘을 제안한다(Ross et al., 2011).",
                    "tag": "3"
                },
                {
                    "index": "488-5",
                    "sentence": "We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.",
                    "sentence_kor": "우리는 그것의 비용 한계에 대한 이론적 분석을 제공하고 또한 텍스트-SQL 문제에 대한 그것의 유망한 성능을 경험적으로 입증한다.",
                    "tag": "4+5"
                },
                {
                    "index": "488-6",
                    "sentence": "Code will be available at https://github.com/sunlab-osu/MISP.",
                    "sentence_kor": "코드는 https://github.com/sunlab-osu/MISP에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "489",
            "abstractID": "EMNLP_abs-489",
            "text": [
                {
                    "index": "489-0",
                    "sentence": "Context-dependent text-to-SQL task has drawn much attention in recent years.",
                    "sentence_kor": "문맥 의존적인 텍스트 대 SQL 작업이 최근 몇 년간 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "489-1",
                    "sentence": "Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs.",
                    "sentence_kor": "컨텍스트 종속 텍스트 대 SQL 작업에 대한 이전 모델은 과거 사용자 입력의 활용에만 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "489-2",
                    "sentence": "In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items.",
                    "sentence_kor": "본 연구에서는 인코더를 사용하여 사용자 입력의 과거 정보를 캡처하는 것 외에도 데이터베이스 스키마 항목의 과거 정보를 활용하는 데이터베이스 스키마 상호 작용 그래프 인코더를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "489-3",
                    "sentence": "In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens.",
                    "sentence_kor": "디코딩 단계에서 다양한 어휘의 중요성을 측정한 다음 SQL 토큰을 예측하는 게이트 메커니즘을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "489-4",
                    "sentence": "We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets.",
                    "sentence_kor": "벤치마크 SPARC 및 CoSQL 데이터 세트에서 모델을 평가한다. SPARC는 두 개의 복잡하고 도메인 간 텍스트-SQL 데이터 세트이다.",
                    "tag": "2"
                },
                {
                    "index": "489-5",
                    "sentence": "Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets.",
                    "sentence_kor": "우리 모델은 이전 최첨단 모델을 큰 차이로 능가하고 두 데이터 세트에서 새로운 최첨단 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "489-6",
                    "sentence": "The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.",
                    "sentence_kor": "비교 및 절제 결과는 모델의 효과와 데이터베이스 스키마 상호 작용 그래프 인코더의 유용성을 입증한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "490",
            "abstractID": "EMNLP_abs-490",
            "text": [
                {
                    "index": "490-0",
                    "sentence": "In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions.",
                    "sentence_kor": "데이터베이스 시스템에 대한 자연어 인터페이스에서 텍스트-SQL 기술은 사용자가 자연어 질문을 사용하여 데이터베이스를 쿼리할 수 있도록 합니다.",
                    "tag": "1"
                },
                {
                    "index": "490-1",
                    "sentence": "Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems.",
                    "sentence_kor": "최근 이 분야에서 상당한 진전이 있었지만 실제 시스템에 배치되면 대부분의 파서가 부족할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "490-2",
                    "sentence": "One main reason stems from the difficulty of fully understanding the users’ natural language questions.",
                    "sentence_kor": "한 가지 주요 이유는 사용자의 자연어 질문을 완전히 이해하는 데 어려움이 있기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "490-3",
                    "sentence": "In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers.",
                    "sentence_kor": "본 논문에서 우리는 인간을 루프에 포함시키고 다중 선택 질문을 사용하여 사용자와 상호 작용하고 임의 파서로 쉽게 작업할 수 있는 새로운 파서 독립적인 대화형 접근법(PIIA)을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "490-4",
                    "sentence": "Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers.",
                    "sentence_kor": "실험은 두 개의 도메인 간 데이터 세트인 WikiSQL과 더 복잡한 Spider를 대상으로 수행되었으며, 5개의 최첨단 파서로 이루어졌다.",
                    "tag": "3"
                },
                {
                    "index": "490-5",
                    "sentence": "These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.",
                    "sentence_kor": "이는 PIIA가 시뮬레이션과 인간 평가를 모두 사용하여 제한된 상호 작용 턴으로 텍스트 대 SQL 성능을 향상시킬 수 있음을 입증했다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "491",
            "abstractID": "EMNLP_abs-491",
            "text": [
                {
                    "index": "491-0",
                    "sentence": "On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots.",
                    "sentence_kor": "WikiSQL 벤치마크에서 최첨단 텍스트 투 SQL 시스템은 일반적으로 슬롯 유형별로 몇 가지 전용 모델을 구축하여 슬롯 채우기 방식을 취합니다.",
                    "tag": "1"
                },
                {
                    "index": "491-1",
                    "sentence": "Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses.",
                    "sentence_kor": "이러한 모듈화된 시스템은 복잡할 뿐만 아니라 SQL 절 간의 상호 의존성을 포착하기 위한 용량도 제한적입니다.",
                    "tag": "1"
                },
                {
                    "index": "491-2",
                    "sentence": "To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries.",
                    "sentence_kor": "이러한 문제를 해결하기 위해 본 논문은 새로운 추출 링크 방식을 제안한다. 여기서 통합 추출기는 링커가 인식된 열을 테이블 스키마에 매핑하여 실행 가능한 SQL 쿼리를 생성하기 전에 질문 문장에 나타나는 모든 유형의 슬롯 언급을 인식한다.",
                    "tag": "1+2"
                },
                {
                    "index": "491-3",
                    "sentence": "Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.",
                    "sentence_kor": "자동으로 생성된 주석으로 교육된 제안된 방법은 WikiSQL 벤치마크에서 1위를 달성한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "492",
            "abstractID": "EMNLP_abs-492",
            "text": [
                {
                    "index": "492-0",
                    "sentence": "(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories.",
                    "sentence_kor": "(T)양상 범주 정서 분석(ACSA) 및 대상 측면 범주 정서 분석(TACSA)을 포함한 ACSA 과제는 사전 정의된 범주에서 정서 극성을 식별하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "492-1",
                    "sentence": "Incremental learning on new categories is necessary for (T)ACSA real applications.",
                    "sentence_kor": "(T)를 위해 새로운 범주에 대한 증분 학습이 필요하다.ACSA 실제 애플리케이션.",
                    "tag": "1"
                },
                {
                    "index": "492-2",
                    "sentence": "Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks.",
                    "sentence_kor": "현재의 멀티태스킹 학습 모델은 (T)에서 우수한 성과를 달성함에도 불구하고ACSA 작업, 그들은 (T)에서 치명적인 망각 문제를 겪는다.ACSA 증분 학습 과제.",
                    "tag": "1"
                },
                {
                    "index": "492-3",
                    "sentence": "In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net).",
                    "sentence_kor": "본 논문에서 우리는 증분 학습을 위해 다중 작업 학습을 실현하기 위해 범주 이름 임베딩 네트워크(CNE-net)를 제안했다.",
                    "tag": "2"
                },
                {
                    "index": "492-4",
                    "sentence": "We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem.",
                    "sentence_kor": "우리는 치명적인 망각 문제를 약화시키기 위해 모든 범주에서 공유하는 인코더와 디코더를 설정했다.",
                    "tag": "3"
                },
                {
                    "index": "492-5",
                    "sentence": "Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.",
                    "sentence_kor": "원점 입력 문장 외에도 작업 차별을 위해 또 다른 입력 기능, 즉 범주 이름을 적용했다.",
                    "tag": "3"
                },
                {
                    "index": "492-6",
                    "sentence": "Our model achieved state-of-the-art on two (T)ACSA benchmark datasets.",
                    "sentence_kor": "우리 모델은 2에서 최첨단(T)을 달성했다.ACSA 벤치마크 데이터 세트.",
                    "tag": "4"
                },
                {
                    "index": "492-7",
                    "sentence": "Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.",
                    "sentence_kor": "또한 (T)에 대한 데이터 세트를 제안했다.ACSA 증분 학습을 통해 다른 강력한 기준선에 비해 최상의 성능을 달성했습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "493",
            "abstractID": "EMNLP_abs-493",
            "text": [
                {
                    "index": "493-0",
                    "sentence": "Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks.",
                    "sentence_kor": "최근 사전 훈련된 언어 모델은 대부분 사전 훈련 후 미세 조정 패러다임을 따르며 다양한 다운스트림 작업에서 뛰어난 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "493-1",
                    "sentence": "However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns.",
                    "sentence_kor": "그러나 사전 훈련 단계는 일반적으로 작업에 구애받지 않으며 미세 조정 단계는 일반적으로 감독 데이터가 불충분하기 때문에 모델이 항상 도메인별 및 작업별 패턴을 잘 포착할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "493-2",
                    "sentence": "In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning.",
                    "sentence_kor": "본 논문에서 우리는 일반적인 사전 훈련과 미세 조정 사이에 선택적 마스킹이 포함된 작업 안내 사전 훈련 단계를 추가하여 3단계 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "493-3",
                    "sentence": "In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns.",
                    "sentence_kor": "이 단계에서 모델은 도메인별 패턴을 학습하기 위해 도메인 내 비지도 데이터에 대한 마스킹 언어 모델링을 통해 학습되며, 작업별 패턴을 학습하기 위한 새로운 선택적 마스킹 전략을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "493-4",
                    "sentence": "Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens.",
                    "sentence_kor": "특히 시퀀스에서 각 토큰의 중요성을 측정하고 중요한 토큰을 선택적으로 마스킹하는 방법을 설계한다.",
                    "tag": "2"
                },
                {
                    "index": "493-5",
                    "sentence": "Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient.",
                    "sentence_kor": "두 가지 감정 분석 작업에 대한 실험 결과는 우리의 방법이 계산 비용의 50% 미만으로 비교할 수 있거나 심지어 더 나은 성능을 달성할 수 있다는 것을 보여주는데, 이것은 우리의 방법이 효과적이고 효율적이라는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "493-6",
                    "sentence": "The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.",
                    "sentence_kor": "본 논문의 소스 코드는 https://github.com/thunlp/SelectiveMasking에서 구할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "494",
            "abstractID": "EMNLP_abs-494",
            "text": [
                {
                    "index": "494-0",
                    "sentence": "Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks.",
                    "sentence_kor": "대부분의 기존 사전 훈련된 언어 표현 모델은 텍스트의 언어 지식을 고려하지 않으며, 이는 NLP 작업에서 언어 이해를 촉진할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "494-1",
                    "sentence": "To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models.",
                    "sentence_kor": "정서 분석의 다운스트림 작업에 도움이 되도록 SentiLARE라는 새로운 언어 표현 모델을 제안한다. SentiWordNet에서 가져온 음성 태그 및 정서 극성을 포함한 단어 수준의 언어 지식을 사전 훈련된 모델에 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "494-2",
                    "sentence": "We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet.",
                    "sentence_kor": "먼저 SentiWordNet을 쿼리하여 음성 부분 태그로 각 단어의 정서 극성을 획득하는 상황 인식 정서 주의 메커니즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "494-3",
                    "sentence": "Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation.",
                    "sentence_kor": "그런 다음, 우리는 지식 인식 언어 표현을 구성하기 위해 라벨 인식 마스킹 언어 모델이라는 새로운 사전 훈련 과제를 고안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "494-4",
                    "sentence": "Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.",
                    "sentence_kor": "실험에 따르면 SentiLARE는 다양한 감정 분석 작업에서 새로운 최첨단 성능을 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "495",
            "abstractID": "EMNLP_abs-495",
            "text": [
                {
                    "index": "495-0",
                    "sentence": "Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner.",
                    "sentence_kor": "검토 텍스트의 측면 기반 정서 분석은 사용자 피드백을 세밀한 방식으로 이해하는 데 큰 가치가 있다.",
                    "tag": "1"
                },
                {
                    "index": "495-1",
                    "sentence": "It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity.",
                    "sentence_kor": "일반적으로 (i) 각 검토에서 측면을 추출하는 것과 (ii) 감정 극성에 따라 측면 기반 검토를 분류하는 두 가지 하위 작업이 있다.",
                    "tag": "1"
                },
                {
                    "index": "495-2",
                    "sentence": "In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples.",
                    "sentence_kor": "본 논문에서, 우리는 측면 기반 정서 분석을 위해 약하게 감독되는 접근방식을 제안한다. 이 접근방식은 라벨링된 예를 사용하지 않고 각 측면/감정을 설명하는 몇 개의 키워드만 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "495-3",
                    "sentence": "Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts.",
                    "sentence_kor": "기존 방법은 하위 작업 중 하나에 대해서만 설계되거나 중복되는 개념을 포함할 수 있는 주제 모델을 기반으로 합니다.",
                    "tag": "3"
                },
                {
                    "index": "495-4",
                    "sentence": "We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data.",
                    "sentence_kor": "우리는 먼저 주제 차별성을 장려하기 위해 정규화를 시행하여 단어 임베딩 공간에 <감정, 측면> 공동 주제 임베딩을 학습한 다음, 신경 모델을 사용하여 임베딩 기반 예측으로 분류자를 사전 훈련하고 레이블이 없는 데이터에 대해 자체 훈련을 실시하여 단어 수준 차별 정보를 일반화할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "495-5",
                    "sentence": "Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.",
                    "sentence_kor": "우리의 종합적인 성능 분석에 따르면 우리의 방법은 양질의 공동 주제를 생성하고 벤치마크 데이터 세트에서 기준선을 크게 능가한다(각각 측면 및 감정 분류에 대해 평균 7.4%, 5.1%)",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "496",
            "abstractID": "EMNLP_abs-496",
            "text": [
                {
                    "index": "496-0",
                    "sentence": "Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments.",
                    "sentence_kor": "풍부한 상호 작용과 논쟁적인 토론이 있는 동료 검토와 반박은 자연스럽게 제 주장을 뒷받침하는 좋은 자료입니다.",
                    "tag": "1"
                },
                {
                    "index": "496-1",
                    "sentence": "However, few works study both of them simultaneously.",
                    "sentence_kor": "그러나 두 가지를 동시에 연구하는 작품은 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "496-2",
                    "sentence": "In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them.",
                    "sentence_kor": "본 논문에서, 우리는 내용, 구조 및 그들 사이의 연결을 연구하기 위해 동료 검토와 반박에 대한 새로운 인수 쌍 추출(APE) 과제를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "496-3",
                    "sentence": "We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task.",
                    "sentence_kor": "우리는 이 작업의 연구를 용이하게 하기 위해 개방형 검토 플랫폼에서 4,764개의 주석이 달린 검토-반박 내용 쌍을 포함하는 까다로운 데이터 세트를 준비한다.",
                    "tag": "3"
                },
                {
                    "index": "496-4",
                    "sentence": "To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task.",
                    "sentence_kor": "인수 제안을 자동으로 탐지하고 이 말뭉치에서 인수 쌍을 추출하기 위해 시퀀스 레이블링 작업과 텍스트 관계 분류 작업의 조합으로 캐스팅했다.",
                    "tag": "3"
                },
                {
                    "index": "496-5",
                    "sentence": "Thus, we propose a multitask learning framework based on hierarchical LSTM networks.",
                    "sentence_kor": "따라서 계층적 LSTM 네트워크를 기반으로 하는 멀티태스크 학습 프레임워크를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "496-6",
                    "sentence": "Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.",
                    "sentence_kor": "광범위한 실험과 분석은 멀티태스킹 프레임워크의 효과를 입증하고 새로운 과제의 과제를 보여주며 향후 연구 방향에 동기를 부여한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "497",
            "abstractID": "EMNLP_abs-497",
            "text": [
                {
                    "index": "497-0",
                    "sentence": "Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious.",
                    "sentence_kor": "신경 문서 수준 DMSC(Multi-spect Mentimal Classification)는 일반적으로 많은 수동 측면 수준 감정 주석을 필요로 하는데, 이는 시간이 많이 걸리고 작업도 많이 걸린다.",
                    "tag": "1"
                },
                {
                    "index": "497-1",
                    "sentence": "As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations.",
                    "sentence_kor": "문서 수준 정서 라벨 데이터는 온라인 서비스에서 널리 사용 가능하므로 무료 문서 수준 주석을 사용하여 DMSC를 수행하는 것이 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "497-2",
                    "sentence": "To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision.",
                    "sentence_kor": "이를 위해 문서 수준의 약한 감독만으로 측면 수준의 정서 분류를 달성할 수 있는 새로운 D-MILN(Diversated Multiple Instance Learning Network)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "497-3",
                    "sentence": "Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision.",
                    "sentence_kor": "특히, 우리는 이 문제를 다중 인스턴스 학습으로 공식화하여 측면 수준과 문서 수준의 감정을 연결하고, 문서 수준 감시의 후방 전파로부터 측면 수준 분류기를 학습하는 방법을 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "497-4",
                    "sentence": "Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training.",
                    "sentence_kor": "훈련 중 문서 수준 신호에 대한 과적합을 방지하기 위해 두 가지 다양한 정규화가 추가로 도입된다.",
                    "tag": "3"
                },
                {
                    "index": "497-5",
                    "sentence": "Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment.",
                    "sentence_kor": "텍스트 정규화는 분류자가 측면 관련 조각을 선택하도록 장려하며, 감성 정규화는 측면 수준의 정서가 문서 수준의 정서와 지나치게 일치하는 것을 방지한다.",
                    "tag": "3"
                },
                {
                    "index": "497-6",
                    "sentence": "Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.",
                    "sentence_kor": "TripAdvisor 및 BeerAdvocate 데이터 세트에 대한 실험 결과는 D-MILN이 최근 약하게 감독된 기준선을 현저하게 능가하며 감독되는 방법과도 비교가 가능하다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "498",
            "abstractID": "EMNLP_abs-498",
            "text": [
                {
                    "index": "498-0",
                    "sentence": "While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community.",
                    "sentence_kor": "과장은 가장 널리 사용되는 수사적 장치 중 하나이지만, 비유적 언어 처리 커뮤니티에서 가장 덜 연구된 장치 중 하나일 것이다.",
                    "tag": "1"
                },
                {
                    "index": "498-1",
                    "sentence": "We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.",
                    "sentence_kor": "(1) 문장 수준의 하이퍼볼 탐지에 초점을 맞춘 말뭉치를 만들고, (2) 말뭉치의 통계적 및 수동 분석을 수행하고, (3) 자동 하이퍼볼 탐지 과제를 해결함으로써 하이퍼볼 연구에 기여한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "499",
            "abstractID": "EMNLP_abs-499",
            "text": [
                {
                    "index": "499-0",
                    "sentence": "The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data.",
                    "sentence_kor": "측면 기반 감정 분석(ABSA)을 위한 감독 모델은 라벨링된 데이터에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "499-1",
                    "sentence": "However, fine-grained labeled data are scarce for the ABSA task.",
                    "sentence_kor": "그러나 세분화된 라벨링 데이터는 ABSA 작업에 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "499-2",
                    "sentence": "To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation.",
                    "sentence_kor": "라벨링된 데이터에 대한 의존도를 완화하기 위해 이전 연구는 주로 기능 기반 적응에 초점을 맞추었는데, 이는 도메인 공유 지식을 사용하여 도메인 간 격차를 해소하기 위한 보조 작업 또는 도메인 적대적 학습을 구성했지만 인스턴스 기반 적응의 속성은 무시했다.",
                    "tag": "1"
                },
                {
                    "index": "499-3",
                    "sentence": "To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper.",
                    "sentence_kor": "이러한 한계를 해결하기 위해 본 백서의 ABSA 과제에 대한 특징 및 인스턴스 기반 적응을 공동으로 수행하는 엔드 투 엔드 프레임워크를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "499-4",
                    "sentence": "Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling.",
                    "sentence_kor": "BERT를 기반으로, 우리는 보조 작업을 구성하기 위해 음성 부분 기능과 구문 종속 관계를 사용하여 도메인 불변 기능 표현을 배우고 시퀀스 라벨링 프레임워크에서 단어 수준 인스턴스 가중치를 공동으로 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "499-5",
                    "sentence": "Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.",
                    "sentence_kor": "네 가지 벤치마크에 대한 실험 결과는 제안된 방법이 교차 도메인 End2End ABSA 및 교차 도메인 측면 추출의 두 작업에서 최첨단과 비교하여 상당한 개선을 달성할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "500",
            "abstractID": "EMNLP_abs-500",
            "text": [
                {
                    "index": "500-0",
                    "sentence": "Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected.",
                    "sentence_kor": "설명 가능한 질문 답변 시스템은 답이 선택된 이유를 보여주는 설명과 함께 답을 예측합니다.",
                    "tag": "1"
                },
                {
                    "index": "500-1",
                    "sentence": "The goal is to enable users to assess the correctness of the system and understand its reasoning process.",
                    "sentence_kor": "목표는 사용자가 시스템의 정확성을 평가하고 시스템의 추론 과정을 이해할 수 있도록 하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "500-2",
                    "sentence": "However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience.",
                    "sentence_kor": "그러나 현재 모델과 평가 설정에 사용자 경험에 심각한 문제를 일으킬 수 있는 답변과 설명의 결합에 관한 단점이 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "500-3",
                    "sentence": "As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling.",
                    "sentence_kor": "해결책으로, 우리는 해답-해설 커플링과 커플링을 정량화하기 위한 두 가지 평가 점수를 강화하기 위한 계층적 모델과 새로운 정규화 용어를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "500-4",
                    "sentence": "We conduct experiments on the HOTPOTQA benchmark data set and perform a user study.",
                    "sentence_kor": "우리는 HOTPOTQA 벤치마크 데이터 세트에 대한 실험을 수행하고 사용자 연구를 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "500-5",
                    "sentence": "The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users.",
                    "sentence_kor": "사용자 연구에 따르면 우리 모델은 시스템의 정확성을 판단하는 사용자의 능력을 증가시키며 F1과 같은 점수는 실제 사용자 환경에서 모델의 유용성을 추정하기에 충분하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "500-6",
                    "sentence": "Our scores are better aligned with user experience, making them promising candidates for model selection.",
                    "sentence_kor": "우리의 점수는 사용자 경험과 더 잘 맞아서 모델 선정에 유망한 후보이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "501",
            "abstractID": "EMNLP_abs-501",
            "text": [
                {
                    "index": "501-0",
                    "sentence": "Knowledge graphs (KGs) can vary greatly from one domain to another.",
                    "sentence_kor": "지식 그래프(KG)는 도메인마다 크게 다를 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "501-1",
                    "sentence": "Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations.",
                    "sentence_kor": "따라서 그래프-텍스트 생성 및 텍스트-그래프 지식 추출(의미적 구문 분석)에 대한 감독 접근법은 항상 도메인별 병렬 그래프-텍스트 데이터의 부족으로 어려움을 겪을 것이다. 동시에, 다른 도메인에서 훈련된 모델을 적용하는 것은 개체와 관계가 거의 또는 전혀 겹치지 않기 때문에 불가능한 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "501-2",
                    "sentence": "This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains.",
                    "sentence_kor": "이 상황은 (1) 주석이 달린 대량의 데이터가 필요하지 않으므로 (2) 다른 도메인에서 잘 작동하기 위해 도메인 적응 기술에 의존할 필요가 없는 접근방식을 요구한다.",
                    "tag": "1"
                },
                {
                    "index": "501-3",
                    "sentence": "To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing.",
                    "sentence_kor": "이를 위해 KG에서 감독되지 않은 텍스트 생성에 대한 첫 번째 접근방식을 제시하고 감독되지 않은 의미 파싱에 사용할 수 있는 방법을 동시에 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "501-4",
                    "sentence": "We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome.",
                    "sentence_kor": "WebNLG v2.1에 대한 접근 방식과 Visual Genome의 장면 그래프를 활용한 새로운 벤치마크를 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "501-5",
                    "sentence": "Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other.",
                    "sentence_kor": "우리 시스템은 한 데이터 세트에서 다른 데이터 세트로 수동 적응하지 않고 두 텍스트 <->그래프 변환 작업에 대해 강력한 기준을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "501-6",
                    "sentence": "In additional experiments, we investigate the impact of using different unsupervised objectives.",
                    "sentence_kor": "추가 실험에서, 우리는 다른 감독되지 않은 목표 사용의 영향을 조사한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "502",
            "abstractID": "EMNLP_abs-502",
            "text": [
                {
                    "index": "502-0",
                    "sentence": "We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer.",
                    "sentence_kor": "텍스트 스타일 전송을 위한 새롭고 단순한 이중 생성기 네트워크 아키텍처인 DGST를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "502-1",
                    "sentence": "Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training.",
                    "sentence_kor": "우리 모델은 두 대의 발전기만 사용하며, 훈련을 위해 판별기 또는 병렬 말뭉치에 의존하지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "502-2",
                    "sentence": "Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.",
                    "sentence_kor": "Yelp 및 IMDb 데이터 세트에 대한 정량적 및 정성적 실험은 우리 모델이 더 복잡한 아키텍처 설계를 가진 몇 가지 강력한 기준선에 비해 경쟁력 있는 성능을 제공한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "503",
            "abstractID": "EMNLP_abs-503",
            "text": [
                {
                    "index": "503-0",
                    "sentence": "With the advancements in natural language processing tasks, math word problem solving has received increasing attention.",
                    "sentence_kor": "자연어 처리과제의 발달로, 수학 단어 문제 풀이가 점점 더 주목을 받고 있다.",
                    "tag": "1"
                },
                {
                    "index": "503-1",
                    "sentence": "Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem.",
                    "sentence_kor": "이전 방법은 유망한 결과를 얻었지만 문제에 의해 직접 제공되지 않는 배경 상식 지식은 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "503-2",
                    "sentence": "In addition, during generation, they focus on local features while neglecting global information.",
                    "sentence_kor": "또한 생성 중에는 글로벌 정보를 무시한 채 로컬 기능에 초점을 맞춥니다.",
                    "tag": "1"
                },
                {
                    "index": "503-3",
                    "sentence": "To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph.",
                    "sentence_kor": "외부 지식과 전역 표현 정보를 통합하기 위해 문제 시퀀스와 범주의 엔티티를 엔티티 그래프로 모델링하는 새로운 지식 인식 시퀀스 투 트리(KA-S2T) 네트워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "503-4",
                    "sentence": "Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations.",
                    "sentence_kor": "이 엔티티 그래프를 기반으로, 그래프 주의 네트워크는 지식 인식 문제 표현을 캡처하는 데 사용된다.",
                    "tag": "2"
                },
                {
                    "index": "503-5",
                    "sentence": "Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information.",
                    "sentence_kor": "또한 상태 집계 메커니즘이 있는 트리 구조 디코더를 사용하여 장거리 종속성 및 전역 표현 정보를 캡처한다.",
                    "tag": "2"
                },
                {
                    "index": "503-6",
                    "sentence": "Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.",
                    "sentence_kor": "Math23K 데이터 세트에 대한 실험 결과는 KA-S2T 모델이 이전에 보고된 최상의 결과보다 더 나은 성능을 달성할 수 있다는 것을 보여주었다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "504",
            "abstractID": "EMNLP_abs-504",
            "text": [
                {
                    "index": "504-0",
                    "sentence": "We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC).",
                    "sentence_kor": "작업을 ESD(Error Span Detection)와 ESC(Error Span Correction)의 두 가지 하위 작업으로 나누어 GEC(Grammatical Error Correction)의 효율성을 개선하기 위한 새로운 언어 독립적인 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "504-1",
                    "sentence": "ESD identifies grammatically incorrect text spans with an efficient sequence tagging model.",
                    "sentence_kor": "ESD는 효율적인 시퀀스 태그 모델을 통해 문법적으로 잘못된 텍스트 범위를 식별합니다.",
                    "tag": "3"
                },
                {
                    "index": "504-2",
                    "sentence": "Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans.",
                    "sentence_kor": "그런 다음 ESC는 seq2seq 모델을 활용하여 주석이 달린 오류 범위가 있는 문장을 입력으로 받아들이고 해당 범위에 대해 수정된 텍스트만 출력한다.",
                    "tag": "3"
                },
                {
                    "index": "504-3",
                    "sentence": "Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.",
                    "sentence_kor": "실험에 따르면 우리의 접근 방식은 추론 시간 비용이 50% 미만인 영어와 중국어 GEC 벤치마크 모두에서 기존 seq2seq 접근 방식과 비교할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "505",
            "abstractID": "EMNLP_abs-505",
            "text": [
                {
                    "index": "505-0",
                    "sentence": "Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning.",
                    "sentence_kor": "BERT와 같은 언어 표현 모델은 일반 텍스트에서 상황별 의미 정보를 효과적으로 캡처할 수 있으며 적절한 미세 조정을 통해 많은 다운스트림 NLP 작업에서 유망한 결과를 얻을 수 있다는 것이 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "505-1",
                    "sentence": "However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse.",
                    "sentence_kor": "그러나 대부분의 기존 언어 표현 모델은 전체 담론의 일관성 있는 이해에 필수적인 상호 참조를 명시적으로 처리할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "505-2",
                    "sentence": "To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 맥락에서 핵심 미분 관계를 포착할 수 있는 새로운 언어 표현 모델인 CorefBERT를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "505-3",
                    "sentence": "The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks.",
                    "sentence_kor": "실험 결과에 따르면 CorefBERT는 기존 기본 모델과 비교하여 다른 일반적인 NLP 작업에 대한 이전 모델과 유사한 성능을 유지하면서 핵심적 추론이 필요한 다양한 다운스트림 NLP 작업에서 일관되게 상당한 개선을 달성할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "505-4",
                    "sentence": "The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.",
                    "sentence_kor": "본 논문의 소스 코드와 실험 세부사항은 https://github.com/thunlp/CorefBERT에서 얻을 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "506",
            "abstractID": "EMNLP_abs-506",
            "text": [
                {
                    "index": "506-0",
                    "sentence": "Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory.",
                    "sentence_kor": "주류 계산 어휘 의미론에서는 단어 감각이 사전 정의된 목록의 이산 항목으로 표현될 수 있다는 가정을 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "506-1",
                    "sentence": "In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions.",
                    "sentence_kor": "본 논문에서 우리는 이것이 그럴 필요가 없다는 것을 보여주고 맥락적으로 적절한 정의를 도출할 수 있는 통일된 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "506-2",
                    "sentence": "In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses.",
                    "sentence_kor": "우리의 모델인 Generationary에서는 광택을 생성하기 위해 영어 사전 훈련된 인코더-디코더 시스템을 미세 조정하는 데 사용하는 새로운 스팬 기반 인코딩 체계를 채택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "506-3",
                    "sentence": "We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context.",
                    "sentence_kor": "우리는 사전 정의된 감각 목록에서 선택할 필요성을 배제하더라도 모델이 효과적으로 사용될 수 있음을 보여준다. 생성은 많은 설정에서 정의 모델링의 생성 작업에서 이전 접근 방식을 능가할 뿐만 아니라 워드 감지 D와 같은 차별적 작업에서 최신 기술과 일치하거나 능가한다.isambigization 및 Word-in-Context.",
                    "tag": "4"
                },
                {
                    "index": "506-4",
                    "sentence": "Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases.",
                    "sentence_kor": "마지막으로, 우리는 무료 형용사 어구에 대한 새로운 정의 데이터 세트를 포함하여 다양한 제로샷 벤치마크에 대한 강력한 이득과 함께 여러 재고의 데이터에 대한 훈련을 통해 생성의 이점을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "506-5",
                    "sentence": "The software and reproduction materials are available at http://generationary.org.",
                    "sentence_kor": "소프트웨어와 복제 자료는 http://generationary.org에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "507",
            "abstractID": "EMNLP_abs-507",
            "text": [
                {
                    "index": "507-0",
                    "sentence": "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture.",
                    "sentence_kor": "BERT와 RoBERTa와 같은 대규모 사전 교육 언어 모델(LM)의 성공은 어떤 유형의 지식을 암묵적으로 포착하는지 밝히기 위해 표현 조사에 대한 관심을 촉발시켰다.",
                    "tag": "1"
                },
                {
                    "index": "507-1",
                    "sentence": "While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context.",
                    "sentence_kor": "이전 연구는 형태론적, 의미론적 및 세계 지식에 초점을 맞췄지만, LM이 맥락의 단어로부터 어휘 유형 수준의 지식을 어느 정도까지 도출하는지는 여전히 불확실하다.",
                    "tag": "1"
                },
                {
                    "index": "507-2",
                    "sentence": "In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance?",
                    "sentence_kor": "본 연구에서는 유형학적으로 다양한 6개의 언어와 5개의 서로 다른 어휘 작업에 걸친 체계적인 경험적 분석을 제시한다. 1) 서로 다른 어휘 지식 추출 전략(단일 언어 대 다국어 소스 LM, 컨텍스트 외 대 컨텍스트 내 인코딩, 특수 토큰 포함 및)을 다루는 방법은 무엇인가? 계층별 평균화) 성능에 영향을 미칩니까?",
                    "tag": "2+3"
                },
                {
                    "index": "507-3",
                    "sentence": "How consistent are the observed effects across tasks and languages?",
                    "sentence_kor": "작업 및 언어 전반에 걸쳐 관찰된 효과는 얼마나 일관성이 있는가?",
                    "tag": "2"
                },
                {
                    "index": "507-4",
                    "sentence": "2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network?",
                    "sentence_kor": "2) 어휘지식은 소수의 매개변수로 저장되어 있는가, 아니면 네트워크 전체에 흩어져 있는가?",
                    "tag": "3"
                },
                {
                    "index": "507-5",
                    "sentence": "3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities?",
                    "sentence_kor": "3) 어휘 과제의 전통적인 정적 단어 벡터에 대한 이러한 표현은 어떻게 수행됩니까4) 독립적으로 훈련된 단일 언어 LM에서 나타나는 어휘 정보는 잠재적인 유사성을 나타냅니까?",
                    "tag": "3"
                },
                {
                    "index": "507-6",
                    "sentence": "Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks.",
                    "sentence_kor": "우리의 주요 결과는 보편적으로 유지되는 패턴과 모범 사례를 나타내지만, 언어와 작업에 따른 현저한 변화를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "507-7",
                    "sentence": "Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
                    "sentence_kor": "또한 하위 트랜스포머 계층이 더 많은 유형 수준의 어휘 지식을 제공한다는 주장을 검증할 뿐만 아니라 이 지식이 여러 계층에 분산되어 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "508",
            "abstractID": "EMNLP_abs-508",
            "text": [
                {
                    "index": "508-0",
                    "sentence": "Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal.",
                    "sentence_kor": "구어 이해 시스템에 대한 현재 언어 간 모델의 유망한 결과에도 불구하고, 소스 언어와 대상 언어 간의 불완전한 언어 간 표현 정렬로 인해 성능이 차선책으로 작용한다.",
                    "tag": "1"
                },
                {
                    "index": "508-1",
                    "sentence": "To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource.",
                    "sentence_kor": "이 문제에 대처하기 위해 외부 자원 없이 언어 간에 단어 수준 및 문장 수준 표현을 추가로 정렬하는 정규화 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "508-2",
                    "sentence": "First, we regularize the representation of user utterances based on their corresponding labels.",
                    "sentence_kor": "첫째, 해당 레이블을 기반으로 사용자 발언의 표현을 정규화한다.",
                    "tag": "3"
                },
                {
                    "index": "508-3",
                    "sentence": "Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables.",
                    "sentence_kor": "둘째, 우리는 잠재 변수를 분리하기 위해 적대적 훈련을 활용하여 잠재 변수 모델(Liu 등, 2019)을 정규화한다.",
                    "tag": "3"
                },
                {
                    "index": "508-4",
                    "sentence": "Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.",
                    "sentence_kor": "언어 간 구어 이해 작업에 대한 실험에 따르면 우리 모델은 퓨샷 시나리오와 제로샷 시나리오 모두에서 현재 최첨단 방법을 능가하며, 목표 언어 훈련 데이터의 3%만 사용하여 퓨샷 설정으로 훈련된 모델은 모든 트레이닌으로 감독되는 훈련과 유사한 성능을 달성한다.g data.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "509",
            "abstractID": "EMNLP_abs-509",
            "text": [
                {
                    "index": "509-0",
                    "sentence": "Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data.",
                    "sentence_kor": "신경 모델에 대한 과거의 발전은 라벨링된 데이터가 충분하다면 명명된 개체 인식은 더 이상 문제가 되지 않는다는 것을 증명했다.",
                    "tag": "1"
                },
                {
                    "index": "509-1",
                    "sentence": "However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive.",
                    "sentence_kor": "그러나 충분한 데이터를 수집하고 주석을 다는 것은 노동 집약적이고, 시간이 많이 걸리며, 비용이 많이 듭니다.",
                    "tag": "1"
                },
                {
                    "index": "509-2",
                    "sentence": "In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective.",
                    "sentence_kor": "본 논문에서 우리는 문장을 실체와 맥락의 두 부분으로 분해하고 인과적 관점에서 그것들과 모델 성능 사이의 관계를 재고한다.",
                    "tag": "2+3"
                },
                {
                    "index": "509-3",
                    "sentence": "Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset.",
                    "sentence_kor": "이를 바탕으로 기존 관측 예제에 대한 개입으로 반사실적 예제를 생성하여 원본 데이터 세트를 개선하는 반사실적 생성기를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "509-4",
                    "sentence": "Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples.",
                    "sentence_kor": "세 개의 데이터 세트에 걸친 실험에 따르면 우리의 방법은 제한된 관찰 예에서 모델의 일반화 능력을 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "509-5",
                    "sentence": "Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels.",
                    "sentence_kor": "또한 구조적 인과 모델을 사용하여 입력 기능과 출력 레이블 간의 가짜 상관 관계를 탐색함으로써 이론적 기초를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "509-6",
                    "sentence": "We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented.",
                    "sentence_kor": "우리는 비증강 및 증강 두 조건 모두에서 모델 성능에 대한 실체 또는 컨텍스트의 인과적 영향을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "509-7",
                    "sentence": "Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation.",
                    "sentence_kor": "흥미롭게도, 우리는 중요하지 않은 상관관계가 상황 표현보다는 개체 표현에 더 위치한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "509-8",
                    "sentence": "As a result, our method eliminates part of the spurious correlations between context representation and output labels.",
                    "sentence_kor": "결과적으로, 우리의 방법은 컨텍스트 표현과 출력 라벨 사이의 잘못된 상관 관계의 일부를 제거한다.",
                    "tag": "4"
                },
                {
                    "index": "509-9",
                    "sentence": "The code is available at https://github.com/xijiz/cfgen.",
                    "sentence_kor": "코드는 https://github.com/xijiz/cfgen에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "510",
            "abstractID": "EMNLP_abs-510",
            "text": [
                {
                    "index": "510-0",
                    "sentence": "The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process.",
                    "sentence_kor": "절차적 텍스트 이해 작업은 프로세스에서 개체/개체의 동적 특성을 이해하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "510-1",
                    "sentence": "Here, the key is to track how the entities interact with each other and how their states are changing along the procedure.",
                    "sentence_kor": "여기서 핵심은 실체가 서로 어떻게 상호작용하고 그 상태가 절차에 따라 어떻게 변화하는지 추적하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "510-2",
                    "sentence": "Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned.",
                    "sentence_kor": "최근의 노력은 절차 텍스트에서 여러 실체를 추적하려는 큰 진전을 이루었지만, 일반적으로 각 실체를 개별적으로 취급하고 한 프로세스 동안 서로 상호작용하는 여러 실체가 있다는 사실을 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "510-3",
                    "sentence": "In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking.",
                    "sentence_kor": "본 논문에서 우리는 상태 추적을 위한 메모리가 장착된 셀을 가진 반복 네트워크인 새로운 IEN(Interactive Entity Network)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "510-4",
                    "sentence": "In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions.",
                    "sentence_kor": "각 IEN 셀에서 우리는 서로 다른 유형의 개체 상호작용을 모델링하기 위해 특정 메모리를 통해 서로 다른 주의 매트릭스를 유지한다.",
                    "tag": "2+3"
                },
                {
                    "index": "510-5",
                    "sentence": "Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes.",
                    "sentence_kor": "중요한 것은, 우리는 실체 작용과 후속 상태 변화 사이의 인과 관계를 탐구하기 위해 순차적인 방법으로 이러한 기억을 업데이트할 수 있다는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "510-6",
                    "sentence": "We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.",
                    "sentence_kor": "벤치마크 데이터 세트에서 모델을 평가하며, 그 결과는 IEN이 여러 엔티티의 상호 작용을 정밀하게 포착하고 엔티티 상호작용과 후속 상태 변경 사이의 관계를 명시적으로 활용하여 최첨단 모델을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "511",
            "abstractID": "EMNLP_abs-511",
            "text": [
                {
                    "index": "511-0",
                    "sentence": "There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.",
                    "sentence_kor": "최근 시간이 지남에 따라 개체 간의 동적 관계를 기록하는 시간 지식 그래프(KG)의 표현 학습에 관심이 높아지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "511-1",
                    "sentence": "Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures.",
                    "sentence_kor": "시간 KG는 종종 계층 구조 및 순환 구조와 같은 복수의 동시 비유클리드 구조를 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "511-2",
                    "sentence": "However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well.",
                    "sentence_kor": "그러나, 시간적 KG에 대한 기존 내장 접근법은 일반적으로 그러한 내재 구조를 잘 포착하지 못할 수 있는 유클리드 공간에서 실체 표현과 그 동적 진화를 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "511-3",
                    "sentence": "To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data.",
                    "sentence_kor": "이를 위해 기본 데이터의 단면 곡선으로부터 구성 공간을 추정하는 리만 다양체의 산물에서 진화하는 실체 표현을 학습하는 비유클리드 임베딩 접근법인 DYERNIE를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "511-4",
                    "sentence": "Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs.",
                    "sentence_kor": "제품 다양체를 통해 시간 KG에 광범위한 기하학적 구조를 더 잘 반영할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "511-5",
                    "sentence": "Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp.",
                    "sentence_kor": "또한 시간 KG의 진화 역학을 포착하기 위해 각 타임스탬프의 탄젠트 공간에 정의된 속도 벡터에 따라 개체 표현이 진화하도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "511-6",
                    "sentence": "We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks.",
                    "sentence_kor": "우리는 시간적 KG의 표현 학습에 대한 기하학적 공간의 기여를 자세히 분석하고 시간적 지식 그래프 완료 작업에 대한 모델을 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "511-7",
                    "sentence": "Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.",
                    "sentence_kor": "세 개의 실제 데이터 세트에 대한 광범위한 실험은 성능이 크게 향상되었음을 보여주며, 다중 관계 그래프 데이터의 역학이 리만 매니폴드의 임베딩 진화에 의해 더 적절하게 모델링될 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "512",
            "abstractID": "EMNLP_abs-512",
            "text": [
                {
                    "index": "512-0",
                    "sentence": "It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe).",
                    "sentence_kor": "단어를 벡터 공간의 요소로 나타내는 것은 사실상 표준이 되었다(word2vec, GloVe).",
                    "tag": "1"
                },
                {
                    "index": "512-1",
                    "sentence": "While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings.",
                    "sentence_kor": "이 접근법은 편리하지만 언어에서는 부자연스럽다. 단어는 잠재적인 계층 구조를 가진 그래프를 형성하며, 이 구조는 단어 임베딩에 의해 드러나고 암호화되어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "512-2",
                    "sentence": "We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end.",
                    "sentence_kor": "우리는 GraphGlove를 소개한다. 단대단적으로 학습되는 감독되지 않은 그래프 단어 표현이다.",
                    "tag": "1"
                },
                {
                    "index": "512-3",
                    "sentence": "In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes.",
                    "sentence_kor": "우리의 설정에서, 각 워드는 가중 그래프의 노드이며 워드 사이의 거리는 해당 노드 사이의 최단 경로 거리이다.",
                    "tag": "1"
                },
                {
                    "index": "512-4",
                    "sentence": "We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm.",
                    "sentence_kor": "우리는 차별화 가능한 가중 그래프의 형태로 데이터 표현을 학습하는 최근 방법을 채택하고 GloVe 훈련 알고리즘을 수정하는 데 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "512-5",
                    "sentence": "We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks.",
                    "sentence_kor": "우리는 그래프 기반 표현이 단어 유사성 및 유추 작업에서 벡터 기반 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "512-6",
                    "sentence": "Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.",
                    "sentence_kor": "우리의 분석에 따르면 학습된 그래프의 구조는 계층적이고 WordNet의 구조와 유사하며 기하학은 매우 사소하지 않으며 다른 로컬 토폴로지를 가진 하위 그래프를 포함하고 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "513",
            "abstractID": "EMNLP_abs-513",
            "text": [
                {
                    "index": "513-0",
                    "sentence": "It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers.",
                    "sentence_kor": "지식 그래프 임베딩은 잠재적으로 해로운 사회적 편견을 암호화하는 것으로 나타났다. 예를 들어 여성은 간호사가 될 가능성이 높고 남성은 은행가가 될 가능성이 높다.",
                    "tag": "1"
                },
                {
                    "index": "513-1",
                    "sentence": "As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases.",
                    "sentence_kor": "그래프 임베딩이 NLP 파이프라인에서 더 널리 사용되기 시작함에 따라 이러한 편견을 제거하는 훈련 방법을 개발할 필요가 있다.",
                    "tag": "1"
                },
                {
                    "index": "513-2",
                    "sentence": "Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially.",
                    "sentence_kor": "이 문제에 대한 이전의 접근방식은 훈련 시간을 8배 이상 크게 증가시키고 모델의 정확도를 크게 감소시킨다.",
                    "tag": "1"
                },
                {
                    "index": "513-3",
                    "sentence": "We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss.",
                    "sentence_kor": "우리는 모든 임베딩이 기본적으로 적대적 손실을 사용하여 성별과 같은 민감한 속성에 중립적이도록 훈련되는 새로운 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "513-4",
                    "sentence": "We then add sensitive attributes back on in whitelisted cases.",
                    "sentence_kor": "그런 다음 화이트리스트 사례에 중요한 속성을 다시 추가합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "513-5",
                    "sentence": "Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.",
                    "sentence_kor": "훈련 시간은 기준선 모델에 비해 약간만 증가하며, 감소된 임베딩은 감소되지 않은 모델만큼 삼중 예측 작업에서 거의 정확하게 수행된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "514",
            "abstractID": "EMNLP_abs-514",
            "text": [
                {
                    "index": "514-0",
                    "sentence": "Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact.",
                    "sentence_kor": "초관계 지식 그래프(KGs)(예: Wikidata)는 주 삼중주와 함께 추가 키-값 쌍을 연결하여 사실의 유효성을 모호하게 하거나 제한할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "514-1",
                    "sentence": "In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs.",
                    "sentence_kor": "본 연구에서는 이러한 초관계 KG를 모델링할 수 있는 메시지 전달 기반 그래프 인코더인 StarE를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "514-2",
                    "sentence": "Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact.",
                    "sentence_kor": "기존 접근 방식과 달리, StarE는 한정자와 3배의 의미적 역할을 그대로 유지하면서 주 3중과 함께 임의의 수의 추가 정보(적격자)를 인코딩할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "514-3",
                    "sentence": "We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K.",
                    "sentence_kor": "또한 초관계 KG에 대한 링크 예측(LP) 성능을 평가하기 위한 기존 벤치마크가 근본적인 결함을 겪고 있으며, 따라서 새로운 Wikidata 기반 데이터 세트인 WD50K를 개발한다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "514-4",
                    "sentence": "Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks.",
                    "sentence_kor": "우리의 실험은 StarE 기반 LP 모델이 여러 벤치마크에서 기존 접근 방식을 능가한다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "514-5",
                    "sentence": "We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.",
                    "sentence_kor": "우리는 또한 트리플 기반 표현에 비해 최대 25 MRR 포인트를 얻은 링크 예측에 적격자를 활용하는 것이 필수적이라는 것을 확인한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "515",
            "abstractID": "EMNLP_abs-515",
            "text": [
                {
                    "index": "515-0",
                    "sentence": "Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news.",
                    "sentence_kor": "대화에서 감정 인식(ERC)에 대한 관심이 다양한 분야에서 높아지고 있는데, 그것은 그것이 사용자 행동을 분석하고 가짜 뉴스를 탐지하는 데 사용될 수 있기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "515-1",
                    "sentence": "Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account.",
                    "sentence_kor": "최근의 많은 ERC 방법은 화자의 발화 사이의 관계를 고려하기 위해 그래프 기반 신경망을 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "515-2",
                    "sentence": "In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT).",
                    "sentence_kor": "특히, 최첨단 방법은 관계형 그래프 주의 네트워크(RGAT)를 사용하여 대화에서 자기 및 화자간 의존성을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "515-3",
                    "sentence": "However, graph-based neural networks do not take sequential information into account.",
                    "sentence_kor": "그러나 그래프 기반 신경망은 순차적 정보를 고려하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "515-4",
                    "sentence": "In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure.",
                    "sentence_kor": "본 논문에서, 우리는 RGAT에 관계형 그래프 구조를 반영하는 순차적 정보를 제공하는 관계형 위치 인코딩을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "515-5",
                    "sentence": "Accordingly, our RGAT model can capture both the speaker dependency and the sequential information.",
                    "sentence_kor": "따라서, 우리의 RGAT 모델은 화자 의존성과 순차적 정보 모두를 포착할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "515-6",
                    "sentence": "Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations.",
                    "sentence_kor": "4개의 ERC 데이터 세트에 대한 실험은 우리의 모델이 대화에서 표현된 감정을 인식하는 데 도움이 된다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "515-7",
                    "sentence": "In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.",
                    "sentence_kor": "또한 우리의 접근 방식은 모든 벤치마크 데이터 세트에서 최첨단 기술을 경험적으로 능가한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "516",
            "abstractID": "EMNLP_abs-516",
            "text": [
                {
                    "index": "516-0",
                    "sentence": "Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity.",
                    "sentence_kor": "예쁘고, 아름답고, 화려한 형용사들은 그들이 수정하지만 다른 강도로 명사의 긍정적인 특성을 묘사한다.",
                    "tag": "1"
                },
                {
                    "index": "516-1",
                    "sentence": "These differences are important for natural language understanding and reasoning.",
                    "sentence_kor": "이러한 차이점들은 자연어의 이해와 추론을 위해 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "516-2",
                    "sentence": "We propose a novel BERT-based approach to intensity detection for scalar adjectives.",
                    "sentence_kor": "우리는 스칼라 형용사의 강도 탐지에 대한 새로운 BERT 기반 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "516-3",
                    "sentence": "We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives.",
                    "sentence_kor": "상황별 표현에서 직접 파생된 벡터로 강도를 모델링하고 스칼라 형용사의 순위를 성공적으로 매길 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "516-4",
                    "sentence": "We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task.",
                    "sentence_kor": "우리는 금본위 데이터 세트와 간접 질문 답변 작업에서 모델을 본질적으로 평가한다.",
                    "tag": "4"
                },
                {
                    "index": "516-5",
                    "sentence": "Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.",
                    "sentence_kor": "우리의 결과는 BERT가 스칼라 형용사의 의미론에 대한 풍부한 지식을 인코딩하고 전용 리소스에 액세스할 수 있는 정적 임베딩 및 이전 모델보다 더 나은 품질 강도 순위를 제공할 수 있음을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "517",
            "abstractID": "EMNLP_abs-517",
            "text": [
                {
                    "index": "517-0",
                    "sentence": "Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently.",
                    "sentence_kor": "사전 훈련된 언어 모델(PrLM)(예: BERT)을 새로운 도메인에 적용하는 것이 최근 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "517-1",
                    "sentence": "Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning.",
                    "sentence_kor": "대부분의 이전 작업에서와 같이 PrLM을 미세 조정하는 대신 미세 조정 없이 PrLM의 기능을 새 도메인에 적용하는 방법을 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "517-2",
                    "sentence": "We explore unsupervised domain adaptation (UDA) in this paper.",
                    "sentence_kor": "본 논문에서 우리는 비지도 도메인 적응(UDA)을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "517-3",
                    "sentence": "With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain.",
                    "sentence_kor": "PrLM의 기능을 사용하여 소스 도메인에서 레이블링된 데이터로 훈련된 모델을 레이블이 없는 대상 도메인으로 조정한다.",
                    "tag": "3"
                },
                {
                    "index": "517-4",
                    "sentence": "Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training.",
                    "sentence_kor": "자가 훈련은 UDA에 널리 사용되며 훈련을 위해 대상 도메인 데이터의 유사 레이블을 예측한다.",
                    "tag": "1"
                },
                {
                    "index": "517-5",
                    "sentence": "However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model.",
                    "sentence_kor": "그러나 예측 의사 라벨에는 불가피하게 노이즈가 포함되어 있어 강력한 모델 교육에 부정적인 영향을 미칠 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "517-6",
                    "sentence": "To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered.",
                    "sentence_kor": "자체 훈련의 견고성을 개선하기 위해 본 논문에서 우리는 PrLM으로부터 차별적 특징을 학습하기 위한 클래스 인식 기능 자기 분산(CFD)을 제시한다. PrLM 기능은 기능 적응 모듈로 자체 분산되고 동일한 클래스의 기능은 더욱 긴밀하게 클러스터된다.",
                    "tag": "2+3"
                },
                {
                    "index": "517-7",
                    "sentence": "We further extend CFd to a cross-language setting, in which language discrepancy is studied.",
                    "sentence_kor": "우리는 또한 CFD를 언어 불일치가 연구되는 교차 언어 환경으로 확장한다.",
                    "tag": "2"
                },
                {
                    "index": "517-8",
                    "sentence": "Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.",
                    "sentence_kor": "두 개의 단일 언어 및 다국어 아마존 리뷰 데이터 세트에 대한 실험은 CFD가 교차 도메인 및 교차 언어 설정에서 자체 훈련 성능을 일관되게 개선할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "518",
            "abstractID": "EMNLP_abs-518",
            "text": [
                {
                    "index": "518-0",
                    "sentence": "In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process.",
                    "sentence_kor": "본 논문에서 우리는 라벨이 부착된 GPT-2와 같은 언어 생성 모델의 안내 출력이 능동적 학습 프로세스를 통해 텍스트 분류기의 성능을 향상시킬 수 있는 새로운 데이터 확대 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "518-1",
                    "sentence": "We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria.",
                    "sentence_kor": "MCTS(Monte Carlo Tree Search)를 최적화 전략으로 사용하고 엔트로피를 최적화 기준 중 하나로 통합하여 데이터 생성 작업을 생성된 출력의 유용성을 극대화하는 최적화 문제로 변환한다.",
                    "tag": "3"
                },
                {
                    "index": "518-2",
                    "sentence": "We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function.",
                    "sentence_kor": "보상 기능을 최적화하지 않는 NGDG(Non-Guided Data Generation) 프로세스에 대해 접근 방식을 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "518-3",
                    "sentence": "Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset.",
                    "sentence_kor": "작은 데이터 집합부터 시작하여, 우리의 결과는 TRC-6 질문 데이터 집합에서 26%, 스탠포드 감성 트리뱅크 SST-2 데이터 집합에서 10%의 MCTS로 향상된 성능을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "518-4",
                    "sentence": "Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.",
                    "sentence_kor": "NGDG와 비교하여 TRC-6 및 SST-2에서 각각 3%와 5%의 증가를 달성할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "519",
            "abstractID": "EMNLP_abs-519",
            "text": [
                {
                    "index": "519-0",
                    "sentence": "Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction.",
                    "sentence_kor": "무의식적 편견은 현대 텍스트와 미디어에서 계속 만연하고 있으며, 편향 보정을 위해 작성자를 도울 수 있는 알고리즘을 요구한다.",
                    "tag": "1"
                },
                {
                    "index": "519-1",
                    "sentence": "For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more proactive and powerful (“_He pursues his dream of being a doctor_”).",
                    "sentence_kor": "예를 들어, 이야기 속의 여성 캐릭터는 종종 수동적이고 힘없는 사람으로 묘사되는 반면, 남성은 보다 능동적이고 강력한 사람으로 묘사된다.",
                    "tag": "1"
                },
                {
                    "index": "519-2",
                    "sentence": "We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals.",
                    "sentence_kor": "우리는 캐릭터 묘사에서 암시적이고 잠재적으로 바람직하지 않은 편견을 시정하기 위해 주어진 텍스트를 다시 쓰는 것을 목표로 하는 새로운 개정 과제인 **Controllable Debiasing***을 공식화한다.",
                    "tag": "1+2"
                },
                {
                    "index": "519-3",
                    "sentence": "We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates.",
                    "sentence_kor": "그런 다음 PowerTransformer를 동사 술어에 관한 묵시적 전력 역학에 대한 실용적인 지식을 인코딩하는 함축 프레임 렌즈(Sap et al., 2017)를 통해 텍스트를 해독하는 접근법으로 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "519-4",
                    "sentence": "One key challenge of our task is the lack of parallel corpora.",
                    "sentence_kor": "우리 과제의 핵심 과제 중 하나는 병렬 말뭉치의 부족이다.",
                    "tag": "1"
                },
                {
                    "index": "519-5",
                    "sentence": "To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models.",
                    "sentence_kor": "이 과제를 해결하기 위해, 우리는 사전 훈련된 언어 모델을 기반으로 재구성 손실에 기초한 패러프레이징 및 자체 감독과 같은 관련 작업과 함께 보조 감독을 사용하는 비지도 접근방식을 채택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "519-6",
                    "sentence": "Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks.",
                    "sentence_kor": "자동 및 인간 평가를 기반으로 한 종합적인 실험을 통해 우리의 접근 방식이 관련 작업의 애블레이션 및 기존 방법을 능가한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "519-7",
                    "sentence": "Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.",
                    "sentence_kor": "게다가, 우리는 영화 대본에서 인물 묘사에서 잘 문서화된 성 편향을 완화하기 위한 단계로서 PowerTransformer의 사용을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "520",
            "abstractID": "EMNLP_abs-520",
            "text": [
                {
                    "index": "520-0",
                    "sentence": "Previous neural coherence models have focused on identifying semantic relations between adjacent sentences.",
                    "sentence_kor": "이전의 신경 일관성 모델은 인접 문장 사이의 의미적 관계를 식별하는 데 초점을 맞추었다.",
                    "tag": "1"
                },
                {
                    "index": "520-1",
                    "sentence": "However, they do not have the means to exploit structural information.",
                    "sentence_kor": "그러나 구조 정보를 활용할 수 있는 수단이 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "520-2",
                    "sentence": "In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations.",
                    "sentence_kor": "본 연구에서는 인간 주석에 의존하지 않고 담화 구조 정보를 고려하는 일관성 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "520-3",
                    "sentence": "We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments.",
                    "sentence_kor": "우리는 언어적 일관성 이론인 중심화 이론에 가깝습니다. 우리는 이것을 담화 부분들 간의 초점 변화를 추적하는데 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "520-4",
                    "sentence": "Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus.",
                    "sentence_kor": "우리의 모델은 먼저 맥락과 관련하여 인식되는 각 문장의 초점을 식별하고 초점의 변화를 추적하여 담화 세그먼트의 구조적 관계를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "520-5",
                    "sentence": "The model then incorporates this structural information into a structure-aware transformer.",
                    "sentence_kor": "그런 다음 모델은 이 구조 정보를 구조 인식 변압기에 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "520-6",
                    "sentence": "We evaluate our model on two tasks, automated essay scoring and assessing writing quality.",
                    "sentence_kor": "우리는 자동 에세이 채점과 글의 질 평가라는 두 가지 과제에 대해 모델을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "520-7",
                    "sentence": "Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks.",
                    "sentence_kor": "우리의 결과는 사전 훈련된 언어 모델 위에 구축된 모델이 두 작업에서 모두 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "520-8",
                    "sentence": "We next statistically examine the identified trees of texts assigned to different quality scores.",
                    "sentence_kor": "다음에는 서로 다른 품질 점수에 할당된 식별된 텍스트 트리를 통계적으로 검사한다.",
                    "tag": "3"
                },
                {
                    "index": "520-9",
                    "sentence": "Finally, we investigate what our model learns in terms of theoretical claims.",
                    "sentence_kor": "마지막으로 이론적 주장 측면에서 모델이 학습하는 내용을 조사한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "521",
            "abstractID": "EMNLP_abs-521",
            "text": [
                {
                    "index": "521-0",
                    "sentence": "The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others.",
                    "sentence_kor": "얼굴의 개념은 개인의 행동뿐만 아니라 다른 사람들과의 상호작용에서 나타나는 개인의 공공적 자아상을 말한다.",
                    "tag": "1"
                },
                {
                    "index": "521-1",
                    "sentence": "Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction.",
                    "sentence_kor": "대화를 통해 얼굴을 모델링하고 얼굴의 상태 변화를 이해하는 것은 상호작용을 통한 인간의 기본적인 욕구 유지 연구에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "521-2",
                    "sentence": "Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models.",
                    "sentence_kor": "브라운과 레빈슨의 공손성 이론(1978년)에 근거해, 설득 대화에서 얼굴 행동을 모델링하기 위한 일반화된 프레임워크를 제안하여 신뢰할 수 있는 코딩 매뉴얼, 주석이 달린 말뭉치 및 계산 모델을 도출한다.",
                    "tag": "2+3"
                },
                {
                    "index": "521-3",
                    "sentence": "The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations.",
                    "sentence_kor": "이 프레임워크는 설득 대화에서 비대칭적인 역할들 사이의 얼굴 행동 활용의 차이에 대한 통찰력을 드러낸다.",
                    "tag": "1"
                },
                {
                    "index": "521-4",
                    "sentence": "Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success).",
                    "sentence_kor": "컴퓨터 모델을 사용하여 얼굴 행동을 성공적으로 식별하고 주요 대화 결과(예: 기부 성공)를 예측할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "521-5",
                    "sentence": "Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.",
                    "sentence_kor": "마지막으로, 우리는 긍정적인 대화 결과의 확률에 대한 예측된 얼굴 행동의 영향을 분석하고 이전 결과를 입증하는 몇 가지 상관 관계를 관찰하기 위해 대화 상태의 잠재적 표현을 모델링한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "522",
            "abstractID": "EMNLP_abs-522",
            "text": [
                {
                    "index": "522-0",
                    "sentence": "We present our HABERTOR model for detecting hatespeech in large scale user-generated content.",
                    "sentence_kor": "대규모 사용자 생성 콘텐츠에서 헤츠피크를 감지하기 위한 HABERTOR 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "522-1",
                    "sentence": "Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task.",
                    "sentence_kor": "BERT 모델의 최근 성공에 영감을 받아 다운스트림 헤츠피크 분류 작업에서 성능을 향상시키기 위해 BERT에 대한 몇 가지 수정을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "522-2",
                    "sentence": "HABERTOR inherits BERT’s architecture, but is different in four aspects: ",
                    "sentence_kor": "HABERTOR는 BERT의 아키텍처를 상속하지만 다음 네 가지 측면에서 다릅니다.",
                    "tag": "1"
                },
                {
                    "index": "522-3",
                    "sentence": "(i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; ",
                    "sentence_kor": "(i) 자체 어휘를 생성하고 가장 큰 규모의 hatspeech 데이터 세트를 사용하여 처음부터 사전 교육을 받는다.",
                    "tag": "3"
                },
                {
                    "index": "522-4",
                    "sentence": "(ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; ",
                    "sentence_kor": "(ii) Quaternion 기반 인수분해 구성요소로 구성되므로 매개변수 수가 훨씬 적고, 훈련 및 회의 속도가 빠르며, 메모리 사용량이 적다.",
                    "tag": "3"
                },
                {
                    "index": "522-5",
                    "sentence": "(iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness;",
                    "sentence_kor": "(iii) 별도의 입력 소스를 위한 풀링 레이어와 함께 제안된 다중 소스 앙상블 헤드를 사용하여 효율성을 더욱 강화한다.",
                    "tag": "3"
                },
                {
                    "index": "522-6",
                    "sentence": "and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness.",
                    "sentence_kor": "그리고 (iv) 견고성을 강화하기 위해 제안된 세분화되고 적응적인 노이즈 크기와 함께 정규화된 적대적 훈련을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "522-7",
                    "sentence": "Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models.",
                    "sentence_kor": "주석이 달린 1.4M개의 대규모 실제 해트스피크 데이터 세트에 대한 실험을 통해, 우리는 HABERTOR가 미세 조정 언어 모델을 포함한 15개의 최첨단 해트스피크 감지 방법보다 더 잘 작동한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "522-8",
                    "sentence": "In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words.",
                    "sentence_kor": "특히, BERT와 비교하여, 우리의 HABERTOR는 훈련/회의 단계에서 4 5배 더 빠르고, 메모리의 1/3 미만을 사용하며, 비록 단어 수의 1% 미만을 사용하여 사전 훈련을 했음에도 불구하고 더 나은 성능을 가지고 있다.",
                    "tag": "4"
                },
                {
                    "index": "522-9",
                    "sentence": "Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.",
                    "sentence_kor": "우리의 일반화 분석에 따르면 HABERTOR는 보이지 않는 다른 해트스피크 데이터 세트로 잘 전송되며, 해트스피크 분류에 대한 BERT의 보다 효율적이고 효과적인 대안이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "523",
            "abstractID": "EMNLP_abs-523",
            "text": [
                {
                    "index": "523-0",
                    "sentence": "Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges.",
                    "sentence_kor": "대규모 다중 레이블 텍스트 분류(LMTC)는 광범위한 자연어 처리(NLP) 애플리케이션을 가지고 있으며 흥미로운 과제를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "523-1",
                    "sentence": "First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets.",
                    "sentence_kor": "첫째, 매우 큰 레이블 세트와 데이터 세트의 왜곡된 레이블 분포로 인해 모든 레이블이 교육 세트에 잘 표현되지는 않는다.",
                    "tag": "1"
                },
                {
                    "index": "523-2",
                    "sentence": "Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity.",
                    "sentence_kor": "또한 라벨 계층 및 인간 라벨링 지침의 차이는 그래프 인식 주석 근접성에 영향을 미칠 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "523-3",
                    "sentence": "Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization.",
                    "sentence_kor": "마지막으로 라벨 계층은 주기적으로 업데이트되므로 제로샷 일반화가 가능한 LMTC 모델이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "523-4",
                    "sentence": "Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks.",
                    "sentence_kor": "현재의 최첨단 LMTC 모델은 (1) LMTC를 플랫 다중 라벨 분류로 취급하며, (2) 라벨 계층을 사용하여 제로샷 학습을 개선할 수 있지만, 이 관행이 크게 연구되지 않고 있으며, (3) 사전 훈련 트랜스포머(EG)와 결합되지 않았다.o 여러 NLP 벤치마크에서 최첨단 결과를 얻을 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "523-5",
                    "sentence": "Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains.",
                    "sentence_kor": "여기서 처음으로 서로 다른 도메인의 세 데이터 세트에 대한 빈도, 소수 및 제로샷 학습에 대해 바닐라 LWAN에서 계층 분류 접근법 및 전송 학습에 이르는 LMTC 방법의 배터리를 경험적으로 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "523-6",
                    "sentence": "We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs.",
                    "sentence_kor": "확률적 라벨 트리(PLT)에 기초한 계층적 방법이 LWAN을 능가한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "523-7",
                    "sentence": "Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN.",
                    "sentence_kor": "또한 트랜스포머 기반 접근 방식이 두 데이터 세트에서 최첨단 방식을 능가한다는 것을 보여주고 BERT와 LWAN을 결합한 새로운 최첨단 방법을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "523-8",
                    "sentence": "Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.",
                    "sentence_kor": "마지막으로, 각 데이터 세트에 대해 소개하는 그래프 인식 주석 근접성 측정값을 고려하여 레이블 계층을 활용하여 거의 및 제로샷 학습을 개선하는 새로운 모델을 제안한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "524",
            "abstractID": "EMNLP_abs-524",
            "text": [
                {
                    "index": "524-0",
                    "sentence": "We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision.",
                    "sentence_kor": "우리는 주어진 과학적 주장을 지지하거나 반박하는 증거를 포함하는 연구 문헌에서 추상화를 선택하고 각 결정을 정당화하는 합리성을 식별하는 새로운 과제인 과학적 주장 검증을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "524-1",
                    "sentence": "To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales.",
                    "sentence_kor": "이 작업을 연구하기 위해 레이블과 합리성이 주석이 달린 증거 포함 추상물과 짝을 이룬 1.4K 전문가 작성 과학 주장 데이터 세트인 SciFact를 구성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "524-2",
                    "sentence": "We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news.",
                    "sentence_kor": "우리는 SciFact의 기본 모델을 개발하고, 간단한 도메인 적응 기법이 위키피디아나 정치 뉴스에서 훈련된 모델에 비해 성능이 상당히 향상된다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "524-3",
                    "sentence": "We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus.",
                    "sentence_kor": "우리는 CORD-19 말뭉치의 증거를 식별하여 시스템이 COVID-19와 관련된 주장을 검증할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "524-4",
                    "sentence": "Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge.",
                    "sentence_kor": "우리의 실험은 SciFact가 전문 도메인 지식을 포함하는 말뭉치를 검색하고 추론하도록 설계된 새로운 시스템 개발을 위한 도전적인 테스트 베드를 제공할 것임을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "524-5",
                    "sentence": "Data and code for this new task are publicly available at https://github.com/allenai/scifact.",
                    "sentence_kor": "이 새로운 작업에 대한 데이터와 코드는 https://github.com/allenai/scifact에서 공개적으로 이용할 수 있다.",
                    "tag": "6"
                },
                {
                    "index": "524-6",
                    "sentence": "A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.",
                    "sentence_kor": "리더보드와 COVID-19 사실 확인 데모는 https://scifact.apps.allenai.org에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "525",
            "abstractID": "EMNLP_abs-525",
            "text": [
                {
                    "index": "525-0",
                    "sentence": "We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing.",
                    "sentence_kor": "(스팬 기반) PropBank 스타일의 의미 역할 레이블링(SRL) 작업을 구문 종속성 구문 분석으로 줄인다.",
                    "tag": "2"
                },
                {
                    "index": "525-1",
                    "sentence": "Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data.",
                    "sentence_kor": "우리의 접근 방식은 영어와 중국어 데이터에 대한 SRL 주석의 98% 이상을 차지하는 세 가지 공통 구문 패턴을 보여주는 경험적 분석에 의해 동기가 부여된다.",
                    "tag": "1"
                },
                {
                    "index": "525-2",
                    "sentence": "Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format.",
                    "sentence_kor": "이러한 관찰을 바탕으로 SRL 주석을 원래 형식으로 매우 정확하게 복구할 수 있는 공동 레이블을 통해 종속성 트리 표현으로 포장하는 변환 체계를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "525-3",
                    "sentence": "This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art.",
                    "sentence_kor": "이 표현을 통해 SRL을 다루기 위해 통계적 의존성 파서를 교육하고 최신 기술로 경쟁력 있는 성능을 달성할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "525-4",
                    "sentence": "Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.",
                    "sentence_kor": "우리의 연구 결과는 지역성의 구문 영역 내에서 의미적 역할 관계를 인코딩하는 구문 의존성 트리의 가능성을 보여주며, 향후 의미적 역할 라벨링에 구문적 방법의 추가 통합 가능성을 지적한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "526",
            "abstractID": "EMNLP_abs-526",
            "text": [
                {
                    "index": "526-0",
                    "sentence": "When does a sequence of events define an everyday scenario and how can this knowledge be induced from text?",
                    "sentence_kor": "일련의 사건들은 언제 매일의 시나리오를 정의하는가? 그리고 이 지식은 어떻게 텍스트로부터 유도될 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "526-1",
                    "sentence": "Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus.",
                    "sentence_kor": "그러한 스크립트를 유도하기 위한 선행 연구는 말뭉치의 사건 인스턴스 간의 상관관계 측정에 의존해왔다.",
                    "tag": "1"
                },
                {
                    "index": "526-2",
                    "sentence": "We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions.",
                    "sentence_kor": "우리는 개념적 및 실제적 의미에서 순전히 상관 기반 접근법이 불충분하다고 주장하고 대신 개입을 통해 공식적으로 정의된 사건 사이의 인과적 영향에 기초한 스크립트 유도 방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "526-3",
                    "sentence": "Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.",
                    "sentence_kor": "인간 및 자동 평가를 통해 인과적 영향에 기초한 방법의 출력이 스크립트가 나타내는 것의 직관과 더 잘 일치한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "527",
            "abstractID": "EMNLP_abs-527",
            "text": [
                {
                    "index": "527-0",
                    "sentence": "NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task.",
                    "sentence_kor": "NLU 모델은 의도한 작업을 제대로 학습하지 않고 데이터 세트별 높은 성능을 달성하기 위해 편견을 이용하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "527-1",
                    "sentence": "Recently proposed debiasing methods are shown to be effective in mitigating this tendency.",
                    "sentence_kor": "최근에 제안된 디비어싱 방법은 이러한 경향을 완화하는 데 효과적인 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "527-2",
                    "sentence": "However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.",
                    "sentence_kor": "그러나 이러한 방법은 편향의 유형을 사전적으로 알아야 한다는 주요 가정에 의존하며, 이로 인해 많은 NLU 작업 및 데이터 세트에 적용이 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "527-3",
                    "sentence": "In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance.",
                    "sentence_kor": "본 연구에서, 우리는 모델이 사전에 알지 못한 채 편견을 주로 활용하는 것을 방지하는 자체 디바이징 프레임워크를 도입하여 이러한 격차를 해소하기 위한 첫 번째 단계를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "527-4",
                    "sentence": "The proposed framework is general and complementary to the existing debiasing methods.",
                    "sentence_kor": "제안된 프레임워크는 일반적이며 기존 디바이징 방법에 보완적이다.",
                    "tag": "3"
                },
                {
                    "index": "527-5",
                    "sentence": "We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases.",
                    "sentence_kor": "우리는 이러한 기존 방법이 특정 편견을 특별히 겨냥하지 않고 도전 데이터 세트(즉, 모델의 편견 의존도를 노출하도록 설계된 예제 세트)의 개선을 유지할 수 있음을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "527-6",
                    "sentence": "Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.",
                    "sentence_kor": "또한, 평가는 프레임워크를 적용하면 전반적인 견고성이 향상된다는 것을 시사한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "528",
            "abstractID": "EMNLP_abs-528",
            "text": [
                {
                    "index": "528-0",
                    "sentence": "The scarcity of large parallel corpora is an important obstacle for neural machine translation.",
                    "sentence_kor": "대형 병렬 말뭉치의 부족은 신경 기계 번역에 중요한 장애물이다.",
                    "tag": "1"
                },
                {
                    "index": "528-1",
                    "sentence": "A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data.",
                    "sentence_kor": "일반적인 해결책은 풍부한 단일 언어 데이터에 대해 훈련된 언어 모델(LM)의 지식을 활용하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "528-2",
                    "sentence": "In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM).",
                    "sentence_kor": "본 연구에서는 신경 번역 모델(TM)에서 이전과 같이 LM을 통합하는 새로운 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "528-3",
                    "sentence": "Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM.",
                    "sentence_kor": "특히, 우리는 정규화 항을 추가하여 TM이 LM과 \"불합치\"할 때 잘못된 예측을 피하면서 TM의 출력 분포를 LM에 따라 개연성이 있도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "528-4",
                    "sentence": "This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language.",
                    "sentence_kor": "이 목표는 지식 증류와 관련이 있으며, 여기서 LM은 TM에게 대상 언어에 대해 가르치는 것으로 볼 수 있습니다.",
                    "tag": "2"
                },
                {
                    "index": "528-5",
                    "sentence": "The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference.",
                    "sentence_kor": "LM은 추론 중에 필요한 이전 작업과 달리 훈련 시간에만 사용되기 때문에 제안된 접근 방식은 디코딩 속도를 손상시키지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "528-6",
                    "sentence": "We present an analysis of the effects that different methods have on the distributions of the TM.",
                    "sentence_kor": "다양한 방법이 TM 분포에 미치는 영향에 대한 분석을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "528-7",
                    "sentence": "Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.",
                    "sentence_kor": "두 개의 저자원 기계 번역 데이터 세트에 대한 결과는 제한된 단일 언어 데이터에도 분명한 개선을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "529",
            "abstractID": "EMNLP_abs-529",
            "text": [
                {
                    "index": "529-0",
                    "sentence": "Word sense disambiguation is a well-known source of translation errors in NMT.",
                    "sentence_kor": "단어 의미 모호화는 NMT에서 번역 오류의 잘 알려진 원인이다.",
                    "tag": "1"
                },
                {
                    "index": "529-1",
                    "sentence": "We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text.",
                    "sentence_kor": "일부 부정확한 모호성 선택은 소스 텍스트를 더 깊이 이해하기 보다는 교육 데이터에서 발견된 데이터 세트 아티팩트, 특히 피상적인 단어 동시 발생에 대한 모델의 지나친 의존 때문이라고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "529-2",
                    "sentence": "We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types.",
                    "sentence_kor": "통계 데이터 속성을 기반으로 모호성 오류를 예측하는 방법을 도입하여 여러 도메인과 모델 유형에 걸쳐 그 효과를 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "529-3",
                    "sentence": "Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models.",
                    "sentence_kor": "또한, 우리는 번역 모델의 견고성을 추가로 조사하기 위해 명확화 오류를 유도하기 위해 문장을 교란시키는 간단한 적대적 공격 전략을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "529-4",
                    "sentence": "Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",
                    "sentence_kor": "우리의 연구 결과는 모호화 견고성이 도메인마다 상당히 다르고 동일한 데이터에 대해 훈련된 다른 모델이 다른 공격에 취약하다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "530",
            "abstractID": "EMNLP_abs-530",
            "text": [
                {
                    "index": "530-0",
                    "sentence": "The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer.",
                    "sentence_kor": "다국어 BERT 및 XLM-R과 같은 최첨단 사전 훈련된 다국어 모델의 주요 목표는 제로샷 또는 퓨샷 교차 언어 전송을 통해 저자원 언어로 NLP 애플리케이션을 활성화하고 부팅하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "530-1",
                    "sentence": "However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training.",
                    "sentence_kor": "그러나 제한된 모델 용량으로 인해 사전 교육 중에 보이지 않는 저자원 언어와 언어에서 전송 성능이 정확히 가장 약하다.",
                    "tag": "1"
                },
                {
                    "index": "530-2",
                    "sentence": "We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations.",
                    "sentence_kor": "모듈 언어와 작업 표현을 학습하여 임의 작업과 언어에 높은 이식성과 매개 변수 효율적인 전송을 가능하게 하는 어댑터 기반 프레임워크인 MAD-X를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "530-3",
                    "sentence": "In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language.",
                    "sentence_kor": "또한 사전 훈련된 다국어 모델을 새로운 언어에 적용하기 위한 새로운 역방향 어댑터 아키텍처와 강력한 기본 방법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "530-4",
                    "sentence": "MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering.",
                    "sentence_kor": "MAD-X는 명명된 개체 인식 및 인과 상식 추론에 대한 유형학적으로 다양한 대표적인 언어 집합에 걸친 교차 언어 전송에서 최첨단 기술을 능가하고 질문 답변에 대한 경쟁 결과를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "530-5",
                    "sentence": "Our code and adapters are available at AdapterHub.ml.",
                    "sentence_kor": "코드 및 어댑터는 AdapterHub.ml에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "531",
            "abstractID": "EMNLP_abs-531",
            "text": [
                {
                    "index": "531-0",
                    "sentence": "Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique.",
                    "sentence_kor": "인간과 기계 번역 모두 언어 간 전송 학습에서 중심적인 역할을 한다. 전문 번역 서비스를 통해 많은 다국어 데이터 세트가 생성되었으며, 기계 번역을 사용하여 테스트 세트 또는 훈련 세트를 번역하는 것이 널리 사용되는 전송 기법이다.",
                    "tag": "1"
                },
                {
                    "index": "531-1",
                    "sentence": "In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models.",
                    "sentence_kor": "본 논문에서, 우리는 그러한 번역 과정이 기존 언어 간 모델에서 눈에 띄는 영향을 미치는 미묘한 아티팩트를 도입할 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "531-2",
                    "sentence": "For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to.",
                    "sentence_kor": "예를 들어, 자연어 추론에서, 전제와 가설을 독립적으로 번역하는 것은 현재 모델이 매우 민감한 그들 사이의 어휘적 중복을 줄일 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "531-3",
                    "sentence": "We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon.",
                    "sentence_kor": "우리는 언어 간 전이 학습의 일부 이전 연구 결과를 이 현상에 비추어 재고할 필요가 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "531-4",
                    "sentence": "Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.",
                    "sentence_kor": "또한 얻은 통찰력을 바탕으로 변환 테스트와 제로샷 접근법에 대한 XNLI의 최첨단 기술을 각각 4.3점과 2.8점 향상시켰다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "532",
            "abstractID": "EMNLP_abs-532",
            "text": [
                {
                    "index": "532-0",
                    "sentence": "Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings.",
                    "sentence_kor": "소셜 미디어의 편재성은 사용자가 전통적인 임상 환경에서 벗어나 자살 생각을 나타낼 수 있는 공간을 조성한다.",
                    "tag": "1"
                },
                {
                    "index": "532-1",
                    "sentence": "Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention.",
                    "sentence_kor": "그러한 관념의 형성을 이해하는 것은 위험에 처한 사용자의 식별과 자살 예방에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "532-2",
                    "sentence": "Suicide ideation is often linked to a history of mental depression.",
                    "sentence_kor": "자살 이데올로기는 종종 정신 우울증과 관련이 있다.",
                    "tag": "1"
                },
                {
                    "index": "532-3",
                    "sentence": "The emotional spectrum of a user’s historical activity on social media can be indicative of their mental state over time.",
                    "sentence_kor": "소셜 미디어에서 사용자의 과거 활동의 감정적 스펙트럼은 시간이 지남에 따라 사용자의 정신 상태를 나타낼 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "532-4",
                    "sentence": "In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context.",
                    "sentence_kor": "본 연구에서는 역사적 맥락으로 언어 모델을 보강하여 영어 트윗에서 자살 의도를 식별하는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "532-5",
                    "sentence": "We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media.",
                    "sentence_kor": "소셜 미디어에서 자살 위험을 사전 선별하기 위한 시간 인식 변압기 기반 모델인 STATNet을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "532-6",
                    "sentence": "STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment.",
                    "sentence_kor": "STATNet은 자살 위험 평가에 대한 감정적 및 일시적 상황적 단서의 효용성을 입증하는 경쟁적 방법을 능가한다.",
                    "tag": "2"
                },
                {
                    "index": "532-7",
                    "sentence": "We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.",
                    "sentence_kor": "우리는 자살 이데올로기 탐지를 위한 STATNet의 경험적, 질적, 실용적, 윤리적 측면을 논의한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "533",
            "abstractID": "EMNLP_abs-533",
            "text": [
                {
                    "index": "533-0",
                    "sentence": "In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics.",
                    "sentence_kor": "본 논문에서, 우리는 정치적으로 분열을 일으키는 주제에 대한 뉴스 기사 보도에서 미묘한 프레임을 식별하기 위한 최소 감독 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "533-1",
                    "sentence": "We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way.",
                    "sentence_kor": "우리는 2014년 Boydstun 등이 제안한 광범위한 정책 프레임을 보다 나은 방식으로 정치 이념의 차이를 포착할 수 있는 세분화된 서브프레임으로 나눌 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "533-2",
                    "sentence": "We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion.",
                    "sentence_kor": "우리는 이민, 총기 규제 및 낙태라는 세 가지 주제에 걸쳐 최소한의 감독을 사용하여 학습한 제안된 서브프레임과 그 임베딩을 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "533-3",
                    "sentence": "We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.",
                    "sentence_kor": "우리는 뉴스 미디어의 이념적 차이를 포착하고 정치적 담론을 분석하는 서브프레임의 능력을 입증한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "534",
            "abstractID": "EMNLP_abs-534",
            "text": [
                {
                    "index": "534-0",
                    "sentence": "Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media.",
                    "sentence_kor": "학계와 산업계에서 많은 사실 확인 시스템이 개발되었지만, 소셜 미디어에서는 여전히 가짜 뉴스가 급증하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "534-1",
                    "sentence": "These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation.",
                    "sentence_kor": "이러한 시스템은 대부분 사실 확인에 초점을 맞추지만 일반적으로 잘못된 정보의 확산의 주요 동인인 온라인 사용자를 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "534-2",
                    "sentence": "How can we use fact-checked information to improve users’ consciousness of fake news to which they are exposed?",
                    "sentence_kor": "팩트 체크 정보를 사용하여 사용자가 노출되는 가짜 뉴스에 대한 의식을 향상시킬 수 있는 방법은 무엇인가?",
                    "tag": "1"
                },
                {
                    "index": "534-3",
                    "sentence": "How can we stop users from spreading fake news?",
                    "sentence_kor": "어떻게 하면 사용자들이 가짜 뉴스를 퍼뜨리는 것을 막을 수 있을까?",
                    "tag": "1"
                },
                {
                    "index": "534-4",
                    "sentence": "To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users.",
                    "sentence_kor": "이러한 질문을 해결하기 위해 온라인 사용자가 게시한 원본 트윗(오보가 포함될 수 있는)의 내용을 다루는 사실 확인 기사를 검색하는 새로운 프레임워크를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "534-5",
                    "sentence": "The search can directly warn fake news posters and online users (e.g. the posters’ followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media.",
                    "sentence_kor": "이 검색은 가짜 뉴스 포스터와 온라인 사용자(예: 포스터 팔로워)에게 잘못된 정보에 대해 직접 경고하고, 가짜 뉴스를 퍼뜨리는 것을 막고, 소셜 미디어에서 검증된 콘텐츠를 확장할 수 있다.",
                    "tag": "5"
                },
                {
                    "index": "534-6",
                    "sentence": "Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets.",
                    "sentence_kor": "우리의 프레임워크는 텍스트와 이미지를 모두 사용하여 사실 확인 기사를 검색하고 실제 데이터 세트에서 유망한 결과를 얻는다.",
                    "tag": "2"
                },
                {
                    "index": "534-7",
                    "sentence": "Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.",
                    "sentence_kor": "코드 및 데이터 세트는 https://github.com/nguyenvo09/EMNLP2020에서 공개됩니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "535",
            "abstractID": "EMNLP_abs-535",
            "text": [
                {
                    "index": "535-0",
                    "sentence": "Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.",
                    "sentence_kor": "현대의 독성 언어 탐지기는 알려진 독성 어휘를 의도적으로 피하는 적대적 공격이나 암묵적 편견의 징후와 같은 위장된 모욕적인 언어를 인식하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "535-1",
                    "sentence": "Building a large annotated dataset for such veiled toxicity can be very expensive.",
                    "sentence_kor": "이러한 베일에 가려진 독성을 위해 주석이 달린 대규모 데이터 세트를 구축하는 데는 비용이 많이 들 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "535-2",
                    "sentence": "In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.",
                    "sentence_kor": "본 연구에서, 우리는 베일에 가려진 독성의 대규모 라벨 말뭉치 없이 기존의 독성 음성 탐지기를 강화하기 위한 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "535-3",
                    "sentence": "Just a handful of probing examples are used to surface orders of magnitude more disguised offenses.",
                    "sentence_kor": "단지 몇 가지 조사 예만이 더 많은 위장한 범죄들을 표면화하기 위해 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "535-4",
                    "sentence": "We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.",
                    "sentence_kor": "우리는 이러한 발견된 불쾌한 예들로 독성 음성 감지기의 훈련 데이터를 증가시켜, 명백한 독성 검출의 효용성은 유지하면서 베일에 가려진 독성에 더 견고하게 만든다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "536",
            "abstractID": "EMNLP_abs-536",
            "text": [
                {
                    "index": "536-0",
                    "sentence": "Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks.",
                    "sentence_kor": "정책 그레이디언트 기반 강화 학습은 언어 생성 작업에 대해 차별화할 수 없는 평가 지표를 직접 최적화하기 위한 유망한 접근 방식임이 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "536-1",
                    "sentence": "However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements.",
                    "sentence_kor": "그러나 특정 메트릭 보상에 대해 최적화하면 대부분 해당 메트릭만 개선되며, 이는 모델이 종종 실제 정성적 개선을 달성하지 못한 채 특정 방식으로 해당 메트릭의 공식화를 게임하고 있음을 시사한다.",
                    "tag": "1"
                },
                {
                    "index": "536-2",
                    "sentence": "Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly.",
                    "sentence_kor": "따라서 모델이 여러 가지 다양한 메트릭 보상을 공동으로 최적화하도록 하는 것이 더 유익하다.",
                    "tag": "1"
                },
                {
                    "index": "536-3",
                    "sentence": "While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards.",
                    "sentence_kor": "이러한 메트릭 보상의 중요성과 스케일링 가중치를 수동으로 결정해야 하기 때문에 이는 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "536-4",
                    "sentence": "Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time.",
                    "sentence_kor": "또한, 시간에 따라 유연하게 변화하는 메트릭 보상의 동적 조합과 커리큘럼을 사용하는 것을 고려하는 것이 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "536-5",
                    "sentence": "Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains.",
                    "sentence_kor": "위의 측면을 고려하여, 본 연구에서는 다중 암 밴디트 접근법(DIRB)을 통해 여러 메트릭 보상의 최적화를 동시에 자동화한다. 각 라운드에서 밴디트는 예상 암 이득에 기초하여 다음에 최적화할 메트릭 보상을 선택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "536-6",
                    "sentence": "We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit).",
                    "sentence_kor": "밴디트에 대해 Exp3 알고리즘을 사용하고 밴디트 보상에 대해 (1) 단일 다중 보상 밴디트(SM-Bandit)와 (2) 계층적 다중 보상 밴디트(HM-Bandit)의 두 가지 접근 방식을 공식화한다.",
                    "tag": "3"
                },
                {
                    "index": "536-7",
                    "sentence": "We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation.",
                    "sentence_kor": "우리는 질문 생성과 데이터-텍스트 생성이라는 두 가지 중요한 NLG 작업에 대한 다양한 자동 메트릭과 인간 평가를 통해 접근 방식의 효과를 경험적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "536-8",
                    "sentence": "Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.",
                    "sentence_kor": "마지막으로 최적화된 보상에 대해 학습된 밴디트 커리큘럼에 대한 해석 가능한 분석을 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "537",
            "abstractID": "EMNLP_abs-537",
            "text": [
                {
                    "index": "537-0",
                    "sentence": "Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines.",
                    "sentence_kor": "대화 데이터에서 정보를 추출하는 것은 특히 어려운 일이다. 왜냐하면 대화의 작업 중심적 특성이 인간에 의한 암묵적 정보의 효과적인 의사소통을 허용하지만 기계에게는 어려운 일이기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "537-1",
                    "sentence": "The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles.",
                    "sentence_kor": "특히 관련 전문지식이 역할 간에 비대칭적으로 분포되어 있는 경우, 대화 내에서 화자의 역할에 따라 발언 간에 난제가 다를 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "537-2",
                    "sentence": "Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue.",
                    "sentence_kor": "또한 대화 초반에 암묵적으로 전달된 정보를 통해 더 많은 공유 컨텍스트를 구축함에 따라 대화 전반에 걸쳐 과제가 증가할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "537-3",
                    "sentence": "In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task.",
                    "sentence_kor": "본 논문에서, 우리는 작업 관련 발언의 식별 및 분류 성능을 높이고, 그렇게 함으로써 다운스트림 정보 추출 작업의 성능에 긍정적인 영향을 미치기 위해 이러한 통찰력을 다루는 새로운 모델링 접근법 MedFilter를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "537-4",
                    "sentence": "We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve).",
                    "sentence_kor": "우리는 MedFilter가 논의에 의학적으로 관련된 기여를 식별하는 데 사용되는 거의 7,000건의 의사-환자 대화 말뭉치에 대해 이 접근방식을 평가한다(PR 곡선 아래 영역에서 SOTA 기준선에 비해 10% 개선 달성).",
                    "tag": "4+5"
                },
                {
                    "index": "537-5",
                    "sentence": "Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.",
                    "sentence_kor": "작업 관련 발언을 식별하면 다운스트림 의료 처리에 도움이 되며 증상, 약물 및 불만사항 추출에 대해 각각 15%, 105% 및 23%의 개선을 달성한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "538",
            "abstractID": "EMNLP_abs-538",
            "text": [
                {
                    "index": "538-0",
                    "sentence": "Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim.",
                    "sentence_kor": "자동화된 사실 추출 및 검증은 신뢰할 수 있는 말뭉치에서 관련 증거 문장을 찾아 클레임의 진실성을 검증하는 것과 관련된 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "538-1",
                    "sentence": "Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification.",
                    "sentence_kor": "기존 모델은 (i) 모든 증거 문장을 연결하여 중복 및 잡음이 있는 정보를 포함하거나 (ii) 각 청구-증거 문장 쌍을 별도로 처리하고 나중에 모두 집계하여 더 정확한 청구 확인을 위해 관련 문장의 초기 조합을 누락시킨다.",
                    "tag": "1"
                },
                {
                    "index": "538-2",
                    "sentence": "Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy.",
                    "sentence_kor": "이전 연구와는 달리, 본 논문에서, 우리는 (각각은 여러 증거 문장을 포함할 수 있음) 증거 세트를 추출하고, 서로 다른 계층의 클레임과 증거 세트를 인코딩하고 참여함으로써 뒷받침, 반박 또는 불충분한 정보를 검증하는 프레임워크인 HESM(Hierarchical Evidence Set Modeling)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "538-3",
                    "sentence": "Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification.",
                    "sentence_kor": "우리의 실험 결과는 HESM이 사실 추출 및 클레임 검증을 위한 7가지 최첨단 방법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "538-4",
                    "sentence": "Our source code is available at https://github.com/ShyamSubramanian/HESM.",
                    "sentence_kor": "소스 코드는 https://github.com/ShyamSubramanian/HESM에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "539",
            "abstractID": "EMNLP_abs-539",
            "text": [
                {
                    "index": "539-0",
                    "sentence": "Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding.",
                    "sentence_kor": "구조화된 데이터에 기초한 사실 검증을 수행하는 것은 많은 실제 응용 프로그램에 중요하며 특히 언어 이해에 기초한 기호 연산과 비공식 추론을 모두 포함하는 경우 어려운 연구 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "539-1",
                    "sentence": "In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models.",
                    "sentence_kor": "본 논문에서, 우리는 프로그램과 실행을 텍스트 추론 모델에 통합하기 위한 프로그램 강화 언어화 및 그래프 주의 네트워크(ProgVGAT)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "539-2",
                    "sentence": "Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables.",
                    "sentence_kor": "특히, 표상의 운영에 포함된 증거를 축적하기 위해 프로그램 실행 모델을 사용한 구두화가 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "539-3",
                    "sentence": "Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision.",
                    "sentence_kor": "이를 기반으로, 우리는 최종 검증 결정을 내리기 위해 구두화된 프로그램 실행, 프로그램 구조, 원본 진술 및 표와 다른 증거 소스를 통합하도록 설계된 그래프 주의 검증 네트워크를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "539-4",
                    "sentence": "To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results.",
                    "sentence_kor": "위의 프레임워크를 지원하기 위해, 우리는 마진 손실을 기반으로 한 새로운 훈련 전략으로 최적화된 프로그램 선택 모듈을 제안하며, 이는 최종 검증 결과를 향상시키는 데 효과적인 것으로 나타났다.",
                    "tag": "3+4"
                },
                {
                    "index": "539-5",
                    "sentence": "Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.",
                    "sentence_kor": "실험 결과에 따르면 제안된 프레임워크는 벤치마크 데이터 세트 TABFACT에서 74.4%의 정확도로 새로운 최첨단 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "540",
            "abstractID": "EMNLP_abs-540",
            "text": [
                {
                    "index": "540-0",
                    "sentence": "Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER.",
                    "sentence_kor": "사실 검증 시스템은 FEVER와 같은 공유 작업으로 인해 관심이 높아지면서 NLP 문헌에서 잘 연구되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "540-1",
                    "sentence": "Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process.",
                    "sentence_kor": "이 작업은 주장의 사실성을 입증하기 위해 추출된 증거에 대한 추론을 필요로 하지만, 추론 과정을 이해하는 작업은 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "540-2",
                    "sentence": "In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence.",
                    "sentence_kor": "본 연구에서는 추출된 증거에 대한 폐쇄적 의존을 강제하는 사실 검증을 위한 새로운 방법론, 특히 FEEVER를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "540-3",
                    "sentence": "We present an extensive evaluation of state-of-the-art verification models under these constraints.",
                    "sentence_kor": "우리는 이러한 제약 조건 하에서 최첨단 검증 모델에 대한 광범위한 평가를 제시한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "541",
            "abstractID": "EMNLP_abs-541",
            "text": [
                {
                    "index": "541-0",
                    "sentence": "We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base.",
                    "sentence_kor": "다국어 엔터티 링크에 대한 새로운 공식을 제안한다. 여기서 언어별 언급은 언어에 구애받지 않는 지식 기반으로 해결된다.",
                    "tag": "2+3"
                },
                {
                    "index": "541-1",
                    "sentence": "We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities.",
                    "sentence_kor": "우리는 100개 이상의 언어와 2000만 개의 엔터티를 포함하는 단일 엔터티 검색 모델을 얻기 위해 개선된 기능 표현, 마이닝 및 보조 엔터티 쌍 작업을 기반으로 하는 이 새로운 환경에서 듀얼 인코더를 교육한다.",
                    "tag": "2+3"
                },
                {
                    "index": "541-2",
                    "sentence": "The model outperforms state-of-the-art results from a far more limited cross-lingual linking task.",
                    "sentence_kor": "이 모델은 훨씬 제한된 언어 간 연결 작업의 최첨단 결과를 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "541-3",
                    "sentence": "Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation.",
                    "sentence_kor": "희귀 개체와 저자원 언어는 이 대규모에서 과제를 제기하므로 제로샷 및 퓨샷 평가에 대한 초점을 강화할 것을 지지한다.",
                    "tag": "4"
                },
                {
                    "index": "541-4",
                    "sentence": "To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.",
                    "sentence_kor": "이를 위해 우리는 설정에 맞는 대규모 새로운 다국어 데이터 세트인 Mewsli-9를 제공하고 주파수 기반 분석이 모델 및 교육 향상에 어떻게 핵심 통찰력을 제공했는지 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "542",
            "abstractID": "EMNLP_abs-542",
            "text": [
                {
                    "index": "542-0",
                    "sentence": "In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing.",
                    "sentence_kor": "본 논문에서, 우리는 BERT를 프로그레시브 모듈 교체로 효과적으로 압축하기 위한 새로운 모델 압축 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "542-1",
                    "sentence": "Our approach first divides the original BERT into several modules and builds their compact substitutes.",
                    "sentence_kor": "우리의 접근 방식은 먼저 원래의 BERT를 여러 모듈로 나누고 그것들의 소형 대체물을 구축한다.",
                    "tag": "3"
                },
                {
                    "index": "542-2",
                    "sentence": "Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules.",
                    "sentence_kor": "그런 다음, 우리는 무작위로 원래의 모듈을 대체품으로 교체하여 원래의 모듈의 동작을 모방하도록 콤팩트 모듈을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "542-3",
                    "sentence": "We progressively increase the probability of replacement through the training.",
                    "sentence_kor": "우리는 훈련을 통해 점차적으로 교체 확률을 높인다.",
                    "tag": "3"
                },
                {
                    "index": "542-4",
                    "sentence": "In this way, our approach brings a deeper level of interaction between the original and compact models.",
                    "sentence_kor": "이러한 방식으로, 우리의 접근 방식은 원래 모델과 소형 모델 사이의 더 깊은 수준의 상호작용을 가져옵니다.",
                    "tag": "4"
                },
                {
                    "index": "542-5",
                    "sentence": "Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function.",
                    "sentence_kor": "BERT 압축에 대한 이전의 지식 증류 접근법과 비교하여, 우리의 접근법은 어떠한 추가 손실 함수도 도입하지 않는다.",
                    "tag": "5"
                },
                {
                    "index": "542-6",
                    "sentence": "Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.",
                    "sentence_kor": "우리의 접근 방식은 GLUE 벤치마크에서 기존 지식 증류 접근 방식을 능가하여 모델 압축에 대한 새로운 관점을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "543",
            "abstractID": "EMNLP_abs-543",
            "text": [
                {
                    "index": "543-0",
                    "sentence": "Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning.",
                    "sentence_kor": "사전 교육을 받은 심층 언어 모델은 먼저 사전 교육한 다음 미세 조정하는 방식으로 큰 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "543-1",
                    "sentence": "But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance.",
                    "sentence_kor": "그러나 이러한 순차적 전송 학습 패러다임은 종종 치명적인 망각 문제에 직면하여 차선의 성능으로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "543-2",
                    "sentence": "To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks.",
                    "sentence_kor": "망각 현상을 줄이기 위해 다중 작업 학습 아이디어를 채택하고 사전 훈련 작업과 다운스트림 작업을 공동으로 학습하는 리콜 및 학습 메커니즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "543-3",
                    "sentence": "Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually.",
                    "sentence_kor": "특히, 우리는 데이터가 없는 사전 훈련 작업에서 지식을 상기하기 위한 사전 훈련 시뮬레이션 메커니즘과 다운스트림 작업에 학습을 점진적으로 집중하기 위한 목표 이동 메커니즘을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "543-4",
                    "sentence": "Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark.",
                    "sentence_kor": "실험에 따르면 이 방법은 GLUE 벤치마크에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "543-5",
                    "sentence": "Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large.",
                    "sentence_kor": "또한 우리의 방법을 사용하면 BERT-베이스가 BERT-Large의 직접 미세 조정보다 더 나은 평균 성능을 달성할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "543-6",
                    "sentence": "Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.",
                    "sentence_kor": "또한 제안된 메커니즘을 Adam 옵티마이저에 통합하는 오픈 소스 RecAdam 옵티마이저를 제공하여 NLP 커뮤니티를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "544",
            "abstractID": "EMNLP_abs-544",
            "text": [
                {
                    "index": "544-0",
                    "sentence": "Active learning strives to reduce annotation costs by choosing the most critical examples to label.",
                    "sentence_kor": "능동적 학습은 라벨을 붙일 가장 중요한 예를 선택하여 주석 비용을 절감하기 위해 노력한다.",
                    "tag": "1"
                },
                {
                    "index": "544-1",
                    "sentence": "Typically, the active learning strategy is contingent on the classification model.",
                    "sentence_kor": "일반적으로, 능동적 학습 전략은 분류 모델에 따라 달라진다.",
                    "tag": "1"
                },
                {
                    "index": "544-2",
                    "sentence": "For instance, uncertainty sampling depends on poorly calibrated model confidence scores.",
                    "sentence_kor": "예를 들어 불확실성 표본 추출은 잘못 보정된 모델 신뢰 점수에 따라 달라집니다.",
                    "tag": "1"
                },
                {
                    "index": "544-3",
                    "sentence": "In the cold-start setting, active learning is impractical because of model instability and data scarcity.",
                    "sentence_kor": "콜드 스타트 환경에서는 모델 불안정성과 데이터 부족 때문에 능동적 학습이 비현실적입니다.",
                    "tag": "1"
                },
                {
                    "index": "544-4",
                    "sentence": "Fortunately, modern NLP provides an additional source of information: pre-trained language models.",
                    "sentence_kor": "다행히도, 현대의 NLP는 사전 훈련된 언어 모델이라는 추가 정보 소스를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "544-5",
                    "sentence": "The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning.",
                    "sentence_kor": "사전 훈련 손실은 모델을 놀라게 하는 예를 찾을 수 있으며 효율적인 미세 조정을 위해 라벨을 붙여야 한다.",
                    "tag": "1"
                },
                {
                    "index": "544-6",
                    "sentence": "Therefore, we treat the language modeling loss as a proxy for classification uncertainty.",
                    "sentence_kor": "따라서, 우리는 언어 모델링 손실을 분류 불확실성의 대용물로 취급한다.",
                    "tag": "2"
                },
                {
                    "index": "544-7",
                    "sentence": "With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification.",
                    "sentence_kor": "BERT를 사용하여 텍스트 분류에 대한 레이블링 비용을 최소화하는 마스킹 언어 모델링 손실을 기반으로 간단한 전략을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "544-8",
                    "sentence": "Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.",
                    "sentence_kor": "다른 기준선에 비해 우리의 접근 방식은 더 적은 샘플링 반복과 계산 시간 내에 더 높은 정확도에 도달한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "545",
            "abstractID": "EMNLP_abs-545",
            "text": [
                {
                    "index": "545-0",
                    "sentence": "Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems.",
                    "sentence_kor": "심층 신경망은 NMT(Neural Machine Translation)에서 대화 시스템에 이르기까지 신뢰할 수 있는 자연어 처리(NLP) 애플리케이션을 구축하는 표준 접근 방식이 되었다.",
                    "tag": "1"
                },
                {
                    "index": "545-1",
                    "sentence": "However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time.",
                    "sentence_kor": "그러나 모델 크기를 증가시켜 정확도를 향상시키려면 많은 하드웨어 계산이 필요하며, 이는 추론 시 NLP 애플리케이션을 상당히 느리게 할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "545-2",
                    "sentence": "To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT.",
                    "sentence_kor": "이 문제를 해결하기 위해 NMT의 추론 시간 대기 시간을 크게 줄이는 새로운 벡터 매트릭스 아키텍처(VVMA)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "545-3",
                    "sentence": "This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations.",
                    "sentence_kor": "이 아키텍처는 대기 시간이 짧은 벡터-벡터 연산과 대기 시간이 긴 벡터-매트릭스 연산 기능을 갖춘 특수 하드웨어를 활용합니다.",
                    "tag": "3"
                },
                {
                    "index": "545-4",
                    "sentence": "It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy.",
                    "sentence_kor": "또한 정확도에 큰 영향을 미치지 않고 효율적인 매트릭스 승수에 의존하는 거의 모든 모델에 대한 매개 변수와 FLOP의 수를 줄인다.",
                    "tag": "4"
                },
                {
                    "index": "545-5",
                    "sentence": "We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four.",
                    "sentence_kor": "우리는 우리의 프레임워크가 NMT에 사용되는 시퀀스 투 시퀀스 및 트랜스포머 모델의 대기 시간을 4배 줄일 수 있음을 시사하는 경험적 결과를 제시한다.",
                    "tag": "4+5"
                },
                {
                    "index": "545-6",
                    "sentence": "Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.",
                    "sentence_kor": "마지막으로 VVMA가 다른 도메인으로 확장된다는 증거를 보여주고 효율적인 사용을 위한 새로운 하드웨어에 대해 논의한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "546",
            "abstractID": "EMNLP_abs-546",
            "text": [
                {
                    "index": "546-0",
                    "sentence": "Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast.",
                    "sentence_kor": "운율(Prosody)은 자연어로 된 풍부한 정보원으로 대조와 같은 현상을 나타내는 지표 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "546-1",
                    "sentence": "In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech.",
                    "sentence_kor": "이 정보를 다운스트림 작업에 사용할 수 있도록 하기 위해서는 운율 사건을 음성으로 탐지하는 방법이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "546-2",
                    "sentence": "We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task.",
                    "sentence_kor": "우리는 이 작업에 대한 CNN 기반 모델을 제시한 Stehwien 외 연구진(2018)의 연구에서 영감을 받아 음역 감지를 위한 새로운 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "546-3",
                    "sentence": "Our model makes greater use of context by using full utterances as input and adding an LSTM layer.",
                    "sentence_kor": "우리 모델은 전체 발음을 입력으로 사용하고 LSTM 계층을 추가하여 컨텍스트를 더 많이 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "546-4",
                    "sentence": "We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result.",
                    "sentence_kor": "우리는 이러한 혁신이 최첨단 결과인 보스턴 대학 라디오 뉴스 코퍼스에서 미국 영어 음성에서 억양 감지의 정확도를 87.5%에서 88.7%로 향상시킨다는 것을 발견했다.",
                    "tag": "4+5"
                },
                {
                    "index": "546-5",
                    "sentence": "We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task.",
                    "sentence_kor": "또한 모든 내용 단어에 피치 액센트를 예측하는 간단한 기준선이 82.2%의 정확도를 산출한다는 것을 발견했으며, 이 기준선이 이 작업에 적합한 기준임을 시사한다.",
                    "tag": "4+5"
                },
                {
                    "index": "546-6",
                    "sentence": "Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.",
                    "sentence_kor": "마지막으로, 우리는 음이 이 작업과 이 말뭉치에 가장 중요한 음향 특징임을 보여주는 절제 테스트를 수행한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "547",
            "abstractID": "EMNLP_abs-547",
            "text": [
                {
                    "index": "547-0",
                    "sentence": "Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting.",
                    "sentence_kor": "자연어 처리로 인해 최근 주식 이동 예측과 변동성 예측이 진전되어 재무 예측이 개선되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "547-1",
                    "sentence": "Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance.",
                    "sentence_kor": "기업의 수익 통화 기록은 리스크 모델링을 위해 잘 연구되어 주식 실적에 대한 독특한 투자 통찰력을 제공합니다.",
                    "tag": "1"
                },
                {
                    "index": "547-2",
                    "sentence": "However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk.",
                    "sentence_kor": "그러나 회사 임원의 말에서 나오는 음성 신호는 재무 위험을 추정하기 위해 자연어 데이터의 방대한 소스를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "547-3",
                    "sentence": "Additionally, most existing approaches ignore the correlations between stocks.",
                    "sentence_kor": "또한 대부분의 기존 접근 방식은 주식 간의 상관 관계를 무시합니다.",
                    "tag": "1"
                },
                {
                    "index": "547-4",
                    "sentence": "Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation.",
                    "sentence_kor": "기존 작업을 기반으로, 우리는 준지도 다중 작업 위험 예측 공식에서 언어, 음성 및 재무 기능을 융합하면서 그래프 컨볼루션을 통해 주식 상호의존성을 설명하는 주식 변동성 예측을 위한 신경 모델을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "547-5",
                    "sentence": "Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.",
                    "sentence_kor": "우리가 제안한 모델인 VolTAGE는 변동성 예측을 위한 다중 모드 학습의 효과를 입증하는 기존 방법을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "548",
            "abstractID": "EMNLP_abs-548",
            "text": [
                {
                    "index": "548-0",
                    "sentence": "Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data.",
                    "sentence_kor": "데이터 부족으로 인해 많은 언어 쌍에서 단대단 접근 방식을 사용하여 음성에서 텍스트로 직접 변환하는 것은 여전히 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "548-1",
                    "sentence": "Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works.",
                    "sentence_kor": "자동 음성 인식(ASR) 작업을 사용하여 인코더 매개 변수를 사전 교육하면 낮은 리소스 설정에서 결과가 개선되지만, 신경 기계 변환(NMT) 작업에서 사전 교육된 매개 변수를 사용하려고 시도하는 것은 이전 연구에서 대부분 성공하지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "548-2",
                    "sentence": "In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.",
                    "sentence_kor": "본 논문에서, 우리는 적대적 정규화기를 사용함으로써 ASR 및 NMT 작업의 인코더 표현을 서로 다른 양식에 있더라도 더 가깝게 만들 수 있으며, 이것이 음성 번역에 사전 훈련된 NMT 디코더를 효과적으로 사용하는 데 어떻게 도움이 되는지 보여줄 것이다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "549",
            "abstractID": "EMNLP_abs-549",
            "text": [
                {
                    "index": "549-0",
                    "sentence": "We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).",
                    "sentence_kor": "GAN(Generative Adversarial Networks)을 사용한 새로운 핵심 문구 생성 접근법을 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "549-1",
                    "sentence": "For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases.",
                    "sentence_kor": "주어진 문서의 경우 발전기는 일련의 키 문구를 생성하고 판별기는 사람이 작성한 키 문구와 기계가 생성한 키 문구를 구분한다.",
                    "tag": "1"
                },
                {
                    "index": "549-2",
                    "sentence": "We evaluated this approach on standard benchmark datasets.",
                    "sentence_kor": "표준 벤치마크 데이터 세트에서 이 접근 방식을 평가했습니다.",
                    "tag": "1"
                },
                {
                    "index": "549-3",
                    "sentence": "We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques.",
                    "sentence_kor": "우리는 우리의 모델이 추상적인 키 프레이즈 생성에서 최첨단 성능을 달성하고 최고의 성능을 발휘하는 추출 기법에 필적한다는 것을 관찰했다.",
                    "tag": "1"
                },
                {
                    "index": "549-4",
                    "sentence": "Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models.",
                    "sentence_kor": "GAN을 사용하여 유망한 결과를 얻었지만 최신 생성 모델보다 크게 나은 것은 아니다.",
                    "tag": "1"
                },
                {
                    "index": "549-5",
                    "sentence": "To our knowledge, this is one of the first works that use GANs for keyphrase generation.",
                    "sentence_kor": "우리가 아는 바로는, 이것은 주요 문구 생성에 GAN을 사용한 최초의 작업 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "549-6",
                    "sentence": "We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.",
                    "sentence_kor": "우리는 우리의 관찰에 대한 상세한 분석을 제시하고 이러한 발견이 다른 연구자들이 핵심 문구 생성 작업에 GAN의 사용을 더 연구하는데 도움이 될 것으로 기대한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "550",
            "abstractID": "EMNLP_abs-550",
            "text": [
                {
                    "index": "550-0",
                    "sentence": "Text classification is a fundamental problem in natural language processing.",
                    "sentence_kor": "텍스트 분류는 자연어 처리의 근본적인 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "550-1",
                    "sentence": "Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus.",
                    "sentence_kor": "최근의 연구는 말뭉치의 글로벌 단어 동시 발생을 포착하기 위해 그래프 신경망(GNN) 기법을 적용했다.",
                    "tag": "1"
                },
                {
                    "index": "550-2",
                    "sentence": "However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph.",
                    "sentence_kor": "그러나 이전 연구는 대규모 말뭉치로 확장되지 않으며 텍스트 그래프의 이질성을 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "550-3",
                    "sentence": "To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer).",
                    "sentence_kor": "이러한 문제를 해결하기 위해 새로운 트랜스포머 기반 이기종 그래프 신경 네트워크, 즉 텍스트 그래프 트랜스포머(TG-트랜스포머)를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "550-4",
                    "sentence": "Our model learns effective node representations by capturing structure and heterogeneity from the text graph.",
                    "sentence_kor": "우리 모델은 텍스트 그래프에서 구조와 이질성을 캡처하여 효과적인 노드 표현을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "550-5",
                    "sentence": "We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus.",
                    "sentence_kor": "대규모 말뭉치를 처리하기 위해 컴퓨팅 및 메모리 비용을 크게 절감하는 미니 배치 텍스트 그래프 샘플링 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "550-6",
                    "sentence": "Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.",
                    "sentence_kor": "여러 벤치마크 데이터 세트에 대해 광범위한 실험이 수행되었으며, 그 결과는 TG-트랜스포머가 텍스트 분류 작업에 대한 최첨단 접근 방식을 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "551",
            "abstractID": "EMNLP_abs-551",
            "text": [
                {
                    "index": "551-0",
                    "sentence": "Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics.",
                    "sentence_kor": "책은 일반적으로 장과 절로 나누어져 일관성 있는 하위 내러티브와 주제를 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "551-1",
                    "sentence": "We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts.",
                    "sentence_kor": "우리는 긴 텍스트를 분할하는 일반적인 작업의 대용으로서 챕터 경계를 예측하는 작업을 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "551-2",
                    "sentence": "We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task.",
                    "sentence_kor": "우리는 신경 추론과 책의 장 제목 헤더를 인식하기 위한 규칙 일치를 결합한 혼합 접근 방식을 사용하여 9,126권의 영어 소설로 구성된 프로젝트 구텐베르크 장 분할 데이터 세트를 구축하여 이 작업에서 F1 점수 0.77을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "551-3",
                    "sentence": "Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents.",
                    "sentence_kor": "구조 단서 제거 후 주석이 달린 이 데이터를 근거 자료로 사용하여 장 분할을 위한 컷 기반 및 신경 방법을 제시하여 책 길이 문서에 대한 정확한 깨짐 예측이라는 어려운 작업에서 F1 점수 0.453을 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "551-4",
                    "sentence": "Finally, we reveal interesting historical trends in the chapter structure of novels.",
                    "sentence_kor": "마지막으로, 우리는 소설의 챕터 구조에서 흥미로운 역사적 경향을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "552",
            "abstractID": "EMNLP_abs-552",
            "text": [
                {
                    "index": "552-0",
                    "sentence": "Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics.",
                    "sentence_kor": "사회경제적, 인구통계학적 배경과 상관없이, 수백만 명의 사람들이 잘 알려지지 않은 주제뿐만 아니라 인기 있는 주제에 대한 정보를 유지하기 위해 매일 위키피디아 기사에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "552-1",
                    "sentence": "Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content.",
                    "sentence_kor": "편집자들에 의해 기사는 백과사전적인 내용으로 신뢰도를 나타내는 몇 가지 품질 등급으로 분류되었다.",
                    "tag": "1"
                },
                {
                    "index": "552-2",
                    "sentence": "This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines.",
                    "sentence_kor": "이 매뉴얼 지정은 백과사전 언어에 대한 깊은 지식이 필요하고 위키 지침의 순환을 탐색해야 하기 때문에 부담스러운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "552-3",
                    "sentence": "In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation.",
                    "sentence_kor": "본 논문에서 우리는 향상된 위키백과 문서 표현을 얻기 위해 문서 텍스트, 메타 데이터 및 이미지와 같은 몇 가지 핵심 정보 출처의 신호를 축적하는 새로운 딥 러닝 모델인 신경 위키백과 품질 모니터(NWQM)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "552-4",
                    "sentence": "We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.",
                    "sentence_kor": "우리는 사용 가능한 수많은 솔루션에 대한 우리의 접근 방식을 비교하고 상세한 절제 연구를 통해 최첨단 접근법에 비해 8% 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "553",
            "abstractID": "EMNLP_abs-553",
            "text": [
                {
                    "index": "553-0",
                    "sentence": "In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task.",
                    "sentence_kor": "재무 영역에서 리스크 모델링과 수익 창출은 정교하고 복잡한 주식 이동 예측 작업에 크게 의존하고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "553-1",
                    "sentence": "Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market.",
                    "sentence_kor": "시장의 확률적 역학과 비정상적 행동을 감안할 때 주식 예측은 복잡하다.",
                    "tag": "1"
                },
                {
                    "index": "553-2",
                    "sentence": "Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks.",
                    "sentence_kor": "주식 이동은 소셜 미디어와 주식 간의 상관관계와 같이 전통적으로 연구된 역사적 가격 이상의 다양한 요인에 의해 영향을 받는다.",
                    "tag": "1"
                },
                {
                    "index": "553-3",
                    "sentence": "The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting.",
                    "sentence_kor": "온라인 콘텐츠와 지식의 편재성이 증가함에 따라 정확한 재고 예측을 위해 이러한 다중 모드 신호를 고려하는 모델의 탐구가 요구된다.",
                    "tag": "1"
                },
                {
                    "index": "553-4",
                    "sentence": "We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion.",
                    "sentence_kor": "계층적 시간적 방식으로 그래프 신경망을 통해 금융 데이터, 소셜 미디어 및 주식 간 관계에서 혼돈된 시간 신호를 강력하게 혼합하는 아키텍처를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "553-5",
                    "sentence": "Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.",
                    "sentence_kor": "실제 S&P 500 인덱스 데이터와 영어 트윗에 대한 실험을 통해 투자 의사 결정 및 거래를 위한 도구로서 모델의 실질적인 적용 가능성을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "554",
            "abstractID": "EMNLP_abs-554",
            "text": [
                {
                    "index": "554-0",
                    "sentence": "State of the art research for date-time entity extraction from text is task agnostic.",
                    "sentence_kor": "텍스트에서 날짜-시간 엔티티 추출을 위한 최신 연구는 작업에 구애받지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "554-1",
                    "sentence": "Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task.",
                    "sentence_kor": "결과적으로 문헌에서 제안된 방법은 텍스트에서 일반적인 날짜-시간 추출에 대해 잘 수행되지만 텍스트에 있는 날짜-시간 엔티티의 부분 집합만 태스크 해결에 적절한 작업 특정 날짜-시간 엔티티 추출에는 적합하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "554-2",
                    "sentence": "Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time.",
                    "sentence_kor": "또한, 일부 업무는 시간 경과에 따라 정확하게 추론하기 위해 날짜-시간 실체와 관련된 부정 제약조건을 식별해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "554-3",
                    "sentence": "We showcase a novel model for extracting task-specific date-time entities along with their negation constraints.",
                    "sentence_kor": "우리는 부정 제약 조건과 함께 작업별 날짜 시간 엔티티를 추출하기 위한 새로운 모델을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "554-4",
                    "sentence": "We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant.",
                    "sentence_kor": "우리는 이메일 기반 디지털 AI 스케줄링 보조자에 대한 회의 일정을 잡는 맥락에서 날짜 이해 작업에 대한 우리 방법의 효과를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "554-5",
                    "sentence": "Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.",
                    "sentence_kor": "우리의 방법은 회의 일정과 관련된 날짜-시간 엔티티를 탐지하는 기준 방법에 비해 19% f-점수의 절대적인 이득을 달성하고 날짜-시간 엔티티에 대한 부정 제약 조건을 탐지하는 기준 방법에 비해 4% 개선된다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "555",
            "abstractID": "EMNLP_abs-555",
            "text": [
                {
                    "index": "555-0",
                    "sentence": "This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates.",
                    "sentence_kor": "본 논문은 압도적인 수의 지원서를 크게 심사하는 데 필요한 시간과 노동력을 줄이고 적합한 지원자의 선택을 개선하기 위한 이력서 분류에 대한 종합적인 연구를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "555-1",
                    "sentence": "A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC).",
                    "sentence_kor": "임상연구조정관(CRC)의 4가지 경력수준으로 지정된 252개 직책에 대한 24,933명의 입사지원서에서 총 6,492명의 이력서를 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "555-2",
                    "sentence": "Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines.",
                    "sentence_kor": "각 이력서는 여러 차례의 3중 주석을 통해 전문가들이 가장 적합한 CRC 위치에 수동으로 주석을 달아 가이드라인을 수립한다.",
                    "tag": "3"
                },
                {
                    "index": "555-3",
                    "sentence": "As a result, a high Kappa score of 61% is achieved for inter-annotator agreement.",
                    "sentence_kor": "결과적으로 주석자 간 합치도에 대해 61%의 높은 카파 점수가 달성됩니다.",
                    "tag": "4"
                },
                {
                    "index": "555-4",
                    "sentence": "Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2).",
                    "sentence_kor": "이 데이터 세트를 고려할 때, 새로운 변압기 기반 분류 모델은 두 가지 작업에 대해 개발된다. 첫 번째 작업은 이력서를 받아 CRC 수준(T1)으로 분류하고 두 번째 작업은 이력서와 직무 설명을 모두 사용하여 애플리케이션이 직무(T2)에 적합한지 여부를 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "555-5",
                    "sentence": "Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2.",
                    "sentence_kor": "섹션 인코딩과 다중 헤드 주의 디코딩을 사용하는 최상의 모델은 T1에 73.3%, T2에 79.2%의 결과를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "555-6",
                    "sentence": "Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.",
                    "sentence_kor": "우리의 분석에 따르면 예측 오류는 대부분 인접한 CRC 수준 사이에서 발생하는데, 이는 전문가들도 구별하기 어려우며, 이는 실제 HR 플랫폼에서 모델의 실질적인 가치를 의미한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "556",
            "abstractID": "EMNLP_abs-556",
            "text": [
                {
                    "index": "556-0",
                    "sentence": "Semantic change detection concerns the task of identifying words whose meaning has changed over time.",
                    "sentence_kor": "의미 변화 감지는 시간이 지남에 따라 의미가 변한 단어를 식별하는 작업과 관련이 있다.",
                    "tag": "1"
                },
                {
                    "index": "556-1",
                    "sentence": "Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time.",
                    "sentence_kor": "신경 임베딩에서 작동하는 현재의 최첨단 접근방식은 시간에 따른 진화를 고려하지 않고 두 개의 뚜렷한 시간 동안 벡터 표현을 비교하여 단어의 의미 변화 수준을 감지한다.",
                    "tag": "1"
                },
                {
                    "index": "556-2",
                    "sentence": "In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time.",
                    "sentence_kor": "본 연구에서, 우리는 의미론적으로 이동된 단어를 탐지하기 위한 세 가지 변형 순차 모델을 제안하며, 이는 시간이 지남에 따라 단어 표현의 변화를 효과적으로 설명한다.",
                    "tag": "2+3"
                },
                {
                    "index": "556-3",
                    "sentence": "Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection.",
                    "sentence_kor": "합성 데이터와 실제 데이터를 사용한 다양한 설정에서 광범위한 실험을 통해 의미론적 변화 감지를 위한 시간을 통한 단어 벡터의 순차 모델링의 중요성을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "556-4",
                    "sentence": "Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.",
                    "sentence_kor": "마지막으로, 우리는 다른 접근방식을 정량적인 방법으로 비교함으로써 단어 표현의 시간적 모델링이 성능에서 분명한 이점을 산출한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "557",
            "abstractID": "EMNLP_abs-557",
            "text": [
                {
                    "index": "557-0",
                    "sentence": "In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation.",
                    "sentence_kor": "본 논문에서 우리는 희박한 단어 표현을 활용함으로써 세분화된 모든 단어 단어 단어 의미 명확화 작업에서 보다 복잡한 작업별 모델의 결과를 능가할 수 있음을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "557-1",
                    "sentence": "Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations.",
                    "sentence_kor": "우리가 제안한 알고리즘은 희박한 상황별 단어 표현을 얻을 수 있도록 하는 지나치게 완전한 의미 기반 벡터 집합에 의존한다.",
                    "tag": "2"
                },
                {
                    "index": "557-2",
                    "sentence": "We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets.",
                    "sentence_kor": "우리는 단어 감각의 공존과 단어 형태의 0이 아닌 좌표를 기반으로 이러한 정보 이론에서 영감을 받은 싱셋 표현을 도입하여 5개의 표준 단어 감각 명확하지 않은 벤치마크 데이터 세트의 조합에 대해 집계된 F-점수 78.8점을 달성할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "557-3",
                    "sentence": "We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks.",
                    "sentence_kor": "또한 4개의 서로 다른 트리 뱅크의 음성 태그에 대해 이를 평가하여 제안된 프레임워크의 일반적인 적용 가능성을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "557-4",
                    "sentence": "Our results indicate a significant improvement over the application of the dense word representations.",
                    "sentence_kor": "우리의 결과는 조밀한 단어 표현의 적용에 비해 크게 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "558",
            "abstractID": "EMNLP_abs-558",
            "text": [
                {
                    "index": "558-0",
                    "sentence": "Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models.",
                    "sentence_kor": "현재 모델의 대용량 메모리 및 런타임 요구 사항으로 인해 긴 문서 코어 참조 해결은 여전히 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "558-1",
                    "sentence": "Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents.",
                    "sentence_kor": "개체의 전역적 표현만을 사용하여 증분 참조 해결을 수행하는 최근 연구는 실질적인 이점을 보여주지만 모든 엔티티를 기억해야 하므로 긴 문서에서는 비실용적일 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "558-2",
                    "sentence": "We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document.",
                    "sentence_kor": "우리는 모든 엔티티를 메모리에 유지하는 것이 불필요하다고 주장하며, 한 번에 소수의 제한된 엔티티만 추적하여 문서의 길이에서 선형 런타임을 보장하는 메모리 증강 신경망을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "558-3",
                    "sentence": "We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy",
                    "sentence_kor": "우리는 (a) 모델이 OnNotes 및 LitBank에서 메모리 및 계산 요구사항이 높은 모델과 경쟁력을 유지하고, (b) 모델은 규칙 기반 전략을 능가하는 효율적인 메모리 관리 전략을 학습한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "559",
            "abstractID": "EMNLP_abs-559",
            "text": [
                {
                    "index": "559-0",
                    "sentence": "Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering.",
                    "sentence_kor": "이벤트와 시간 표현식 사이의 시간적 관계를 추출하는 것은 이벤트 타임라인 구성 및 시간 관련 질문 답변과 같은 많은 응용 프로그램이 있다.",
                    "tag": "1"
                },
                {
                    "index": "559-1",
                    "sentence": "It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019).",
                    "sentence_kor": "BERT(Devlin et al., 2019)와 같은 심층 맥락 언어 모델(LM)에 의해 포착될 수 있는 문장 또는 담화 수준에서 구문 및 의미 정보가 필요한 어려운 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "559-2",
                    "sentence": "In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a).",
                    "sentence_kor": "본 논문에서 우리는 BERT 기반 시간 의존성 분석기의 여러 변형을 개발하고 BERT가 시간 의존성 구문 분석을 크게 개선한다는 것을 보여준다(Zhang and Xue, 2018a).",
                    "tag": "1"
                },
                {
                    "index": "559-3",
                    "sentence": "We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short.",
                    "sentence_kor": "또한 심층 상황별 신경 LM이 도움이 되는 이유와 부족한 부분에 대한 자세한 분석을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "559-4",
                    "sentence": "Source code and resources are made available at https://github.com/bnmin/tdp_ranking.",
                    "sentence_kor": "소스 코드 및 리소스는 https://github.com/bnmin/tdp_ranking에서 사용할 수 있습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "560",
            "abstractID": "EMNLP_abs-560",
            "text": [
                {
                    "index": "560-0",
                    "sentence": "The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>.",
                    "sentence_kor": "개방형 정보 추출(OIE)의 목표는 자연어 텍스트에서 사실을 추출하고, 이들을 <주제, 명제, 목적어> 형식의 구조화된 3배로 표현하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "560-1",
                    "sentence": "For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven, composed, Ode to Joy>.",
                    "sentence_kor": "예를 들어, \"베토벤은 기쁨의 송가를 작곡했다.\"라는 문장을 감안할 때, 우리는 3중 <베토벤, 작곡된, 기쁨의 송가>를 추출할 것으로 기대된다.",
                    "tag": "3"
                },
                {
                    "index": "560-2",
                    "sentence": "In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases).",
                    "sentence_kor": "본 연구에서는 서로 다른 신경망 아키텍처와 훈련 접근방식을 체계적으로 비교하고 OIE16 벤치마크(Stanovsky 및 Dagan, 2016)에서 현재 최고 모델의 성능을 각각 0.421 F1 점수와 0.420 AUC-PR(즉, 두 경우 모두 200% 이상) 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "560-3",
                    "sentence": "Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.",
                    "sentence_kor": "더욱이, 우리는 적절한 문제와 손실 공식화가 종종 네트워크 아키텍처보다 성능에 더 큰 영향을 미친다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "561",
            "abstractID": "EMNLP_abs-561",
            "text": [
                {
                    "index": "561-0",
                    "sentence": "Active learning is an important technique for low-resource sequence labeling tasks.",
                    "sentence_kor": "능동 학습은 저자원 시퀀스 라벨링 작업에 중요한 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "561-1",
                    "sentence": "However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations.",
                    "sentence_kor": "그러나 현재 활성 시퀀스 라벨링 방법은 각 반복에서 쿼리된 샘플을 단독으로 사용하므로 인간 주석을 활용하는 비효율적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "561-2",
                    "sentence": "We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling.",
                    "sentence_kor": "활성 시퀀스 라벨링의 레이블 효율성을 개선하기 위해 간단하지만 효과적인 데이터 확대 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "561-3",
                    "sentence": "Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration.",
                    "sentence_kor": "우리의 방법인 SeqMix는 각 반복에서 레이블이 지정된 시퀀스를 추가로 생성하여 쿼리된 샘플을 증가시킨다.",
                    "tag": "3"
                },
                {
                    "index": "561-4",
                    "sentence": "The key difficulty is to generate plausible sequences along with token-level labels.",
                    "sentence_kor": "핵심 어려움은 토큰 레벨 라벨과 함께 그럴듯한 시퀀스를 생성하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "561-5",
                    "sentence": "In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples.",
                    "sentence_kor": "SeqMix에서는 쿼리된 샘플의 시퀀스 및 토큰 레벨 레이블 모두에 대해 혼합을 수행하여 이 문제를 해결한다.",
                    "tag": "3"
                },
                {
                    "index": "561-6",
                    "sentence": "Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not.",
                    "sentence_kor": "또한 시퀀스 혼합 중에 판별기를 설계하여 생성된 시퀀스의 타당성 여부를 판단한다.",
                    "tag": "3"
                },
                {
                    "index": "561-7",
                    "sentence": "Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores.",
                    "sentence_kor": "명명된 개체 인식 및 이벤트 감지 작업에 대한 우리의 실험은 SeqMix가 F1 점수 측면에서 표준 활성 시퀀스 라벨링 방법을 2.27%-3.75% 개선할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "561-8",
                    "sentence": "The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.",
                    "sentence_kor": "SeqMix의 코드와 데이터는 https://github.com/rz-zhang/SeqMix에서 확인할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "562",
            "abstractID": "EMNLP_abs-562",
            "text": [
                {
                    "index": "562-0",
                    "sentence": "In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs).",
                    "sentence_kor": "본 연구에서는 상식 지식 기반(KB)에서 감독되지 않은 텍스트로의 경로 및 텍스트 전송 경로에 대한 이중 학습 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "562-1",
                    "sentence": "We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers.",
                    "sentence_kor": "우리는 약하게 감독되는 데이터 세트를 만들어 약한 감독이 미치는 영향을 조사하고 약간의 감독이라도 모델 성능을 크게 향상시키고 더 나은 품질의 전송을 가능하게 할 수 있다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "562-2",
                    "sentence": "We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models.",
                    "sentence_kor": "우리는 다양한 모델 아키텍처와 평가 지표를 검토하여 생성 모델에 맞춘 새로운 상식 KB 완성 지표를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "562-3",
                    "sentence": "Extensive experimental results show that the proposed method compares very favorably to the existing baselines.",
                    "sentence_kor": "광범위한 실험 결과는 제안된 방법이 기존 기준선에 비해 매우 유리하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "562-4",
                    "sentence": "This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.",
                    "sentence_kor": "이 접근법은 자동 KB 구성/확장을 위한 보다 진보된 시스템과 일관된 텍스트 설명으로의 KB 변환의 역연산을 향한 실행 가능한 단계이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "563",
            "abstractID": "EMNLP_abs-563",
            "text": [
                {
                    "index": "563-0",
                    "sentence": "Data-to-text generation has recently attracted substantial interests due to its wide applications.",
                    "sentence_kor": "데이터-텍스트 생성은 광범위한 애플리케이션으로 인해 최근 상당한 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "563-1",
                    "sentence": "Existing methods have shown impressive performance on an array of tasks.",
                    "sentence_kor": "기존 방법은 일련의 작업에서 인상적인 성능을 보여 주었다.",
                    "tag": "1"
                },
                {
                    "index": "563-2",
                    "sentence": "However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains.",
                    "sentence_kor": "그러나 이들은 각 작업에 상당한 양의 레이블링 데이터에 의존하므로 취득 비용이 많이 들고 따라서 새로운 작업 및 도메인으로 적용을 제한한다.",
                    "tag": "1"
                },
                {
                    "index": "563-3",
                    "sentence": "In this paper, we propose to leverage pre-training and transfer learning to address this issue.",
                    "sentence_kor": "본 논문에서 우리는 이 문제를 해결하기 위해 사전 교육 및 이전 학습을 활용할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "563-4",
                    "sentence": "We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text.",
                    "sentence_kor": "우리는 지식 기반 사전 훈련(KGPT)을 제안한다. 이 훈련은 1) 지식 기반 텍스트를 생성하기 위한 일반적인 지식 기반 생성 모델로 구성된다.",
                    "tag": "2+3"
                },
                {
                    "index": "563-5",
                    "sentence": "2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web.",
                    "sentence_kor": "2) 웹에서 기어들어온 방대한 지식 수집 텍스트 말뭉치에 대한 사전 교육 패러다임.",
                    "tag": "3"
                },
                {
                    "index": "563-6",
                    "sentence": "The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text.",
                    "sentence_kor": "사전 교육된 모델은 작업별 텍스트를 생성하기 위해 다양한 데이터-텍스트 생성 작업에 대해 미세 조정될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "563-7",
                    "sentence": "We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.",
                    "sentence_kor": "우리는 그 효과를 평가하기 위해 완전한 감독, 제로샷, 퓨샷의 세 가지 설정을 채택한다.",
                    "tag": "3"
                },
                {
                    "index": "563-8",
                    "sentence": "Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines.",
                    "sentence_kor": "완전히 감독된 환경에서 우리 모델은 알려진 기준선에 비해 현저한 이득을 얻을 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "563-9",
                    "sentence": "Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.",
                    "sentence_kor": "제로샷 설정에서 예제를 보지 않은 모델은 다른 모든 기준선이 실패하는 동안 WebNLG에서 30개 이상의 ROUGE-L을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "563-10",
                    "sentence": "Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models.",
                    "sentence_kor": "퓨샷 설정 하에서 우리 모델은 기준선 모델과 동일한 수준의 성능을 달성하기 위해 라벨링된 예제의 약 15분의 1만 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "563-11",
                    "sentence": "These experiments consistently prove the strong generalization ability of our proposed framework.",
                    "sentence_kor": "이러한 실험은 제안된 프레임워크의 강력한 일반화 능력을 일관되게 입증한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "564",
            "abstractID": "EMNLP_abs-564",
            "text": [
                {
                    "index": "564-0",
                    "sentence": "Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation.",
                    "sentence_kor": "BERT 및 GPT-2와 같은 대규모 사전 교육 언어 모델은 언어 표현 학습 및 자유 형식 텍스트 생성에서 우수한 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "564-1",
                    "sentence": "However, these models cannot be directly employed to generate text under specified lexical constraints.",
                    "sentence_kor": "그러나 이러한 모델은 지정된 어휘 제약 조건 하에서 텍스트를 생성하기 위해 직접 사용할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "564-2",
                    "sentence": "To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation.",
                    "sentence_kor": "이 과제를 해결하기 위해 POINTER(ProOverging INsertion 기반 변환)를 제시한다.ER)는 엄격하게 제한된 텍스트 생성을 위한 단순하지만 새로운 삽입 기반 접근법이다.",
                    "tag": "1+2"
                },
                {
                    "index": "564-3",
                    "sentence": "The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner.",
                    "sentence_kor": "제안된 방법은 병렬 방식으로 기존 토큰 사이에 새 토큰을 점진적으로 삽입하여 작동합니다.",
                    "tag": "3"
                },
                {
                    "index": "564-4",
                    "sentence": "This procedure is recursively applied until a sequence is completed.",
                    "sentence_kor": "이 절차는 시퀀스가 완료될 때까지 반복적으로 적용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "564-5",
                    "sentence": "The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable.",
                    "sentence_kor": "그 결과로 발생하는 거친-미세 계층 구조는 생성 프로세스를 직관적이고 해석할 수 있게 한다.",
                    "tag": "4"
                },
                {
                    "index": "564-6",
                    "sentence": "We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks.",
                    "sentence_kor": "우리는 12GB Wikipedia 데이터 세트에서 제안된 점진적 삽입 기반 목표를 사용하여 모델을 사전 교육하고 다운스트림 하드 제약 생성 작업에서 미세 조정한다.",
                    "tag": "3"
                },
                {
                    "index": "564-7",
                    "sentence": "Non-autoregressive decoding yields a logarithmic time complexity during inference time.",
                    "sentence_kor": "비 자기 회귀 디코딩은 추론 시간 동안 로그 시간 복잡성을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "564-8",
                    "sentence": "Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation.",
                    "sentence_kor": "뉴스와 Yelp 데이터 세트에 대한 실험 결과는 포인터가 제한된 텍스트 생성에서 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "564-9",
                    "sentence": "We released the pre-trained models and the source code to facilitate future research.",
                    "sentence_kor": "우리는 향후 연구를 용이하게 하기 위해 사전 훈련된 모델과 소스 코드를 공개했습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "565",
            "abstractID": "EMNLP_abs-565",
            "text": [
                {
                    "index": "565-0",
                    "sentence": "Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation.",
                    "sentence_kor": "BERT, MASS, BART와 같은 자체 지도 사전 훈련이 자연어 이해와 생성을 위한 강력한 기술로 부상했다.",
                    "tag": "1"
                },
                {
                    "index": "565-1",
                    "sentence": "Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens.",
                    "sentence_kor": "기존의 사전 교육 기법은 자동 인코딩 및/또는 자동 회귀 목표를 사용하여 일부 마스킹된 토큰으로 손상된 텍스트에서 원래 단어 토큰을 복구하여 Transformer 기반 모델을 훈련시킨다.",
                    "tag": "1"
                },
                {
                    "index": "565-2",
                    "sentence": "The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context.",
                    "sentence_kor": "기존 기법의 훈련 목표는 주어진 맥락에서 새로운 텍스트를 생성하기 위한 생성적 질문 답변 및 대화 응답 생성과 같은 많은 언어 생성 작업의 목표와 일치하지 않는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "565-3",
                    "sentence": "This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context.",
                    "sentence_kor": "이 연구는 PALM에게 레이블이 지정되지 않은 대규모 말뭉치에서 자동 인코딩 및 자기 회귀 언어 모델을 공동으로 사전 학습하는 새로운 계획을 제시한다. 이는 문맥에 따라 조건화된 새 텍스트를 생성하도록 특별히 설계된 것이다.",
                    "tag": "1+2"
                },
                {
                    "index": "565-4",
                    "sentence": "The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text.",
                    "sentence_kor": "새로운 계획은 원본 텍스트 재구성 이상의 생성에서 사전 훈련과 미세 조정 사이의 기존 노이즈 제거 체계에서 도입된 불일치를 완화한다.",
                    "tag": "4"
                },
                {
                    "index": "565-5",
                    "sentence": "An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.",
                    "sentence_kor": "광범위한 실험 세트는 PALM이 생성 질문 답변(공식 MARCO 리더보드의 1위), CNN/DailyMail의 추상적 요약, SQuAD의 질문 생성 및 대화 응답 생성을 포함하는 다양한 언어 생성 벤치마크에서 새로운 최첨단 결과를 달성한다는 것을 보여준다. 코넬 영화 대화.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "566",
            "abstractID": "EMNLP_abs-566",
            "text": [
                {
                    "index": "566-0",
                    "sentence": "Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications.",
                    "sentence_kor": "어휘적으로 제한된 생성은 대상 문장이 특정 단어를 포함하거나 주어진 문장의 의역하는 것과 같은 어휘적 제약 조건을 충족하도록 요구하며, 이는 많은 실제 자연어 생성 애플리케이션에서 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "566-1",
                    "sentence": "Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation.",
                    "sentence_kor": "이전 연구는 일반적으로 빔 검색 기반 방법 또는 확률적 검색 방법을 어휘적으로 제한된 생성에 적용한다.",
                    "tag": "1"
                },
                {
                    "index": "566-2",
                    "sentence": "However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution.",
                    "sentence_kor": "그러나 검색 공간이 너무 크면 빔 검색 기반 메서드는 항상 제한된 최적 솔루션을 찾지 못합니다.",
                    "tag": "1"
                },
                {
                    "index": "566-3",
                    "sentence": "At the same time, stochastic search methods always cost too many steps to find the correct optimization direction.",
                    "sentence_kor": "동시에, 확률적 검색 방법은 항상 올바른 최적화 방향을 찾기 위해 너무 많은 단계가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "566-4",
                    "sentence": "In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem.",
                    "sentence_kor": "본 논문에서, 우리는 사전적으로 제한된 생성을 비지도 그레이디언트 유도 최적화 문제로 해결하기 위한 새로운 방법 G2LC를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "566-5",
                    "sentence": "We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word).",
                    "sentence_kor": "우리는 차별화 가능한 목적 함수를 제안하고 구배를 사용하여 순서에서 어떤 위치를 변경해야 하는지 결정한다(삭제 또는 삽입/다른 단어로 대체).",
                    "tag": "2+3"
                },
                {
                    "index": "566-6",
                    "sentence": "The word updating process of the inserted/replaced word also benefits from the guidance of gradient.",
                    "sentence_kor": "삽입/바꾸기 단어의 단어 업데이트 프로세스도 기울기의 지침에서 이점을 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "566-7",
                    "sentence": "Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model.",
                    "sentence_kor": "또한, 우리의 방법은 사전 훈련된 생성 모델의 추론 단계에서 사용할 수 있는 유연한 병렬 데이터 훈련이 없다.",
                    "tag": "3"
                },
                {
                    "index": "566-8",
                    "sentence": "We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation.",
                    "sentence_kor": "우리는 키워드 대 문장 생성과 비지도 패러프레이즈 생성이라는 두 세대 작업에 G2LC를 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "566-9",
                    "sentence": "The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.",
                    "sentence_kor": "실험 결과는 우리의 방법이 이전의 어휘적으로 제한된 방법과 비교하여 최첨단 방법을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "567",
            "abstractID": "EMNLP_abs-567",
            "text": [
                {
                    "index": "567-0",
                    "sentence": "Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps.",
                    "sentence_kor": "교사 강제성으로 훈련된 시퀀스 생성 모델은 시간 단계에 걸친 노출 편향 및 차별화 결여와 관련된 문제로 어려움을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "567-1",
                    "sentence": "Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps.",
                    "sentence_kor": "우리가 제안한 방법인 TeaForN(TeaForN)은 N 예측 단계를 기반으로 모델 매개 변수를 업데이트할 수 있는 2차 시간 축을 따라 디코딩하도록 훈련된 N 디코더 스택을 사용하여 이 두 가지 문제를 직접 해결한다.",
                    "tag": "2+3"
                },
                {
                    "index": "567-2",
                    "sentence": "TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup.",
                    "sentence_kor": "TeaForN은 광범위한 디코더 아키텍처에서 사용할 수 있으며 표준 교사 강제 설정에서 최소한의 수정이 필요합니다.",
                    "tag": "3"
                },
                {
                    "index": "567-3",
                    "sentence": "Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.",
                    "sentence_kor": "경험적으로, 우리는 TeaForN이 하나의 기계 번역 벤치마크, WMT 2014 영어-프랑스어 및 두 개의 뉴스 요약 벤치마크인 CNN/Dailymail 및 Gigaword에서 생성 품질을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "568",
            "abstractID": "EMNLP_abs-568",
            "text": [
                {
                    "index": "568-0",
                    "sentence": "Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces.",
                    "sentence_kor": "텍스트 기반 게임은 자율 에이전트가 자연어로 작동하고 거대한 액션 공간을 처리해야 하는 고유한 과제를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "568-1",
                    "sentence": "In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state.",
                    "sentence_kor": "본 논문에서, 우리는 각 게임 상태에서 콤팩트한 액션 후보 세트를 생성하기 위한 상황별 액션 언어 모델(CALM)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "568-2",
                    "sentence": "Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history.",
                    "sentence_kor": "우리의 핵심 통찰력은 인간 게임플레이에 대한 언어 모델을 훈련시키는 것이다. 여기서 사람들은 언어적 우선 순위와 게임 역사를 조건으로 한 유망한 액션에 대한 일반적인 게임 감각을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "568-3",
                    "sentence": "We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards.",
                    "sentence_kor": "우리는 CALM을 생성된 액션 후보를 재평가하여 게임 내 보상을 극대화하는 강화 학습 에이전트와 결합한다.",
                    "tag": "3"
                },
                {
                    "index": "568-4",
                    "sentence": "We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training.",
                    "sentence_kor": "우리는 훈련 중 CALM에서 보이지 않는 게임에 대해 Jericho 벤치마크를 사용하여 접근 방식을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "568-5",
                    "sentence": "Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model.",
                    "sentence_kor": "우리의 방법은 이전 최첨단 모델에 비해 평균 게임 점수가 69% 향상되었다.",
                    "tag": "4"
                },
                {
                    "index": "568-6",
                    "sentence": "Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions.",
                    "sentence_kor": "놀랍게도 이 게임들 중 절반에서 CALM은 실측 자료로 인정받을 수 있는 조치에 접근할 수 있는 다른 모델들과 경쟁적이거나 더 낫다.",
                    "tag": "4+5"
                },
                {
                    "index": "568-7",
                    "sentence": "Code and data are available at https://github.com/princeton-nlp/calm-textgame.",
                    "sentence_kor": "코드와 데이터는 https://github.com/princeton-nlp/calm-textgame에서 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "569",
            "abstractID": "EMNLP_abs-569",
            "text": [
                {
                    "index": "569-0",
                    "sentence": "Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding.",
                    "sentence_kor": "마스킹 언어 모델, VILBERT, LXMERT 및 UNITER와 같은 비전 및 언어 대응 모델의 성공을 반영하여 시각적 질문 답변 및 시각적 접지와 같은 다양한 다중 모드 차별 작업에서 최첨단 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "569-1",
                    "sentence": "Recent work has also successfully adapted such models towards the generative task of image captioning.",
                    "sentence_kor": "최근 연구는 또한 이러한 모델을 이미지 캡션 생성 작업에 성공적으로 적용했다.",
                    "tag": "1"
                },
                {
                    "index": "569-2",
                    "sentence": "This begs the question: Can these models go the other way and generate images from pieces of text?",
                    "sentence_kor": "이것은 질문을 요구합니다: 이 모델들이 다른 방향으로 나아가서 텍스트 조각으로부터 이미지를 생성할 수 있을까요?",
                    "tag": "1"
                },
                {
                    "index": "569-3",
                    "sentence": "Our analysis of a popular representative from this model family – LXMERT – finds that it is unable to generate rich and semantically meaningful imagery with its current training setup.",
                    "sentence_kor": "이 모델군의 인기 있는 대표자 LXMERT를 분석한 결과, 현재 교육 설정으로는 풍부하고 의미론적으로 의미 있는 이미지를 생성할 수 없는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "569-4",
                    "sentence": "We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint.",
                    "sentence_kor": "우리는 시각적 표현 이산화, 광범위한 마스킹 비율의 균일한 마스킹 사용, 올바른 사전 훈련 데이터 세트 정렬을 포함한 교육 개선 기능을 갖춘 LXMERT의 확장인 X-LXMERT를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "569-5",
                    "sentence": "X-LXMERT’s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT.",
                    "sentence_kor": "X-LXMERT의 이미지 생성 기능은 최첨단 생성 모델의 상태에 필적하는 반면, 질문 답변 및 캡션 기능은 LXMERT에 필적하는 수준으로 남아있다.",
                    "tag": "4"
                },
                {
                    "index": "569-6",
                    "sentence": "Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.",
                    "sentence_kor": "마지막으로, 우리는 X-UNITER를 생성하기 위해 UNITER에 이미지 생성 기능을 추가하여 이러한 훈련 개선의 일반성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "570",
            "abstractID": "EMNLP_abs-570",
            "text": [
                {
                    "index": "570-0",
                    "sentence": "In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering.",
                    "sentence_kor": "본 논문에서는 다중 홉 질문 답변을 위한 계층적 그래프 네트워크(HGN)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "570-1",
                    "sentence": "To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders.",
                    "sentence_kor": "여러 단락에 걸쳐 흩어져 있는 텍스트에서 단서를 수집하기 위해, 계층 그래프는 사전 훈련된 상황별 인코더로 초기화된 다양한 수준의 세분성(질문, 단락, 문장, 엔티티)에 노드를 구성하여 작성된다.",
                    "tag": "3"
                },
                {
                    "index": "570-2",
                    "sentence": "Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction).",
                    "sentence_kor": "이 계층적 그래프가 주어지면, 초기 노드 표현은 그래프 전파를 통해 업데이트되며, 다중 홉 추론은 각 후속 하위 작업(예: 단락 선택, 지원 사실 추출, 답변 예측)에 대한 그래프 가장자리 통과를 통해 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "570-3",
                    "sentence": "By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously.",
                    "sentence_kor": "이기종 노드를 통합 그래프로 엮어 노드 세분성의 계층적 차별화를 통해 HGN은 다른 질문 답변 하위 작업을 동시에 지원할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "570-4",
                    "sentence": "Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.",
                    "sentence_kor": "HotpotQA 벤치마크에 대한 실험은 제안된 모델이 기존의 멀티홉 QA 접근 방식을 능가하는 새로운 최첨단 모델을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "571",
            "abstractID": "EMNLP_abs-571",
            "text": [
                {
                    "index": "571-0",
                    "sentence": "Has there been real progress in multi-hop question-answering?",
                    "sentence_kor": "멀티홉 질의응답에 진정한 진전이 있었는가?",
                    "tag": "1"
                },
                {
                    "index": "571-1",
                    "sentence": "Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts.",
                    "sentence_kor": "모델들은 종종 데이터셋 아티팩트를 이용하여 여러 입증 사실에 걸쳐 정보를 연결하지 않고 정답을 도출합니다.",
                    "tag": "1"
                },
                {
                    "index": "571-2",
                    "sentence": "This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets.",
                    "sentence_kor": "이로 인해 실제 진행 상황을 측정하는 능력이 제한되고 다중 홉 QA 데이터 세트를 구축하는 목적이 상실된다.",
                    "tag": "1"
                },
                {
                    "index": "571-3",
                    "sentence": "We make three contributions towards addressing this.",
                    "sentence_kor": "우리는 이 문제를 해결하기 위해 세 가지 기여를 합니다.",
                    "tag": "1"
                },
                {
                    "index": "571-4",
                    "sentence": "First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts.",
                    "sentence_kor": "첫째, 우리는 뒷받침하는 사실의 하위 집합에 걸쳐 연결이 끊긴 추론과 같은 바람직하지 않은 행동을 공식화한다.",
                    "tag": "2"
                },
                {
                    "index": "571-5",
                    "sentence": "This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning.",
                    "sentence_kor": "이를 통해 단절된 추론을 통해 모델이 얼마나 속임수를 쓸 수 있는지 측정하기 위한 모델 애그노스틱 프로브를 개발할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "571-6",
                    "sentence": "Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning.",
                    "sentence_kor": "둘째, 대비 지원 충분성의 개념을 사용하여 연결이 끊긴 추론의 양을 줄이는 기존 데이터 세트의 자동 변환을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "571-7",
                    "sentence": "Third, our experiments suggest that there hasn’t been much progress in multi-hop QA in the reading comprehension setting.",
                    "sentence_kor": "셋째, 우리의 실험은 독해 환경에서 멀티홉 QA에 큰 진전이 없음을 시사한다.",
                    "tag": "3"
                },
                {
                    "index": "571-8",
                    "sentence": "For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline.",
                    "sentence_kor": "최근의 대규모 모델(XLNet)의 경우, 핫팟QA에 대한 답변 F1 점수 72점 중 18점 만이 다단계 추론을 통해 얻어지는 것으로 나타나는데, 이는 단순한 RNN 기준선과 거의 같다.",
                    "tag": "3"
                },
                {
                    "index": "571-9",
                    "sentence": "Our transformation substantially reduces disconnected reasoning (19 points in answer F1).",
                    "sentence_kor": "우리의 변환은 단절된 추론(응답 F1의 19점)을 상당히 감소시킨다.",
                    "tag": "4"
                },
                {
                    "index": "571-10",
                    "sentence": "It is complementary to adversarial approaches, yielding further reductions in conjunction.",
                    "sentence_kor": "이는 적대적 접근방식을 보완하여 연계하여 추가 감소를 초래한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "572",
            "abstractID": "EMNLP_abs-572",
            "text": [
                {
                    "index": "572-0",
                    "sentence": "We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering.",
                    "sentence_kor": "우리는 어려운 질문을 기존 QA 시스템이 대답할 수 있는 더 간단한 하위 질문으로 분해하여 질문 답변(QA)을 개선하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "572-1",
                    "sentence": "Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet.",
                    "sentence_kor": "분해와 함께 질문에 라벨을 붙이는 것은 번거롭기 때문에, 우리는 하위 질문을 생성하기 위해 감독되지 않은 접근법을 취하여 인터넷에서 수백만 개의 질문을 활용할 수 있게 한다.",
                    "tag": "1+2"
                },
                {
                    "index": "572-2",
                    "sentence": "Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions.",
                    "sentence_kor": "특히, 우리는 하나의 어려운 다중 홉 질문을 많은 단순 단일 홉 하위 질문에 매핑하는 방법을 배우는 ONUS(One-to-N Unsuped Sequence Transference) 알고리즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "572-3",
                    "sentence": "We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer.",
                    "sentence_kor": "우리는 기성 QA 모델로 하위 질문에 답하고 최종 답변으로 결합하는 재구성 모델에 결과 답변을 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "572-4",
                    "sentence": "We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets.",
                    "sentence_kor": "원래 도메인 외부 및 다중 홉 개발 세트의 강력한 기준선에 비해 HotpotQA에서 대규모 QA가 개선되었음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "572-5",
                    "sentence": "ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency.",
                    "sentence_kor": "ONUS는 QA에 대한 감독 및 휴리스틱 분해 방법의 효용성을 일치시키고 유창하게 이러한 방법을 초과하면서 다양한 종류의 질문을 분해하는 방법을 자동으로 학습한다.",
                    "tag": "4"
                },
                {
                    "index": "572-6",
                    "sentence": "Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.",
                    "sentence_kor": "질적으로, 우리는 QA 시스템이 예측을 하는 이유를 밝히는 데 하위 질문을 사용하는 것이 유망하다는 것을 발견한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "573",
            "abstractID": "EMNLP_abs-573",
            "text": [
                {
                    "index": "573-0",
                    "sentence": "This work deals with the challenge of learning and reasoning over multi-hop question answering (QA).",
                    "sentence_kor": "이 작업은 다중 홉 질문 답변(QA)에 대한 학습 및 추론 과제를 다룬다.",
                    "tag": "1"
                },
                {
                    "index": "573-1",
                    "sentence": "We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly.",
                    "sentence_kor": "단락 간 추론 경로를 학습하고 뒷받침하는 사실과 답을 공동으로 찾기 위해 문장의 의미 구조를 기반으로 한 그래프 추론 네트워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "573-2",
                    "sentence": "The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges.",
                    "sentence_kor": "제안된 그래프는 유형 문장(질문, 제목 및 기타 문장)의 노드를 포함하고, 노드로서 인수를 포함하며 가장자리로 술어를 포함하는 문장당 의미 역할 라벨링 하위 그래프를 포함하는 이질적인 문서 수준 그래프이다.",
                    "tag": "3"
                },
                {
                    "index": "573-3",
                    "sentence": "Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths.",
                    "sentence_kor": "SRL 술어에서 유래한 인수의 유형, 인수 구문 및 에지의 의미를 그래프 인코더에 통합하면 추론 경로의 설명 및 찾기에 도움이 된다.",
                    "tag": "3"
                },
                {
                    "index": "573-4",
                    "sentence": "Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.",
                    "sentence_kor": "우리가 제안한 접근 방식은 최신 모델에 비해 HotpotQA 산만기 설정 벤치마크에서 경쟁력 있는 성능을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "574",
            "abstractID": "EMNLP_abs-574",
            "text": [
                {
                    "index": "574-0",
                    "sentence": "Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content.",
                    "sentence_kor": "트윗에 대한 감정 분류는 종종 저특이성, 소음 및 다국어 콘텐츠의 문제를 처리해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "574-1",
                    "sentence": "This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues.",
                    "sentence_kor": "이 연구는 트윗의 다중 표현을 생성하고 위의 문제를 해결하기 위해 이질적인 다중 계층 네트워크 기반 표현을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "574-2",
                    "sentence": "The generated representations are further ensembled and classified using a neural-based early fusion approach.",
                    "sentence_kor": "생성된 표현은 신경 기반 조기 융합 접근방식을 사용하여 추가로 앙상블화 및 분류된다.",
                    "tag": "3"
                },
                {
                    "index": "574-3",
                    "sentence": "Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network.",
                    "sentence_kor": "또한 다중 계층 네트워크에 적합한 노드 임베딩 및 트윗 표현을 위한 중심성 인식 랜덤 워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "574-4",
                    "sentence": "From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts.",
                    "sentence_kor": "다양한 실험 분석을 통해 제안된 방법이 트윗에 존재하는 저특이성, 잡음이 많은 텍스트 및 다국어 콘텐츠 문제를 다룰 수 있으며 텍스트 기반 콘텐츠보다 더 나은 분류 성능을 제공한다는 것이 명백하다.",
                    "tag": "4"
                },
                {
                    "index": "574-5",
                    "sentence": "Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.",
                    "sentence_kor": "또한, 제안된 중심성 인식 기반 무작위 보행은 편견 없는 다른 편향된 상대보다 더 나은 표현을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "575",
            "abstractID": "EMNLP_abs-575",
            "text": [
                {
                    "index": "575-0",
                    "sentence": "Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence.",
                    "sentence_kor": "대상 의견 단어 추출(TOWE)은 문장에서 주어진 측면 용어에 대한 의견 단어를 찾는 것을 목표로 하는 측면 기반 정서 분석(ABSA)의 하위 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "575-1",
                    "sentence": "Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research.",
                    "sentence_kor": "TOWE에 대한 성공에도 불구하고, 현재의 딥러닝 모델은 이전 연구에서 TOWE에 유용한 것으로 입증된 문장의 구문 정보를 이용하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "575-2",
                    "sentence": "In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words.",
                    "sentence_kor": "본 연구에서는 구문 기반 의견 가능성 점수와 단어 사이의 구문 연결을 활용하여 TOWE의 딥 러닝 모델에 문장의 구문 구조를 통합할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "575-3",
                    "sentence": "We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE.",
                    "sentence_kor": "또한 TOWE에서 단어 간의 표현 구분을 기반으로 딥 러닝 모델의 성능을 개선하기 위한 새로운 정규화 기법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "575-4",
                    "sentence": "The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.",
                    "sentence_kor": "제안된 모델은 광범위하게 분석되어 4개의 벤치마크 데이터 세트에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "576",
            "abstractID": "EMNLP_abs-576",
            "text": [
                {
                    "index": "576-0",
                    "sentence": "Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly.",
                    "sentence_kor": "공감 반응 생성에 대한 현재 접근법은 입력 텍스트에 표현된 감정 집합을 모든 감정이 균일하게 처리되는 평평한 구조로 본다.",
                    "tag": "1"
                },
                {
                    "index": "576-1",
                    "sentence": "We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content.",
                    "sentence_kor": "우리는 공감적 반응이 종종 긍정성 또는 부정성과 내용에 따라 사용자의 감정을 다양하게 모방한다고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "576-2",
                    "sentence": "We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art.",
                    "sentence_kor": "우리는 이러한 극성 기반 감정 군집과 감정 모방에 대한 고려가 최첨단 기술에 비해 반응의 공감 및 맥락 관련성을 향상시킨다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "576-3",
                    "sentence": "Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work.",
                    "sentence_kor": "또한, 우리는 이전 작업보다 감정적으로 더 다양한 공감 반응을 생성하는 감정 혼합에 확률성을 도입한다.",
                    "tag": "2"
                },
                {
                    "index": "576-4",
                    "sentence": "We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations.",
                    "sentence_kor": "우리는 자동 및 인간 기반 평가를 모두 사용하여 공감 반응 생성에 이러한 요인의 중요성을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "576-5",
                    "sentence": "The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.",
                    "sentence_kor": "MIME 구현은 https://github.com/declare-lab/MIME에서 공개적으로 이용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "577",
            "abstractID": "EMNLP_abs-577",
            "text": [
                {
                    "index": "577-0",
                    "sentence": "In this work, we aim at equipping pre-trained language models with structured knowledge.",
                    "sentence_kor": "본 연구에서는 사전 훈련된 언어 모델에 체계적인 지식을 갖추는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "577-1",
                    "sentence": "We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.",
                    "sentence_kor": "지식 그래프의 지침을 사용하여 원시 텍스트를 통해 학습하는 두 가지 자체 감독 과제를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "577-2",
                    "sentence": "Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text.",
                    "sentence_kor": "엔티티 레벨 마스킹 언어 모델을 기반으로 하여, 우리의 첫 번째 기여는 텍스트의 기반이 되는 관계적 지식을 활용하는 엔티티 마스킹 체계이다.",
                    "tag": "3"
                },
                {
                    "index": "577-3",
                    "sentence": "This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions.",
                    "sentence_kor": "이는 연결된 지식 그래프를 사용하여 유용한 엔터티를 선택한 다음 해당 엔터티의 언급을 마스킹함으로써 달성됩니다.",
                    "tag": "3"
                },
                {
                    "index": "577-4",
                    "sentence": "In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model.",
                    "sentence_kor": "또한 지식 그래프를 사용하여 마스킹된 엔터티에 대한 산만기를 얻고 마스킹된 언어 모델과 공동으로 최적화된 새로운 산만기 억제 순위 목표를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "577-5",
                    "sentence": "In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text.",
                    "sentence_kor": "기존 패러다임과는 대조적으로, 우리의 접근 방식은 사전 훈련 중에만 지식 그래프를 암묵적으로 사용하여 원시 텍스트로부터의 학습을 통해 구조화된 지식을 가진 언어 모델을 주입한다.",
                    "tag": "3"
                },
                {
                    "index": "577-6",
                    "sentence": "It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples.",
                    "sentence_kor": "이는 미세 조정 및 추론 중에 개체 연결과 통합을 수행하는 검색 기반 방법보다 더 효율적이며 연결된 그래프 3중으로 직접 학습하는 방법보다 더 효과적으로 일반화된다.",
                    "tag": "4"
                },
                {
                    "index": "577-7",
                    "sentence": "Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.",
                    "sentence_kor": "실험에 따르면 제안된 모델은 질문 답변 및 지식 기반 완료를 포함한 5가지 벤치마크에서 향상된 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "578",
            "abstractID": "EMNLP_abs-578",
            "text": [
                {
                    "index": "578-0",
                    "sentence": "Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.",
                    "sentence_kor": "심층 신경 네트워크 모델은 명명된 개체 인식이 수공예 기능 없이도 놀라운 성능을 달성하는 데 도움이 되었다.",
                    "tag": "1"
                },
                {
                    "index": "578-1",
                    "sentence": "However, existing systems require large amounts of human annotated training data.",
                    "sentence_kor": "그러나 기존 시스템에는 대량의 인적 주석이 달린 교육 데이터가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "578-2",
                    "sentence": "Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources.",
                    "sentence_kor": "인간 주석을 외부 지식(예: NE 사전, 음성 일부 태그)으로 대체하려는 노력이 이루어졌으며, 이와 같은 효과적인 자원을 확보하는 것도 또 다른 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "578-3",
                    "sentence": "In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.",
                    "sentence_kor": "본 연구에서는 사전 훈련된 단어 임베딩에서 정보 힌트만 얻으면 되는 완전히 감독되지 않은 NE 인식 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "578-4",
                    "sentence": "We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks.",
                    "sentence_kor": "먼저 개체 범위 감지 및 유형 예측을 위해 단어 임베딩에 가우스 은닉 마르코프 모델과 심층 자동 인코딩 가우스 혼합 모델을 적용한 다음 강화 학습을 기반으로 인스턴스 선택기를 추가로 설계하여 긍정적인 문장과 노이즈가 많은 문장을 구분하고 신경계를 통해 이러한 거친 주석을 다듬는다. 네트워크",
                    "tag": "3"
                },
                {
                    "index": "578-5",
                    "sentence": "Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.",
                    "sentence_kor": "두 개의 CoNLL 벤치마크 NER 데이터 세트(CoNLL-2003 영어 데이터 세트 및 CoNLL-2002 스페인어 데이터 세트)에 대한 광범위한 실험은 제안된 라이트 NE 인식 모델이 주석이 달린 어휘 또는 말뭉치를 사용하지 않고도 놀라운 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "579",
            "abstractID": "EMNLP_abs-579",
            "text": [
                {
                    "index": "579-0",
                    "sentence": "Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications.",
                    "sentence_kor": "현재 텍스트 분류 방법은 일반적으로 훈련 데이터로 많은 수의 인간 라벨 문서가 필요하며, 실제 애플리케이션에서 비용이 많이 들고 얻기 어려울 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "579-1",
                    "sentence": "Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified.",
                    "sentence_kor": "인간은 라벨이 붙은 예를 보지 않고 분류를 수행할 수 있지만 분류될 범주를 설명하는 작은 단어 집합에만 기초한다.",
                    "tag": "1"
                },
                {
                    "index": "579-2",
                    "sentence": "In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents.",
                    "sentence_kor": "본 논문에서는 라벨링된 문서를 사용하지 않고 라벨링되지 않은 데이터에 대한 분류 모델을 훈련하기 위해 각 클래스의 라벨 이름만 사용할 수 있는 가능성을 살펴본다.",
                    "tag": "1+2"
                },
                {
                    "index": "579-3",
                    "sentence": "We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification.",
                    "sentence_kor": "사전 훈련된 신경 언어 모델을 범주 이해를 위한 일반 언어 지식 소스와 문서 분류를 위한 표현 학습 모델로 모두 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "579-4",
                    "sentence": "Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training.",
                    "sentence_kor": "우리의 방법 (1)은 의미론적으로 관련된 단어를 라벨 이름과 연관시키고 (2) 범주 표시 단어를 찾아 암시적인 범주를 예측하도록 모델을 훈련시키며 (3) 자가 훈련을 통해 모델을 일반화한다.",
                    "tag": "3"
                },
                {
                    "index": "579-5",
                    "sentence": "We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.",
                    "sentence_kor": "우리는 우리 모델이 라벨링된 문서를 사용하지 않고 라벨링되지 않은 데이터로부터 학습하는 항목 및 감정 분류를 포함한 4개의 벤치마크 데이터 세트에서 약 90%의 정확도를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "580",
            "abstractID": "EMNLP_abs-580",
            "text": [
                {
                    "index": "580-0",
                    "sentence": "Advances on deep generative models have attracted significant research interest in neural topic modeling.",
                    "sentence_kor": "심층 생성 모델의 발전은 신경 주제 모델링에 대한 상당한 연구 관심을 끌었다.",
                    "tag": "1"
                },
                {
                    "index": "580-1",
                    "sentence": "The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics.",
                    "sentence_kor": "최근 제안된 적대적-신경적 토픽 모델은 잠재 주제의 의미 패턴을 포착하기 전에 적대적으로 훈련된 발전기 네트워크를 사용하여 주제를 모델링하고 디리클레를 채택한다.",
                    "tag": "1"
                },
                {
                    "index": "580-2",
                    "sentence": "It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels.",
                    "sentence_kor": "일관성 있는 주제를 발견하는 데 효과적이지만 주어진 문서에 대한 주제 분포를 추론하거나 사용 가능한 문서 레이블을 활용할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "580-3",
                    "sentence": "To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT.",
                    "sentence_kor": "이러한 한계를 극복하기 위해 ToMCAT(Cycle-consistent Adversarial Training) 및 감독 버전 sToMCAT를 통한 토픽 모델링을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "580-4",
                    "sentence": "ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.",
                    "sentence_kor": "ToMCAT는 항목을 해석하기 위해 제너레이터 네트워크를 사용하고 문서 항목을 추론하기 위해 인코더 네트워크를 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "580-5",
                    "sentence": "Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other.",
                    "sentence_kor": "적대적 훈련과 주기 일치 제약 조건은 발전기와 인코더가 서로 조정되는 실제 샘플을 생산하도록 장려하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "580-6",
                    "sentence": "sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics.",
                    "sentence_kor": "sToMCAT는 주제 모델링 프로세스에 문서 레이블을 통합하여 ToMCAT를 확장하여 보다 일관된 주제를 찾을 수 있도록 지원합니다.",
                    "tag": "4"
                },
                {
                    "index": "580-7",
                    "sentence": "The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification.",
                    "sentence_kor": "제안된 모델의 효과는 감독/감독되지 않은 주제 모델링 및 텍스트 분류에서 평가된다.",
                    "tag": "4"
                },
                {
                    "index": "580-8",
                    "sentence": "The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.",
                    "sentence_kor": "실험 결과는 우리 모델이 일관성 있고 유익한 주제를 모두 생산할 수 있다는 것을 보여주며, 많은 경쟁 기준을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "581",
            "abstractID": "EMNLP_abs-581",
            "text": [
                {
                    "index": "581-0",
                    "sentence": "In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time.",
                    "sentence_kor": "본 논문에서, 우리는 게시 시간에 따라 일련의 게시물로 구성된 이벤트 수준의 소셜 미디어에서 에 대한 자동 루머 감지를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "581-1",
                    "sentence": "It is common that the state of an event is dynamically evolving.",
                    "sentence_kor": "이벤트 상태는 동적으로 진화하는 것이 일반적입니다.",
                    "tag": "3"
                },
                {
                    "index": "581-2",
                    "sentence": "However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event’s life cycle.",
                    "sentence_kor": "그러나 이 과제에 대한 기존 방법의 대부분은 이 문제를 무시하고 사건 수명 주기의 모든 게시물을 기반으로 글로벌 표현을 확립했다.",
                    "tag": "1"
                },
                {
                    "index": "581-3",
                    "sentence": "Such coarse-grained methods failed to capture the event’s unique features in different states.",
                    "sentence_kor": "이러한 거친 방법은 서로 다른 상태에서 이벤트의 고유한 특징을 캡처하지 못했습니다.",
                    "tag": "1"
                },
                {
                    "index": "581-4",
                    "sentence": "To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation.",
                    "sentence_kor": "이러한 한계를 해결하기 위해 세분화된 이벤트 상태 감지 및 세분화를 기반으로 루머 탐지를 위한 상태 독립적이고 시간 진화하는 네트워크(STN)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "581-5",
                    "sentence": "Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events.",
                    "sentence_kor": "일련의 게시물로 구성된 이벤트가 주어지면 STN은 먼저 해당 상태 시퀀스를 예측하고 이벤트를 여러 상태 독립 하위 이벤트로 분할한다.",
                    "tag": "3"
                },
                {
                    "index": "581-6",
                    "sentence": "For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction.",
                    "sentence_kor": "각 하위 이벤트에 대해 STN은 해당 하위 이벤트에 대한 특징 표현을 학습하도록 인코더를 독립적으로 훈련시키고 루머 예측을 위해 현재 하위 이벤트의 표현을 이전 이벤트와 점진적으로 융합한다.",
                    "tag": "3"
                },
                {
                    "index": "581-7",
                    "sentence": "This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection.",
                    "sentence_kor": "이 프레임워크는 초기 단계에서 이벤트 표현을 보다 정확하게 학습할 수 있으며 조기 소문 탐지를 가능하게 한다.",
                    "tag": "4"
                },
                {
                    "index": "581-8",
                    "sentence": "Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems.",
                    "sentence_kor": "두 벤치마크 데이터 세트에 대한 실험에 따르면 STN은 일부 강력한 기준 시스템과 비교하여 소문 탐지 정확도를 크게 향상시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "581-9",
                    "sentence": "We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.",
                    "sentence_kor": "또한 STN이 비교에서 더 높은 이점을 보여주는 초기 소문 감지 성능을 측정하기 위한 새로운 평가 메트릭을 설계한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "582",
            "abstractID": "EMNLP_abs-582",
            "text": [
                {
                    "index": "582-0",
                    "sentence": "Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding.",
                    "sentence_kor": "소스 코드와 자연어를 동시에 모델링하는 것은 자동화된 소프트웨어 개발과 이해에 있어 많은 흥미로운 응용 프로그램들을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "582-1",
                    "sentence": "Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style.",
                    "sentence_kor": "이러한 기술을 달성하기 위해, 우리는 파이썬 방법 텍스트 대 텍스트 전송 변압기인 PyMT5를 소개한다. 이 변환기는 파이썬 방법 기능 조합의 모든 쌍, 즉 자연어 문서 문자열(도크스트링)에서 전체 방법을 예측하고 코드를 임의의 c의 문서 문자열로 요약할 수 있는 단일 모델 사이에서 변환하도록 훈련된다.옴몬 스타일",
                    "tag": "1+2"
                },
                {
                    "index": "582-2",
                    "sentence": "We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized.",
                    "sentence_kor": "우리는 2,600만 파이썬 방법과 770만 개의 메소드-닥스트링 쌍으로 구성된 대규모 병렬 말뭉치의 분석 및 모델링 노력을 제시하며, 문서열 및 방법 생성의 경우 PyMT5가 영어 사전 교육되거나 임의로 초기화된 유사한 크기의 자동 회귀 언어 모델(GPT2)을 능가한다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "582-3",
                    "sentence": "On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.",
                    "sentence_kor": "CodeSearchNet 테스트 세트에서 우리의 최고 모델은 구문적으로 올바른 방법 본문을 92.1% 예측하고, 방법 생성의 경우 BLEU 점수 8.59점, 문서 문자열 생성(요약화)의 경우 16.3점을 달성했으며, 방법 생성의 경우 ROUGE-L F-점수 24.8점, 문서 문자열 생성의 경우 36.7점을 달성했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "583",
            "abstractID": "EMNLP_abs-583",
            "text": [
                {
                    "index": "583-0",
                    "sentence": "Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information.",
                    "sentence_kor": "질문 생성을 위한 기존 연구는 입력 텍스트를 팩트 정보를 명시적으로 모델링하지 않고 토큰 시퀀스로 인코딩한다.",
                    "tag": "1"
                },
                {
                    "index": "583-1",
                    "sentence": "These models tend to generate irrelevant and uninformative questions.",
                    "sentence_kor": "이러한 모델들은 관련이 없고 유익한 질문을 유발하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "583-2",
                    "sentence": "In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way.",
                    "sentence_kor": "본 논문에서 우리는 질문 생성을 위해 텍스트에 사실을 포괄적으로 통합하는 방법을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "583-3",
                    "sentence": "We present a novel task of question generation given a query path in the knowledge graph constructed from the input text.",
                    "sentence_kor": "입력 텍스트로 구성된 지식 그래프에서 쿼리 경로가 주어진 질문 생성이라는 새로운 작업을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "583-4",
                    "sentence": "We divide the task into two steps, namely, query representation learning and query-based question generation.",
                    "sentence_kor": "우리는 과제를 두 단계, 즉 질의 표현 학습과 질의 기반 질문 생성으로 나눈다.",
                    "tag": "3"
                },
                {
                    "index": "583-5",
                    "sentence": "We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation.",
                    "sentence_kor": "쿼리를 형성하기 위해 관련 사실을 식별하기 위한 시퀀스 라벨링 문제로 쿼리 표현 학습을 공식화하고 질문 생성을 위해 RNN 기반 생성기를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "583-6",
                    "sentence": "We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework.",
                    "sentence_kor": "우리는 먼저 두 모듈을 엔드투엔드 방식으로 공동으로 교육하고, 변형 프레임워크에서 이 두 모듈 사이의 상호작용을 추가로 시행한다.",
                    "tag": "3"
                },
                {
                    "index": "583-7",
                    "sentence": "We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex.",
                    "sentence_kor": "우리는 SQuAD 위에 실험 데이터 세트를 구성하며, 결과는 우리 모델이 다른 최첨단 접근 방식을 능가하고 목표 질문이 복잡할 때 성능 마진이 더 크다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "583-8",
                    "sentence": "Human evaluation also proves that our model is able to generate relevant and informative questions.",
                    "sentence_kor": "인간 평가는 또한 우리의 모델이 적절하고 유익한 질문을 생성할 수 있다는 것을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "584",
            "abstractID": "EMNLP_abs-584",
            "text": [
                {
                    "index": "584-0",
                    "sentence": "Recognizing the flow of time in a story is a crucial aspect of understanding it.",
                    "sentence_kor": "이야기에서 시간의 흐름을 인식하는 것은 그것을 이해하는 데 있어 중요한 측면이다.",
                    "tag": "1"
                },
                {
                    "index": "584-1",
                    "sentence": "Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases.",
                    "sentence_kor": "시간과 관련된 이전 작업은 주로 사건의 시간적 표현이나 상대적인 시퀀싱을 식별하는 데 초점을 맞추었지만, 여기서는 명시적인 시간 설명 문구가 없는 경우에도 벽시계 시간으로 책의 각 라인에 계산적으로 주석을 달 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "584-2",
                    "sentence": "To do so, we construct a data set of hourly time phrases from 52,183 fictional books.",
                    "sentence_kor": "그렇게 하기 위해, 우리는 52,183권의 허구적인 책에서 시간당 구절의 데이터 세트를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "584-3",
                    "sentence": "We then construct a time-of-day classification model that achieves an average error of 2.27 hours.",
                    "sentence_kor": "그런 다음 평균 오차 2.27시간을 달성하는 일일 시간 분류 모델을 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "584-4",
                    "sentence": "Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day.",
                    "sentence_kor": "또한, 우리는 중단점의 동적 프로그래밍을 사용하여 책을 전체적으로 분석함으로써 책을 대략적으로 특정 시간에 해당하는 세그먼트로 분할할 수 있음을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "584-5",
                    "sentence": "This approach improves upon baselines by over two hour.",
                    "sentence_kor": "이 접근 방식은 기준선을 기준으로 2시간 이상 개선됩니다.",
                    "tag": "4"
                },
                {
                    "index": "584-6",
                    "sentence": "Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past.",
                    "sentence_kor": "마지막으로, 우리는 과거 시간 활동의 흥미로운 추세를 보여주기 위해 역사상 다른 기간별로 분류된 문헌의 말뭉치에 우리의 모델을 적용한다.",
                    "tag": "4"
                },
                {
                    "index": "584-7",
                    "sentence": "Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.",
                    "sentence_kor": "여러 관측 결과 중에서 10P 이후에 발생하는 사건의 비율을 알 수 있다.M은 전구와 도시 조명의 출현과 동시에 1880년을 뛰어넘었다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "585",
            "abstractID": "EMNLP_abs-585",
            "text": [
                {
                    "index": "585-0",
                    "sentence": "Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts.",
                    "sentence_kor": "자연어는 구성성으로 특징지어진다: 복잡한 표현의 의미는 구성 부분의 의미로부터 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "585-1",
                    "sentence": "To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English.",
                    "sentence_kor": "언어 처리 아키텍처의 구성 능력 평가를 용이하게 하기 위해 영어 일부를 기반으로 한 의미 구문 분석 데이터 세트인 COGS를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "585-2",
                    "sentence": "The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures.",
                    "sentence_kor": "COGS의 평가 부분에는 구성 일반화에 의해서만 해결될 수 있는 여러 체계적인 격차가 포함되어 있다. 여기에는 익숙한 구문 구조의 새로운 조합 또는 익숙한 단어와 익숙한 구조의 새로운 조합이 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "585-3",
                    "sentence": "In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%).",
                    "sentence_kor": "트랜스포머와 LSTM을 사용한 실험에서 COGS 테스트 세트의 분포 내 정확도는 거의 완벽에 가까웠지만(96-99%) 일반화 정확도는 상당히 낮았고(+-6-8%) 랜덤 시드에 대한 높은 민감도를 보였다.",
                    "tag": "4"
                },
                {
                    "index": "585-4",
                    "sentence": "These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.",
                    "sentence_kor": "이러한 연구 결과는 현대 표준 NLP 모델이 구성 일반화 용량에 한계가 있음을 나타내며 COGS를 진행 상태를 측정하는 좋은 방법으로 배치한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "586",
            "abstractID": "EMNLP_abs-586",
            "text": [
                {
                    "index": "586-0",
                    "sentence": "Pre-trained contextual representations like BERT have achieved great success in natural language processing.",
                    "sentence_kor": "BERT와 같이 사전 훈련된 상황 표현은 자연어 처리에서 큰 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "586-1",
                    "sentence": "However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences.",
                    "sentence_kor": "그러나 미세 조정 없이 사전 훈련된 언어 모델의 문장 임베딩은 문장의 의미 의미를 제대로 포착하지 못하는 것으로 밝혀졌다.",
                    "tag": "1"
                },
                {
                    "index": "586-2",
                    "sentence": "In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited.",
                    "sentence_kor": "본 논문에서 우리는 BERT 임베딩의 의미 정보가 충분히 활용되지 않았다고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "586-3",
                    "sentence": "We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically.",
                    "sentence_kor": "우리는 먼저 마스킹된 언어 모델 사전 훈련 목표와 이론적으로 의미 유사성 작업 사이의 이론적 연관성을 밝힌 다음 BERT 문장 임베딩을 경험적으로 분석한다.",
                    "tag": "1"
                },
                {
                    "index": "586-4",
                    "sentence": "We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity.",
                    "sentence_kor": "우리는 BERT가 항상 문장의 부드러운 이방성 의미 공간을 유도하여 의미 유사성의 성능을 손상시킨다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "586-5",
                    "sentence": "To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective.",
                    "sentence_kor": "이 문제를 해결하기 위해 비지도 목표로 학습한 흐름을 정규화하여 비등방성 문장 임베딩 분포를 매끄럽고 등방성 가우스 분포로 변환할 것을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "586-6",
                    "sentence": "Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.",
                    "sentence_kor": "실험 결과에 따르면 우리가 제안한 BERT-흐름 방법은 다양한 의미론적 텍스트 유사성 작업에서 최첨단 문장 임베딩에 비해 상당한 성능 향상을 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "586-7",
                    "sentence": "The code is available at https://github.com/bohanli/BERT-flow.",
                    "sentence_kor": "코드는 https://github.com/bohanli/BERT-flow에서 이용할 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "587",
            "abstractID": "EMNLP_abs-587",
            "text": [
                {
                    "index": "587-0",
                    "sentence": "Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens.",
                    "sentence_kor": "신경 언어 모델은 종종 최대우도 추정(MLE)으로 훈련되며, 여기서 다음 단어는 실제 단어 토큰에서 조건부로 생성된다.",
                    "tag": "1"
                },
                {
                    "index": "587-1",
                    "sentence": "During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias.",
                    "sentence_kor": "그러나 시험하는 동안 모델은 대신에 이전에 생성된 토큰을 조건으로 하여 노출 바이어스라고 불리는 결과를 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "587-2",
                    "sentence": "To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes.",
                    "sentence_kor": "훈련과 테스트 사이의 이러한 격차를 줄이기 위해 이 두 모드에서 생성된 시퀀스와 일치하도록 최적의 전송(OT)을 사용할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "587-3",
                    "sentence": "We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation.",
                    "sentence_kor": "우리는 모방 학습 해석을 통해 훈련 중에 학생 강제 계획을 추가할 필요성을 검토한다.",
                    "tag": "3"
                },
                {
                    "index": "587-4",
                    "sentence": "An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences.",
                    "sentence_kor": "텍스트 시퀀스의 구조 및 상황 정보를 기반으로 긴 시퀀스에 대한 OT 학습을 개선하기 위한 확장이 추가로 제안된다.",
                    "tag": "2+3"
                },
                {
                    "index": "587-5",
                    "sentence": "The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.",
                    "sentence_kor": "제안된 방법의 효과는 기계 번역, 텍스트 요약 및 텍스트 생성 작업에서 검증된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "588",
            "abstractID": "EMNLP_abs-588",
            "text": [
                {
                    "index": "588-0",
                    "sentence": "Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references.",
                    "sentence_kor": "기존 참조 메트릭스(예: BLEU 및 MoverScore)의 성공에도 불구하고, 악명 높은 일대다 문제 때문에 스토리 또는 대화 상자 생성을 포함한 개방형 텍스트 생성에 대한 인간의 판단과 잘 상관되지 않는다. 동일한 입력에는 문자 그대로 또는 의미론에서 크게 다를 수 있는 많은 그럴듯한 출력이 있다.e 지정된 참조 수가 제한됩니다.",
                    "tag": "1"
                },
                {
                    "index": "588-1",
                    "sentence": "To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference.",
                    "sentence_kor": "이 문제를 완화하기 위해, 우리는 어떠한 참조 없이 생성된 이야기의 품질을 측정하는 Open-eNded 스토리 생성을 평가하기 위한 학습 가능한 UN 참조 metrIC인 UNION을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "588-2",
                    "sentence": "Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories.",
                    "sentence_kor": "BERT 위에 구축된 UNION은 인간이 쓴 이야기와 부정적인 샘플을 구별하고 부정적인 이야기의 동요를 복구하도록 훈련 받는다.",
                    "tag": "3"
                },
                {
                    "index": "588-3",
                    "sentence": "We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence.",
                    "sentence_kor": "우리는 반복되는 플롯, 상충되는 논리 및 장거리 일관성을 포함하여 기존 NLG 모델에서 일반적으로 관찰되는 오류를 모방하여 마이너스 샘플을 구성하는 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "588-4",
                    "sentence": "Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.",
                    "sentence_kor": "두 개의 스토리 데이터 세트에 대한 실험은 UNION이 생성된 스토리의 품질을 평가하기 위한 신뢰할 수 있는 척도로, 인간의 판단과 더 잘 상관되며 기존 최첨단 메트릭보다 더 일반화할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "589",
            "abstractID": "EMNLP_abs-589",
            "text": [
                {
                    "index": "589-0",
                    "sentence": "Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive.",
                    "sentence_kor": "신경 텍스트 생성의 최근 발전에도 불구하고, 인간 언어의 풍부한 다양성을 인코딩하는 것은 여전히 이해하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "589-1",
                    "sentence": "We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective.",
                    "sentence_kor": "우리는 차선의 텍스트 생성은 주로 불균형 토큰 분포에 기인하며, 특히 최대 우도 목표로 훈련했을 때 학습 모델을 잘못 지시한다고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "589-2",
                    "sentence": "As a simple yet effective remedy, we propose two novel methods, Fˆ2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution.",
                    "sentence_kor": "간단하지만 효과적인 해결책으로, 왜곡된 주파수 분포에도 균형 잡힌 훈련을 위해 F22-Softmax와 MefMax라는 두 가지 새로운 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "589-3",
                    "sentence": "MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes.",
                    "sentence_kor": "MefMax는 주파수 클래스에 고유하게 토큰을 할당하여 주파수가 유사한 토큰을 그룹화하고 클래스 간에 주파수 질량을 균등화하려고 합니다.",
                    "tag": "3"
                },
                {
                    "index": "589-4",
                    "sentence": "Fˆ2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class.",
                    "sentence_kor": "FΔ2-Softmax는 대상 토큰의 확률 분포를 (1) 주파수 클래스의 두 조건부 확률과 (2) 대상 주파수 클래스의 토큰의 곱으로 분해한다.",
                    "tag": "3"
                },
                {
                    "index": "589-5",
                    "sentence": "Models learn more uniform probability distributions because they are confined to subsets of vocabularies.",
                    "sentence_kor": "모형은 어휘의 부분 집합으로 제한되기 때문에 보다 균일한 확률 분포를 학습합니다.",
                    "tag": "4"
                },
                {
                    "index": "589-6",
                    "sentence": "Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.",
                    "sentence_kor": "7가지 관련 지표에 대한 상당한 성능 이득은 다양성뿐만 아니라 생성된 텍스트의 품질을 개선하는 데 있어 접근 방식의 우위를 시사한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "590",
            "abstractID": "EMNLP_abs-590",
            "text": [
                {
                    "index": "590-0",
                    "sentence": "The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability.",
                    "sentence_kor": "데이터 대 텍스트 작업은 주어진 구조화된 데이터를 설명하기 위해 사람이 읽을 수 있는 텍스트를 생성하여 해석성을 높이는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "590-1",
                    "sentence": "However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain.",
                    "sentence_kor": "그러나 일반적인 생성 작업은 얻기 어렵고 비용이 많이 드는 잘 정렬된 데이터가 필요하기 때문에 몇 가지 특정 도메인에 국한된다.",
                    "tag": "1"
                },
                {
                    "index": "590-2",
                    "sentence": "Using partially-aligned data is an alternative way of solving the dataset scarcity problem.",
                    "sentence_kor": "부분적으로 정렬된 데이터를 사용하는 것이 데이터 세트 부족 문제를 해결하는 대안적인 방법입니다.",
                    "tag": "1+2"
                },
                {
                    "index": "590-3",
                    "sentence": "This kind of data is much easier to obtain since it can be produced automatically.",
                    "sentence_kor": "이러한 종류의 데이터는 자동으로 생성될 수 있기 때문에 훨씬 더 쉽게 얻을 수 있습니다.",
                    "tag": "1+2"
                },
                {
                    "index": "590-4",
                    "sentence": "However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure.",
                    "sentence_kor": "그러나 이러한 종류의 데이터를 사용하는 것은 기존 모델에 어려움을 야기하는 초과 생성 문제를 유발하며, 생성 절차 중에 관련 없는 발췌문을 추가하는 경향이 있다.",
                    "tag": "3"
                },
                {
                    "index": "590-5",
                    "sentence": "In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains.",
                    "sentence_kor": "자동으로 주석이 달린 부분 데이터 세트를 효과적으로 활용하기 위해 기존 생성 작업을 PADTG(Partial-Aligned Data-to-Text Generation)라는 정제된 작업으로 확장하는데, 이 작업은 자동으로 주석이 달린 데이터를 교육에 활용하므로 애플리케이션 도메인을 상당히 확장하기 때문에 더욱 실용적이다.",
                    "tag": "3"
                },
                {
                    "index": "590-6",
                    "sentence": "To tackle this new task, we propose a novel distant supervision generation framework.",
                    "sentence_kor": "이 새로운 과제를 해결하기 위해 우리는 새로운 원격 감독 생성 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "590-7",
                    "sentence": "It firstly estimates the input data’s supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively.",
                    "sentence_kor": "먼저 추정기로 각 대상 단어에 대한 입력 데이터의 지원성을 추정한 다음 지원 어댑터와 균형 잡힌 빔 검색을 적용하여 각각 훈련 및 생성 단계에서 초과 생성 문제를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "590-8",
                    "sentence": "We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata.",
                    "sentence_kor": "또한 부분적으로 정렬된 데이터 세트를 제공한다(이 논문의 데이터와 소스 코드는 Wikipedia의 문장을 샘플링하고 Wikidata의 각 문장에 해당하는 KB 3배를 자동으로 추출하여 https://github.com/fuzihaofzh/distant_supervision_nlg)에서 얻을 수 있다).",
                    "tag": "3+4"
                },
                {
                    "index": "590-9",
                    "sentence": "The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.",
                    "sentence_kor": "실험 결과는 우리의 프레임워크가 모든 기본 모델을 능가할 뿐만 아니라 부분적으로 정렬된 데이터를 활용하는 실현 가능성을 검증한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "591",
            "abstractID": "EMNLP_abs-591",
            "text": [
                {
                    "index": "591-0",
                    "sentence": "Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly.",
                    "sentence_kor": "기존의 페르소나 기반 대화 상자 모델은 인간이 원활하게 수행할 수 있는 특정 페르소나 설명의 단순한 의미를 포착하지 못하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "591-1",
                    "sentence": "For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break.",
                    "sentence_kor": "예를 들어, 하이킹에 대한 관심이 자연에 대한 사랑이나 휴식에 대한 갈망을 의미할 수 있다고 최첨단 모델들은 추론할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "591-2",
                    "sentence": "In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions.",
                    "sentence_kor": "본 논문에서, 우리는 기존의 상식적인 지식 기반을 사용하여 사용 가능한 페르소나 문장을 확장하여 더 풍부하고 확장된 페르소나 설명에 대한 액세스 권한을 가진 대화 상자 모델을 주입할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "591-3",
                    "sentence": "Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response.",
                    "sentence_kor": "또한 대화 응답을 합성하는 동안 모델이 개인 설정 문장 중에서 이산적인 선택을 하도록 장려하여 페르소나에 대한 세분화된 접지를 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "591-4",
                    "sentence": "Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions.",
                    "sentence_kor": "이러한 선택은 데이터에서 관찰되지 않기 때문에, 우리는 이산 잠재 랜덤 변수를 사용하여 이를 모델링하고 수백 개의 페르소나 확장에서 표본을 추출하기 위해 변형 학습을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "591-5",
                    "sentence": "Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.",
                    "sentence_kor": "우리 모델은 대화 상자 품질과 다양성 측면에서 페르소나-챗 데이터 세트의 경쟁 기준을 능가하는 동시에 페르소나 일관성과 제어 가능한 대화 상자 생성을 달성한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "592",
            "abstractID": "EMNLP_abs-592",
            "text": [
                {
                    "index": "592-0",
                    "sentence": "Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems.",
                    "sentence_kor": "구조화된 신념 상태는 작업 지향 대화 시스템에서 사용자 목표 추적 및 데이터베이스 쿼리에 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "592-1",
                    "sentence": "However, training belief trackers often requires expensive turn-level annotations of every user utterance.",
                    "sentence_kor": "그러나 훈련 신념 추적기는 종종 모든 사용자 발언에 대한 값비싼 턴 레벨 주석을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "592-2",
                    "sentence": "In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning.",
                    "sentence_kor": "본 논문에서 우리는 준지도 학습을 위해 라벨이 부착되지 않은 대화 데이터를 활용하여 종단 간 대화 시스템 구축에서 믿음 상태 레이블에 대한 의존도를 완화하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "592-3",
                    "sentence": "We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs.",
                    "sentence_kor": "우리는 LABES(LAtent BELief State) 모델이라고 불리는 확률론적 대화 상자 모델을 제안한다. 여기서 믿음 상태는 이산 잠재 변수로 표현되고 사용자 입력이 주어진 시스템 응답과 공동으로 모델링된다.",
                    "tag": "2+3"
                },
                {
                    "index": "592-4",
                    "sentence": "Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework.",
                    "sentence_kor": "이러한 잠재적 변수 모델링을 통해 원칙적인 변형 학습 프레임워크 하에서 준지도 학습을 개발할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "592-5",
                    "sentence": "Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES.",
                    "sentence_kor": "또한 LAVES의 복사 확장 Seq2Seq 모델 인스턴스화인 LAVES-S2S를 소개한다.",
                    "tag": "4"
                },
                {
                    "index": "592-6",
                    "sentence": "In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales.",
                    "sentence_kor": "감독 실험에서 LAVES-S2S는 서로 다른 규모의 세 가지 벤치마크 데이터 세트에서 강력한 결과를 얻는다.",
                    "tag": "4"
                },
                {
                    "index": "592-7",
                    "sentence": "In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines.",
                    "sentence_kor": "라벨이 부착되지 않은 대화 상자 데이터를 활용할 때 준감독 LAVES-S2S는 감독 전용 및 준감독 기준선을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "592-8",
                    "sentence": "Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.",
                    "sentence_kor": "놀랍게도 MultiWOZ의 성능 손실 없이 주석 수요를 50%까지 줄일 수 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "593",
            "abstractID": "EMNLP_abs-593",
            "text": [
                {
                    "index": "593-0",
                    "sentence": "Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems.",
                    "sentence_kor": "대화 일관성을 자동으로 평가하는 것은 고품질 개방형 도메인 대화 시스템을 개발하기 위해서는 어렵지만 수요가 많은 능력이다.",
                    "tag": "1"
                },
                {
                    "index": "593-1",
                    "sentence": "However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows.",
                    "sentence_kor": "그러나 현재 평가 지표는 대화 흐름의 세분화된 주제 전환 역학을 명시적으로 고려하지 않고 표면 특징이나 발화 수준 의미론만 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "593-2",
                    "sentence": "Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics.",
                    "sentence_kor": "여기서, 우리는 먼저 대화의 주제들로 구성된 그래프 구조가 설득력 있는 지표를 생성하는 보다 자연스러운 방법인 기본적인 의사소통 논리를 정확하게 묘사할 수 있다는 것을 고려한다.",
                    "tag": "1+2"
                },
                {
                    "index": "593-3",
                    "sentence": "Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation.",
                    "sentence_kor": "주제 수준 대화 그래프를 사용하여, 우리는 자동 대화 평가를 위한 그래프 강화 표현을 나타내는 새로운 평가 메트릭 GRADE를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "593-4",
                    "sentence": "Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence.",
                    "sentence_kor": "특히 GRADE는 거친 발화 수준의 상황별 표현과 세분화된 주제 수준 그래프 표현을 모두 통합하여 대화 일관성을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "593-5",
                    "sentence": "The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights.",
                    "sentence_kor": "그래프 표현은 k-홉 인접 표현과 홉-어텐션 가중치를 포함하여 상식 그래프의 증거로 강화된 주제 수준 대화 그래프를 추론하여 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "593-6",
                    "sentence": "Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments.",
                    "sentence_kor": "실험 결과에 따르면 우리의 GRADE는 인간 판단과 피어슨 및 스피어맨 상관관계 측면에서 다양한 대화 모델을 측정하는 데 있어 다른 최첨단 측정 기준을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "593-7",
                    "sentence": "Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.",
                    "sentence_kor": "또한, 우리는 자동 측정 기준에 대한 향후 연구를 용이하게 하기 위해 새로운 대규모 인간 평가 벤치마크를 발표한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "594",
            "abstractID": "EMNLP_abs-594",
            "text": [
                {
                    "index": "594-0",
                    "sentence": "Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior.",
                    "sentence_kor": "기계 생성 합리성의 품질을 평가하기 위한 두 가지 주요 접근법은 1) 인간 합리성을 금본위제로 사용하는 것과 2) 합리성이 모델 행동에 미치는 영향에 기초한 자동화된 측정법이다.",
                    "tag": "1"
                },
                {
                    "index": "594-1",
                    "sentence": "An open question, however, is how human rationales fare with these automatic metrics.",
                    "sentence_kor": "그러나 미해결 질문은 인간이 이러한 자동 측정 기준을 어떻게 사용하는가 하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "594-2",
                    "sentence": "Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics.",
                    "sentence_kor": "다양한 데이터 세트와 모델을 분석하면, 인간의 합리성이 반드시 이러한 메트릭에서 잘 수행되지는 않는다는 것을 알 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "594-3",
                    "sentence": "To unpack this finding, we propose improved metrics to account for model-dependent baseline performance.",
                    "sentence_kor": "이 발견을 풀기 위해 모델 의존적 기준 성능을 고려하여 개선된 측정 기준을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "594-4",
                    "sentence": "We then propose two methods to further characterize rationale quality, one based on model retraining and one on using “fidelity curves” to reveal properties such as irrelevance and redundancy.",
                    "sentence_kor": "그런 다음 이론적 품질을 더욱 특성화하기 위한 두 가지 방법, 즉 모델 재교육을 기반으로 하는 방법과 무관성 및 중복성과 같은 속성을 드러내기 위해 \"신뢰도 곡선\"을 사용하는 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "594-5",
                    "sentence": "Our work leads to actionable suggestions for evaluating and characterizing rationales.",
                    "sentence_kor": "우리의 작업은 합리성을 평가하고 특성화하기 위한 실행 가능한 제안으로 이어진다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "595",
            "abstractID": "EMNLP_abs-595",
            "text": [
                {
                    "index": "595-0",
                    "sentence": "We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization.",
                    "sentence_kor": "신경 추상적 요약을 통해 수천 단어를 초과하는 긴 문서의 추상적 요약을 생성하는 방법을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "595-1",
                    "sentence": "We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary.",
                    "sentence_kor": "요약을 생성하기 전에 간단한 추출 단계를 수행하며, 요약 생성 작업을 수행하기 전에 관련 정보에 대한 변압기 언어 모델을 조건화하는 데 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "595-2",
                    "sentence": "We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores.",
                    "sentence_kor": "우리는 또한 이 접근 방식이 복사 메커니즘을 사용하는 이전 작업과 비교하여 더 높은 ROUGE 점수를 획득하는 더 추상적인 요약을 산출한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "595-3",
                    "sentence": "We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two.",
                    "sentence_kor": "우리는 강력한 기본 방법, 이전 최신 작업 및 변압기, 추출 기법 및 두 가지 조합만을 사용하는 접근방식을 포함한 다양한 변형과 광범위한 비교를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "595-4",
                    "sentence": "We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets.",
                    "sentence_kor": "우리는 arXiv 논문, PubMed 논문, 뉴스룸 및 BigPatent 데이터 세트의 네 가지 요약 작업과 데이터 세트를 사용하여 이러한 모델을 검토한다.",
                    "tag": "1"
                },
                {
                    "index": "595-5",
                    "sentence": "We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts.",
                    "sentence_kor": "변압기 기반 방법은 더 적은 수의 n그램 복사본으로 요약을 생성하여 사람이 생성한 추상 자료와 더 유사한 n그램 복사 통계를 도출한다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "595-6",
                    "sentence": "We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance.",
                    "sentence_kor": "우리는 변압기가 일관성과 유창성 면에서 높은 순위를 차지하지만, 순수 추출 방법은 정보성과 관련성 면에서 더 높은 점수를 받는다는 것을 발견하는 인간 평가를 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "595-7",
                    "sentence": "We hope that these architectures and experiments may serve as strong points of comparison for future work.",
                    "sentence_kor": "우리는 이러한 아키텍처와 실험이 향후 작업에 있어 비교의 강점이 되기를 바란다.",
                    "tag": "1"
                },
                {
                    "index": "595-8",
                    "sentence": "Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.",
                    "sentence_kor": "참고: 위의 추상화는 본 논문의 이전 초안을 바탕으로 저자와 본 논문에 제시된 모델 중 하나가 공동으로 작성했습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "596",
            "abstractID": "EMNLP_abs-596",
            "text": [
                {
                    "index": "596-0",
                    "sentence": "Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE.",
                    "sentence_kor": "사전 훈련된 신경 추상 요약 시스템은 적어도 ROUGE 측면에서 뉴스 요약 성능에 대한 추출 전략을 지배해왔다.",
                    "tag": "1"
                },
                {
                    "index": "596-1",
                    "sentence": "However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text.",
                    "sentence_kor": "그러나 시스템에서 생성된 추상적 요약은 원본 텍스트와 관련하여 잘못된 사실을 생성하는 사실 불일치의 함정에 직면하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "596-2",
                    "sentence": "To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection.",
                    "sentence_kor": "이 과제를 해결하기 위해, 우리는 질문 답변 모델에서 학습한 지식을 활용하여 범위 선택을 통해 시스템 생성 요약을 수정하는 두 가지 사실 수정 모델 제품군인 Span-Fact를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "596-3",
                    "sentence": "Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t.",
                    "sentence_kor": "우리의 모델은 의미적 일관성을 보장하기 위해 실체를 반복적으로 또는 자동으로 대체하는 단일 또는 다중 마스크 전략을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "596-4",
                    "sentence": "the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models.",
                    "sentence_kor": "추상적 요약 모델에 의해 생성된 요약의 구문 구조를 유지하면서 원본 텍스트.",
                    "tag": "3"
                },
                {
                    "index": "596-5",
                    "sentence": "Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.",
                    "sentence_kor": "실험에 따르면 우리 모델은 자동 메트릭과 인간 평가 측면에서 요약 품질을 저하시키지 않고 시스템 생성 요약의 사실적 일관성을 크게 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "597",
            "abstractID": "EMNLP_abs-597",
            "text": [
                {
                    "index": "597-0",
                    "sentence": "The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents.",
                    "sentence_kor": "요약 알고리즘을 평가하기 위한 가장 일반적인 측정 기준은 요약이 실제로 원본 문서와 일치하는지 여부를 설명하지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "597-1",
                    "sentence": "We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.",
                    "sentence_kor": "우리는 사실적 일관성을 검증하고 소스 문서와 생성된 요약 간의 충돌을 식별하기 위해 약하게 감독되는 모델 기반 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "597-2",
                    "sentence": "Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it.",
                    "sentence_kor": "교육 데이터는 소스 문서의 문장에 일련의 규칙 기반 변환을 적용하여 생성된다.그런 다음 사실 일관성 모델을 세 가지 작업에 대해 공동으로 훈련한다. 1) 각 요약 문장이 사실적으로 일관성이 있는지 예측하고 2) 두 경우 모두 소스 문서에서 일관성 예측을 뒷받침하는 범위를 추출하고 3) 일관성이 없는 것으로 간주되는 각 요약 문장에 대해 일관성이 없는 범위를 추출한다.",
                    "tag": "3"
                },
                {
                    "index": "597-3",
                    "sentence": "Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking.",
                    "sentence_kor": "이 모델을 여러 신경 모델에 의해 생성된 요약으로 전송하면 이 확장성이 높은 접근 방식이 자연어 추론 및 사실 확인과 같은 관련 도메인의 데이터 세트를 사용하여 강력한 감독 훈련을 받은 모델을 포함하여 이전 모델보다 성능이 우수하다는 것을 알 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "597-4",
                    "sentence": "Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",
                    "sentence_kor": "또한 인간 평가는 보조 스팬 추출 작업이 사실 일관성을 검증하는 과정에서 유용한 도움을 제공한다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "597-5",
                    "sentence": "We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.",
                    "sentence_kor": "우리는 또한 사실 일관성 검증을 위한 수동 주석 데이터 세트, 데이터 생성 훈련을 위한 코드 및 훈련된 모델 가중치를 https://github.com/salesforce/factCC에서 공개한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "598",
            "abstractID": "EMNLP_abs-598",
            "text": [
                {
                    "index": "598-0",
                    "sentence": "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization.",
                    "sentence_kor": "수동 평가의 예비로서 자동화된 평가 지표는 텍스트 요약과 같은 텍스트 생성 작업 개발에 필수적인 부분이다.",
                    "tag": "1"
                },
                {
                    "index": "598-1",
                    "sentence": "However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers.",
                    "sentence_kor": "그러나 이 분야가 발전하는 동안 표준 메트릭스는 거의 20년 동안 대부분의 요약 논문에서 ROUGE가 표준 평가였습니다.",
                    "tag": "1"
                },
                {
                    "index": "598-2",
                    "sentence": "In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings.",
                    "sentence_kor": "본 논문에서, 우리는 텍스트 요약을 위한 평가 방법을 재평가하려고 시도한다. 즉, 시스템 수준 및 요약 수준 평가 설정에 대해 최근 널리 사용되는 데이터 세트에서 추상적이고 추출적인 시스템 출력을 사용하여 자동 메트릭의 신뢰성을 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "598-3",
                    "sentence": "We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.",
                    "sentence_kor": "우리는 오래된 데이터 세트의 평가 메트릭에 대한 결론이 반드시 최신 데이터 세트와 시스템에 적용되는 것은 아니라는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "598-4",
                    "sentence": "We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).",
                    "sentence_kor": "우리는 25개의 최고 점수 신경 요약 시스템(추상 14개 및 추출 11개)에서 수집된 인간 판단 데이터 세트를 공개한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "599",
            "abstractID": "EMNLP_abs-599",
            "text": [
                {
                    "index": "599-0",
                    "sentence": "A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo.",
                    "sentence_kor": "요즘 인기 있는 멀티미디어 뉴스 포맷은 CNN, BBC, 트위터, 웨이보를 포함한 소셜 미디어를 포함한 영향력 있는 뉴스 매체에 의해 사용되는 생생한 비디오와 그에 상응하는 뉴스 기사를 사용자에게 제공하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "599-1",
                    "sentence": "In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively.",
                    "sentence_kor": "이러한 경우, 비디오의 적절한 표지 프레임을 자동으로 선택하고 기사의 적절한 텍스트 요약을 생성하는 것은 편집자들이 시간을 절약하는 데 도움이 될 수 있으며 독자들은 보다 효과적으로 결정을 내릴 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "599-2",
                    "sentence": "Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem.",
                    "sentence_kor": "따라서 본 논문에서는 이러한 문제를 해결하기 위해 비디오 기반 다중 모드 출력 요약(VMSO) 과제를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "599-3",
                    "sentence": "The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article.",
                    "sentence_kor": "이 작업의 주요 과제는 동영상의 시간적 의존성을 기사의 의미와 함께 모델링하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "599-4",
                    "sentence": "To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator.",
                    "sentence_kor": "이를 위해 이중 상호 작용 모듈과 다중 모드 생성기로 구성된 이중 상호 작용 기반 다중 모드 요약기(DIMS)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "599-5",
                    "sentence": "In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level.",
                    "sentence_kor": "이중 상호작용 모듈에서, 우리는 비디오 내의 로컬 의미 정보를 캡처하는 조건부 자기 주의 메커니즘과 뉴스 텍스트와 비디오 사이의 의미 관계를 높은 수준에서 처리하는 글로벌 주의 메커니즘을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "599-6",
                    "sentence": "Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.",
                    "sentence_kor": "대규모 실제 VMSMO 데이터 세트에 대해 수행된 광범위한 실험은 DIMS가 자동 메트릭과 인간 평가 측면에서 모두 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "600",
            "abstractID": "SPA_abs-1",
            "text": [
                {
                    "index": "1-0",
                    "sentence": "We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task.",
                    "sentence_kor": "작업을 턴 기반 스팬 추출 작업으로 프레임화하는 대화 상자 슬롯 채우기를 위한 경량 모델인 Span-ConveRT를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "1-1",
                    "sentence": "This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019).",
                    "sentence_kor": "이 공식은 ConveRT와 같은 사전 교육된 대화 모델에서 코드화된 대화 지식을 간단히 통합할 수 있다(Henderson 등, 2019).",
                    "tag": "1"
                },
                {
                    "index": "1-2",
                    "sentence": "In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.",
                    "sentence_kor": "슬롯 채우기 작업을 위한 스팬 추출에 대한 더 많은 작업을 고무하기 위해, 우리는 레스토랑 예약 도메인의 실제 대화로부터 수집된 8,198개의 발화들로 구성된 새로운 도전적인 데이터 세트인 RESTAURANTS-8K도 출시한다.",
                    "tag": "3"
                },
                {
                    "index": "1-3",
                    "sentence": "We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor.",
                    "sentence_kor": "우리는 Span-ConveRT의 이러한 지식을 활용하는 것이 특히 퓨샷 학습 시나리오에 유용하다는 것을 보여준다. 1) 대상 도메인에서 표현을 처음부터 훈련시키는 스팬 추출기와 2) BERT 기반 스팬 추출기에 대한 일관된 이득을 보고한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "601",
            "abstractID": "SPA_abs-2",
            "text": [
                {
                    "index": "2-0",
                    "sentence": "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition.",
                    "sentence_kor": "다중 도메인 대화 상태 추적을 위한 제로샷 전송 학습을 통해 높은 데이터 획득 비용 없이 새로운 도메인을 처리할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "2-1",
                    "sentence": "This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain.",
                    "sentence_kor": "본 논문은 도메인 내 훈련 데이터가 모두 추상 대화 모델과 도메인의 온톨로지로부터 합성되는 대화 상태 추적을 위한 새로운 제로 쇼트 전송 학습 기법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "2-2",
                    "sentence": "We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset.",
                    "sentence_kor": "우리는 합성 데이터를 통한 데이터 증가가 MultiWOZ 2.1 데이터 세트의 TRADE 모델과 BERT 기반 SUMBT 모델 모두에 대한 제로샷 학습의 정확도를 향상시킬 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "2-3",
                    "sentence": "We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset.",
                    "sentence_kor": "SUMBT 모델에서 합성된 도메인 내 데이터만으로 전체 교육 데이터 세트로 얻은 정확도의 약 2/3에 도달할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "2-4",
                    "sentence": "We improve the zero-shot learning state of the art on average across domains by 21%.",
                    "sentence_kor": "우리는 도메인 전체의 평균 제로샷 학습 상태를 21% 개선한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "602",
            "abstractID": "SPA_abs-3",
            "text": [
                {
                    "index": "3-0",
                    "sentence": "This work proposes a standalone, complete Chinese discourse parser for practical applications.",
                    "sentence_kor": "본 연구는 실용적 적용을 위한 독립적이고 완전한 중국어 담화 분석기를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "3-1",
                    "sentence": "We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies.",
                    "sentence_kor": "우리는 다양한 측면에서 중국어 담화 파싱에 접근하고 사전 훈련된 텍스트 인코더를 통합할 뿐만 아니라 새로운 훈련 전략을 채택하여 시프트 감소 파서를 개선한다.",
                    "tag": "1+2"
                },
                {
                    "index": "3-2",
                    "sentence": "We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition.",
                    "sentence_kor": "우리는 시프트 감소 파서를 훈련하기 위해 동적 오라클 절차를 개정하고 수사적 관계 인식을 향상시키기 위해 감독되지 않은 데이터 확대를 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "3-3",
                    "sentence": "Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.",
                    "sentence_kor": "실험 결과는 우리의 중국어 담화 분석기가 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "603",
            "abstractID": "SPA_abs-4",
            "text": [
                {
                    "index": "4-0",
                    "sentence": "Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues.",
                    "sentence_kor": "암묵적 담화 관계 인식은 강력한 언어적 단서로서 연결부족이 부족하기 때문에 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "4-1",
                    "sentence": "Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal.",
                    "sentence_kor": "이전 방법은 주로 두 개의 인수를 별도로 인코딩하거나 주석이 달린 관계 신호를 완전히 이용하지 않은 작업에 대한 특정 상호 작용 패턴을 추출한다.",
                    "tag": "1"
                },
                {
                    "index": "4-2",
                    "sentence": "Therefore, we propose a novel TransS-driven joint learning architecture to address the issues.",
                    "sentence_kor": "따라서 문제를 해결하기 위해 새로운 TransS 기반 공동 학습 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "4-3",
                    "sentence": "Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task.",
                    "sentence_kor": "특히, 다단계 인코더를 기반으로 1) 인수-관계 인스턴스의 잠재적 기하학적 구조 정보를 캐낼 수 있는 저차원 임베딩 공간(TransS라고 함)에서 담화 관계를 변환하고 2) 담화 이해를 돕기 위해 주장의 의미적 특징을 추가로 활용한다. 3) 공동 학습 1) 및 2)작업 수행을 개선하기 위해 더 나은 인수 표현을 얻기 위해 서로를 보강합니다.",
                    "tag": "3"
                },
                {
                    "index": "4-4",
                    "sentence": "Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.",
                    "sentence_kor": "Penn 담화 트리뱅크(PDTB)에 대한 광범위한 실험 결과는 우리 모델이 여러 최첨단 시스템에 비해 경쟁력 있는 결과를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "604",
            "abstractID": "SPA_abs-5",
            "text": [
                {
                    "index": "5-0",
                    "sentence": "Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy.",
                    "sentence_kor": "비 자기 회귀(NAR) 모델은 시퀀스의 모든 토큰을 병렬로 생성하여 자기 회귀(AR) 모델에 비해 생성 속도가 빠르지만 정확도는 낮다.",
                    "tag": "1"
                },
                {
                    "index": "5-1",
                    "sentence": "Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS).",
                    "sentence_kor": "신경 기계 번역(NMT), 자동 음성 인식(ASR), 텍스트 대 음성(TTS)과 같은 다양한 작업에서 AR과 NAR 모델 간의 격차를 메우기 위해 지식 증류와 소스-표적 정렬을 포함한 다양한 기술이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "5-2",
                    "sentence": "With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others.",
                    "sentence_kor": "이러한 기법의 도움으로 NAR 모델은 일부 작업에서는 AR 모델의 정확도를 따라잡을 수 있지만 일부 작업에서는 따라잡을 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "5-3",
                    "sentence": "In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer:",
                    "sentence_kor": "본 연구에서는 NAR 시퀀스 생성의 어려움을 이해하고 다음과 같이 답변하기 위한 연구를 수행한다.",
                    "tag": "1+2"
                },
                {
                    "index": "5-4",
                    "sentence": "(1) Why NAR models can catch up with AR models in some tasks but not all?",
                    "sentence_kor": "(1) NAR 모델이 전부는 아니지만 일부 작업에서 AR 모델을 따라잡을 수 있는 이유는 무엇입니까?",
                    "tag": "1"
                },
                {
                    "index": "5-5",
                    "sentence": "(2) Why techniques like knowledge distillation and source-target alignment can help NAR models.",
                    "sentence_kor": "(2) 지식 증류 및 출처-표적 정렬과 같은 기술이 NAR 모델에 도움이 될 수 있는 이유는 무엇인가?",
                    "tag": "1"
                },
                {
                    "index": "5-6",
                    "sentence": "Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens.",
                    "sentence_kor": "AR 모델과 NAR 모델의 주된 차이점은 NAR 모델은 대상 토큰 간의 의존성을 사용하지 않는 반면 AR 모델은 의존성을 사용하지 않기 때문에 직관적으로 NAR 시퀀스 생성의 어려움은 대상 토큰 간의 의존성의 강도에 크게 좌우된다.",
                    "tag": "6"
                },
                {
                    "index": "5-7",
                    "sentence": "To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks.",
                    "sentence_kor": "이러한 의존성을 정량화하기 위해 다양한 NAR 시퀀스 생성 작업의 어려움을 특성화하는 CoMMA라는 분석 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "5-8",
                    "sentence": "We have several interesting findings:",
                    "sentence_kor": "몇 가지 흥미로운 결과가 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "5-9",
                    "sentence": "1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least.",
                    "sentence_kor": "1) NMT, ASR, TTS 과제 중 ASR의 대상토큰 의존성이 가장 높으며 TTS의 의존성은 가장 낮다.",
                    "tag": "4"
                },
                {
                    "index": "5-10",
                    "sentence": "2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models.",
                    "sentence_kor": "2) 지식 증류는 목표 순서의 목표-토큰 의존성을 감소시켜 NAR 모델의 정확도를 향상시킨다.",
                    "tag": "1"
                },
                {
                    "index": "5-11",
                    "sentence": "3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.",
                    "sentence_kor": "3) 소스-대상 정렬 제약은 소스 토큰에 대한 대상 토큰의 의존성을 촉진하여 NAR 모델의 훈련을 용이하게 한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "605",
            "abstractID": "SPA_abs-6",
            "text": [
                {
                    "index": "6-0",
                    "sentence": "Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations.",
                    "sentence_kor": "이미지 캡션과 같은 교차 모달 언어 생성 작업은 영어가 아닌 주석의 부족과 결합된 데이터 부족 모델의 추세로 인해 영어가 아닌 언어를 지원하는 능력에 직접적인 타격을 받는다.",
                    "tag": "1"
                },
                {
                    "index": "6-1",
                    "sentence": "We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage.",
                    "sentence_kor": "도메인 및 언어 범위의 웹 스케일에서 솔루션을 생성하기 위해 영어로 된 기존 언어 생성 주석을 번역 기능과 결합하기 위한 잠재적 솔루션을 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "6-2",
                    "sentence": "We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption.",
                    "sentence_kor": "우리는 PLUGS(Pivot-Language Generation Stabilization)라고 하는 접근 방식을 설명한다. 이 방법은 훈련 시 기존의 영어 주석(골드 데이터)과 기계 번역 버전(실버 데이터)을 모두 직접 활용한다. 런타임 시 영어 캡션을 먼저 생성한 다음 해당 대상 언어 캡션을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "6-3",
                    "sentence": "We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset.",
                    "sentence_kor": "우리는 PLugS 모델이 Open Images 데이터 세트의 이미지를 사용하는 대규모 도메인 테스트 세트 아래에서 5개의 서로 다른 대상 언어에서 수행된 평가에서 다른 후보 솔루션보다 우수하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "6-4",
                    "sentence": "Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.",
                    "sentence_kor": "또한, 우리는 PLuGS 모델에 의해 생성된 영어 캡션이 원래의 단일 언어 영어 모델에서 생성된 캡션보다 더 낫다는 흥미로운 효과를 발견한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "606",
            "abstractID": "SPA_abs-7",
            "text": [
                {
                    "index": "7-0",
                    "sentence": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data.",
                    "sentence_kor": "구조화된 데이터 또는 지식에서 자연어 생성(NLG)에 대한 신경 기반 엔드 투 엔드 접근법은 데이터가 많이 필요하므로 제한된 데이터로 실제 애플리케이션에 채택하기가 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "7-1",
                    "sentence": "In this work, we propose the new task of few-shot natural language generation.",
                    "sentence_kor": "본 연구에서는 퓨샷 자연어 생성이라는 새로운 과제를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "7-2",
                    "sentence": "Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains.",
                    "sentence_kor": "인간이 표 형식의 데이터를 요약하는 방식에 의해 동기 부여되어, 간단하면서도 효과적인 접근방식을 제안하고 이것이 강력한 성능을 보여줄 뿐만 아니라 도메인 전반에 걸쳐 좋은 일반화를 제공한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "7-3",
                    "sentence": "The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge",
                    "sentence_kor": "모델 아키텍처의 설계는 두 가지 측면에 기초한다: 입력 데이터에서 내용 선택과 사전 지식에서 얻을 수 있는 일관성 있는 문장을 구성하기 위한 언어 모델링",
                    "tag": "3"
                },
                {
                    "index": "7-4",
                    "sentence": "With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement.",
                    "sentence_kor": "여러 영역에 걸친 200개의 교육 예제를 통해 우리의 접근 방식이 매우 합리적인 성능을 달성하고 평균 8.0 BLEU 포인트 향상으로 가장 강력한 기준선을 능가한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "7-5",
                    "sentence": "Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG",
                    "sentence_kor": "우리의 코드와 데이터는 https://github.com/czyssrs/Few-Shot-NLG에서 찾을 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "607",
            "abstractID": "SPA_abs-8",
            "text": [
                {
                    "index": "8-0",
                    "sentence": "Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask.",
                    "sentence_kor": "질문 답변(QA)은 대화 QA(ConvQA) 하위 작업에서 특정 연구에 초점을 맞추는 개방형 도메인 대화 에이전트의 중요한 측면이다.",
                    "tag": "1"
                },
                {
                    "index": "8-1",
                    "sentence": "One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents.",
                    "sentence_kor": "최근 ConvQA 노력의 주목할 만한 한계는 대상 말뭉치에서 응답 범위를 추출하여 고품질 대화 에이전트의 자연어 생성(NLG) 측면을 무시하는 응답이다.",
                    "tag": "1"
                },
                {
                    "index": "8-2",
                    "sentence": "In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness.",
                    "sentence_kor": "본 연구에서는 SEQ2 내에서 QA 응답을 위치시키는 방법을 제안한다.SEQ NLG 접근 방식은 정확성을 유지하면서 문법에 대한 답변을 유창하게 생성합니다.",
                    "tag": "2"
                },
                {
                    "index": "8-3",
                    "sentence": "From a technical perspective, we use data augmentation to generate training data for an end-to-end system.",
                    "sentence_kor": "기술적 관점에서, 우리는 엔드 투 엔드 시스템에 대한 교육 데이터를 생성하기 위해 데이터 확대를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "8-4",
                    "sentence": "Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019).",
                    "sentence_kor": "구체적으로, 우리는 질문별 후보 답변 응답을 생성하기 위해 구문 변환(STS)을 개발하고 BERT 기반 분류기를 사용하여 순위를 매긴다(Devlin et al., 2019).",
                    "tag": "3"
                },
                {
                    "index": "8-5",
                    "sentence": "Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses.",
                    "sentence_kor": "SQuAD 2.0 데이터에 대한 인간 평가(Rajpurkar et al., 2018)는 제안된 모델이 대화 응답 생성에서 기준 CoQA 및 QuAC 모델을 능가한다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "8-6",
                    "sentence": "We further show our model’s scalability by conducting tests on the CoQA dataset.",
                    "sentence_kor": "또한 CoQA 데이터 세트에 대한 테스트를 수행하여 모델의 확장성을 보여준다.",
                    "tag": "6"
                },
                {
                    "index": "8-7",
                    "sentence": "The code and data are available at https://github.com/abaheti95/QADialogSystem.",
                    "sentence_kor": "코드와 데이터는 https://github.com/abaheti95/QADialogSystem에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "608",
            "abstractID": "SPA_abs-9",
            "text": [
                {
                    "index": "9-0",
                    "sentence": "One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation.",
                    "sentence_kor": "질문 답변(QA)에서 가장 중요한 과제 중 하나는 라벨링된 데이터의 부족이다. 사람 주석이 있는 대상 텍스트 도메인에 대한 질문-답변(QA) 쌍을 얻는 데 비용이 많이 들기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "9-1",
                    "sentence": "An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia).",
                    "sentence_kor": "문제를 해결하기 위한 다른 접근법은 문제 상황 또는 대량의 구조화되지 않은 텍스트(예: 위키피디아)에서 자동으로 생성된 QA 쌍을 사용하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "9-2",
                    "sentence": "In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency.",
                    "sentence_kor": "본 연구에서는 구조화되지 않은 텍스트가 컨텍스트로 주어진 QA 쌍을 생성하는 동시에 생성된 QA 쌍 간의 상호 정보를 극대화하여 일관성을 보장하기 위한 계층적 조건부 변이 자동 인코더(HCVAE)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "9-3",
                    "sentence": "We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models.",
                    "sentence_kor": "생성된 QA 쌍(QA 기반 평가)만 사용하거나 생성된 QA 쌍과 인간 레이블 쌍(반지도 학습)을 모두 사용하여 여러 벤치마크 데이터 세트에서 정보 최대화 계층적 조건 변이 자동 인코더(Info-HCVAE)를 검증한다.최첨단 기본 모델",
                    "tag": "3"
                },
                {
                    "index": "9-4",
                    "sentence": "The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.",
                    "sentence_kor": "그 결과 우리 모델은 훈련에 데이터의 일부만 사용하여 두 작업의 모든 기준선에 비해 인상적인 성능 향상을 얻었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "609",
            "abstractID": "SPA_abs-10",
            "text": [
                {
                    "index": "10-0",
                    "sentence": "Paraphrasing natural language sentences is a multifaceted process:",
                    "sentence_kor": "자연어 문장을 바꾸어 쓰는 것은 다면적인 과정이다.",
                    "tag": "1"
                },
                {
                    "index": "10-1",
                    "sentence": "it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization.",
                    "sentence_kor": "그것은 개별 단어 또는 짧은 구절 교체, 내용의 국지적 재배열 또는 주제화 또는 수동화와 같은 높은 수준의 재구성을 포함할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "10-2",
                    "sentence": "Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner.",
                    "sentence_kor": "과거의 접근법은 해석 가능한 방식으로 가능성을 바꾸어 표현하기 위해 애쓰고 있다.",
                    "tag": "1"
                },
                {
                    "index": "10-3",
                    "sentence": "Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model.",
                    "sentence_kor": "기계 번역의 사전 정렬 문헌에서 영감을 받은 우리의 연구는 구문 변환을 사용하여 소스 문장을 부드럽게 \"재 정렬\"하고 우리의 신경 패러프레이징 모델을 안내한다.",
                    "tag": "2"
                },
                {
                    "index": "10-4",
                    "sentence": "First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model.",
                    "sentence_kor": "첫째, 입력 문장이 주어지면 인코더-디코더 모델을 사용하여 실현 가능한 구문 재정렬 세트를 도출한다.",
                    "tag": "3"
                },
                {
                    "index": "10-5",
                    "sentence": "This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks.",
                    "sentence_kor": "이 모델은 문장의 부분적으로 어휘적이고 부분적으로 통사적인 보기를 통해 작동하며 큰 청크의 순서를 변경할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "10-6",
                    "sentence": "Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order.",
                    "sentence_kor": "다음으로 제안된 각 재배열을 사용하여 일련의 위치 임베딩 순서를 생성하며, 이는 최종 인코더-디코더 패러프레이즈 모델이 특정 순서로 소스 단어를 처리하도록 권장한다.",
                    "tag": "3"
                },
                {
                    "index": "10-7",
                    "sentence": "Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.",
                    "sentence_kor": "우리의 평가는 자동 및 인간 모두 제안된 시스템이 기본 접근법의 품질을 유지하면서 생성된 패러프레이즈의 다양성을 상당히 증가시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "610",
            "abstractID": "SPA_abs-11",
            "text": [
                {
                    "index": "11-0",
                    "sentence": "Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents.",
                    "sentence_kor": "조건부 텍스트 생성은 인간이 생성된 콘텐츠의 속성을 제어할 수 있는 가능성을 제공하는 자연어 생성(NLG)의 주제로 많은 관심을 끌었다.",
                    "tag": "1"
                },
                {
                    "index": "11-1",
                    "sentence": "Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion.",
                    "sentence_kor": "현재의 조건부 생성 모델은 공동 종단 간 학습 패션으로 인해 새로운 조건을 처리할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "11-2",
                    "sentence": "When a new condition added, these techniques require full retraining.",
                    "sentence_kor": "새로운 조건이 추가되면 이러한 기법은 완전한 재교육이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "11-3",
                    "sentence": "In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation.",
                    "sentence_kor": "본 논문에서는 유연한 조건부 텍스트 생성을 위한 프리트레인 및 플러그인 가변 자동 인코더(PPVAE)라는 새로운 프레임워크를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "11-4",
                    "sentence": "PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation.",
                    "sentence_kor": "PPVAE는 텍스트 생성 모듈을 조건 표현 모듈에서 분리하여 \"1 대 다수\" 조건부 생성을 허용합니다.",
                    "tag": "3"
                },
                {
                    "index": "11-5",
                    "sentence": "When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications.",
                    "sentence_kor": "때 신선한 조건 떠오르면서만 가벼운 네트워크 훈련을 많이 받고 PPVAE고 바람직한의 실세계 적용을 위한 효율적인 것을 플러그 인으로 일한다 필요가 있다.",
                    "tag": "4"
                },
                {
                    "index": "11-6",
                    "sentence": "Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.",
                    "sentence_kor": "광범위한 실험을 통해 PPVAE가 기존 대안보다 우수하다는 것이 입증되었습니다. 조건성과 다양성은 개선되었지만 훈련 노력은 감소했습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "611",
            "abstractID": "SPA_abs-12",
            "text": [
                {
                    "index": "12-0",
                    "sentence": "Masked language model and autoregressive language model are two types of language models.",
                    "sentence_kor": "마스크된 언어 모델과 자기 회귀 언어 모델은 두 가지 유형의 언어 모델이다.",
                    "tag": "1"
                },
                {
                    "index": "12-1",
                    "sentence": "While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG).",
                    "sentence_kor": "BERT와 같은 사전 훈련된 마스킹 언어 모델은 자연어 이해(NLU) 작업의 라인을 압도하는 반면 GPT와 같은 자기 회귀 언어 모델은 특히 자연어 생성(NLG)에서 능력이 있다.",
                    "tag": "1"
                },
                {
                    "index": "12-2",
                    "sentence": "In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM).",
                    "sentence_kor": "본 논문에서 우리는 확률적으로 마스킹된 언어 모델(PMLM)이라고 부르는 마스킹 언어 모델에 대한 확률론적 마스킹 방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "12-3",
                    "sentence": "We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM.",
                    "sentence_kor": "우리는 u-PMLM이라는 마스킹 비율에 대한 균일한 사전 분포를 가진 특정 PMLM을 구현합니다.",
                    "tag": "3"
                },
                {
                    "index": "12-4",
                    "sentence": "We prove that u-PMLM is equivalent to an autoregressive permutated language model.",
                    "sentence_kor": "우리는 u-PMLM이 자기 회귀 순열 언어 모델과 동일하다는 것을 증명한다.",
                    "tag": "4"
                },
                {
                    "index": "12-5",
                    "sentence": "One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation.",
                    "sentence_kor": "이 모델의 한 가지 주요 장점은 놀라울 정도로 우수한 품질로 임의의 순서로 텍스트 생성을 지원하여 기존의 단방향 생성에 비해 잠재적으로 새로운 애플리케이션을 사용할 수 있다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "12-6",
                    "sentence": "Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.",
                    "sentence_kor": "또한 사전 교육을 받은 u-PMLM은 다운스트림 NLU 작업에서도 BERT를 능가합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "612",
            "abstractID": "SPA_abs-13",
            "text": [
                {
                    "index": "13-0",
                    "sentence": "Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways.",
                    "sentence_kor": "신경 텍스트 생성 모델링의 최근 발전은 그러한 접근법이 악의적인 방법으로 사용될 수 있는 방법과 관련된 많은 사회적 우려를 낳았다.",
                    "tag": "1"
                },
                {
                    "index": "13-1",
                    "sentence": "It is therefore desirable to develop a deeper understanding of the fundamental properties of such models.",
                    "sentence_kor": "따라서 이러한 모델의 기본 특성에 대해 더 깊이 이해하는 것이 바람직하다.",
                    "tag": "1"
                },
                {
                    "index": "13-2",
                    "sentence": "The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area.",
                    "sentence_kor": "모델링 선택의 결과로 기계 생성 텍스트에 나타나는 아티팩트에 대한 연구는 초기 연구 영역이다.",
                    "tag": "1"
                },
                {
                    "index": "13-3",
                    "sentence": "To this end, the extent and degree to which these artifacts surface in generated text is still unclear.",
                    "sentence_kor": "이를 위해 생성된 텍스트에 이러한 아티팩트가 나타나는 범위와 정도는 여전히 불분명합니다.",
                    "tag": "1"
                },
                {
                    "index": "13-4",
                    "sentence": "In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text.",
                    "sentence_kor": "생성 텍스트 모델과 해당 아티팩트를 더 잘 이해하기 위해 주어진 모델의 여러 변형 중 어떤 것이 일부 텍스트를 생성했는지 구별하는 새로운 작업을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "13-5",
                    "sentence": "Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate.",
                    "sentence_kor": "특히, 모델링 선택(예: 샘플링 방법, Top-k 확률, 모델 아키텍처 등)이 생성되는 텍스트에 탐지 가능한 아티팩트를 남기는지 여부를 관찰하기 위해 광범위한 진단 테스트를 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "13-6",
                    "sentence": "Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone.",
                    "sentence_kor": "엄격한 실험을 통해 뒷받침되는 우리의 주요 발견은 그러한 인공물이 존재하고 생성된 텍스트만 보고 다른 모델링 선택을 추론할 수 있다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "13-7",
                    "sentence": "This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.",
                    "sentence_kor": "이는 신경 텍스트 생성기가 이전에 생각했던 것보다 실제로 다양한 모델링 선택에 더 민감할 수 있음을 시사한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "613",
            "abstractID": "SPA_abs-14",
            "text": [
                {
                    "index": "14-0",
                    "sentence": "While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need.",
                    "sentence_kor": "제품과 서비스에 대한 온라인 검토가 중요한 정보 소스가 되지만 잠재적 소비자가 정보 요구를 충족시키기 위해 장황한 검토를 이용하는 것은 비효율적이다.",
                    "tag": "1"
                },
                {
                    "index": "14-1",
                    "sentence": "We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences.",
                    "sentence_kor": "우리는 정보 활용의 새로운 방법, 즉 해당 검토 문장으로 대답할 수 있는 질문을 생성하는 방법으로 질문 생성을 탐구할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "14-2",
                    "sentence": "One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences.",
                    "sentence_kor": "이 생성 과제의 한 가지 주요 과제는 훈련 데이터 부족, 즉 사용자가 제시한 질문과 검토 문장 사이의 명시적 매핑 관계이다.",
                    "tag": "3"
                },
                {
                    "index": "14-3",
                    "sentence": "To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation.",
                    "sentence_kor": "생성 모델에 대한 적절한 교육 인스턴스를 얻기 위해 적응형 인스턴스 전송 및 증강을 포함한 반복 학습 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "14-4",
                    "sentence": "To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation.",
                    "sentence_kor": "검토의 주요 측면에 대한 요점 질문을 생성하기 위해 측면 주석 부담 없이 비지도 방식으로 추출한 관련 기능을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "14-5",
                    "sentence": "Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.",
                    "sentence_kor": "인기 있는 전자상거래 사이트의 다양한 범주의 데이터에 대한 실험은 제안된 검토 기반 질문 생성 과제의 잠재력뿐만 아니라 프레임워크의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "614",
            "abstractID": "SPA_abs-15",
            "text": [
                {
                    "index": "15-0",
                    "sentence": "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc.",
                    "sentence_kor": "구조 대 시퀀스 프레임워크를 사용하는 기존의 선도 코드 주석 생성 접근법은 코드 해석의 유형 정보(예: 연산자, 문자열 등)를 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "15-1",
                    "sentence": "However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information.",
                    "sentence_kor": "그러나 유형 정보 간의 계층적 의존성으로 인해 유형 정보를 기존 프레임워크에 도입하는 것은 중요하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "15-2",
                    "sentence": "In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node.",
                    "sentence_kor": "위의 문제를 해결하기 위해 소스 코드를 각 노드와 관련된 유형 정보가 있는 N-아리 트리로 간주하는 코드 주석 생성 작업에 대한 유형 보조 안내 인코더-디코더 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "15-3",
                    "sentence": "Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code.",
                    "sentence_kor": "특히, 우리의 프레임워크는 소스 코드의 적응형 요약을 가능하게 하는 유형 관련 인코더와 유형 제한 디코더를 특징으로 한다.",
                    "tag": "3"
                },
                {
                    "index": "15-4",
                    "sentence": "We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework.",
                    "sentence_kor": "또한 제안된 프레임워크의 훈련 문제를 해결하기 위한 계층적 강화 학습 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "15-5",
                    "sentence": "Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.",
                    "sentence_kor": "광범위한 평가는 자동 평가 측정 기준과 사례 연구를 모두 사용하여 프레임워크의 최첨단 성능을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "615",
            "abstractID": "SPA_abs-16",
            "text": [
                {
                    "index": "16-0",
                    "sentence": "We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing.",
                    "sentence_kor": "우리는 모의 어닐링을 통해 비지도 패러프레이징을 수행하는 새로운 접근법인 UPSA를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "16-1",
                    "sentence": "We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases.",
                    "sentence_kor": "우리는 의역 생성을 최적화 문제로 모델링하고 의미 유사성, 표현 다양성 및 의역 언어의 유창성을 포함하는 정교한 목적 함수를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "16-2",
                    "sentence": "UPSA searches the sentence space towards this objective by performing a sequence of local editing.",
                    "sentence_kor": "UPSA는 일련의 로컬 편집을 수행하여 이 목표를 향한 문장 공간을 검색합니다.",
                    "tag": "3"
                },
                {
                    "index": "16-3",
                    "sentence": "We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter.",
                    "sentence_kor": "우리는 Quora, Wikianswers, MSCOCO 및 Twitter와 같은 다양한 데이터 세트에 대한 접근 방식을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "16-4",
                    "sentence": "Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations.",
                    "sentence_kor": "광범위한 결과에 따르면 UPSA는 자동 및 인간 평가 측면에서 이전의 감독되지 않은 방법과 비교하여 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "16-5",
                    "sentence": "Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.",
                    "sentence_kor": "또한, 우리의 접근 방식은 대부분의 기존 도메인 적응 감독 모델을 능가하여 UPSA의 일반화 가능성을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "616",
            "abstractID": "SPA_abs-17",
            "text": [
                {
                    "index": "17-0",
                    "sentence": "Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections.",
                    "sentence_kor": "텍스트 분할은 문서의 텍스트를 일관성 있는 섹션으로 분할하여 잠재 구조를 밝혀내는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "17-1",
                    "sentence": "Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly.",
                    "sentence_kor": "텍스트 분할에 대한 이전 연구가 문서 분할과 세그먼트 라벨링 작업을 별도로 고려하는 경우, 우리는 작업이 보완적인 정보를 포함하고 있으며 공동으로 가장 잘 다루어진다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "17-2",
                    "sentence": "We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments.",
                    "sentence_kor": "문서와 레이블을 공동으로 분할할 수 있는 세그먼트 풀링 LSTM(S-LSTM)을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "17-3",
                    "sentence": "In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments.",
                    "sentence_kor": "공동 훈련을 지원하기 위해 예측 및 실측 자료 세그먼트를 정렬하여 오류를 복구하도록 모델을 가르치는 방법을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "17-4",
                    "sentence": "We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.",
                    "sentence_kor": "우리는 S-LSTM이 세그먼트 오류를 평균 30% 줄이는 동시에 세그먼트 레이블링도 개선한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "617",
            "abstractID": "SPA_abs-18",
            "text": [
                {
                    "index": "18-0",
                    "sentence": "Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers.",
                    "sentence_kor": "사용자가 제공한 몇 가지 종자 단어를 기반으로 약하게 감독된 텍스트 분류가 최근 연구자들로부터 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "18-1",
                    "sentence": "Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked.",
                    "sentence_kor": "기존 방법은 주로 문맥이 없는 방식으로 의사 라벨을 생성하므로(예: 문자열 일치) 인간 언어의 모호하고 문맥 의존적인 특성은 오랫동안 간과되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "18-2",
                    "sentence": "In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification.",
                    "sentence_kor": "본 논문에서 우리는 텍스트 분류에 대한 상황별 약한 감독을 제공하는 새로운 프레임워크 ConWea를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "18-3",
                    "sentence": "Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus.",
                    "sentence_kor": "특히, 우리는 단어 발생과 시드 단어 정보에 대한 상황별 표현을 활용하여 동일한 단어의 여러 해석을 자동으로 차별화하여 상황별 말뭉치를 만든다.",
                    "tag": "3"
                },
                {
                    "index": "18-4",
                    "sentence": "This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner.",
                    "sentence_kor": "이 상황별 말뭉치는 분류자를 훈련시키고 반복적인 방식으로 시드 단어를 확장하는 데 추가로 활용된다.",
                    "tag": "3"
                },
                {
                    "index": "18-5",
                    "sentence": "This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized.",
                    "sentence_kor": "이 프로세스는 새로운 상황별, 높은 라벨 표시 키워드를 추가할 뿐만 아니라 초기 시드 워드를 모호하게 만들어 약한 감독을 완전히 상황별화시킵니다.",
                    "tag": "4"
                },
                {
                    "index": "18-6",
                    "sentence": "Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.",
                    "sentence_kor": "실제 데이터 세트에 대한 광범위한 실험과 사례 연구는 특히 클래스 레이블이 세분화된 경우 상황별 약한 감독을 사용할 필요성과 상당한 이점을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "618",
            "abstractID": "SPA_abs-19",
            "text": [
                {
                    "index": "19-0",
                    "sentence": "Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task.",
                    "sentence_kor": "텍스트 분류는 자연어 처리(NLP)에서 기본이며 그래프 신경망(GNN)이 최근 이 작업에 적용되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "19-1",
                    "sentence": "However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words.",
                    "sentence_kor": "그러나 기존의 그래프 기반 작업은 각 문서 내의 상황별 단어 관계를 포착할 수도 없고 새로운 단어의 귀납적 학습을 수행할 수도 없다.",
                    "tag": "1"
                },
                {
                    "index": "19-2",
                    "sentence": "Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN.",
                    "sentence_kor": "따라서 본 연구에서는 이러한 문제를 극복하기 위해 GNN을 통한 유도 텍스트 분류를 위한 TextING을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "19-3",
                    "sentence": "We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document.",
                    "sentence_kor": "먼저 각 문서에 대해 개별 그래프를 만든 다음 GNN을 사용하여 로컬 구조를 기반으로 세분화된 단어 표현을 학습하여 새 문서에서 보이지 않는 단어에 대한 임베딩을 효과적으로 생성할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "19-4",
                    "sentence": "Finally, the word nodes are aggregated as the document embedding.",
                    "sentence_kor": "마지막으로, 단어 노드가 문서 임베딩으로 집계됩니다.",
                    "tag": "3"
                },
                {
                    "index": "19-5",
                    "sentence": "Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.",
                    "sentence_kor": "4개의 벤치마크 데이터 세트에 대한 광범위한 실험은 우리의 방법이 최첨단 텍스트 분류 방법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "619",
            "abstractID": "SPA_abs-20",
            "text": [
                {
                    "index": "20-0",
                    "sentence": "Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA).",
                    "sentence_kor": "최근 몇 년 동안 잠재 디리클레 할당(LDA)과 같은 전통적인 주제 모델에서처럼 모델 추론을 위한 복잡한 수학적 파생을 피하기 때문에 텍스트에서 주제 자동 추출을 위해 신경 주제 모델을 사용하는 것에 대한 관심이 급증하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "20-1",
                    "sentence": "However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document.",
                    "sentence_kor": "그러나 이러한 모델은 일반적으로 잠재 주제 공간에 대해 부적절한 사전(예: 가우스 또는 로지스틱 정규)을 가정하거나 주어진 문서에 대한 주제 분포를 추론할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "20-2",
                    "sentence": "To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling.",
                    "sentence_kor": "이러한 한계를 해결하기 위해, 우리는 신경 주제 모델링에 양방향 적대적 훈련을 적용하는 첫 번째 시도를 나타내는 BAT(Bidirectional Adversarial Topic) 모델이라는 신경 주제 모델링 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "20-3",
                    "sentence": "The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution.",
                    "sentence_kor": "제안된 BAT는 문서 주제 분포와 문서 단어 분포 사이에 양방향 예측을 구축한다.",
                    "tag": "2+3"
                },
                {
                    "index": "20-4",
                    "sentence": "It uses a generator to capture the semantic patterns from texts and an encoder for topic inference.",
                    "sentence_kor": "텍스트에서 의미 패턴을 캡처하기 위해 생성기를 사용하고 주제 추론을 위해 인코더를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "20-5",
                    "sentence": "Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT.",
                    "sentence_kor": "또한 단어 관련성 정보를 통합하기 위해 가우스(가우스-B)를 사용한 양방향 적대적 토픽 모델AT)는 BAT에서 확장됩니다.",
                    "tag": "3"
                },
                {
                    "index": "20-6",
                    "sentence": "To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments.",
                    "sentence_kor": "BAT와 가우스-BAT의 효과를 검증하기 위해 세 가지 벤치마크 말뭉치를 실험에 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "20-7",
                    "sentence": "The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines.",
                    "sentence_kor": "실험 결과는 BAT와 가우스-B를 보여준다.AT는 몇 가지 경쟁 기준을 능가하는 보다 일관된 주제를 확보합니다.",
                    "tag": "4"
                },
                {
                    "index": "20-8",
                    "sentence": "Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.",
                    "sentence_kor": "또한 추출된 주제를 기반으로 텍스트 클러스터링을 수행할 때, 우리 모델은 가우스-B에 의해 달성된 보다 현저한 개선으로 모든 기준선을 능가한다.정확도에서 거의 6%의 증가가 관찰되는 경우.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "620",
            "abstractID": "SPA_abs-21",
            "text": [
                {
                    "index": "21-0",
                    "sentence": "Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks.",
                    "sentence_kor": "텍스트 표현을 위한 고급 사전 교육 모델은 다양한 텍스트 분류 작업에서 최첨단 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "21-1",
                    "sentence": "However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts.",
                    "sentence_kor": "단, 텍스트와 라벨링 표준의 의미적 유사성 간의 불일치는 분류자에게 영향을 미친다. 즉, 분류자가 의미론적으로 유사한 텍스트에 다른 라벨을 할당해야 하는 경우 성능이 저하된다.",
                    "tag": "1"
                },
                {
                    "index": "21-2",
                    "sentence": "To address this problem, we propose a simple multitask learning model that uses negative supervision.",
                    "sentence_kor": "이 문제를 해결하기 위해, 우리는 부정적인 감독을 사용하는 간단한 다중 작업 학습 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "21-3",
                    "sentence": "Specifically, our model encourages texts with different labels to have distinct representations.",
                    "sentence_kor": "특히, 우리 모델은 라벨이 다른 텍스트에 뚜렷한 표현을 권장한다.",
                    "tag": "3"
                },
                {
                    "index": "21-4",
                    "sentence": "Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.",
                    "sentence_kor": "종합적인 실험에 따르면 우리 모델은 단일 및 다중 레이블 분류, 문장 및 문서 분류, 세 가지 언어로 된 분류 모두에서 최첨단 사전 교육 모델을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "621",
            "abstractID": "SPA_abs-22",
            "text": [
                {
                    "index": "22-0",
                    "sentence": "Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word.",
                    "sentence_kor": "신경 기계 번역(NMT)은 대상 문장을 단어별로 생성하기 위해 보편적인 방법으로 소스 문장을 인코딩한다.",
                    "tag": "1"
                },
                {
                    "index": "22-1",
                    "sentence": "However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words).",
                    "sentence_kor": "그러나 NMT는 문장 의미에서의 단어의 중요성을 고려하지 않는다. 예를 들어, 일부 단어(예: 내용 단어)는 다른 단어(예: 기능 단어)보다 더 중요한 의미를 표현한다.",
                    "tag": "1"
                },
                {
                    "index": "22-2",
                    "sentence": "To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance.",
                    "sentence_kor": "이러한 한계를 해결하기 위해, 우리는 먼저 단어 빈도 정보를 활용하여 문장의 내용과 기능 단어를 구별한 다음, 번역 성능을 향상시키기 위해 내용 단어 인식 NMT를 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "22-3",
                    "sentence": "Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.",
                    "sentence_kor": "WMT14 영어-독일어, WMT14 영어-프랑스어 및 WMT17 중국어-영어 번역 작업에 대한 경험적 결과는 제안된 방법이 트랜스포머 기반 NMT의 성능을 크게 개선할 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "622",
            "abstractID": "SPA_abs-23",
            "text": [
                {
                    "index": "23-0",
                    "sentence": "The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks.",
                    "sentence_kor": "마스크된 언어 모델은 다양한 자연어 처리 작업에 대한 효과로 인해 주목할 만한 관심을 받아왔다.",
                    "tag": "1"
                },
                {
                    "index": "23-1",
                    "sentence": "However, few works have adopted this technique in the sequence-to-sequence models.",
                    "sentence_kor": "그러나 시퀀스 투 시퀀스 모델에서 이 기술을 채택한 작품은 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "23-2",
                    "sentence": "In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT).",
                    "sentence_kor": "본 연구에서는 공동으로 마스킹된 시퀀스 대 시퀀스 모델을 소개하고 비 자기 회귀 신경 기계 변환에 대한 그 적용을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "23-3",
                    "sentence": "Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality.",
                    "sentence_kor": "특히, 우리는 먼저 NAT 모델에서 인코더와 디코더의 기능을 경험적으로 연구하여 인코더가 변환 품질과 관련하여 디코더보다 더 중요한 역할을 한다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "23-4",
                    "sentence": "Therefore, we propose to train the encoder more rigorously by masking the encoder input while training.",
                    "sentence_kor": "따라서 훈련 중에 인코더 입력을 마스킹하여 인코더를 보다 엄격하게 훈련시킬 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "23-5",
                    "sentence": "As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words.",
                    "sentence_kor": "디코더의 경우, 중복 단어 번역 문제를 완화하기 위해 n그램 손실 함수로 디코더 입력의 연속 마스킹을 기반으로 훈련할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "23-6",
                    "sentence": "The two types of masks are applied to the model jointly at the training stage.",
                    "sentence_kor": "두 가지 유형의 마스크는 교육 단계에서 모델에 공동으로 적용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "23-7",
                    "sentence": "We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.",
                    "sentence_kor": "우리는 5개의 벤치마크 기계 번역 작업에 대한 실험을 수행하며, 우리 모델은 자기 회귀 모델에 비해 5배 이상 빠른 속도로 WMT14 영어-독일어/독일어-영어 작업에서 27.69/32.24 BLEU 점수를 달성할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "623",
            "abstractID": "SPA_abs-24",
            "text": [
                {
                    "index": "24-0",
                    "sentence": "The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT).",
                    "sentence_kor": "다중 헤드 주의 메커니즘을 기반으로 하는 트랜스포머 변환 모델(Vaswani 등, 2017)은 병렬로 효과적으로 계산될 수 있으며 신경 기계 변환(NMT)의 성능을 크게 향상시켰다.",
                    "tag": "1"
                },
                {
                    "index": "24-1",
                    "sentence": "Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018).",
                    "sentence_kor": "주의 네트워크는 직관적으로 RNN보다 짧은 네트워크 경로를 통해 원거리 단어를 연결할 수 있지만, 경험적 분석은 여전히 장거리 종속성을 완전히 포착하는 데 어려움이 있음을 보여준다(Tang et al., 2018).",
                    "tag": "1"
                },
                {
                    "index": "24-2",
                    "sentence": "Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships.",
                    "sentence_kor": "단어 대신 구문을 모델링하는 것이 더 큰 변환 블록(\"구문\")과 그 재정렬 능력을 통해 통계 기계 번역(SMT) 접근 방식을 크게 개선했다는 점을 고려하면, 구 수준에서 NMT 모델링은 모델이 장거리 관계를 포착하는 데 도움이 되는 직관적인 제안이다.",
                    "tag": "1+2"
                },
                {
                    "index": "24-3",
                    "sentence": "In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations.",
                    "sentence_kor": "본 논문에서는 먼저 해당 토큰 표현에서 구문 표현을 생성할 수 있는 주의 깊은 구문 표현 생성 메커니즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "24-4",
                    "sentence": "In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships.",
                    "sentence_kor": "또한 생성된 구문 표현을 트랜스포머 변환 모델에 통합하여 장거리 관계를 캡처할 수 있는 능력을 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "24-5",
                    "sentence": "In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach.",
                    "sentence_kor": "우리의 실험에서, 우리는 접근 방식의 효과를 보여주는 강력한 트랜스포머 기준선 위에 WMT 14 영독 및 영불 작업에서 상당한 개선을 얻었다.",
                    "tag": "4"
                },
                {
                    "index": "24-6",
                    "sentence": "Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps.",
                    "sentence_kor": "우리의 접근 방식은 Transformer Base 모델이 Transformer Big 모델 수준에서 성능을 발휘하고, 긴 문장에서는 훨씬 더 뛰어나지만 매개 변수와 교육 단계가 상당히 적습니다.",
                    "tag": "4+5"
                },
                {
                    "index": "24-7",
                    "sentence": "The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.",
                    "sentence_kor": "문구 표현이 큰 환경에서도 도움이 된다는 사실은 그것들이 장거리 관계에 귀중한 기여를 한다는 우리의 추측을 더욱 뒷받침한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "624",
            "abstractID": "SPA_abs-25",
            "text": [
                {
                    "index": "25-0",
                    "sentence": "The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure.",
                    "sentence_kor": "트랜스포머 변환 모델은 다중 레이어 인코더/디코더 구조로 인한 최적화 어려움을 완화하기 위해 잔여 연결 및 계층 정규화를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "25-1",
                    "sentence": "Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge.",
                    "sentence_kor": "이전 연구에 따르면 잔여 연결 및 계층 정규화에도 딥 트랜스포머는 여전히 교육에 어려움을 겪고 있으며, 특히 12개 이상의 인코더/디코더 레이어가 있는 트랜스포머 모델은 수렴하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "25-2",
                    "sentence": "In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers.",
                    "sentence_kor": "이 논문에서 우리는 먼저 공식 구현에서 잔존 연결 및 계층 정규화의 계산 순서를 변경하는 간단한 수정이 심층 트랜스포머의 최적화를 크게 용이하게 할 수 있음을 경험적으로 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "25-3",
                    "sentence": "We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.",
                    "sentence_kor": "그런 다음 계산 순서의 미묘한 차이를 상당히 상세하게 비교하고 훈련 수렴을 효과적으로 보장하는 트랜스포머 매개 변수의 초기화에 대한 립시츠 제약 조건을 활용하는 매개 변수 초기화 방법을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "25-4",
                    "sentence": "In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers.",
                    "sentence_kor": "이전 연구 결과와 달리, 우리는 립시츠 매개변수 초기화를 통해 원래 계산 순서를 가진 딥 트랜스포머가 수렴될 수 있으며 최대 24개의 레이어로 상당한 BLEU 개선을 얻을 수 있음을 추가로 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "25-5",
                    "sentence": "In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.",
                    "sentence_kor": "딥 인코더에 초점을 맞춘 이전의 연구와 달리, 우리의 접근 방식은 트랜스포머도 딥 디코더의 이점을 얻을 수 있게 한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "625",
            "abstractID": "SPA_abs-26",
            "text": [
                {
                    "index": "26-0",
                    "sentence": "A neural machine translation (NMT) system is expensive to train, especially with high-resource settings.",
                    "sentence_kor": "신경 기계 변환(NMT) 시스템은 특히 고자원 설정으로 훈련하는 데 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "26-1",
                    "sentence": "As the NMT architectures become deeper and wider, this issue gets worse and worse.",
                    "sentence_kor": "NMT 아키텍처가 점점 더 깊어지고 넓어짐에 따라, 이 문제는 점점 더 심각해진다.",
                    "tag": "1"
                },
                {
                    "index": "26-2",
                    "sentence": "In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method.",
                    "sentence_kor": "본 논문에서는 새로운 표준 기반 커리큘럼 학습 방법을 도입하여 NMT 훈련의 효율성을 향상시키는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "26-3",
                    "sentence": "We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence.",
                    "sentence_kor": "우리는 단어 임베딩의 표준(일명 길이 또는 모듈)을 1) 문장의 난이도 2) 모델의 역량 3) 문장의 무게의 척도로 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "26-4",
                    "sentence": "The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties.",
                    "sentence_kor": "표준 기반 문장 난이도는 언어 동기와 모델 기반 문장 난이도의 장점을 모두 취한다.",
                    "tag": "4"
                },
                {
                    "index": "26-5",
                    "sentence": "It is easy to determine and contains learning-dependent features.",
                    "sentence_kor": "쉽게 결정할 수 있으며 학습에 의존하는 특징을 포함하고 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "26-6",
                    "sentence": "The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT.",
                    "sentence_kor": "표준 기반 모델 역량은 NMT가 완전히 자동화된 방식으로 커리큘럼을 학습하도록 하는 반면, 표준 기반 문장 가중치는 NMT의 벡터 표현 학습을 더욱 강화한다.",
                    "tag": "4+5"
                },
                {
                    "index": "26-7",
                    "sentence": "Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).",
                    "sentence_kor": "WMT'14 영어-독일어 및 WMT'17 중국어-영어 번역 작업에 대한 실험 결과는 제안된 방법이 BLEU 점수(+1.17/+1.56)와 훈련 속도(2.22x/3.33x) 측면에서 강력한 기준선을 능가한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "626",
            "abstractID": "SPA_abs-27",
            "text": [
                {
                    "index": "27-0",
                    "sentence": "Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently.",
                    "sentence_kor": "동시 번역은 많은 중요한 응용 시나리오를 가지고 있으며 최근 학계와 업계 모두에서 많은 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "27-1",
                    "sentence": "Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative.",
                    "sentence_kor": "그러나 대부분의 기존 프레임워크는 변환 품질과 지연 시간 사이의 균형을 맞추는 데 어려움을 겪는다. 즉, 디코딩 정책은 일반적으로 너무 공격적이거나 너무 보수적이다.",
                    "tag": "1"
                },
                {
                    "index": "27-2",
                    "sentence": "We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information.",
                    "sentence_kor": "우리는 청중을 최신 정보로 추적하도록 하기 위해 각 단계에서 특정 추가 단어 마운트를 항상 (과다) 생성하는 시기적절한 수정 능력을 가진 기회주의적 디코딩 기법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "27-3",
                    "sentence": "At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality.",
                    "sentence_kor": "동시에 높은 번역 품질을 보장하기 위해 더 많은 소스 컨텍스트를 관찰할 때 과도하게 생성된 이전 단어의 오류를 적시에 수정한다.",
                    "tag": "3"
                },
                {
                    "index": "27-4",
                    "sentence": "Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.",
                    "sentence_kor": "실험에 따르면 우리 기술은 중국어-영어 및 영어-중국어 번역에서 수정률이 8% 미만이며 대기 시간을 크게 단축하고 BLEU를 최대 +3.1회 증가시킨다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "627",
            "abstractID": "SPA_abs-28",
            "text": [
                {
                    "index": "28-0",
                    "sentence": "We develop a formal hierarchy of the expressive capacity of RNN architectures.",
                    "sentence_kor": "우리는 RNN 아키텍처의 표현 용량의 공식적인 계층을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "28-1",
                    "sentence": "The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine.",
                    "sentence_kor": "계층 구조는 두 가지 형식 속성, 즉 RNN의 메모리를 측정하는 공간 복잡성과 반복 업데이트가 가중 유한 상태 기계에 의해 설명될 수 있는지 여부에 따라 정의된 합리적인 반복을 기반으로 한다.",
                    "tag": "1"
                },
                {
                    "index": "28-2",
                    "sentence": "We place several RNN variants within this hierarchy.",
                    "sentence_kor": "이 계층 구조 내에 여러 RNN 변형을 배치한다.",
                    "tag": "3"
                },
                {
                    "index": "28-3",
                    "sentence": "For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016).",
                    "sentence_kor": "예를 들어 LSTM이 합리적이지 않음을 입증하여 관련 QRNN과 공식적으로 분리한다(Bradbury et al., 2016).",
                    "tag": "3"
                },
                {
                    "index": "28-4",
                    "sentence": "We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions.",
                    "sentence_kor": "또한 여러 레이어를 쌓거나 서로 다른 풀링 기능으로 구성함으로써 이러한 모델의 표현 능력이 어떻게 확장되는지 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "28-5",
                    "sentence": "Our results build on the theory of “saturated” RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy.",
                    "sentence_kor": "우리의 결과는 \"포화\" RNN 이론을 기반으로 한다(Merrill, 2019). 이러한 연구 결과를 불포화 RNN으로 공식적으로 확장하는 것은 향후 연구에 남겨두지만, 우리는 불포화 RNN의 실제 학습 가능 용량이 유사한 계층에 따른다고 가정한다.",
                    "tag": "4+5"
                },
                {
                    "index": "28-6",
                    "sentence": "We provide empirical results to support this conjecture.",
                    "sentence_kor": "우리는 이 추측을 뒷받침하는 경험적 결과를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "28-7",
                    "sentence": "Experimental findings from training unsaturated networks on formal languages support this conjecture.",
                    "sentence_kor": "공식 언어로 불포화 네트워크를 훈련한 실험 결과는 이 추측을 뒷받침한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "628",
            "abstractID": "SPA_abs-29",
            "text": [
                {
                    "index": "29-0",
                    "sentence": "We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters.",
                    "sentence_kor": "텍스트 데이터의 순위-주파수 관계는 f r r-((r+))-,를 따른다. 여기서 f는 토큰 빈도이고 r은 빈도별 순위이며 (eters, ,, ))는 매개변수로 한다.",
                    "tag": "1"
                },
                {
                    "index": "29-1",
                    "sentence": "The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f).",
                    "sentence_kor": "공식은 d2 (x+y)/dx2가 전형적인 임펄스 함수라는 경험적 관측에 기초하여 도출되며, 여기서 (x,y)=(log r, log f)이다.",
                    "tag": "1"
                },
                {
                    "index": "29-2",
                    "sentence": "The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0.",
                    "sentence_kor": "공식은 Δ=0일 때 멱함수이고 Δ=0일 때 Zipf-Mandelbrot 법칙입니다.",
                    "tag": "1"
                },
                {
                    "index": "29-3",
                    "sentence": "We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora.",
                    "sentence_kor": "우리는 다국어 말뭉치에 대한 조사를 통해 𝛼는 구문의 분석적 특징과 ++ to는 자연어의 형태학적 특징과 관련이 있음을 보여준다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "629",
            "abstractID": "SPA_abs-30",
            "text": [
                {
                    "index": "30-0",
                    "sentence": "Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training.",
                    "sentence_kor": "태그 지정 및 기계 판독 이해와 같은 많은 NLP 작업은 심각한 데이터 불균형 문제에 직면해 있다. 부정적인 예가 긍정적인 예보다 훨씬 많으며 쉬운 예제의 수가 훈련을 압도한다.",
                    "tag": "1"
                },
                {
                    "index": "30-1",
                    "sentence": "The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.",
                    "sentence_kor": "가장 일반적으로 사용되는 교차 엔트로피(CE) 기준은 사실 정확도 지향 목표이므로 훈련과 시험 사이에 불일치를 발생시킨다. 훈련 시 각 훈련 인스턴스는 목표 기능에 동일하게 기여하는 반면 시험 시간에는 F1 점수가 긍정적인 예에 대해 더 많이 우려한다.",
                    "tag": "1"
                },
                {
                    "index": "30-2",
                    "sentence": "In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks.",
                    "sentence_kor": "본 논문에서, 우리는 데이터 불균형 NLP 작업에 대한 표준 교차 엔트로피 목표를 대체하기 위해 주사위 손실을 사용할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "30-3",
                    "sentence": "Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue.",
                    "sentence_kor": "주사위 손실은 Sørensen--Dice 계수 또는 Tversky 지수에 기초하고, 이는 잘못된 긍정과 잘못된 부정에 유사한 중요성을 부여하며, 데이터 불균형 문제에 더 면역성이 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "30-4",
                    "sentence": "To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.",
                    "sentence_kor": "훈련에서 쉬운 음성 예제의 지배적인 영향을 더욱 완화하기 위해 훈련 예제를 동적 조정 가중치와 연결하여 쉬운 음성 예제를 강조할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "30-5",
                    "sentence": "Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.",
                    "sentence_kor": "이론적 분석에 따르면 이 전략은 평가에서 F1 점수와 훈련에서 주사위 손실 사이의 격차를 줄여준다.",
                    "tag": "4"
                },
                {
                    "index": "30-6",
                    "sentence": "With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks.",
                    "sentence_kor": "제안된 교육 목표를 통해 광범위한 데이터 불균형 NLP 작업에서 상당한 성능 향상을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "30-7",
                    "sentence": "Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.",
                    "sentence_kor": "특히, 음성 태깅 작업의 부분에 대해서는 CTB5, CTB6, UD1.4에 대한 SOTA 결과, 명명된 개체 인식 작업에 대한 SOTA 결과, 기계 판독 및 분석 작업에 대한 경쟁 결과와 함께, CoNLL03, OnNotes5.0, MSRA 및 OnNotes4.0에 대한 SOTA 결과도 얻을 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "630",
            "abstractID": "SPA_abs-31",
            "text": [
                {
                    "index": "31-0",
                    "sentence": "This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance.",
                    "sentence_kor": "이 논문은 명시적 구문별 지침 없이 말뭉치에서 구문 학습 가능성에 대한 논의에 이론적으로 기여한다.",
                    "tag": "1+2"
                },
                {
                    "index": "31-1",
                    "sentence": "Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information.",
                    "sentence_kor": "우리의 접근법은 문법성(통사적 정보)과 의미/기도 정보를 정의하고 분리하는 데 사용하는 말뭉치의 관측 가능한 구조에서 비롯된다.",
                    "tag": "3"
                },
                {
                    "index": "31-2",
                    "sentence": "We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.",
                    "sentence_kor": "자율 구문의 형식적 특성을 설명하고 모델 형식에 대한 사전 가설 없이 간단한 최적화 프로세스로 구문 기반 어휘 범주를 검색할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "631",
            "abstractID": "SPA_abs-32",
            "text": [
                {
                    "index": "32-0",
                    "sentence": "We examine a methodology using neural language models (LMs) for analyzing the word order of language.",
                    "sentence_kor": "우리는 언어의 어순을 분석하기 위해 신경 언어 모델(LM)을 사용하는 방법론을 검토한다.",
                    "tag": "1+2"
                },
                {
                    "index": "32-1",
                    "sentence": "This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods.",
                    "sentence_kor": "이 LM 기반 방법은 카운트 기반 방법의 전처리기 오류 전파와 같이 기존 방법이 직면한 어려움을 극복할 수 있는 잠재력을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "32-2",
                    "sentence": "In this study, we explore whether the LM-based method is valid for analyzing the word order.",
                    "sentence_kor": "이 연구에서는 LM 기반 방법이 어순 분석에 유효한지 여부를 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "32-3",
                    "sentence": "As a case study, this study focuses on Japanese due to its complex and flexible word order.",
                    "sentence_kor": "사례 연구로서, 이 연구는 복잡하고 유연한 어순 때문에 일본어에 초점을 맞추고 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "32-4",
                    "sentence": "To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies.",
                    "sentence_kor": "LM 기반 방법을 검증하기 위해 (i) LM과 인간 어순 선호도 사이의 유사성 및 (ii) 이전 언어 연구와 LM 기반 방법을 사용하여 얻은 결과의 일관성을 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "32-5",
                    "sentence": "Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool.",
                    "sentence_kor": "우리의 실험을 통해, 우리는 LM이 분석 도구로서의 사용에 충분한 어순 지식을 보여준다는 잠정적인 결론을 내린다.",
                    "tag": "4"
                },
                {
                    "index": "32-6",
                    "sentence": "Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.",
                    "sentence_kor": "마지막으로, LM 기반 방법을 사용하여 대규모 실험에 의해 아직 분석되지 않은 표준 어순과 주제화 사이의 관계를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "632",
            "abstractID": "SPA_abs-33",
            "text": [
                {
                    "index": "33-0",
                    "sentence": "This paper solves the fake news detection problem under a more realistic scenario on social media.",
                    "sentence_kor": "본 논문은 소셜 미디어의 보다 현실적인 시나리오에서 가짜 뉴스 탐지 문제를 해결한다.",
                    "tag": "1"
                },
                {
                    "index": "33-1",
                    "sentence": "Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern.",
                    "sentence_kor": "소스 단문 트윗과 텍스트 코멘트가 없는 해당 리트윗 사용자의 시퀀스를 감안할 때, 소스 트윗이 가짜인지 여부를 예측하고 의심스러운 리트위터와 그들이 우려하는 단어에 대한 증거를 강조하여 설명을 생성하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "33-2",
                    "sentence": "We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal.",
                    "sentence_kor": "우리는 목표를 달성하기 위해 새로운 신경망 기반 모델인 그래프 인식 공동 주의 네트워크(GCAN)를 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "33-3",
                    "sentence": "Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average.",
                    "sentence_kor": "실제 트윗 데이터 세트에 대해 수행된 광범위한 실험에 따르면 GCAN은 평균 16%의 정확도로 최첨단 방법을 크게 능가할 수 있다.",
                    "tag": "4+5"
                },
                {
                    "index": "33-4",
                    "sentence": "In addition, the case studies also show that GCAN can produce reasonable explanations.",
                    "sentence_kor": "또한, 사례 연구는 GCAN이 합리적인 설명을 할 수 있다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "633",
            "abstractID": "SPA_abs-34",
            "text": [
                {
                    "index": "34-0",
                    "sentence": "Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views.",
                    "sentence_kor": "소셜미디어에서 논란이 되고 있는 게시물을 찾아내는 것은 민심을 수렴하고 사건의 영향을 평가하며 양극화된 시각을 완화하기 위한 근본적인 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "34-1",
                    "sentence": "However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set.",
                    "sentence_kor": "그러나 기존 방법은 1) 콘텐츠 관련 게시물의 의미 정보를 효과적으로 통합하지 못하고 2) 응답 관계 모델링을 위한 구조적 정보를 보존하며 3) 교육 세트의 내용과 다른 주제에서 게시물을 적절하게 처리한다.",
                    "tag": "1"
                },
                {
                    "index": "34-2",
                    "sentence": "To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection.",
                    "sentence_kor": "처음 두 가지 한계를 극복하기 위해, 우리는 사후 수준 논란 탐지를 위한 주제, 게시물 및 의견의 그래프 구조와 내용에서 정보를 통합하는 TPC-GCN(Topic-Post-Comment Graph Convolutional Network)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "34-3",
                    "sentence": "As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically.",
                    "sentence_kor": "세 번째 제한 사항에 대해서는 모델을 DTPC-GCN(Disentangled TPC-GCN)으로 확장하여 주제 관련 및 주제 관련 기능을 분리한 다음 동적으로 융합한다.",
                    "tag": "2+3"
                },
                {
                    "index": "34-4",
                    "sentence": "Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods.",
                    "sentence_kor": "두 개의 실제 데이터 세트에 대한 광범위한 실험은 우리 모델이 기존 방법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "34-5",
                    "sentence": "Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.",
                    "sentence_kor": "결과와 사례 분석은 우리의 모델이 의미론적 및 구조적 정보를 상당한 일반화 가능성과 통합할 수 있다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "634",
            "abstractID": "SPA_abs-35",
            "text": [
                {
                    "index": "35-0",
                    "sentence": "Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers.",
                    "sentence_kor": "현재 논란이 되고 있는 주제에 대한 언론 매체와 영향력 있는 사람들의 입장을 발견하는 것은 사회 통계학자들과 정책 입안자들에게 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "35-1",
                    "sentence": "Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly.",
                    "sentence_kor": "많은 감독 솔루션이 관점을 결정하기 위해 존재하지만 교육 데이터에 수동으로 주석을 다는 것은 비용이 많이 듭니다.",
                    "tag": "1"
                },
                {
                    "index": "35-2",
                    "sentence": "In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic.",
                    "sentence_kor": "본 논문에서, 우리는 리트윗 행동을 활용하여 편광 주제에 관한 트위터 사용자의 입장을 확인하기 위해 감독되지 않은 학습을 사용하는 계단식 방법을 제안한다. 그런 다음, 온라인 미디어와 인기 있는 트위터 사용자의 일반적인 정치적 성향 모두를 특성화하기 위해 사용자 레이블을 기반으로 한 감독 학습을 사용한다. 목표의 양극화 주제에 대한 그들의 입장으로서.",
                    "tag": "2+3"
                },
                {
                    "index": "35-3",
                    "sentence": "We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.",
                    "sentence_kor": "우리는 모델의 예측을 미디어 바이어스/팩트 체크 웹사이트의 골드 라벨과 비교하여 82.6%의 정확도를 달성하여 모델을 평가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "635",
            "abstractID": "SPA_abs-36",
            "text": [
                {
                    "index": "36-0",
                    "sentence": "The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science.",
                    "sentence_kor": "두 텍스트 본문을 비교하고 그들 사이의 용법이 다른 단어를 찾는 문제는 디지털 인문학과 컴퓨터 사회과학에서 종종 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "36-1",
                    "sentence": "This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large.",
                    "sentence_kor": "이는 일반적으로 각 말뭉치에 단어 임베딩을 훈련하고 벡터 공간을 정렬하며 정렬된 공간에서 코사인 거리가 큰 단어를 찾아 접근한다.",
                    "tag": "1"
                },
                {
                    "index": "36-2",
                    "sentence": "However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results.",
                    "sentence_kor": "그러나 이러한 방법은 종종 어휘를 광범위하게 필터링해야 잘 수행할 수 있으며, 이 연구에서 알 수 있듯이 불안정하고 따라서 신뢰성이 떨어지는 결과를 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "36-3",
                    "sentence": "We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word.",
                    "sentence_kor": "우리는 벡터 공간 정렬을 사용하지 않고 대신 각 단어의 이웃을 고려하는 대체 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "36-4",
                    "sentence": "The method is simple, interpretable and stable.",
                    "sentence_kor": "방법은 간단하고 해석 가능하며 안정적입니다.",
                    "tag": "6"
                },
                {
                    "index": "36-5",
                    "sentence": "We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).",
                    "sentence_kor": "다양한 말뭉치 분할 기준(나이, 트윗 작성자의 성별 및 직업, 트윗 시간)과 다른 언어(영어, 프랑스어 및 히브리어)를 고려하여 9가지 설정에서 그 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "636",
            "abstractID": "SPA_abs-37",
            "text": [
                {
                    "index": "37-0",
                    "sentence": "Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging.",
                    "sentence_kor": "감정 조절 가능한 응답 생성은 개방된 도메인 대화를 더 공감하고 참여시키는 것을 목표로 하는 매력적이고 가치 있는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "37-1",
                    "sentence": "Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process.",
                    "sentence_kor": "기존 방법은 주로 표준 교차 엔트로피 손실에 정규화 항을 추가하여 감정 표현을 강화하여 훈련 과정에 영향을 미친다.",
                    "tag": "1"
                },
                {
                    "index": "37-2",
                    "sentence": "However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified.",
                    "sentence_kor": "그러나 콘텐츠 일관성에 대한 추가적인 고려가 부족하기 때문에 대응 생성 과제의 공통적인 문제인 안전 대응이 강화된다.",
                    "tag": "1+2"
                },
                {
                    "index": "37-3",
                    "sentence": "Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence.",
                    "sentence_kor": "게다가, 질의와 응답 사이의 관계를 모델링하는 데 도움이 될 수 있는 질의 감정은 이전 모델에서 간단히 무시되어 일관성을 더욱 손상시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "37-4",
                    "sentence": "To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively.",
                    "sentence_kor": "이러한 문제를 완화하기 위해, 우리는 감정을 조절할 수 있는 응답 생성을 이중 과제로 확장하여 감정 반응과 감정 쿼리를 대신 생성하는 CDL(Curriculum Dual Learning)이라는 새로운 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "37-5",
                    "sentence": "CDL utilizes two rewards focusing on emotion and content to improve the duality.",
                    "sentence_kor": "CDL은 이중성을 개선하기 위해 감정과 내용에 초점을 맞춘 두 가지 보상을 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "37-6",
                    "sentence": "Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions.",
                    "sentence_kor": "또한 다양한 감정표현의 어려움을 바탕으로 점차적으로 수준 높은 반응을 이끌어내기 위해 커리큘럼 학습을 적용하고 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "37-7",
                    "sentence": "Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.",
                    "sentence_kor": "실험 결과에 따르면 CDL은 일관성, 다양성 및 감정 요소와의 관계 측면에서 기준선을 크게 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "637",
            "abstractID": "SPA_abs-38",
            "text": [
                {
                    "index": "38-0",
                    "sentence": "Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches.",
                    "sentence_kor": "대화 상태 추적(DST)의 최근 연구는 사전 정의된 온톨로지 기반 접근 방식의 확장성과 일반화 문제를 해결하기 위한 개방형 어휘 기반 설정에 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "38-1",
                    "sentence": "However, they are inefficient in that they predict the dialogue state at every turn from scratch.",
                    "sentence_kor": "그러나 대화 국면을 처음부터 끝까지 예측한다는 점에서 비효율적이다.",
                    "tag": "1"
                },
                {
                    "index": "38-2",
                    "sentence": "Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST.",
                    "sentence_kor": "여기서는 대화 상태를 명시적인 고정 크기 메모리로 간주하고 보다 효율적인 DST를 위해 선택적으로 덮어쓰는 메커니즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "38-3",
                    "sentence": "This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations.",
                    "sentence_kor": "이 메커니즘은 두 단계로 구성된다. (1) 각 메모리 슬롯의 상태 연산 예측, (2) 새로운 값으로 메모리를 덮어쓰며, 그 중 예측 상태 연산에 따라 생성되는 것은 소수에 불과하다.",
                    "tag": "3"
                },
                {
                    "index": "38-4",
                    "sentence": "Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder.",
                    "sentence_kor": "우리의 방법은 DST를 두 개의 하위 작업으로 분해하고 디코더가 작업 중 한 가지에만 집중하도록 유도하여 디코더의 부담을 줄인다.",
                    "tag": "3"
                },
                {
                    "index": "38-5",
                    "sentence": "This enhances the effectiveness of training and DST performance.",
                    "sentence_kor": "이는 교육 및 DST 수행의 효과를 향상시킵니다.",
                    "tag": "4"
                },
                {
                    "index": "38-6",
                    "sentence": "Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting.",
                    "sentence_kor": "SOM-DST(Dialogue State Tracking을 위한 메모리 선택적으로 덮어쓰기) 모델은 개방형 어휘 기반 DST 설정에서 MultiWOZ 2.0에서 51.72%, MultiWOZ 2.1에서 53.01%로 최첨단 공동 목표 정확도를 달성한다.",
                    "tag": "5"
                },
                {
                    "index": "38-7",
                    "sentence": "In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.",
                    "sentence_kor": "또한, 우리는 현재와 실제 상황 사이의 정확도 격차를 분석하고 DST 성능을 높이기 위해 상태 운영 예측을 개선하는 것이 유망한 방향임을 제안한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "638",
            "abstractID": "SPA_abs-39",
            "text": [
                {
                    "index": "39-0",
                    "sentence": "The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal.",
                    "sentence_kor": "목표 지향적인 대화 시스템은 사용자 목표를 달성하기 위해 다양한 상황에서 대화 흐름을 추적하고 효과적인 대화를 수행하는데 최적화되어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "39-1",
                    "sentence": "The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually.",
                    "sentence_kor": "그러한 대화 시스템을 구축하는 전통적인 접근법은 그것의 모듈들이 개별적으로 최적화되는 파이프라인 모듈형 구조를 취하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "39-2",
                    "sentence": "However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system.",
                    "sentence_kor": "그러나 이러한 최적화 계획이 반드시 전체 시스템의 전반적인 성능 향상을 제공하는 것은 아닙니다.",
                    "tag": "1"
                },
                {
                    "index": "39-3",
                    "sentence": "On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus.",
                    "sentence_kor": "반면에 단일 신경 구조를 가진 종단 간 대화 시스템은 말뭉치에서 사용할 수 있는 전체 주석을 고려하지 않고 입출력 발언으로만 훈련되는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "39-4",
                    "sentence": "This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.",
                    "sentence_kor": "이 계획은 시스템이 외부 시스템과 통합되어야 하거나 시스템이 특정 응답을 발생시킨 이유에 대한 해석 가능한 정보를 제공해야 하는 목표 지향적 대화를 어렵게 한다.",
                    "tag": "1"
                },
                {
                    "index": "39-5",
                    "sentence": "In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above.",
                    "sentence_kor": "본 논문에서, 우리는 위의 두 가지 과제를 모두 해결하는 대화 시스템을 위한 엔드 투 엔드 신경 아키텍처를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "39-6",
                    "sentence": "In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).",
                    "sentence_kor": "인간평가에서 대화시스템은 68.32의 성공률, 언어이해 점수 4.149, 응답적합성 점수 4.287을 달성하여 제8회 대화시스템 기술 과제(DSTC8)에서 엔드투엔드 다중 도메인 대화시스템 과제에서 시스템을 1위로 선정하였습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "639",
            "abstractID": "SPA_abs-40",
            "text": [
                {
                    "index": "40-0",
                    "sentence": "Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system.",
                    "sentence_kor": "응답이 해당 프롬프트에 대해 주제에서 벗어난 것인지 여부를 예측하는 것을 목표로 하는 업무인 주제에서 벗어난 구어 응답 감지는 자동 구어 평가 시스템에 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "40-1",
                    "sentence": "In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training.",
                    "sentence_kor": "많은 실제 교육 애플리케이션에서 주제 외 음성 응답 탐지기는 보이는 프롬프트뿐만 아니라 훈련 중에 보이지 않는 프롬프트에 대해서도 주제 외 응답에 대해 높은 리콜을 달성해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "40-2",
                    "sentence": "In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts.",
                    "sentence_kor": "본 논문에서, 우리는 보이는 프롬프트와 보이지 않는 프롬프트 모두에서 주제 외적인 회상을 갖는 주제 외 구어 응답 탐지를 위한 새로운 접근방식을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "40-3",
                    "sentence": "We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts.",
                    "sentence_kor": "우리는 새로운 모델인 GCBiA(Gated Convolutional 양방향 주의 기반 모델)를 소개하는데, 이 모델은 바이 어텐션 메커니즘과 컨볼루션을 적용하여 프롬프트와 응답의 핵심 문구를 추출하고 주요 계층 간의 게이트 단위와 잔류 연결을 도입하여 응답과 프롬프트의 관련성을 더 잘 표현한다.",
                    "tag": "3+4"
                },
                {
                    "index": "40-4",
                    "sentence": "Moreover, a new negative sampling method is proposed to augment training data.",
                    "sentence_kor": "또한 훈련 데이터를 강화하기 위해 새로운 음성 샘플링 방법이 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "40-5",
                    "sentence": "Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.",
                    "sentence_kor": "실험 결과는 우리의 새로운 접근 방식이 눈에 보이는 프롬프트와 보이지 않는 프롬프트 모두에 대해 극도로 높은 주제별 리콜을 가진 오프토픽 응답을 탐지하는 데 있어 상당한 개선을 달성할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "640",
            "abstractID": "SPA_abs-41",
            "text": [
                {
                    "index": "41-0",
                    "sentence": "Existing end-to-end dialog systems perform less effectively when data is scarce.",
                    "sentence_kor": "기존의 종단 간 대화 시스템은 데이터가 부족한 경우 성능이 저하됩니다.",
                    "tag": "1"
                },
                {
                    "index": "41-1",
                    "sentence": "To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems.",
                    "sentence_kor": "소수의 교육 사례만으로 실제 온라인 서비스에서 수용할 수 있는 성공을 거두려면 빠른 적응성과 신뢰성 있는 성능이 대화 시스템에 매우 바람직하다.",
                    "tag": "1"
                },
                {
                    "index": "41-2",
                    "sentence": "In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration.",
                    "sentence_kor": "본 논문에서 우리는 메타 학습 접근 방식과 인간-기계 협업의 장점을 결합한 메타 대화 시스템(MDS)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "41-3",
                    "sentence": "We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning.",
                    "sentence_kor": "우리는 저자원 목표 지향 대화 상자 학습을 위한 새로운 확장 bABI 데이터 세트와 변환된 MultiWOZ 데이터 세트에서 우리의 방법을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "41-4",
                    "sentence": "Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.",
                    "sentence_kor": "실험 결과에 따르면 MDS는 비 메타 학습 기준선을 크게 능가하며 확장 bABI 데이터 세트에서 10개의 대화 상자만으로 회전당 정확도가 90%를 넘을 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "641",
            "abstractID": "SPA_abs-42",
            "text": [
                {
                    "index": "42-0",
                    "sentence": "Neural-based context-aware models for slot tagging have achieved state-of-the-art performance.",
                    "sentence_kor": "슬롯 태깅을 위한 신경 기반 상황 인식 모델은 최첨단 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "42-1",
                    "sentence": "However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario.",
                    "sentence_kor": "그러나 특히 퓨샷 시나리오에서 OOV(Out-of-Vocab) 단어의 존재는 신경 기반 모델의 성능을 크게 저하시킨다.",
                    "tag": "1"
                },
                {
                    "index": "42-2",
                    "sentence": "In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge.",
                    "sentence_kor": "본 논문에서, 우리는 입력 텍스트의 상황적 표현과 대규모 어휘적 배경 지식을 통합하기 위한 새로운 지식 강화 슬롯 태깅 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "42-3",
                    "sentence": "Besides, we use multi-level graph attention to explicitly model lexical relations.",
                    "sentence_kor": "또한 어휘 관계를 명시적으로 모델링하기 위해 다단계 그래프 주의를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "42-4",
                    "sentence": "The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.",
                    "sentence_kor": "실험에 따르면 우리가 제안한 지식 통합 메커니즘은 두 개의 공개 벤치마크 데이터 세트에서 서로 다른 크기의 교육 데이터를 사용하여 설정에 걸쳐 일관된 개선을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "642",
            "abstractID": "SPA_abs-43",
            "text": [
                {
                    "index": "43-0",
                    "sentence": "Many studies have applied reinforcement learning to train a dialog policy and show great promise these years.",
                    "sentence_kor": "많은 연구들이 대화 정책을 훈련시키고 큰 가능성을 보여주기 위해 강화 학습을 적용했다.",
                    "tag": "1"
                },
                {
                    "index": "43-1",
                    "sentence": "One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms.",
                    "sentence_kor": "한 가지 일반적인 접근법은 사용자 시뮬레이터를 사용하여 강화 학습 알고리즘을 위해 시뮬레이션된 사용자 경험을 많이 얻는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "43-2",
                    "sentence": "However, modeling a realistic user simulator is challenging.",
                    "sentence_kor": "그러나 현실적인 사용자 시뮬레이터를 모델링하는 것은 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "43-3",
                    "sentence": "A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator.",
                    "sentence_kor": "규칙 기반 시뮬레이터는 복잡한 작업을 위해 많은 도메인 전문지식을 필요로 하며, 데이터 기반 시뮬레이터는 상당한 데이터를 필요로 하며 시뮬레이터를 평가하는 방법도 불분명하다.",
                    "tag": "1"
                },
                {
                    "index": "43-4",
                    "sentence": "To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents.",
                    "sentence_kor": "사전에 사용자 시뮬레이터를 명시적으로 구축하지 않기 위해 시스템과 사용자를 대화 에이전트로 간주하는 Multi-에이전트 대화 상자 정책 학습을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "43-5",
                    "sentence": "Two agents interact with each other and are jointly learned simultaneously.",
                    "sentence_kor": "두 에이전트가 상호 작용하고 동시에 학습됩니다.",
                    "tag": "3"
                },
                {
                    "index": "43-6",
                    "sentence": "The method uses the actor-critic framework to facilitate pretraining and improve scalability.",
                    "sentence_kor": "이 방법은 사전 교육을 용이하게 하고 확장성을 개선하기 위해 행위자-비판 프레임워크를 사용합니다.",
                    "tag": "3"
                },
                {
                    "index": "43-7",
                    "sentence": "We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog.",
                    "sentence_kor": "또한 작업 지향 대화 상자에서 각 에이전트의 역할별 도메인 지식을 통합하기 위해 역할 인식 보상 분해를 위한 하이브리드 가치 네트워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "43-8",
                    "sentence": "Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.",
                    "sentence_kor": "결과는 우리의 방법이 시스템 정책과 사용자 정책을 동시에 성공적으로 구축할 수 있으며, 두 에이전트가 대화 상호작용을 통해 높은 작업 성공률을 달성할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "643",
            "abstractID": "SPA_abs-44",
            "text": [
                {
                    "index": "44-0",
                    "sentence": "Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set.",
                    "sentence_kor": "신경 생성 모델은 대규모 데이터 세트가 주어진 경우 대화 상자 생성 작업에서 유망한 성능을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "44-1",
                    "sentence": "However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings.",
                    "sentence_kor": "그러나 고품질 대화 상자 데이터의 부족과 값비싼 데이터 주석 프로세스로 인해 실제 환경에서의 적용이 크게 제한됩니다.",
                    "tag": "1"
                },
                {
                    "index": "44-2",
                    "sentence": "We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance.",
                    "sentence_kor": "대화 상자 생성 성능을 향상시키기 위해 패러프레이즈 모델과 응답 생성 모델을 공동으로 교육하는 증강 응답 생성(PARG) 프레임워크를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "44-3",
                    "sentence": "We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels.",
                    "sentence_kor": "또한 대화 상자 상태 및 대화 상자 작업 레이블을 기반으로 훈련 데이터 세트를 자동으로 구성하는 방법을 설계한다.",
                    "tag": "1"
                },
                {
                    "index": "44-4",
                    "sentence": "PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019).",
                    "sentence_kor": "PARG는 TSCP(Lei et al., 2018) 및 DAMD(Zhang et al., 2019)와 같은 다양한 대화 상자 생성 모델에 적용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "44-5",
                    "sentence": "Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ.",
                    "sentence_kor": "실험 결과에 따르면 제안된 프레임워크는 CamRest676 및 MultiWOZ에서 이러한 최첨단 대화 상자 모델을 추가로 개선한다.",
                    "tag": "1"
                },
                {
                    "index": "44-6",
                    "sentence": "PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.",
                    "sentence_kor": "또한 PARG는 특히 낮은 자원 설정에서 대화 상자 생성 작업에서 다른 데이터 확대 방법을 크게 능가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "644",
            "abstractID": "SPA_abs-45",
            "text": [
                {
                    "index": "45-0",
                    "sentence": "Neural conversation models are known to generate appropriate but non-informative responses in general.",
                    "sentence_kor": "신경 대화 모델은 적절하지만 일반적으로 비정보적인 반응을 생성하는 것으로 알려져 있다.",
                    "tag": "1"
                },
                {
                    "index": "45-1",
                    "sentence": "A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document.",
                    "sentence_kor": "정보성이 크게 향상될 수 있는 시나리오는 읽기를 통한 대화(CbR)이며, 여기서 주어진 외부 문서와 관련하여 대화가 이루어진다.",
                    "tag": "1"
                },
                {
                    "index": "45-2",
                    "sentence": "In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory.",
                    "sentence_kor": "이전 연구에서 외부 문서는 (1) 문서의 정보와 대화 컨텍스트를 통합하는 상황 인식 문서 메모리를 만든 다음 (2) 메모리에 대한 응답을 생성하는 방식으로 활용된다.",
                    "tag": "1"
                },
                {
                    "index": "45-3",
                    "sentence": "In this paper, we propose to create the document memory with some anticipated responses in mind.",
                    "sentence_kor": "본 논문에서는 예상되는 응답을 염두에 두고 문서 메모리를 만들 것을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "45-4",
                    "sentence": "This is achieved using a teacher-student framework.",
                    "sentence_kor": "이것은 교사-학생 프레임워크를 사용하여 달성된다.",
                    "tag": "1"
                },
                {
                    "index": "45-5",
                    "sentence": "The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information.",
                    "sentence_kor": "교사는 외부 문서, 상황 및 지상 진실 응답을 제공받고 세 가지 정보 출처로부터 응답 인식 문서 메모리를 구축하는 방법을 배운다.",
                    "tag": "1"
                },
                {
                    "index": "45-6",
                    "sentence": "The student learns to construct a response-anticipated document memory from the first two sources, and teacher’s insight on memory creation.",
                    "sentence_kor": "학생은 처음 두 출처로부터 응답 예상 문서 기억과 기억 생성에 대한 선생님의 통찰력을 구성하는 방법을 배운다.",
                    "tag": "1"
                },
                {
                    "index": "45-7",
                    "sentence": "Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.",
                    "sentence_kor": "경험적 결과에 따르면 우리 모델은 CbR 작업에 대해 이전의 최첨단 모델을 능가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "645",
            "abstractID": "SPA_abs-46",
            "text": [
                {
                    "index": "46-0",
                    "sentence": "Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems.",
                    "sentence_kor": "대화 정책 최적화는 종종 과제 지향 대화 시스템에서 과제가 완료될 때까지 피드백을 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "46-1",
                    "sentence": "This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues.",
                    "sentence_kor": "감독 신호(또는 보상)는 대화 끝에 제공되기 때문에 중간 대화 턴 훈련에는 충분하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "46-2",
                    "sentence": "To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards.",
                    "sentence_kor": "이 문제를 해결하기 위해, 턴 바이 턴 보상을 제공하는 최적의 정책의 상태-행동 쌍으로부터 학습하는 보상 학습이 도입되었다.",
                    "tag": "1"
                },
                {
                    "index": "46-3",
                    "sentence": "This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive.",
                    "sentence_kor": "이 접근법은 노동 집약적인 인간 대 인간 대화(즉, 전문가 데모)의 완전한 상태 행동 주석을 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "46-4",
                    "sentence": "To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning.",
                    "sentence_kor": "이러한 한계를 극복하기 위해 준감독 정책 학습을 위한 새로운 보상 학습 접근법을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "46-5",
                    "sentence": "The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations.",
                    "sentence_kor": "제안된 접근법은 주석 유무에 관계없이 전문가 시연을 기반으로 대화 진행 상황(즉, 상태-행동 순서)을 모델링하는 보상 함수로 역학 모델을 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "46-6",
                    "sentence": "The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations.",
                    "sentence_kor": "동적 모델은 대화 진행이 전문가 시연과 일치하는지 여부를 예측하여 보상을 계산한다.",
                    "tag": "1"
                },
                {
                    "index": "46-7",
                    "sentence": "We further propose to learn action embeddings for a better generalization of the reward function.",
                    "sentence_kor": "우리는 또한 보상 함수의 더 나은 일반화를 위해 행동 임베딩을 배울 것을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "46-8",
                    "sentence": "The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.",
                    "sentence_kor": "제안된 접근 방식은 벤치마크 다중 도메인 데이터 세트인 MultiWOZ에서 경쟁력 있는 정책 학습 기준을 능가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "646",
            "abstractID": "SPA_abs-47",
            "text": [
                {
                    "index": "47-0",
                    "sentence": "In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations.",
                    "sentence_kor": "모듈식 대화 시스템에서 자연어 이해(NLU)와 자연어 생성(NLG)은 두 가지 중요한 구성 요소이며, 여기서 NLU는 주어진 텍스트에서 의미론을 추출하고 NLG는 입력 의미 표현을 기반으로 해당 자연어 문장을 구성하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "47-1",
                    "sentence": "However, the dual property between understanding and generation has been rarely explored.",
                    "sentence_kor": "그러나 이해와 세대 간의 이중적 특성은 거의 연구되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "47-2",
                    "sentence": "The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework.",
                    "sentence_kor": "이전 연구는 이중 지도 학습 프레임워크를 통해 성능을 향상시키기 위해 NLU와 NLG의 이중성을 활용한 첫 번째 시도이다.",
                    "tag": "1"
                },
                {
                    "index": "47-3",
                    "sentence": "However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion.",
                    "sentence_kor": "그러나 이전 연구에서는 여전히 두 구성 요소를 감독 방식으로 학습했다. 대신, 본 논문은 이러한 이중성을 효과적으로 활용하기 위한 일반적인 학습 프레임워크를 도입하여 언어 이해 및 생성 모델을 공동 방식으로 훈련하기 위해 감독 및 감독되지 않은 학습 알고리즘을 통합하는 유연성을 제공한다.",
                    "tag": "1+2"
                },
                {
                    "index": "47-4",
                    "sentence": "The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.",
                    "sentence_kor": "벤치마크 실험은 제안된 접근 방식이 NLU와 NLG 모두의 성능을 향상시킬 수 있음을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "47-5",
                    "sentence": "The source code is available at: https://github.com/MiuLab/DuaLUG.",
                    "sentence_kor": "소스 코드는 https://github.com/MiuLab/DuaLUG에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "647",
            "abstractID": "SPA_abs-48",
            "text": [
                {
                    "index": "48-0",
                    "sentence": "The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research.",
                    "sentence_kor": "대화상자에 대한 의미 있는 자동 평가 지표의 부족으로 인해 열린 도메인 대화상자 연구가 지연되었다.",
                    "tag": "1"
                },
                {
                    "index": "48-1",
                    "sentence": "Standard language generation metrics have been shown to be ineffective for evaluating dialog models.",
                    "sentence_kor": "표준 언어 생성 지표는 대화 상자 모델 평가에 비효율적인 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "48-2",
                    "sentence": "To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog.",
                    "sentence_kor": "이를 위해 본 논문은 대화 상자에 대한 감독되지 않고 참조가 없는 평가 지표인 USR을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "48-3",
                    "sentence": "USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog.",
                    "sentence_kor": "USR은 몇 가지 바람직한 대화 품질을 측정하기 위해 감독되지 않은 모델을 훈련시키는 기준이 없는 측정 기준이다.",
                    "tag": "3"
                },
                {
                    "index": "48-4",
                    "sentence": "USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0).",
                    "sentence_kor": "USR은 Topic-Chat(턴 레벨: 0.42, 시스템 레벨: 1.0)과 PersonaChat(턴 레벨: 0.48 및 시스템 레벨: 1.0) 모두에 대한 인간의 판단과 밀접한 상관관계가 있는 것으로 나타났다.",
                    "tag": "4"
                },
                {
                    "index": "48-5",
                    "sentence": "USR additionally produces interpretable measures for several desirable properties of dialog.",
                    "sentence_kor": "USR은 또한 몇 가지 바람직한 대화 상자 속성에 대한 해석 가능한 측정값을 생성한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "648",
            "abstractID": "SPA_abs-49",
            "text": [
                {
                    "index": "49-0",
                    "sentence": "Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts.",
                    "sentence_kor": "단어의 사전 정의를 자동으로 생성하는 것을 목표로 하는 정의 생성은 최근 사전의 구성을 돕고 사람들이 익숙하지 않은 텍스트를 이해하는 것을 돕기 위해 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "49-1",
                    "sentence": "However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results.",
                    "sentence_kor": "그러나 이전 연구에서는 정의의 \"구성 요소\"를 명시적으로 모델링하는 것을 거의 고려하지 않아 특정 생성 결과가 불충분하다.",
                    "tag": "1"
                },
                {
                    "index": "49-2",
                    "sentence": "In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation.",
                    "sentence_kor": "본 논문에서, 우리는 ESD, 즉 정의 생성을 위한 명시적 의미 분해를 제안하는데, 이는 단어의 의미를 명시적으로 의미 구성 요소로 분해하고 정의 생성을 위한 이산 잠재 변수로 모델링한다.",
                    "tag": "2+3"
                },
                {
                    "index": "49-3",
                    "sentence": "Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.",
                    "sentence_kor": "실험 결과에 따르면 WordNet 및 Oxford 벤치마크에서 이전의 강력한 기준을 능가하는 최고의 결과를 달성했습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "649",
            "abstractID": "SPA_abs-50",
            "text": [
                {
                    "index": "50-0",
                    "sentence": "Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss.",
                    "sentence_kor": "신경 언어 모델은 일반적으로 로그 손실을 최소화하여 대규모 말뭉치의 분포 특성과 일치하도록 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "50-1",
                    "sentence": "While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts).",
                    "sentence_kor": "이 접근 방식은 최적화는 간단하지만 모델이 노이즈가 많고 잘못된 참조(예: 잘못된 주석 및 환각 사실)를 포함하여 데이터 세트의 모든 변형을 재현하도록 강제한다.",
                    "tag": "1"
                },
                {
                    "index": "50-2",
                    "sentence": "Even a small fraction of noisy data can degrade the performance of log loss.",
                    "sentence_kor": "노이즈가 많은 데이터의 극히 일부라도 로그 손실의 성능을 저하시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "50-3",
                    "sentence": "As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references.",
                    "sentence_kor": "대안으로, 이전 연구에서는 생성된 샘플의 구별성을 최소화하는 것이 잘못된 참조를 처리할 수 있는 원칙적이고 강력한 손실임을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "50-4",
                    "sentence": "However, distinguishability has not been used in practice due to challenges in optimization and estimation.",
                    "sentence_kor": "그러나 최적화와 추정의 어려움 때문에 구별성은 실제로 사용되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "50-5",
                    "sentence": "We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability.",
                    "sentence_kor": "우리는 구별성을 최적화하기 위한 방법으로 높은 로그 손실 예를 적응적으로 제거하는 간단하고 확장 가능한 절차인 손실 절단을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "50-6",
                    "sentence": "Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task.",
                    "sentence_kor": "경험적으로, 우리는 손실 절단이 요약 작업의 구별 가능성에 대한 기존 기준선을 능가한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "50-7",
                    "sentence": "Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.",
                    "sentence_kor": "또한 손실 절단 모델에 의해 생성된 샘플이 기준선을 초과하고 인간의 참조와 일치하는 사실적 정확도 등급을 가지고 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "650",
            "abstractID": "SPA_abs-51",
            "text": [
                {
                    "index": "51-0",
                    "sentence": "Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models.",
                    "sentence_kor": "레이블이 지정된 가장자리를 가진 그래프에 대한 효율적인 구조 인코딩은 많은 그래프 기반 모델에서 중요하지만 어려운 점이다.",
                    "tag": "1"
                },
                {
                    "index": "51-1",
                    "sentence": "This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR).",
                    "sentence_kor": "본 연구는 AMR-텍스트 생성 – 추상적 의미 표현(AMR)에서 자연어를 복구하는 것을 목표로 하는 그래프-시퀀스 작업에 초점을 맞추고 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "51-2",
                    "sentence": "Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations:",
                    "sentence_kor": "기존의 그래프 대 시퀀스 접근법은 일반적으로 그래프 신경망을 인코더로 활용하는데, 여기에는 두 가지 제한이 있다.",
                    "tag": "1"
                },
                {
                    "index": "51-3",
                    "sentence": "1) The message propagation process in AMR graphs is only guided by the first-order adjacency information.",
                    "sentence_kor": "1) AMR 그래프의 메시지 전달과정은 1차 인접정보에 의해서만 유도된다.",
                    "tag": "1"
                },
                {
                    "index": "51-4",
                    "sentence": "2) The relationships between labeled edges are not fully considered.",
                    "sentence_kor": "2) 라벨이 부착된 가장자리 사이의 관계를 충분히 고려하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "51-5",
                    "sentence": "In this work, we propose a novel graph encoding framework which can effectively explore the edge relations.",
                    "sentence_kor": "본 연구에서는 에지 관계를 효과적으로 탐색할 수 있는 새로운 그래프 인코딩 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "51-6",
                    "sentence": "We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs.",
                    "sentence_kor": "또한 AMR 그래프의 풍부한 구조를 인코딩하기 위해 고차 이웃 정보가 있는 그래프 주의 네트워크를 채택한다.",
                    "tag": "3"
                },
                {
                    "index": "51-7",
                    "sentence": "Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets.",
                    "sentence_kor": "실험 결과에 따르면 우리의 접근 방식은 영어 AMR 벤치마크 데이터 세트에서 새로운 최첨단 성능을 얻는다.",
                    "tag": "4"
                },
                {
                    "index": "51-8",
                    "sentence": "The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.",
                    "sentence_kor": "또한 절제 분석은 에지 관계와 고차 정보 모두 그래프 대 시퀀스 모델링에 도움이 된다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "651",
            "abstractID": "SPA_abs-52",
            "text": [
                {
                    "index": "52-0",
                    "sentence": "Neural text generation has made tremendous progress in various tasks.",
                    "sentence_kor": "신경 텍스트 생성은 다양한 작업에서 엄청난 발전을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "52-1",
                    "sentence": "One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating.",
                    "sentence_kor": "대부분의 작업의 한 가지 공통적인 특징은 본문이 생성 시 일부 엄격한 형식으로 제한되지 않는다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "52-2",
                    "sentence": "However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc.",
                    "sentence_kor": "그러나, 우리는 가사 (음악이 주어진다고 가정), 소네트, 송치 등과 같은 특별한 텍스트 패러다임에 직면할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "52-3",
                    "sentence": "The typical characteristics of these texts are in three folds:",
                    "sentence_kor": "이러한 텍스트의 일반적인 특성은 세 가지 접힘으로 구성됩니다.",
                    "tag": "1"
                },
                {
                    "index": "52-4",
                    "sentence": "(1) They must comply fully with the rigid predefined formats.",
                    "sentence_kor": "(1) 반드시 사전에 정해진 엄격한 형식을 준수해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "52-5",
                    "sentence": "(2) They must obey some rhyming schemes.",
                    "sentence_kor": "(2) 운율 체계를 따라야 한다.",
                    "tag": "1"
                },
                {
                    "index": "52-6",
                    "sentence": "(3) Although they are restricted to some formats, the sentence integrity must be guaranteed.",
                    "sentence_kor": "(3) 일부 형식에 제한되나, 문장의 건전성이 보장되어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "52-7",
                    "sentence": "To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated.",
                    "sentence_kor": "우리가 아는 한, 미리 정의된 엄격한 형식에 기초한 텍스트 생성은 잘 조사되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "52-8",
                    "sentence": "Therefore, we propose a simple and elegant framework named SongNet to tackle this problem.",
                    "sentence_kor": "따라서 이 문제를 해결하기 위해 SongNet이라는 간단하고 우아한 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "52-9",
                    "sentence": "The backbone of the framework is a Transformer-based auto-regressive language model.",
                    "sentence_kor": "프레임워크의 백본은 트랜스포머 기반 자동 회귀 언어 모델이다.",
                    "tag": "3"
                },
                {
                    "index": "52-10",
                    "sentence": "Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity.",
                    "sentence_kor": "기호 집합은 특히 형식, 운율 및 문장 무결성에 대한 모델링 성능을 향상시키도록 맞춤 설계됩니다.",
                    "tag": "1"
                },
                {
                    "index": "52-11",
                    "sentence": "We improve the attention mechanism to impel the model to capture some future information on the format.",
                    "sentence_kor": "우리는 모델이 형식에 대한 미래 정보를 캡처하도록 하기 위해 주의 메커니즘을 개선한다.",
                    "tag": "1"
                },
                {
                    "index": "52-12",
                    "sentence": "A pre-training and fine-tuning framework is designed to further improve the generation quality.",
                    "sentence_kor": "사전 교육 및 미세 조정 프레임워크는 발전 품질을 더욱 향상시키도록 설계되었다.",
                    "tag": "1"
                },
                {
                    "index": "52-13",
                    "sentence": "Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.",
                    "sentence_kor": "두 개의 수집된 말뭉치에 대해 수행된 광범위한 실험은 우리가 제안한 프레임워크가 자동 측정 기준과 인간 평가 측면에서 훨씬 더 나은 결과를 생성한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "652",
            "abstractID": "SPA_abs-53",
            "text": [
                {
                    "index": "53-0",
                    "sentence": "Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form.",
                    "sentence_kor": "질문 생성(QG)은 기본적으로 단순한 구문 변환이지만 의미론의 많은 측면은 어떤 질문을 형성하기에 좋은지에 영향을 미친다.",
                    "tag": "1"
                },
                {
                    "index": "53-1",
                    "sentence": "We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs.",
                    "sentence_kor": "보편적 의존성, 얕은 의미 구문 분석, 어휘 리소스 및 선언적 문장을 질문-답변 쌍으로 변환하는 사용자 정의 규칙을 활용하는 투명한 구문 규칙 집합인 Syn-QG를 개발하여 이러한 관찰을 구현한다.",
                    "tag": "1"
                },
                {
                    "index": "53-2",
                    "sentence": "We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems.",
                    "sentence_kor": "우리는 PropBank 인수 설명과 VerbNet 상태 술어를 사용하여 얕은 의미론적 내용을 통합하며, 이는 설명적 성격의 질문을 생성하고 기존 시스템보다 추리적이고 의미론적으로 풍부한 질문을 생성하는 데 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "53-3",
                    "sentence": "In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules.",
                    "sentence_kor": "구문적 유창성을 개선하고 문법적으로 틀린 질문을 제거하기 위해, 우리는 이러한 구문적 규칙의 출력에 역번역을 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "53-4",
                    "sentence": "A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.",
                    "sentence_kor": "일련의 크라우드 소싱 평가 결과, 우리 시스템은 이전 QG 시스템보다 더 많은 수의 문법적이고 관련성 있는 질문을 생성할 수 있으며 역번역하면 관련 없는 질문을 생성하는 약간의 비용으로 문법성을 크게 향상시킬 수 있다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "653",
            "abstractID": "SPA_abs-54",
            "text": [
                {
                    "index": "54-0",
                    "sentence": "Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution.",
                    "sentence_kor": "짧은 텍스트 스트림 클러스터링은 무한 길이, 희박한 데이터 표현 및 클러스터 진화라는 고유한 특성 때문에 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "54-1",
                    "sentence": "Existing approaches often exploit short text streams in a batch way.",
                    "sentence_kor": "기존 접근 방식은 종종 배치 방식으로 짧은 텍스트 스트림을 활용합니다.",
                    "tag": "1"
                },
                {
                    "index": "54-2",
                    "sentence": "However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve.",
                    "sentence_kor": "그러나 주제가 진화할 때 사전 지식이 없기 때문에 최적의 배치 크기를 결정하는 것은 일반적으로 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "54-3",
                    "sentence": "In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering.",
                    "sentence_kor": "또한 그래픽 모델에서 전통적인 독립 단어 표현은 짧은 텍스트 군집화에 \"항 모호성\" 문제를 일으키는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "54-4",
                    "sentence": "Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way.",
                    "sentence_kor": "따라서 본 논문에서 우리는 단어 발생 의미 정보(즉, 컨텍스트)를 새로운 그래픽 모델에 통합하고 각 짧은 텍스트가 온라인 방식으로 자동으로 도착하는 클러스터인 OSDM이라는 짧은 섹스트 스트림 클러스터링을 위한 온라인 의미 강화 디리클레 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "54-5",
                    "sentence": "Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.",
                    "sentence_kor": "광범위한 결과에 따르면 OSDM은 합성 및 실제 데이터셋 모두에서 많은 최신 알고리즘에 비해 성능이 우수합니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "654",
            "abstractID": "SPA_abs-55",
            "text": [
                {
                    "index": "55-0",
                    "sentence": "Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint.",
                    "sentence_kor": "생성 의미 해싱은 빠른 검색 속도와 작은 메모리 설치 공간 덕분에 대규모 정보 검색에 유망한 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "55-1",
                    "sentence": "For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes.",
                    "sentence_kor": "훈련의 추적성을 위해 기존 생성 해시 방법은 대부분 사후 분포에 대한 인수분해 형태를 가정하여 해시 코드 비트 사이에 독립성을 강화한다.",
                    "tag": "1"
                },
                {
                    "index": "55-2",
                    "sentence": "From the perspectives of both model representation and code space size, independence is always not the best assumption.",
                    "sentence_kor": "모델 표현과 코드 공간 크기 모두의 관점에서, 독립성이 항상 최선의 가정은 아니다.",
                    "tag": "1"
                },
                {
                    "index": "55-3",
                    "sentence": "In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior.",
                    "sentence_kor": "본 논문에서는 해시 코드 비트 간의 상관 관계를 소개하기 위해 볼츠만 기계의 분포를 변형 후분위로 사용할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "55-4",
                    "sentence": "To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution.",
                    "sentence_kor": "훈련의 난치성 문제를 해결하기 위해 먼저 볼츠만 기계의 분포를 가우스와 같은 분포와 베르누이 분포의 계층적 연결로 보강하여 재매개미터화하는 대략적인 방법을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "55-5",
                    "sentence": "Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO).",
                    "sentence_kor": "이를 바탕으로 증거 하한(ELBO)에 대해 점근적으로 정확한 하한이 추가로 도출된다.",
                    "tag": "4"
                },
                {
                    "index": "55-6",
                    "sentence": "With these novel techniques, the entire model can be optimized efficiently.",
                    "sentence_kor": "이러한 새로운 기법으로 전체 모델을 효율적으로 최적화할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "55-7",
                    "sentence": "Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.",
                    "sentence_kor": "광범위한 실험 결과는 해시 코드 내에서 서로 다른 비트 간의 상관 관계를 효과적으로 모델링함으로써 우리 모델이 상당한 성능 향상을 달성할 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "655",
            "abstractID": "SPA_abs-56",
            "text": [
                {
                    "index": "56-0",
                    "sentence": "We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis.",
                    "sentence_kor": "인간과 기계 간의 대화형 프로세스를 통해 텍스트 분석을 위한 용어 사전을 구성하는 방법을 제안하며, 이는 일반적인 텍스트 분석에 필요한 정밀도로 유연한 사전을 만드는 데 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "56-1",
                    "sentence": "This paper introduces the first formulation of interactive dictionary construction to address this issue.",
                    "sentence_kor": "이 논문은 이 문제를 해결하기 위한 대화형 사전 구성의 첫 번째 공식화를 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "56-2",
                    "sentence": "To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms.",
                    "sentence_kor": "상호작용을 최적화하기 위해 소수의 표본 항에서 시작하여 분석가의 의도를 효과적으로 포착하는 새로운 알고리즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "56-3",
                    "sentence": "Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task.",
                    "sentence_kor": "알고리즘과 함께 사전 작성 작업에 대한 대화형 방법에 대한 체계적인 평가를 제공하는 자동 평가 프레임워크도 설계한다.",
                    "tag": "3"
                },
                {
                    "index": "56-4",
                    "sentence": "Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.",
                    "sentence_kor": "실제 시나리오 기반 말뭉치와 사전을 사용한 실험은 우리의 알고리즘이 기준 방법을 능가하며 적은 수의 상호작용에서도 작동한다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "656",
            "abstractID": "SPA_abs-57",
            "text": [
                {
                    "index": "57-0",
                    "sentence": "This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches.",
                    "sentence_kor": "이 논문은 가지가 무한히 많은 트리에 주제 분포를 갖는 트리 구조 신경 주제 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "57-1",
                    "sentence": "Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks.",
                    "sentence_kor": "우리 모델은 이중 순환 신경망을 적용하여 무한 조상 및 이란성 주제 분포를 매개 변수화한다.",
                    "tag": "3"
                },
                {
                    "index": "57-2",
                    "sentence": "With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010).",
                    "sentence_kor": "변형 베이즈 자동 인코딩의 도움으로, 우리 모델은 이전의 트리 구조 주제 모델과 비교하여 잠재 주제와 트리 구조를 유도할 때 데이터 확장성을 개선하고 경쟁력 있는 성능을 달성한다(Blei et al., 2010).",
                    "tag": "3"
                },
                {
                    "index": "57-3",
                    "sentence": "This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.",
                    "sentence_kor": "이 작업은 트리 구조 주제 모델을 확장하여 다운스트림 작업을 위한 신경 모델과 통합할 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "657",
            "abstractID": "SPA_abs-58",
            "text": [
                {
                    "index": "58-0",
                    "sentence": "We focus on the task of Frequently Asked Questions (FAQ) retrieval.",
                    "sentence_kor": "우리는 자주 묻는 질문(FAQ) 검색 작업에 중점을 둔다.",
                    "tag": "1"
                },
                {
                    "index": "58-1",
                    "sentence": "A given user query can be matched against the questions and/or the answers in the FAQ.",
                    "sentence_kor": "주어진 사용자 질의를 FAQ의 질문 및/또는 답변과 일치시킬 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "58-2",
                    "sentence": "We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models.",
                    "sentence_kor": "FAQ 쌍을 활용하여 두 개의 BERT 모델을 교육하는 완전히 감독되지 않은 방법을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "58-3",
                    "sentence": "The two models match user queries to FAQ answers and questions, respectively.",
                    "sentence_kor": "두 모델은 사용자 질의와 FAQ 답변 및 질문을 각각 일치시킵니다.",
                    "tag": "1"
                },
                {
                    "index": "58-4",
                    "sentence": "We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases.",
                    "sentence_kor": "우리는 고품질의 질문 패러프레이즈를 자동으로 생성하여 후자의 누락된 라벨링 데이터를 완화한다.",
                    "tag": "1"
                },
                {
                    "index": "58-5",
                    "sentence": "We show that our model is on par and even outperforms supervised models on existing datasets.",
                    "sentence_kor": "우리는 우리의 모델이 동등한 수준이며 심지어 기존 데이터 세트에서 감독되는 모델보다 성능이 우수하다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "658",
            "abstractID": "SPA_abs-59",
            "text": [
                {
                    "index": "59-0",
                    "sentence": "Humor plays an important role in human languages and it is essential to model humor when building intelligence systems.",
                    "sentence_kor": "유머는 인간 언어에서 중요한 역할을 하며 지능 시스템을 구축할 때 유머를 본떠서 만드는 것은 필수적이다.",
                    "tag": "1"
                },
                {
                    "index": "59-1",
                    "sentence": "Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity.",
                    "sentence_kor": "유머의 다른 형태들 중에서, 말장난은 이중 엔트리와 높은 음성 유사성을 가진 단어들을 사용함으로써 유머 효과를 위한 단어 놀이를 한다.",
                    "tag": "1"
                },
                {
                    "index": "59-2",
                    "sentence": "However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks.",
                    "sentence_kor": "그러나 말장난을 식별하고 모델링하는 것은 일반적으로 암묵적 의미론 또는 음운론적 트릭을 포함하는 말장난으로서 어려운 일이다.",
                    "tag": "1"
                },
                {
                    "index": "59-3",
                    "sentence": "In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence.",
                    "sentence_kor": "본 논문에서 우리는 인간의 유머를 지각하고, 문장에 말장난이 포함되어 있는지 탐지하고, 문장에서 이를 찾기 위해 발음 주의 상황별 말장난 인식(PCPR)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "59-4",
                    "sentence": "PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols.",
                    "sentence_kor": "PCPR은 주변 문맥과 해당 음성 기호 사이의 연관성을 포착하여 문장의 각 단어에 대한 상황별 표현을 도출한다.",
                    "tag": "2"
                },
                {
                    "index": "59-5",
                    "sentence": "Extensive experiments are conducted on two benchmark datasets.",
                    "sentence_kor": "두 개의 벤치마크 데이터 세트에 대해 광범위한 실험이 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "59-6",
                    "sentence": "Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks.",
                    "sentence_kor": "결과는 제안된 접근 방식이 말장난 탐지 및 위치 작업에서 최첨단 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "59-7",
                    "sentence": "In-depth analyses verify the effectiveness and robustness of PCPR.",
                    "sentence_kor": "심층 분석은 PCPR의 효과와 견고성을 검증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "659",
            "abstractID": "SPA_abs-60",
            "text": [
                {
                    "index": "60-0",
                    "sentence": "Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations.",
                    "sentence_kor": "BERT가 다양한 지도 학습 과제에서 성공적인 성능 향상을 달성했음에도 불구하고, BERT는 여전히 상황별 언어 표현 계산을 위한 감독되지 않은 과제에 대한 반복적인 추론에 의해 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "60-1",
                    "sentence": "To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA).",
                    "sentence_kor": "이러한 한계를 해결하기 위해 트랜스포머 기반 텍스트 자동 인코더(T-TA)라는 새로운 심층 양방향 언어 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "60-2",
                    "sentence": "The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT.",
                    "sentence_kor": "T-TA는 반복 없이 상황별 언어 표현을 계산하고 BERT와 같은 심층 양방향 아키텍처의 이점을 표시한다.",
                    "tag": "3"
                },
                {
                    "index": "60-3",
                    "sentence": "In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task.",
                    "sentence_kor": "CPU 환경의 계산 시간 실험에서 제안된 T-TA는 재순위화 작업에서 BERT 유사 모델보다 6배 이상 빠르고 의미 유사성 작업에서 12배 더 빠르다.",
                    "tag": "3+4"
                },
                {
                    "index": "60-4",
                    "sentence": "Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks.",
                    "sentence_kor": "또한 T-TA는 위의 작업에서 BERT의 정확도보다 경쟁력 있거나 훨씬 더 나은 정확도를 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "60-5",
                    "sentence": "Code is available at https://github.com/joongbo/tta.",
                    "sentence_kor": "코드는 https://github.com/joongbo/tta에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "660",
            "abstractID": "SPA_abs-61",
            "text": [
                {
                    "index": "61-0",
                    "sentence": "Operational risk management is one of the biggest challenges nowadays faced by financial institutions.",
                    "sentence_kor": "운영 리스크 관리는 오늘날 금융 기관이 직면하고 있는 가장 큰 과제 중 하나입니다.",
                    "tag": "1"
                },
                {
                    "index": "61-1",
                    "sentence": "There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability.",
                    "sentence_kor": "라벨링/레이블링되지 않은 데이터의 불균형 및 해석 가능성 부족을 포함하여 자동 운영 위험 예측을 위한 텍스트 분류 시스템 구축에는 몇 가지 주요 과제가 있다.",
                    "tag": "2"
                },
                {
                    "index": "61-2",
                    "sentence": "To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC).",
                    "sentence_kor": "이러한 과제를 해결하기 위해 다중 헤드 주의 메커니즘을 운영 위험 분류(SemiORC)에 대한 준지도 변형 추론과 통합하는 준지도 텍스트 분류 프레임워크를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "61-3",
                    "sentence": "We empirically evaluate the framework on a real-world dataset.",
                    "sentence_kor": "우리는 실제 데이터 세트의 프레임워크를 경험적으로 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "61-4",
                    "sentence": "The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations.",
                    "sentence_kor": "결과는 우리의 방법이 라벨이 부착되지 않은 데이터를 더 잘 활용하고 시각적으로 해석 가능한 문서 표현을 배울 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "61-5",
                    "sentence": "SemiORC also outperforms other baseline methods on operational risk classification.",
                    "sentence_kor": "또한 SemiORC는 운영 위험 분류에서 다른 기준 방법을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "661",
            "abstractID": "SPA_abs-62",
            "text": [
                {
                    "index": "62-0",
                    "sentence": "Identifying user geolocation in online social networks is an essential task in many location-based applications.",
                    "sentence_kor": "온라인 소셜 네트워크에서 사용자 위치 파악은 많은 위치 기반 애플리케이션에서 필수적인 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "62-1",
                    "sentence": "Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior.",
                    "sentence_kor": "기존 방법은 텍스트와 네트워크 구조의 유사성에 의존하지만, 모델 동작을 이해하는 데 중요한 해당 결과에 대한 해석성의 부족으로 어려움을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "62-2",
                    "sentence": "In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users.",
                    "sentence_kor": "본 연구에서는 시험 사용자의 위치를 예측할 때 사용자 훈련의 중요성을 파악하여 GNN 기반 모델의 동작을 해석하는 영향 함수를 채택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "62-3",
                    "sentence": "This methodology helps with providing meaningful explanations on prediction results.",
                    "sentence_kor": "이 방법론은 예측 결과에 대한 의미 있는 설명을 제공하는 데 도움이 됩니다.",
                    "tag": "3"
                },
                {
                    "index": "62-4",
                    "sentence": "Furthermore, it also initiates an attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes.",
                    "sentence_kor": "또한 개별 노드의 영향을 조사하여 소위 \"블랙박스\" GNN 기반 모델을 발굴하려는 시도를 시작한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "662",
            "abstractID": "SPA_abs-63",
            "text": [
                {
                    "index": "63-0",
                    "sentence": "Language modeling is the technique to estimate the probability of a sequence of words.",
                    "sentence_kor": "언어 모델링은 일련의 단어들의 확률을 추정하는 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "63-1",
                    "sentence": "A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages.",
                    "sentence_kor": "이중 언어 모델은 언어 간 단어의 순차적 의존성을 모델링할 것으로 예상되는데, 이는 언어 간 다양한 구문 구조뿐만 아니라 적절한 훈련 데이터가 선천적으로 부족하기 때문에 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "63-2",
                    "sentence": "We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency.",
                    "sentence_kor": "단일 언어 및 교차 언어 순차 종속성을 모델링하기 위해 준 번역 목표를 가진 언어 모델링 목표를 동시에 수행하는 이중 언어 주의 언어 모델(BALM)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "63-3",
                    "sentence": "The attention mechanism learns the bilingual context from a parallel corpus.",
                    "sentence_kor": "주의 메커니즘은 병렬 말뭉치에서 이중 언어 컨텍스트를 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "63-4",
                    "sentence": "BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result.",
                    "sentence_kor": "BALM은 가장 잘 보고된 결과에 대해 20.5%의 복잡성을 줄여 SEAME 코드 스위치 데이터베이스에서 최첨단 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "63-5",
                    "sentence": "We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.",
                    "sentence_kor": "우리는 또한 BALM을 이중 언어 어휘 유도 및 언어 표준화 작업에 적용하여 아이디어를 검증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "663",
            "abstractID": "SPA_abs-64",
            "text": [
                {
                    "index": "64-0",
                    "sentence": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language.",
                    "sentence_kor": "CSC(Chinese Spelling Check)는 중국어 자연어의 철자 오류를 탐지하고 수정하는 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "64-1",
                    "sentence": "Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters.",
                    "sentence_kor": "기존의 방법들은 한자 사이의 유사성 지식을 통합하려는 시도를 해왔다.",
                    "tag": "1"
                },
                {
                    "index": "64-2",
                    "sentence": "However, they take the similarity knowledge as either an external input resource or just heuristic rules.",
                    "sentence_kor": "그러나 유사성 지식은 외부 입력 리소스 또는 경험적 규칙만 사용합니다.",
                    "tag": "1"
                },
                {
                    "index": "64-3",
                    "sentence": "This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN).",
                    "sentence_kor": "본 논문은 특수 그래프 컨볼루션 네트워크(SpellGCN)를 통해 CSC용 언어 모델에 음운 및 시각적 유사성 지식을 통합할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "64-4",
                    "sentence": "The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers.",
                    "sentence_kor": "모델은 문자 위에 그래프를 작성하고, SpellGCN은 이 그래프를 상호 종속 문자 분류기 집합으로 매핑하는 방법을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "64-5",
                    "sentence": "These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable.",
                    "sentence_kor": "이러한 분류자는 BERT와 같은 다른 네트워크에 의해 추출된 표현에 적용되어 전체 네트워크를 종단간 훈련이 가능하도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "64-6",
                    "sentence": "Experiments are conducted on three human-annotated datasets. Our method achieves superior performance against previous models by a large margin.",
                    "sentence_kor": "실험은 세 개의 인간 주석 데이터 세트에 대해 수행된다. 우리의 방법은 이전 모델에 비해 큰 차이로 우수한 성능을 달성한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "664",
            "abstractID": "SPA_abs-65",
            "text": [
                {
                    "index": "65-0",
                    "sentence": "Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability.",
                    "sentence_kor": "맞춤법 오류 수정은 기본적으로 인간 수준의 언어 이해 능력이 필요하기 때문에 중요하지만 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "65-1",
                    "sentence": "Without loss of generality we consider Chinese spelling error correction (CSC) in this paper.",
                    "sentence_kor": "일반성을 잃지 않고 본 논문에서 중국어 맞춤법 오류 수정(CSC)을 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "65-2",
                    "sentence": "A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model.",
                    "sentence_kor": "작업에 대한 최첨단 방법은 언어 표현 모델인 BERT에 기초하여 문장의 각 위치에 있는 수정 후보 목록(비수정 포함)에서 문자를 선택한다.",
                    "tag": "3"
                },
                {
                    "index": "65-3",
                    "sentence": "The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling.",
                    "sentence_kor": "그러나 BERT가 각 위치에 오류가 있는지 여부를 탐지하기에 충분한 능력을 가지고 있지 않기 때문에 방법의 정확도는 차선책이 될 수 있다. 이는 마스크 언어 모델링을 사용하여 사전 훈련하기 때문인 것으로 보인다.",
                    "tag": "3"
                },
                {
                    "index": "65-4",
                    "sentence": "In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique.",
                    "sentence_kor": "본 연구에서는 앞서 언급한 문제를 해결하기 위한 새로운 신경 아키텍처를 제안한다. 이 네트워크는 BERT에 기반한 오류 감지 네트워크와 오류 수정을 위한 네트워크로 구성되며, 전자는 우리가 소프트 마스킹 기술이라고 부르는 후자에 연결된다.",
                    "tag": "2+3"
                },
                {
                    "index": "65-5",
                    "sentence": "Our method of using ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems.",
                    "sentence_kor": "당사의 '소프트 마스크 BERT' 사용 방법은 일반적이며, 다른 언어 감지-수정 문제에 사용될 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "65-6",
                    "sentence": "Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.",
                    "sentence_kor": "우리가 만들고 출시할 예정인 하나의 대규모 데이터 세트를 포함한 두 데이터 세트에 대한 실험 결과는 우리가 제안한 방법의 성능이 BERT만을 기반으로 하는 것을 포함한 기준선보다 훨씬 더 낫다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "665",
            "abstractID": "SPA_abs-66",
            "text": [
                {
                    "index": "66-0",
                    "sentence": "Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language.",
                    "sentence_kor": "CSC(Chinese Spelling Check)는 중국어 자연어의 철자 오류를 탐지하고 수정하는 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "66-1",
                    "sentence": "Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters.",
                    "sentence_kor": "기존의 방법들은 한자 사이의 유사성 지식을 통합하려는 시도를 해왔다.",
                    "tag": "1"
                },
                {
                    "index": "66-2",
                    "sentence": "However, they take the similarity knowledge as either an external input resource or just heuristic rules.",
                    "sentence_kor": "그러나 유사성 지식은 외부 입력 리소스 또는 경험적 규칙만 사용합니다.",
                    "tag": "1"
                },
                {
                    "index": "66-3",
                    "sentence": "This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN).",
                    "sentence_kor": "본 논문은 특수 그래프 컨볼루션 네트워크(SpellGCN)를 통해 CSC용 언어 모델에 음운 및 시각적 유사성 지식을 통합할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "66-4",
                    "sentence": "The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers.",
                    "sentence_kor": "모델은 문자 위에 그래프를 작성하고, SpellGCN은 이 그래프를 상호 종속 문자 분류기 집합으로 매핑하는 방법을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "66-5",
                    "sentence": "These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable.",
                    "sentence_kor": "이러한 분류자는 BERT와 같은 다른 네트워크에 의해 추출된 표현에 적용되어 전체 네트워크를 종단간 훈련이 가능하도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "66-6",
                    "sentence": "Experiments are conducted on three human-annotated datasets.",
                    "sentence_kor": "실험은 세 개의 인간 주석 데이터 세트에 대해 수행된다.",
                    "tag": "3"
                },
                {
                    "index": "66-7",
                    "sentence": "Our method achieves superior performance against previous models by a large margin.",
                    "sentence_kor": "우리의 방법은 이전 모델에 비해 큰 차이로 우수한 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "666",
            "abstractID": "SPA_abs-67",
            "text": [
                {
                    "index": "67-0",
                    "sentence": "Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC).",
                    "sentence_kor": "문장 표현(SR)은 기계 판독 이해(MRC)에서 가장 중요하고 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "67-1",
                    "sentence": "MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge.",
                    "sentence_kor": "MRC 시스템은 일반적으로 문장 자체에 포함된 정보만 활용하는 반면 인간은 의미적 지식을 활용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "67-2",
                    "sentence": "To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling.",
                    "sentence_kor": "격차를 해소하기 위해, 우리는 문장 모델링을 용이하게 하기 위해 프레임 의미 지식을 사용하는 새로운 프레임 기반 문장 표현(FSR) 방법을 제안했다.",
                    "tag": "2+3"
                },
                {
                    "index": "67-3",
                    "sentence": "Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema.",
                    "sentence_kor": "특히 어휘 단위(LU)만 모델링하는 기존 방법과 달리 프레임의 LU와 프레임 대 프레임(F-to-F) 관계를 모두 활용하는 프레임 표현 모델은 주의 스키마가 있는 프레임과 문장을 모델링하도록 설계된다.",
                    "tag": "3"
                },
                {
                    "index": "67-4",
                    "sentence": "Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations.",
                    "sentence_kor": "우리가 제안한 FSR 방법은 다중 프레임 의미 정보를 통합하여 훨씬 더 나은 문장 표현을 얻을 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "67-5",
                    "sentence": "Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.",
                    "sentence_kor": "우리의 광범위한 실험 결과는 그것이 기계 판독 이해 작업에서 최첨단 기술보다 더 잘 수행된다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "667",
            "abstractID": "SPA_abs-68",
            "text": [
                {
                    "index": "68-0",
                    "sentence": "In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data.",
                    "sentence_kor": "본 논문에서, 우리는 구조화된 데이터에 대한 질문 답변을 위한 말뭉치를 효율적으로 구성하기 위한 새로운 방법론을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "68-1",
                    "sentence": "For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT).",
                    "sentence_kor": "이를 위해, 우리는 OT(Operation Trees)라고 하는 데이터베이스의 논리적 쿼리 계획을 기반으로 하는 중간 표현을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "68-2",
                    "sentence": "This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate.",
                    "sentence_kor": "이 표현을 사용하면 생성하는 쿼리 유형의 유연성을 잃지 않고 주석 프로세스를 반전할 수 있습니다.",
                    "tag": "3+4"
                },
                {
                    "index": "68-3",
                    "sentence": "Furthermore, it allows for fine-grained alignment of the tokens to the operations.",
                    "sentence_kor": "또한 토큰을 작업에 세밀하게 정렬할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "68-4",
                    "sentence": "Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens.",
                    "sentence_kor": "따라서 문맥 자유 문법에서 무작위로 OT를 생성하고 주석자는 적절한 질문을 작성하고 토큰을 할당하기만 하면 된다.",
                    "tag": "3"
                },
                {
                    "index": "68-5",
                    "sentence": "We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries.",
                    "sentence_kor": "우리는 데이터베이스에 대한 자연어 인터페이스를 평가하기 위한 대규모 의미 구문 분석 말뭉치인 말뭉치 OTA(Operation Trees and Token Assignment)를 Spider 및 LC-QuaD 2.0과 비교하고 우리의 방법론을 쿼리의 복잡성을 유지하면서 주석 속도를 3배 이상 증가시킨다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "68-6",
                    "sentence": "Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.",
                    "sentence_kor": "마지막으로, 데이터에 대한 최첨단 의미 구문 분석 모델을 교육하고 데이터 세트가 어려운 데이터 세트이며 토큰 정렬을 활용하여 성능을 크게 향상시킬 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "668",
            "abstractID": "SPA_abs-69",
            "text": [
                {
                    "index": "69-0",
                    "sentence": "Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models.",
                    "sentence_kor": "개방형 도메인 질문 답변은 구문 검색 문제로 공식화될 수 있는데, 여기서 우리는 엄청난 확장성 및 속도 이점을 기대할 수 있지만 기존 구문 표현 모델의 한계로 인해 정확도가 떨어지는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "69-1",
                    "sentence": "In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc).",
                    "sentence_kor": "본 논문에서는 상황에 맞는 희소 표현(Sparc)으로 각 구문 임베딩의 품질을 향상시키는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "69-2",
                    "sentence": "Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space.",
                    "sentence_kor": "용어 빈도 기반(예: tf-idf)이거나 직접 학습(예: 수천 차원)된 이전의 희소 벡터와 달리, 우리는 정류된 자기 주의를 활용하여 n그램 어휘 공간에서 희소 벡터를 간접적으로 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "69-3",
                    "sentence": "By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.",
                    "sentence_kor": "Sparc로 이전 문구 검색 모델(Seo et al., 2019)을 보강하여 큐레이티드 4% 이상의 개선을 보여준다.TRC 및 SQuAD-Open.",
                    "tag": "3+4"
                },
                {
                    "index": "69-4",
                    "sentence": "Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.",
                    "sentence_kor": "교황청TRC 점수는 추론 속도가 최소 45배 빠른 가장 잘 알려진 검색 및 읽기 모델보다 훨씬 더 좋다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "669",
            "abstractID": "SPA_abs-70",
            "text": [
                {
                    "index": "70-0",
                    "sentence": "Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community.",
                    "sentence_kor": "여러 데이터 세트를 동시에 해결할 수 있는 일반 독해 시스템을 구축하는 것이 최근 연구 커뮤니티에서 열망하는 목표이다.",
                    "tag": "1"
                },
                {
                    "index": "70-1",
                    "sentence": "Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up.",
                    "sentence_kor": "이전 작업은 데이터 세트를 유지하기 위한 모델 아키텍처 또는 일반화에 초점을 맞추었으며, 대부분 다중 작업 학습 설정의 세부 사항을 넘겼다.",
                    "tag": "1"
                },
                {
                    "index": "70-2",
                    "sentence": "We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model’s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning.",
                    "sentence_kor": "단일 작업 성능과 관련된 데이터 세트에서 다중 작업 모델의 현재 성능에 비례하는 훈련을 위한 인스턴스를 선택하는 간단한 동적 샘플링 전략이 이전 다중 작업 샘플링 전략에 비해 상당한 이득을 제공하여 다중 작업 학습에서 흔히 발생하는 치명적인 망각을 완화한다는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "70-3",
                    "sentence": "We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level.",
                    "sentence_kor": "또한 서로 다른 작업의 인스턴스를 각 에포크와 배치 사이에 가능한 한 많이 인터리빙할 수 있도록 하는 것이 에포크 또는 배치 수준에서 작업 동질성을 강제하는 것보다 멀티태스킹 성능에 분명한 이점이 있음을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "70-4",
                    "sentence": "Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.",
                    "sentence_kor": "우리의 최종 모델은 최근 출시된 다중 작업 읽기 이해 벤치마크인 ORB의 최고 모델보다 성능이 크게 향상되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "670",
            "abstractID": "SPA_abs-71",
            "text": [
                {
                    "index": "71-0",
                    "sentence": "Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages.",
                    "sentence_kor": "다국어 사전 교육 모델은 풍부한 소스 언어(예: 영어)의 교육 데이터를 활용하여 낮은 리소스 언어에서 성능을 향상시킬 수 있습니다.",
                    "tag": "2+3"
                },
                {
                    "index": "71-1",
                    "sentence": "However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary.",
                    "sentence_kor": "그러나 다국어 기계 판독 이해(MRC)의 전송 품질은 주로 단어 수준 응답 경계를 감지하기 위한 MRC의 요구 사항 때문에 문장 분류 작업보다 훨씬 더 나쁘다.",
                    "tag": "1"
                },
                {
                    "index": "71-2",
                    "sentence": "In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web.",
                    "sentence_kor": "본 논문에서, 우리는 추가 구문 경계 감독을 만들기 위해 미세 조정 단계에서 두 가지 보조 작업을 제안한다. (1) 질문이나 구절을 다른 언어로 번역하고 교차 언어 질문-통과 쌍을 구축하는 혼합 MRC 작업, (2) 웹에서 채굴한 지식 구문을 활용하여 언어에 구애받지 않는 지식 마스킹 작업이다.",
                    "tag": "2+3"
                },
                {
                    "index": "71-3",
                    "sentence": "Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.",
                    "sentence_kor": "또한, 두 개의 언어 간 MRC 데이터 세트에 대한 광범위한 실험은 제안된 접근 방식의 효과를 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "671",
            "abstractID": "SPA_abs-72",
            "text": [
                {
                    "index": "72-0",
                    "sentence": "The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions.",
                    "sentence_kor": "대화 기계 판독의 목표는 설명 질문을 요구할 수 있는 지식 기반 텍스트가 주어진 사용자 질문에 대답하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "72-1",
                    "sentence": "Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them.",
                    "sentence_kor": "기존 접근법은 질문 관련 규칙을 추출하고 이에 대한 추론을 하는 데 어려움을 겪기 때문에 의사결정에 한계가 있다.",
                    "tag": "1"
                },
                {
                    "index": "72-2",
                    "sentence": "In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision.",
                    "sentence_kor": "본 논문에서는 규칙 텍스트에 나열된 조건이 이미 결정을 내리기 위해 충족되었는지 여부를 추적하기 위해 새로운 명시적 기억 추적기(EMT)로 구성된 대화 기계 판독의 새로운 프레임워크를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "72-3",
                    "sentence": "Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions.",
                    "sentence_kor": "또한, 우리의 프레임워크는 문장 수준의 수반 점수를 활용하여 토큰 수준 분포를 가중치하는 대략적인 추론 전략을 채택하여 해명 질문을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "72-4",
                    "sentence": "On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4.",
                    "sentence_kor": "ShARC 벤치마크(블라인드, 홀드아웃) 테스트 세트에서 EMT는 마이크로 평균 의사 결정 정확도 74.6%와 49.5 BLEU4의 새로운 최첨단 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "72-5",
                    "sentence": "We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows.",
                    "sentence_kor": "우리는 또한 EMT가 대화 흐름으로 수반 지향 추론 프로세스를 시각화함으로써 더 해석 가능하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "72-6",
                    "sentence": "Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.",
                    "sentence_kor": "코드와 모델은 https://github.com/Yifan-Gao/explicit_memory_tracker에서 공개된다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "672",
            "abstractID": "SPA_abs-73",
            "text": [
                {
                    "index": "73-0",
                    "sentence": "Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.",
                    "sentence_kor": "대규모 사전 교육 언어 모델(LM)은 상당한 양의 언어 정보를 인코딩하는 것으로 알려져 있다.",
                    "tag": "1"
                },
                {
                    "index": "73-1",
                    "sentence": "However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.",
                    "sentence_kor": "그러나 수치 추론과 같은 고도의 추론 기술은 언어 모델링 목표에서만 배우기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "73-2",
                    "sentence": "Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.",
                    "sentence_kor": "결과적으로, 수치 추론을 위한 기존 모델은 제한된 유연성을 가진 전문화된 아키텍처를 사용해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "73-3",
                    "sentence": "In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.",
                    "sentence_kor": "이 연구에서 우리는 수치 추론이 자동 데이터 생성에 적합하다는 것을 보여주며, 따라서 많은 양의 데이터를 생성하고 다중 작업 설정에서 훈련을 통해 사전 훈련된 LM에 이 기술을 주입할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "73-4",
                    "sentence": "We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.",
                    "sentence_kor": "우리는 이 데이터에 대해 모델 GenBERT를 사전 교육하면 DROP(49.3 –> 72.3 F1)의 성능이 크게 향상되어 단순하고 범용 인코더-디코더 아키텍처를 사용하면서 비슷한 크기의 최첨단 모델과 일치하는 성능에 도달한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "73-5",
                    "sentence": "Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.",
                    "sentence_kor": "또한 GenBERT는 표준 RC 작업에서 높은 성능을 유지하면서 수학 단어 문제 데이터 세트에 잘 일반화된다.",
                    "tag": "1"
                },
                {
                    "index": "73-6",
                    "sentence": "Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.",
                    "sentence_kor": "우리의 접근 방식은 기술이 자동 데이터 증가를 수용할 수 있을 때마다 사전 훈련된 대규모 LM에 기술을 주입하기 위한 일반적인 방법을 제공한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "673",
            "abstractID": "SPA_abs-74",
            "text": [
                {
                    "index": "74-0",
                    "sentence": "Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions.",
                    "sentence_kor": "최근 대화의 질문에 대한 대답이 진전되었음에도 불구하고, 대부분의 선행 작업은 후속 질문에 초점을 맞추지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "74-1",
                    "sentence": "Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently.",
                    "sentence_kor": "실용적인 대화식 질문 답변 시스템은 종종 진행 중인 대화에서 후속 질문을 받는데, 시스템이 더 효과적인 답변을 찾기 위해 질문이 현재 대화의 후속 질문인지 여부를 결정할 수 있는 것이 중요하다.",
                    "tag": "1+2"
                },
                {
                    "index": "74-2",
                    "sentence": "In this paper, we introduce a new follow-up question identification task.",
                    "sentence_kor": "본 논문에서는 새로운 후속 질문 식별 과제를 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "74-3",
                    "sentence": "We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question.",
                    "sentence_kor": "관련 단락, 대화 내역 및 후보 후속 질문 사이의 쌍방향 상호 작용을 포착하여 후속 질문의 적합성을 결정하는 3방향 주의 풀링 네트워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "74-4",
                    "sentence": "It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question.",
                    "sentence_kor": "모델이 특정 후보 후속 질문에 점수를 매기면서 주제 연속성과 주제 전환을 포착할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "74-5",
                    "sentence": "Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.",
                    "sentence_kor": "실험에 따르면 제안된 3방향 주의 풀링 네트워크는 모든 기준 시스템을 상당한 여유도로 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "674",
            "abstractID": "SPA_abs-75",
            "text": [
                {
                    "index": "75-0",
                    "sentence": "Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations.",
                    "sentence_kor": "지식 기반에서 복잡한 질문에 대답하는 이전의 연구는 일반적으로 두 가지 유형의 복잡성을 별도로 다룬다. 즉, 제약 조건이 있는 질문과 여러 홉의 관계가 있는 질문이다.",
                    "tag": "1"
                },
                {
                    "index": "75-1",
                    "sentence": "In this paper, we handle both types of complexity at the same time.",
                    "sentence_kor": "본 논문에서 우리는 두 가지 유형의 복잡성을 동시에 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "75-2",
                    "sentence": "Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs.",
                    "sentence_kor": "쿼리 그래프에 제약 조건을 조기에 통합하면 검색 공간을 보다 효과적으로 제거할 수 있다는 관찰에 자극을 받아 쿼리 그래프를 보다 유연하게 생성할 수 있는 수정된 단계별 쿼리 그래프 생성 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "75-3",
                    "sentence": "Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.",
                    "sentence_kor": "우리의 실험은 우리의 방법이 세 가지 벤치마크 KBQA 데이터 세트에서 최첨단 기술을 달성한다는 것을 분명히 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "675",
            "abstractID": "SPA_abs-76",
            "text": [
                {
                    "index": "76-0",
                    "sentence": "Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image.",
                    "sentence_kor": "이미지 캡션을 평가하는 것은 각 이미지에 대해 여러 개의 올바른 캡션이 있기 때문에 부분적으로 매우 어렵습니다.",
                    "tag": "1"
                },
                {
                    "index": "76-1",
                    "sentence": "Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions.",
                    "sentence_kor": "대부분의 기존 일대일 측정 기준은 실측 자료 캡션 사이의 본질적 차이를 고려하지 않고 참조 캡션과 생성 캡션 사이의 불일치에 불이익을 주는 방식으로 작동한다.",
                    "tag": "1"
                },
                {
                    "index": "76-2",
                    "sentence": "It usually leads to over-penalization and thus a bad correlation to human judgment.",
                    "sentence_kor": "그것은 보통 과잉 처벌로 이어져 인간의 판단과 나쁜 상관관계를 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "76-3",
                    "sentence": "Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance.",
                    "sentence_kor": "최근, 최신 일대일 메트릭 BERTScore는 시스템 수준 작업에서 높은 인적 상관 관계를 달성하고 일부 문제는 더 나은 성능을 위해 해결할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "76-4",
                    "sentence": "In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation.",
                    "sentence_kor": "본 논문에서는 이러한 과제를 처리하고 이미지 캡션 평가에 적합한 몇 가지 새로운 기능으로 BERTScore를 확장할 수 있는 BERTScore를 기반으로 한 새로운 메트릭을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "76-5",
                    "sentence": "The experimental results show that our metric achieves state-of-the-art human judgment correlation.",
                    "sentence_kor": "실험 결과는 우리의 측정 지표가 최첨단 인간 판단 상관 관계를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "676",
            "abstractID": "SPA_abs-77",
            "text": [
                {
                    "index": "77-0",
                    "sentence": "Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar.",
                    "sentence_kor": "매핑 기반 교차 언어 단어 임베딩에 대한 기존 접근법은 소스 및 대상 임베딩 공간이 구조적으로 유사하다는 가정에 기초한다.",
                    "tag": "1"
                },
                {
                    "index": "77-1",
                    "sentence": "The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines.",
                    "sentence_kor": "임베딩 공간의 구조는 맥락 창의 선택이 결정하는 각 단어의 동시 발생 통계에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "77-2",
                    "sentence": "Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work.",
                    "sentence_kor": "컨텍스트 창과 매핑 기반 교차 언어 임베딩 간의 이러한 분명한 연결에도 불구하고, 이들의 관계는 이전 작업에서 그리 복잡하지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "77-3",
                    "sentence": "In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows.",
                    "sentence_kor": "본 연구에서는 다양한 컨텍스트 창으로 훈련된 이중 언어 임베딩에 대해 다양한 언어, 도메인 및 작업으로 철저한 평가를 제공한다.",
                    "tag": "2+3"
                },
                {
                    "index": "77-4",
                    "sentence": "The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.",
                    "sentence_kor": "우리 연구결과의 하이라이트는 소스 창 크기와 대상 창 크기를 모두 늘리면 이중 언어 어휘 사용, 특히 빈번한 명사에 대한 성능이 향상된다는 것이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "677",
            "abstractID": "SPA_abs-78",
            "text": [
                {
                    "index": "78-0",
                    "sentence": "A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training.",
                    "sentence_kor": "WSD(Word Sense Disambigization)의 주요 장애물은 단어 감각이 균일하게 분포되지 않아 기존 모델이 일반적으로 훈련 중에 드물거나 보이지 않는 감각에서 성능이 떨어진다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "78-1",
                    "sentence": "We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense.",
                    "sentence_kor": "(1) 대상 단어와 (2) 각 의미의 사전 정의 또는 광택을 독립적으로 내장하는 바이 인코더 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "78-2",
                    "sentence": "The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding.",
                    "sentence_kor": "인코더는 동일한 표현 공간에서 공동으로 최적화되므로 각 대상 단어 임베딩에 가장 가까운 의미 임베딩을 찾아 의미 명확화를 수행할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "78-3",
                    "sentence": "Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work.",
                    "sentence_kor": "우리 시스템은 영어 전체 단어 WSD에서 이전의 최첨단 모델을 능가한다. 이러한 이점은 주로 희귀 감각의 성능 향상에서 비롯되며, 이전 작업에 비해 빈도가 낮은 감각에 대한 오류 감소로 이어진다.",
                    "tag": "1"
                },
                {
                    "index": "78-4",
                    "sentence": "This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.",
                    "sentence_kor": "이는 희귀 감각의 정의를 모델링함으로써 더 효과적으로 모호해질 수 있음을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "678",
            "abstractID": "SPA_abs-79",
            "text": [
                {
                    "index": "79-0",
                    "sentence": "In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications.",
                    "sentence_kor": "본 논문에서는 코드 전환 패턴을 사용하여 다양한 다운스트림 NLP 애플리케이션을 개선하는 방법을 시연한다.",
                    "tag": "2+3"
                },
                {
                    "index": "79-1",
                    "sentence": "In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks.",
                    "sentence_kor": "특히 유머, 빈정거림 및 혐오 음성 감지 작업을 개선하기 위해 다양한 전환 기능을 인코딩한다.",
                    "tag": "3"
                },
                {
                    "index": "79-2",
                    "sentence": "We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.",
                    "sentence_kor": "우리는 이러한 단순한 언어 관찰이 다른 유사한 NLP 애플리케이션을 개선하는 데도 잠재적으로 도움이 될 수 있다고 믿는다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "679",
            "abstractID": "SPA_abs-80",
            "text": [
                {
                    "index": "80-0",
                    "sentence": "Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized.",
                    "sentence_kor": "최근 많은 방법들이 널리 알려진 설명 가능한 주장 검증을 위해 적절한 신경망에 의해 신뢰할 수 있는 출처로부터 효과적인 증거를 발견한다.",
                    "tag": "1"
                },
                {
                    "index": "80-1",
                    "sentence": "However, in these methods, the discovery process of evidence is nontransparent and unexplained.",
                    "sentence_kor": "그러나 이러한 방법에서 증거의 발견 과정은 불투명하고 설명되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "80-2",
                    "sentence": "Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims.",
                    "sentence_kor": "이와 동시에, 발견된 증거는 전체 청구 시퀀스의 해석 가능성을 목표로 하고 있지만 청구의 거짓 부분에 초점을 맞추기에는 불충분하다.",
                    "tag": "1"
                },
                {
                    "index": "80-3",
                    "sentence": "In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification.",
                    "sentence_kor": "본 논문에서, 우리는 설명 가능한 클레임 검증을 위한 증거를 발견하기 위한 의사결정 트리 기반 공동 주의 모델(DTCA)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "80-4",
                    "sentence": "Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way.",
                    "sentence_kor": "특히, 우리는 먼저 신뢰성이 높은 주석을 투명하고 해석 가능한 방식으로 증거로 선택하기 위해 의사 결정 트리 기반 증거 모델(DTE)을 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "80-5",
                    "sentence": "Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim.",
                    "sentence_kor": "그런 다음 우리는 선택된 증거가 청구와 상호작용하도록 공동 주의 자기 주의 네트워크(CaSa)를 설계한다. 즉, 1) 최적의 결정 임계값을 결정하고 더 강력한 증거를 얻기 위한 DTE 훈련과 2) 청구에서 거짓 부분을 찾기 위해 증거를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "80-6",
                    "sentence": "Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.",
                    "sentence_kor": "두 개의 공개 데이터 세트인 RumourEval과 PHEME에 대한 실험은 DTCA가 클레임 검증 결과에 대한 설명을 제공할 뿐만 아니라 최첨단 성능을 달성하여 F1 점수를 각각 3.11%, 2.41% 이상 향상시킨다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "680",
            "abstractID": "SPA_abs-81",
            "text": [
                {
                    "index": "81-0",
                    "sentence": "User intent classification plays a vital role in dialogue systems.",
                    "sentence_kor": "사용자 의도 분류는 대화 시스템에서 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "81-1",
                    "sentence": "Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun.",
                    "sentence_kor": "많은 현실적인 시나리오에서 사용자 의도가 시간에 따라 자주 변경될 수 있기 때문에 연구가 막 시작된 곳에서는 알려지지 않은(새로운) 의도 감지가 필수적인 문제가 되었다.",
                    "tag": "1"
                },
                {
                    "index": "81-2",
                    "sentence": "This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection.",
                    "sentence_kor": "본 논문은 알 수 없는 의도 감지를 위한 의미 강화 가우스 혼합물 모델(SEG)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "81-3",
                    "sentence": "In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection.",
                    "sentence_kor": "특히 가우스 혼합물 분포로 발화 임베딩을 모델링하고 동적 클래스 의미 정보를 가우스 평균에 주입하여 다운스트림 이상치 탐지를 용이하게 하는 더 많은 클래스 집중 임베딩을 학습할 수 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "81-4",
                    "sentence": "Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection.",
                    "sentence_kor": "밀도 기반 특이치 탐지 알고리즘과 함께 SEG는 알 수 없는 의도 감지를 위해 2개 언어로 된 3개의 실제 작업 지향 대화 데이터 세트에서 경쟁 결과를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "81-5",
                    "sentence": "On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance.",
                    "sentence_kor": "또한 SEG를 알 수 없는 의도 식별자로 기존의 일반화된 제로샷 의도 분류 모델에 통합하여 성능을 향상시킬 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "81-6",
                    "sentence": "A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.",
                    "sentence_kor": "최첨단 방법인 ReCapsNet에 대한 사례 연구는 SEG가 분류 성능을 훨씬 더 높은 수준으로 끌어올릴 수 있다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "681",
            "abstractID": "SPA_abs-82",
            "text": [
                {
                    "index": "82-0",
                    "sentence": "Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions.",
                    "sentence_kor": "지식 기반에서 텍스트 생성은 지식의 3배를 자연어 설명으로 변환하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "82-1",
                    "sentence": "Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table.",
                    "sentence_kor": "대부분의 기존 메서드는 생성된 텍스트 설명과 원본 테이블 간의 충실도를 무시하여 생성된 정보가 표의 내용을 넘어섭니다.",
                    "tag": "1"
                },
                {
                    "index": "82-2",
                    "sentence": "In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal.",
                    "sentence_kor": "본 논문에서 우리는 처음으로 목표를 달성하기 위한 새로운 트랜스포머 기반 생성 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "82-3",
                    "sentence": "The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model.",
                    "sentence_kor": "충실도를 적용하는 방법의 핵심 기법에는 트랜스포머 모델에 기초한 새로운 테이블-텍스트 최적 전송 일치 손실과 테이블-텍스트 임베딩 유사성 손실이 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "82-4",
                    "sentence": "Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem.",
                    "sentence_kor": "또한 충실도를 평가하기 위해 표 대 텍스트 생성 문제에 특화된 새로운 자동 측정 기준을 제안한다.",
                    "tag": "3+4"
                },
                {
                    "index": "82-5",
                    "sentence": "We also provide detailed analysis on each component of our model in our experiments.",
                    "sentence_kor": "또한 실험에서 모델의 각 구성 요소에 대한 자세한 분석을 제공합니다.",
                    "tag": "4"
                },
                {
                    "index": "82-6",
                    "sentence": "Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.",
                    "sentence_kor": "자동 및 인간 평가에 따르면 우리의 프레임워크가 최첨단보다 큰 폭으로 성능이 우수할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "682",
            "abstractID": "SPA_abs-83",
            "text": [
                {
                    "index": "83-0",
                    "sentence": "This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification.",
                    "sentence_kor": "본 논문은 몇 개의 짧은 텍스트 분류를 위한 동적 메모리 유도 네트워크(DMIN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "83-1",
                    "sentence": "The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification.",
                    "sentence_kor": "이 모델은 정적 메모리를 통한 동적 라우팅 메커니즘을 개발하여 보이지 않는 클래스에 더 잘 적응할 수 있도록 하며, 이는 몇 가지 짧은 분류를 위한 중요한 기능이다.",
                    "tag": "3+4"
                },
                {
                    "index": "83-2",
                    "sentence": "The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning.",
                    "sentence_kor": "이 모델은 또한 메타 학습의 일반화 능력을 향상시키기 위해 지도 학습 가중치 및 쿼리 정보로 유도 프로세스를 확장한다.",
                    "tag": "2+3"
                },
                {
                    "index": "83-3",
                    "sentence": "The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets.",
                    "sentence_kor": "제안된 모델은 miniRCV1 및 ODIC 데이터 세트에서 2~4% 향상된 최첨단 성능을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "83-4",
                    "sentence": "Detailed analysis is further performed to show how the proposed network achieves the new performance.",
                    "sentence_kor": "제안된 네트워크가 새로운 성능을 달성하는 방법을 보여주기 위해 상세한 분석이 추가로 수행된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "683",
            "abstractID": "SPA_abs-84",
            "text": [
                {
                    "index": "84-0",
                    "sentence": "Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases.",
                    "sentence_kor": "키프레이즈 생성(KG)은 문서의 주요 아이디어를 일련의 키 프레이즈로 요약하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "84-1",
                    "sentence": "A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce.",
                    "sentence_kor": "문서가 주어졌을 때 모델이 일련의 키 프레이즈를 예측하고 동시에 생성할 적절한 키 프레이즈 수를 결정해야 하는 새로운 설정이 최근에 이 문제에 도입되었다.",
                    "tag": "1"
                },
                {
                    "index": "84-2",
                    "sentence": "Previous work in this setting employs a sequential decoding process to generate keyphrases.",
                    "sentence_kor": "이 설정의 이전 작업은 키 프레이즈를 생성하기 위해 순차 디코딩 프로세스를 사용합니다.",
                    "tag": "1"
                },
                {
                    "index": "84-3",
                    "sentence": "However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document.",
                    "sentence_kor": "그러나 이러한 디코딩 방법은 문서의 키프레이즈 집합에 존재하는 고유한 계층 구성성을 무시합니다.",
                    "tag": "1"
                },
                {
                    "index": "84-4",
                    "sentence": "Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources.",
                    "sentence_kor": "또한 이전 작업은 중복된 핵심 문구를 생성하는 경향이 있으며, 이는 시간과 컴퓨팅 리소스를 낭비한다.",
                    "tag": "1"
                },
                {
                    "index": "84-5",
                    "sentence": "To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism.",
                    "sentence_kor": "이러한 한계를 극복하기 위해 계층적 디코딩 프로세스와 소프트 또는 하드 배제 메커니즘을 포함하는 배타적 계층 디코딩 프레임워크를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "84-6",
                    "sentence": "The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set.",
                    "sentence_kor": "계층적 디코딩 프로세스는 키프레이즈 집합의 계층 구성성을 명시적으로 모델링하는 것입니다.",
                    "tag": "3"
                },
                {
                    "index": "84-7",
                    "sentence": "Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases.",
                    "sentence_kor": "소프트 및 하드 제외 메커니즘 모두 생성된 키 신장의 다양성을 향상시키기 위해 창 크기 내에서 이전에 예측한 키 신장을 추적한다.",
                    "tag": "3"
                },
                {
                    "index": "84-8",
                    "sentence": "Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.",
                    "sentence_kor": "여러 KG 벤치마크 데이터 세트에 대한 광범위한 실험은 덜 중복되고 더 정확한 키 문구를 생성하는 방법의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "684",
            "abstractID": "SPA_abs-85",
            "text": [
                {
                    "index": "85-0",
                    "sentence": "Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy.",
                    "sentence_kor": "계층적 텍스트 분류는 분류학적 계층 구조를 가진 다중 레이블 텍스트 분류의 필수적이지만 어려운 하위 작업입니다.",
                    "tag": "1"
                },
                {
                    "index": "85-1",
                    "sentence": "Existing methods have difficulties in modeling the hierarchical label structure in a global view.",
                    "sentence_kor": "기존 방법은 전역 보기에서 계층적 레이블 구조를 모델링하는 데 어려움이 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "85-2",
                    "sentence": "Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space.",
                    "sentence_kor": "또한 텍스트 형상 공간과 레이블 공간 간의 상호 작용을 완전히 사용할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "85-3",
                    "sentence": "In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies.",
                    "sentence_kor": "본 논문에서 우리는 계층을 지시된 그래프로 공식화하고 레이블 종속성을 모델링하기 위한 계층 인식 구조 인코더를 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "85-4",
                    "sentence": "Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants.",
                    "sentence_kor": "계층 인코더를 기반으로 두 가지 변형을 가진 새로운 종단 간 계층 인식 글로벌 모델(HiAGM)을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "85-5",
                    "sentence": "A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features.",
                    "sentence_kor": "다중 레이블 주의 변형(HiAGM-LA)은 계층 인코더를 통해 계층 인식 레이블 임베딩을 학습하고 레이블 인식 텍스트 기능의 유도 퓨전을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "85-6",
                    "sentence": "A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders.",
                    "sentence_kor": "텍스트 기능을 계층 인코더에 직접 공급하는 연역적 변형으로 텍스트 기능 전파 모델(HiAGM-TP)이 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "85-7",
                    "sentence": "Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.",
                    "sentence_kor": "이전 작업과 비교하여 HiAGM-LA와 HiAGM-TP 모두 세 가지 벤치마크 데이터 세트에서 중요하고 일관된 개선을 달성했습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "685",
            "abstractID": "SPA_abs-86",
            "text": [
                {
                    "index": "86-0",
                    "sentence": "Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval.",
                    "sentence_kor": "시퀀스 투 시퀀스 모델은 키프레이즈 생성에 상당한 진전을 이루었지만, 문서 검색에 도움이 될 만큼 충분히 신뢰할 수 있는지는 여전히 알 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "86-1",
                    "sentence": "This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models.",
                    "sentence_kor": "본 연구는 이러한 모델이 검색 성능을 크게 향상시킬 수 있다는 경험적 증거를 제공하고 핵심 문구 생성 모델의 한계를 더 잘 이해할 수 있는 새로운 외부 평가 프레임워크를 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "86-2",
                    "sentence": "Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains.",
                    "sentence_kor": "이 프레임워크를 사용하여 텍스트 키 문구에 없는 문서를 보완하고 도메인 전체에 걸쳐 모델을 일반화할 때 직면하는 어려움을 지적하고 논의한다.",
                    "tag": "2+3"
                },
                {
                    "index": "86-3",
                    "sentence": "Our code is available at https://github.com/boudinfl/ir-using-kg",
                    "sentence_kor": "우리의 코드는 https://github.com/boudinfl/ir-using-kg에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "686",
            "abstractID": "SPA_abs-87",
            "text": [
                {
                    "index": "87-0",
                    "sentence": "There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics.",
                    "sentence_kor": "언어학에서 복잡하고 어려운 것으로 판단되는 문제인 파생물의 형태학적 형태성(MWF) 모델링에 대한 연구는 거의 없었다.",
                    "tag": "1"
                },
                {
                    "index": "87-1",
                    "sentence": "We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation.",
                    "sentence_kor": "파생에서 접사와 스템의 호환성에 대한 정보를 캡처하는 임베딩을 학습하는 그래프 자동 인코더를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "87-2",
                    "sentence": "The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.",
                    "sentence_kor": "자동 인코더는 구문 및 의미 정보를 정신 어휘의 연관 정보와 결합하여 영어로 MWF를 놀랍도록 잘 모델링한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "687",
            "abstractID": "SPA_abs-88",
            "text": [
                {
                    "index": "88-0",
                    "sentence": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis.",
                    "sentence_kor": "감정 어휘는 단어의 정서적 의미를 설명하고 따라서 고급 감정과 감정 분석을 위한 중심 작품이 된다.",
                    "tag": "1"
                },
                {
                    "index": "88-1",
                    "sentence": "Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications.",
                    "sentence_kor": "그러나 수동으로 큐레이션된 어휘는 소수의 언어에서만 사용할 수 있으며, 세계 대부분의 언어에는 다운스트림 애플리케이션을 위한 귀중한 리소스가 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "88-2",
                    "sentence": "Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature.",
                    "sentence_kor": "설상가상으로, 이들의 범위는 종종 그들이 포함하는 어휘 단위와 그들이 특징짓는 감정 변수 측면에서 모두 제한적이다.",
                    "tag": "1"
                },
                {
                    "index": "88-3",
                    "sentence": "In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language.",
                    "sentence_kor": "이러한 병목현상을 극복하기 위해, 우리는 대상 언어에 대해 거의 임의로 큰 감정 어휘를 만드는 방법론을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "88-4",
                    "sentence": "Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model.",
                    "sentence_kor": "우리의 접근 방식에는 소스 언어 감정 어휘, 이중 언어 단어 번역 모델 및 대상 언어 내장 모델만 필요하다.",
                    "tag": "3"
                },
                {
                    "index": "88-5",
                    "sentence": "Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each.",
                    "sentence_kor": "91개 언어에 대한 이러한 요구사항을 충족함으로써, 우리는 각각 100k개 이상의 어휘 항목을 가진 8개의 감정 변수로 구성된 대표적으로 풍부한 커버리지 어휘를 생성할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "88-6",
                    "sentence": "We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables.",
                    "sentence_kor": "우리는 유형학적으로 다양한 12개 언어에 걸친 26개 데이터 세트의 인간 판단에 따라 자동으로 생성된 어휘를 평가했고, 우리의 접근 방식이 어휘 작성에 대한 최첨단 단일 언어 접근 방식과 일치하는 결과를 산출하며, 일부 언어와 변수에 대해서는 인간의 신뢰성을 능가한다는 것을 발견했다.",
                    "tag": "4+5"
                },
                {
                    "index": "88-7",
                    "sentence": "Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.",
                    "sentence_kor": "코드 및 데이터는 DOI 10.5281/zenodo에 따라 보관된 https://github.com/JULIELab/MEmoLon에서 사용할 수 있습니다.3779901.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "688",
            "abstractID": "SPA_abs-89",
            "text": [
                {
                    "index": "89-0",
                    "sentence": "Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem.",
                    "sentence_kor": "자동화된 메트릭을 통해 MT(Machine Translation)를 안정적으로 평가하는 것은 오랜 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "89-1",
                    "sentence": "One of the main challenges is the fact that multiple outputs can be equally valid.",
                    "sentence_kor": "주요 과제 중 하나는 다중 출력이 동등하게 유효할 수 있다는 사실이다.",
                    "tag": "1"
                },
                {
                    "index": "89-2",
                    "sentence": "Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references.",
                    "sentence_kor": "이 문제를 최소화하기 위한 시도에는 MT 출력과 참조 문자열의 일치를 완화하는 메트릭과 다중 참조 사용이 포함됩니다.",
                    "tag": "1"
                },
                {
                    "index": "89-3",
                    "sentence": "The latter has been shown to significantly improve the performance of evaluation metrics.",
                    "sentence_kor": "후자는 평가 지표의 성능을 크게 향상시키는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "89-4",
                    "sentence": "However, collecting multiple references is expensive and in practice a single reference is generally used.",
                    "sentence_kor": "그러나 여러 개의 참조를 수집하려면 비용이 많이 들고 실제로 단일 참조가 일반적으로 사용됩니다.",
                    "tag": "1"
                },
                {
                    "index": "89-5",
                    "sentence": "In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether.",
                    "sentence_kor": "본 논문에서 우리는 대안적 접근방식을 제안한다. 인간 참조의 언어적 변화를 모델링하는 대신 MT 모델 불확실성을 이용하여 다중 다양한 변환을 생성하고 이를 참조 번역 대리자로 사용한다. (ii) 기존 메트릭 스코르를 보완하기 위한 번역 변동성의 정량화를 얻기 위해.es 또는 (iii) 참조를 모두 대체한다.",
                    "tag": "2+3"
                },
                {
                    "index": "89-6",
                    "sentence": "We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.",
                    "sentence_kor": "우리는 많은 인기 있는 평가 지표의 경우 우리의 변동성 추정치가 인간 품질 판단과의 상관관계를 15%까지 크게 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "689",
            "abstractID": "SPA_abs-90",
            "text": [
                {
                    "index": "90-0",
                    "sentence": "We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE.",
                    "sentence_kor": "멀티모달 QE의 텍스트 및 시각적 양식을 모두 탐색하는 기계 번역용 품질 추정(QE)에 대한 접근 방식을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "90-1",
                    "sentence": "We compare various multimodality integration and fusion strategies.",
                    "sentence_kor": "우리는 다양한 다중 모드 통합과 융합 전략을 비교한다.",
                    "tag": "1"
                },
                {
                    "index": "90-2",
                    "sentence": "For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.",
                    "sentence_kor": "문장 수준 및 문서 수준 예측의 경우, 최첨단 신경 및 기능 기반 QE 프레임워크가 추가 양식을 사용할 때 더 나은 결과를 얻는다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "690",
            "abstractID": "SPA_abs-91",
            "text": [
                {
                    "index": "91-0",
                    "sentence": "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph.",
                    "sentence_kor": "우리는 AMR 파싱을 입력 시퀀스와 점진적으로 구성된 그래프에 대한 일련의 이중 결정으로 처리하는 새로운 엔드 투 엔드 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "91-1",
                    "sentence": "At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept.",
                    "sentence_kor": "각 단계에서 모델은 (1) 추상화할 입력 시퀀스의 어느 부분과 (2) 새로운 개념을 구성하기 위한 출력 그래프에서 두 가지 중요한 질문에 답하는 것을 목표로 하는 주의, 추론 및 구성의 여러 라운드를 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "91-2",
                    "sentence": "We show that the answers to these two questions are mutually causalities.",
                    "sentence_kor": "우리는 이 두 질문에 대한 답이 상호 인과 관계라는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "91-3",
                    "sentence": "We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy.",
                    "sentence_kor": "우리는 두 가지 관점 모두에서 더 나은 답변을 얻어 구문 분석 정확도를 크게 향상시키는 데 도움이 되는 반복 추론을 기반으로 모델을 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "91-4",
                    "sentence": "Our experimental results significantly outperform all previously reported Smatch scores by large margins.",
                    "sentence_kor": "우리의 실험 결과는 이전에 보고된 모든 매치 점수를 큰 폭으로 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "91-5",
                    "sentence": "Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT.",
                    "sentence_kor": "놀랍게도, 대규모 사전 교육 언어 모델(예: BERT)의 도움 없이 우리 모델은 이미 BERT를 사용한 이전의 최첨단 모델을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "91-6",
                    "sentence": "With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).",
                    "sentence_kor": "BERT의 도움을 받아 LDC2017T10(AMR 2.0)의 경우 80.2%, LDC2014T12(AMR 1.0)의 경우 75.4%로 최첨단 결과를 추진할 수 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "691",
            "abstractID": "SPA_abs-92",
            "text": [
                {
                    "index": "92-0",
                    "sentence": "Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English).",
                    "sentence_kor": "교차 언어 요약은 하나의 언어(예: 중국어)로 문서를 다른 언어(예: 영어)로 요약하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "92-1",
                    "sentence": "In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary.",
                    "sentence_kor": "본 논문에서, 우리는 언어 간 요약을 얻는 과정에서 번역 패턴에서 영감을 얻은 새로운 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "92-2",
                    "sentence": "We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary.",
                    "sentence_kor": "먼저 원본 텍스트의 일부 단어를 확인한 다음 대상 언어로 번역하고 요약하여 최종 요약을 가져옵니다.",
                    "tag": "3"
                },
                {
                    "index": "92-3",
                    "sentence": "Specifically, we first employ the encoder-decoder attention distribution to attend to the source words.",
                    "sentence_kor": "특히, 먼저 소스 단어를 주의하기 위해 인코더-디코더 주의 분포를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "92-4",
                    "sentence": "Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word.",
                    "sentence_kor": "둘째, 각 소스 단어의 번역 후보를 얻는 데 도움이 되는 번역 확률을 얻기 위한 세 가지 전략을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "92-5",
                    "sentence": "Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words.",
                    "sentence_kor": "마지막으로, 각 요약 단어는 신경 분포 또는 소스 단어의 번역 후보에서 생성된다.",
                    "tag": "3"
                },
                {
                    "index": "92-6",
                    "sentence": "Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.",
                    "sentence_kor": "중국어-영어 및 영어-중국어 요약 작업에 대한 실험 결과는 우리가 제안한 방법이 기준선을 크게 능가하여 최첨단과 유사한 성능을 달성할 수 있다는 것을 보여주었다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "692",
            "abstractID": "SPA_abs-93",
            "text": [
                {
                    "index": "93-0",
                    "sentence": "We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.).",
                    "sentence_kor": "우리는 인간이 작성한 참조 요약이나 인간 주석(예: 선호도, 등급 등)을 필요로 하지 않는 감독되지 않은 다중 문서 요약 평가 지표를 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "93-1",
                    "sentence": "We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques.",
                    "sentence_kor": "우리는 상황별 임베딩과 소프트 토큰 정렬 기법을 사용하여, 의사 참조 요약과 의미적 유사성을 측정하여 요약의 품질을 평가하는 SUPERT를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "93-2",
                    "sentence": "Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%.",
                    "sentence_kor": "최첨단 비지도 평가 지표와 비교하여 SUPERT는 인간 등급과 18-39% 더 잘 상관된다.",
                    "tag": "1"
                },
                {
                    "index": "93-3",
                    "sentence": "Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers.",
                    "sentence_kor": "또한 SUPERT를 보상으로 사용하여 신경 기반 강화 학습 요약기를 안내하여 최첨단 비지도 요약기에 비해 우수한 성능을 낸다.",
                    "tag": "1"
                },
                {
                    "index": "93-4",
                    "sentence": "All source code is available at https://github.com/yg211/acl20-ref-free-eval.",
                    "sentence_kor": "모든 소스 코드는 https://github.com/yg211/acl20-ref-free-eval에서 이용할 수 있습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "693",
            "abstractID": "SPA_abs-94",
            "text": [
                {
                    "index": "94-0",
                    "sentence": "Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary.",
                    "sentence_kor": "복사 모듈은 최근의 추상적 요약 모델에 광범위하게 장착되어 있어 디코더가 소스에서 요약으로 단어를 추출할 수 있도록 한다.",
                    "tag": "1"
                },
                {
                    "index": "94-1",
                    "sentence": "Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge.",
                    "sentence_kor": "일반적으로 인코더-디코더 주의는 복사 배포로 제공되지만 소스의 중요한 단어가 복사되도록 보장하는 방법은 여전히 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "94-2",
                    "sentence": "In this work, we propose a Transformer-based model to enhance the copy mechanism.",
                    "sentence_kor": "본 연구에서는 복사 메커니즘을 강화하기 위한 트랜스포머 기반 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "94-3",
                    "sentence": "Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer.",
                    "sentence_kor": "특히, 우리는 트랜스포머의 자기 주의 계층에 의해 구축된 방향 그래프를 사용하여 각 소스 워드의 중요성을 정도의 중심성에 기초하여 식별한다.",
                    "tag": "3"
                },
                {
                    "index": "94-4",
                    "sentence": "We use the centrality of each source word to guide the copy process explicitly.",
                    "sentence_kor": "각 소스 워드의 중심성을 사용하여 복사 프로세스를 명시적으로 안내합니다.",
                    "tag": "3"
                },
                {
                    "index": "94-5",
                    "sentence": "Experimental results show that the self-attention graph provides useful guidance for the copy distribution.",
                    "sentence_kor": "실험 결과에 따르면 자기 주의 그래프는 복사 분포에 유용한 지침을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "94-6",
                    "sentence": "Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.",
                    "sentence_kor": "우리가 제안한 모델은 CNN/Daily Mail 데이터 세트와 Gigaword 데이터 세트에서 기준 방법을 크게 능가한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "694",
            "abstractID": "SPA_abs-95",
            "text": [
                {
                    "index": "95-0",
                    "sentence": "Open Domain dialog system evaluation is one of the most important challenges in dialog research.",
                    "sentence_kor": "개방형 도메인 대화 상자 시스템 평가는 대화 상자 연구에서 가장 중요한 과제 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "95-1",
                    "sentence": "Existing automatic evaluation metrics, such as BLEU are mostly reference-based.",
                    "sentence_kor": "BLEU와 같은 기존의 자동 평가 지표는 대부분 참조 기반이다.",
                    "tag": "1"
                },
                {
                    "index": "95-2",
                    "sentence": "They calculate the difference between the generated response and a limited number of available references.",
                    "sentence_kor": "생성된 응답과 제한된 수의 사용 가능한 참조 간의 차이를 계산합니다.",
                    "tag": "1"
                },
                {
                    "index": "95-3",
                    "sentence": "Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots.",
                    "sentence_kor": "리커트 점수 기반 자체 보고 사용자 등급은 Amazon Alexa Prize 챗봇과 같은 사회적 대화 시스템에서 널리 채택되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "95-4",
                    "sentence": "However, self-reported user rating suffers from bias and variance among different users.",
                    "sentence_kor": "그러나 자체 보고 사용자 등급은 서로 다른 사용자 간의 편견과 분산을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "95-5",
                    "sentence": "To alleviate this problem, we formulate dialog evaluation as a comparison task.",
                    "sentence_kor": "이 문제를 완화하기 위해 대화 상자 평가를 비교 작업으로 공식화한다.",
                    "tag": "2+3"
                },
                {
                    "index": "95-6",
                    "sentence": "We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them.",
                    "sentence_kor": "또한 자동 대화 상자 평가를 위한 비교 모델인 CMADE(자동 대화 상자 평가를 위한 비교 모델)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "95-7",
                    "sentence": "Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples.",
                    "sentence_kor": "특히, 우리는 먼저 더 나은 대화 상자 기능 표현을 배우기 위해 자체 감독 방법을 사용한 다음 KNN과 섀플리를 사용하여 혼동되는 샘플을 제거한다.",
                    "tag": "3"
                },
                {
                    "index": "95-8",
                    "sentence": "Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.",
                    "sentence_kor": "우리의 실험은 CMADE가 대화 상자 비교 작업에서 89.2%의 정확도를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "695",
            "abstractID": "SPA_abs-96",
            "text": [
                {
                    "index": "96-0",
                    "sentence": "Human conversations contain many types of information, e.g., knowledge, common sense, and language habits.",
                    "sentence_kor": "인간의 대화는 지식, 상식, 언어 습관과 같은 많은 종류의 정보를 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "96-1",
                    "sentence": "In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding.",
                    "sentence_kor": "본 논문에서는 대화 쌍 <포스트, 응답>을 사용하여 단어 임베딩을 학습하는 PR-Embedding이라는 대화 단어 임베딩 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "96-2",
                    "sentence": "Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.",
                    "sentence_kor": "이전 작업과 달리 PR-Embeding은 두 개의 서로 다른 의미 공간의 벡터를 사용하여 포스트 및 응답의 단어를 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "96-3",
                    "sentence": "To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.",
                    "sentence_kor": "쌍 간의 정보를 수집하기 위해 먼저 통계 기계 번역에서 단어 정렬 모델을 도입하여 문장 간 창을 생성한 다음 단어 수준과 문장 수준에서 임베딩을 훈련시킨다.",
                    "tag": "3"
                },
                {
                    "index": "96-4",
                    "sentence": "We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.",
                    "sentence_kor": "검색 기반 대화 시스템의 싱글 턴 및 멀티 턴 응답 선택 작업에 대한 방법을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "96-5",
                    "sentence": "The experiment results show that PR-Embedding can improve the quality of the selected response.",
                    "sentence_kor": "실험 결과는 PR-임베딩이 선택한 반응의 품질을 향상시킬 수 있음을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "696",
            "abstractID": "SPA_abs-97",
            "text": [
                {
                    "index": "97-0",
                    "sentence": "In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot).",
                    "sentence_kor": "본 논문에서는 라벨이 부착된 지원 문장(예: 퓨샷) 몇 개만으로 슬롯 태깅을 살펴본다.",
                    "tag": "2"
                },
                {
                    "index": "97-1",
                    "sentence": "Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels.",
                    "sentence_kor": "퓨샷 슬롯 태그는 레이블 간의 의존성을 모델링해야 하기 때문에 다른 퓨샷 분류 문제와 비교하여 고유한 과제에 직면한다.",
                    "tag": "1"
                },
                {
                    "index": "97-2",
                    "sentence": "But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets.",
                    "sentence_kor": "그러나 라벨 세트의 불일치 때문에 이전에 학습한 라벨 종속성을 보이지 않는 도메인에 적용하는 것은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "97-3",
                    "sentence": "To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores.",
                    "sentence_kor": "이를 해결하기 위해, 우리는 추상 레이블 의존성 패턴을 전환 점수로 전송하기 위해 조건부 랜덤 필드(CRF)에 축소된 의존성 전송 메커니즘을 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "97-4",
                    "sentence": "In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label. To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels.",
                    "sentence_kor": "퓨샷 설정에서 CRF의 방출 점수는 각 라벨의 표현과 단어의 유사성으로 계산할 수 있다. 이와 같은 유사성을 계산하기 위해 레이블을 나타내는 데 라벨 이름 의미론을 활용하여 최첨단 퓨샷 분류 모델인 TapNet을 기반으로 라벨 강화 작업 적응 투영 네트워크(L-TapNet)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "97-5",
                    "sentence": "Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.",
                    "sentence_kor": "실험 결과에 따르면 우리 모델은 원샷 설정에서 가장 강력한 퓨샷 학습 기준선을 14.64 F1 점수만큼 크게 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "697",
            "abstractID": "SPA_abs-98",
            "text": [
                {
                    "index": "98-0",
                    "sentence": "Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems.",
                    "sentence_kor": "심층 강화 학습은 대화 관리자 교육에 대한 유망한 접근법이지만, 현재의 방법은 다중 도메인 대화 시스템의 대규모 상태 및 작업 공간과 씨름한다.",
                    "tag": "1"
                },
                {
                    "index": "98-1",
                    "sentence": "Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests.",
                    "sentence_kor": "어려운 Atari 게임에서 높은 점수를 받는 알고리즘인 DQfD(Deep Q-Learning from 데모)를 기반으로 대화 데이터를 활용하여 에이전트가 사용자의 요청에 성공적으로 응답하도록 안내한다.",
                    "tag": "2+3"
                },
                {
                    "index": "98-2",
                    "sentence": "We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators.",
                    "sentence_kor": "우리는 전문가 데모 참가자를 훈련시키기 위해 라벨 부착, 축소 라벨 부착 및 라벨 부착되지 않은 데이터를 사용하여 필요한 데이터에 대한 가정을 점진적으로 적게 한다.",
                    "tag": "3"
                },
                {
                    "index": "98-3",
                    "sentence": "We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment.",
                    "sentence_kor": "데이터 세트와 환경 간의 도메인 격차를 극복할 수 있도록 DQfD의 확장인 강화 미세 조정 학습을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "98-4",
                    "sentence": "Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.",
                    "sentence_kor": "도전적인 다중 도메인 대화 시스템 프레임워크에서의 실험은 우리의 접근 방식을 검증하고, 도메인 외 데이터에 대한 교육을 받은 경우에도 높은 성공률을 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "698",
            "abstractID": "SPA_abs-99",
            "text": [
                {
                    "index": "99-0",
                    "sentence": "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors.",
                    "sentence_kor": "잡담 대화 시스템의 참여성과 일관성을 개선하려는 지속적인 노력에도 불구하고, 현재 작업의 대부분은 단순히 인간과 유사한 반응을 모방하는 데 초점을 맞추고 있으며, 대화자 간의 이해 모델링 측면은 연구되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "99-1",
                    "sentence": "The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation.",
                    "sentence_kor": "대신, 인지과학 연구는 이해는 높은 수준의 잡담 대화를 위한 필수적인 신호라는 것을 암시한다.",
                    "tag": "1"
                },
                {
                    "index": "99-2",
                    "sentence": "Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding.",
                    "sentence_kor": "이에 동기를 부여하여 이해를 명시적으로 모델링하기 위한 송신기-수신기 기반 프레임워크인 P with2 Bot을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "99-3",
                    "sentence": "Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation.",
                    "sentence_kor": "특히 Pˆ2 Bot은 개인화된 대화 생성의 질을 향상시키기 위해 상호 페르소나 인식을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "99-4",
                    "sentence": "Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.",
                    "sentence_kor": "대규모 공개 데이터 세트인 Persona-Chat에 대한 실험은 자동 메트릭과 인간 평가 모두에서 최첨단 기준선을 크게 향상시키면서 접근 방식의 효과를 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "699",
            "abstractID": "SPA_abs-100",
            "text": [
                {
                    "index": "100-0",
                    "sentence": "Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given.",
                    "sentence_kor": "아나포라 해상도 브리징에 대한 대부분의 이전 연구(Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a)는 쌍방향 모델을 사용하여 문제를 해결하고 골드 멘션 정보가 제공되었다고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "100-1",
                    "sentence": "In this paper, we cast bridging anaphora resolution as question answering based on context.",
                    "sentence_kor": "본 논문에서 우리는 맥락에 기초한 질문 답변으로 브리징 아나포라 해결을 캐스팅했다.",
                    "tag": "2+3"
                },
                {
                    "index": "100-2",
                    "sentence": "This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself).",
                    "sentence_kor": "이렇게 하면, 아나포르는 금으로 된 언급 정보를 전혀 알지 못하더라도, 주어진 아나포어의 선례를 찾을 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "100-3",
                    "sentence": "We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning.",
                    "sentence_kor": "우리는 전송 학습의 힘을 활용하는 이 작업에 대한 질문 응답 프레임워크(BARQA)를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "100-4",
                    "sentence": "Furthermore, we propose a novel method to generate a large amount of “quasi-bridging” training data.",
                    "sentence_kor": "또한, 우리는 대량의 \"준 브리지\" 훈련 데이터를 생성하는 새로운 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "100-5",
                    "sentence": "We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).",
                    "sentence_kor": "우리는 이 데이터 세트에 대해 사전 교육되고 소량의 도메인 내 데이터 세트에서 미세 조정된 모델이 두 개의 브리지 코포라(ISNotes, Markert et al., 2012)와 BASHI(Ro ssiger, 2018)에서 아나포라 해상도를 연결하기 위한 새로운 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "700",
            "abstractID": "SPA_abs-101",
            "text": [
                {
                    "index": "101-0",
                    "sentence": "Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels.",
                    "sentence_kor": "최근의 대화 일관성 모델은 독백 텍스트(예: 명목적 실체)에 대해 설계된 일관성 특징을 사용하여 발화를 표현한 다음 대화 관련 특징(예: 대화 행위 라벨)으로 명시적으로 강화한다.",
                    "tag": "1"
                },
                {
                    "index": "101-1",
                    "sentence": "It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels.",
                    "sentence_kor": "이는 (a) 발언의 의미론이 실체 언급으로 제한되고, (b) 일관성 모델의 성능은 입력 대화 행위 라벨의 품질에 크게 의존한다는 두 가지 단점을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "101-2",
                    "sentence": "We address these issues by introducing a novel approach to dialogue coherence assessment.",
                    "sentence_kor": "우리는 대화 일관성 평가에 대한 새로운 접근방식을 도입함으로써 이러한 문제들을 다룬다.",
                    "tag": "1"
                },
                {
                    "index": "101-3",
                    "sentence": "We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment.",
                    "sentence_kor": "일관성 평가를 위한 정보 발화 표현을 얻기 위해 다중 작업 학습 시나리오의 보조 과제로 대화 행위 예측을 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "101-4",
                    "sentence": "Our approach alleviates the need for explicit dialogue act labels during evaluation.",
                    "sentence_kor": "우리의 접근방식은 평가 중에 명시적인 대화 행위 라벨의 필요성을 완화한다.",
                    "tag": "4"
                },
                {
                    "index": "101-5",
                    "sentence": "The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.",
                    "sentence_kor": "실험 결과는 우리 모델(20개 이상의 정확도 포인트)이 DailyDialogue 말뭉치에서 강력한 경쟁자를 크게 능가하고, 스위치보드 말뭉치에서 그들과 동등한 성능을 발휘하여 일관성에 관한 대화의 순위를 매긴다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "101-6",
                    "sentence": "We release our source code.",
                    "sentence_kor": "소스 코드를 공개합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "701",
            "abstractID": "SPA_abs-102",
            "text": [
                {
                    "index": "102-0",
                    "sentence": "We propose a graph-based method to tackle the dependency tree linearization task.",
                    "sentence_kor": "종속성 트리 선형화 작업을 다루기 위한 그래프 기반 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "102-1",
                    "sentence": "We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs.",
                    "sentence_kor": "우리는 이 작업을 TSP(여행 판매원 문제)로 공식화하고 바이아핀 주의 모델을 사용하여 에지 비용을 계산한다.",
                    "tag": "3"
                },
                {
                    "index": "102-2",
                    "sentence": "We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree.",
                    "sentence_kor": "우리는 각 하위 트리에 대한 TSP를 해결하고 솔루션을 투영 트리로 결합하여 디코딩을 용이하게 한다.",
                    "tag": "3"
                },
                {
                    "index": "102-3",
                    "sentence": "We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences.",
                    "sentence_kor": "그런 다음 비투사형 전환 기반 구문 분석에서 영감을 받아 사후 처리로 전환 시스템을 설계하여 비투사형 문장을 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "102-4",
                    "sentence": "Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.",
                    "sentence_kor": "우리가 제안한 방법은 훈련과 디코딩에서 10배 더 빠르면서도 최첨단 선형화기를 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "702",
            "abstractID": "SPA_abs-103",
            "text": [
                {
                    "index": "103-0",
                    "sentence": "This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage.",
                    "sentence_kor": "본 논문은 입력 경로에 대한 여러 정보에 대한 추론이 필요한 복잡한 질문을 생성하는 것을 목표로 하는 심층 질문 생성(DQG)의 문제를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "103-1",
                    "sentence": "In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN).",
                    "sentence_kor": "문서의 전역 구조를 포착하고 추론을 용이하게 하기 위해 먼저 입력 문서의 의미 수준 그래프를 구성한 다음 주의 기반 GGNN(Att-GGNN)을 도입하여 의미 그래프를 인코딩하는 새로운 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "103-2",
                    "sentence": "Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding.",
                    "sentence_kor": "그 후에, 우리는 내용 선택과 질문 디코딩의 공동 훈련을 수행하기 위해 문서 레벨과 그래프 레벨 표현을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "103-3",
                    "sentence": "On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance.",
                    "sentence_kor": "HotpotQA의 심층 질문 중심 데이터 세트에서 우리 모델은 여러 사실에 대한 추론이 필요한 질문에 대해 성능을 크게 향상시켜 최첨단 성능을 제공한다.",
                    "tag": "4+5"
                },
                {
                    "index": "103-4",
                    "sentence": "The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.",
                    "sentence_kor": "이 코드는 https://github.com/WING-NUS/SG-Deep-Question-Generation에서 공개적으로 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "703",
            "abstractID": "SPA_abs-104",
            "text": [
                {
                    "index": "104-0",
                    "sentence": "Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training.",
                    "sentence_kor": "명명된 개체 인식(NER) 성능은 훈련 중에 관찰된 텍스트와 다른 대상 도메인에 적용될 때 급격히 저하되는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "104-1",
                    "sentence": "When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain.",
                    "sentence_kor": "도메인 내 라벨링 데이터를 사용할 수 있는 경우 전송 학습 기법을 사용하여 기존 NER 모델을 대상 도메인에 적용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "104-2",
                    "sentence": "But what should one do when there is no hand-labelled data for the target domain?",
                    "sentence_kor": "그러나 대상 도메인에 대한 수동 레이블링 데이터가 없는 경우 어떻게 해야 할까요?",
                    "tag": "1"
                },
                {
                    "index": "104-3",
                    "sentence": "This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision.",
                    "sentence_kor": "본 논문은 약한 감독을 통해 라벨링된 데이터가 없는 경우 NER 모델을 학습할 수 있는 간단하지만 강력한 접근 방식을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "104-4",
                    "sentence": "The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain.",
                    "sentence_kor": "이 접근법은 광범위한 라벨링 기능에 의존하여 대상 영역의 텍스트에 자동으로 주석을 달 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "104-5",
                    "sentence": "These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions.",
                    "sentence_kor": "그런 다음 이러한 주석은 레이블링 함수의 다양한 정확도와 혼동을 포착하는 숨겨진 마르코프 모델을 사용하여 함께 병합됩니다.",
                    "tag": "1"
                },
                {
                    "index": "104-6",
                    "sentence": "A sequence labelling model can finally be trained on the basis of this unified annotation.",
                    "sentence_kor": "이 통합 주석을 기반으로 시퀀스 라벨링 모델을 최종적으로 교육할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "104-7",
                    "sentence": "We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.",
                    "sentence_kor": "우리는 두 개의 영어 데이터 세트(CoNLL 2003 및 로이터와 블룸버그의 뉴스 기사)에 대한 접근 방식을 평가하고 영역 외 신경 NER 모델에 비해 엔티티 레벨 F1 점수가 약 7% 향상되었음을 입증한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "704",
            "abstractID": "SPA_abs-105",
            "text": [
                {
                    "index": "105-0",
                    "sentence": "Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities.",
                    "sentence_kor": "문서 수준 관계 추출을 위해서는 문서의 여러 문장 내에서 또는 여러 문장에 걸쳐 정보를 통합하고 문장 간 실체 간의 복잡한 상호 작용을 포착해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "105-1",
                    "sentence": "However, effective aggregation of relevant information in the document remains a challenging research question.",
                    "sentence_kor": "그러나 문서에서 관련 정보의 효과적인 집계는 여전히 어려운 연구 질문으로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "105-2",
                    "sentence": "Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies.",
                    "sentence_kor": "기존 접근 방식은 종속성을 모델링하기 위해 구조화되지 않은 텍스트의 구문 트리, 공동 참조 또는 휴리스틱을 기반으로 정적 문서 수준 그래프를 구성한다.",
                    "tag": "1"
                },
                {
                    "index": "105-3",
                    "sentence": "Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph.",
                    "sentence_kor": "추론을 위해 풍부한 비로컬 상호 작용을 포착할 수 없는 이전 방법과 달리, 우리는 잠재 문서 수준 그래프를 자동으로 유도하여 문장 전반에 걸쳐 관계적 추론을 강화하는 새로운 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "105-4",
                    "sentence": "We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning.",
                    "sentence_kor": "우리는 모델이 다중 홉 추론을 위해 관련 정보를 점진적으로 집계할 수 있는 개선 전략을 추가로 개발한다.",
                    "tag": "3+4"
                },
                {
                    "index": "105-5",
                    "sentence": "Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset.",
                    "sentence_kor": "특히, 우리 모델은 대규모 문서 수준 데이터 세트(DocRED)에서 59.05의 F1 점수를 달성하여 이전 결과에 비해 크게 개선되었으며 CDR 및 GDA 데이터 세트에서 새로운 최첨단 결과를 산출한다.",
                    "tag": "4+5"
                },
                {
                    "index": "105-6",
                    "sentence": "Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",
                    "sentence_kor": "또한 광범위한 분석에 따르면 모델이 문장 간 관계를 보다 정확하게 발견할 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "705",
            "abstractID": "SPA_abs-106",
            "text": [
                {
                    "index": "106-0",
                    "sentence": "In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary.",
                    "sentence_kor": "본 논문에서 우리는 병렬 문장은 없지만 지상 진실 이중 언어 사전을 참조할 수 있는 기계 번역(MT)이라는 새로운 과제를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "106-1",
                    "sentence": "Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences.",
                    "sentence_kor": "이중 언어 사전을 찾아 번역하는 것을 배우는 단일 언어 화자의 능력에 자극을 받아, MT 시스템이 병렬 문장에서 독립적이면서도 이중 언어 사전과 대규모 단일 언어 말뭉치를 사용하여 얼마나 많은 잠재력을 달성할 수 있는지 확인하는 과제를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "106-2",
                    "sentence": "We propose anchored training (AT) to tackle the task.",
                    "sentence_kor": "우리는 이 과제를 다루기 위해 고정된 훈련(AT)을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "106-3",
                    "sentence": "AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language.",
                    "sentence_kor": "AT는 이중 언어 사전을 사용하여 소스 언어와 대상 언어 사이의 격차를 줄이기 위한 고정 지점을 설정한다.",
                    "tag": "3"
                },
                {
                    "index": "106-4",
                    "sentence": "Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT.",
                    "sentence_kor": "다양한 언어 쌍에 대한 실험은 사전 기반 단어 대 단어 번역, 사전 감독 교차 언어 단어 임베딩 변환 및 비감독 MT를 포함한 다양한 기준선보다 우리의 접근 방식이 훨씬 더 낫다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "106-5",
                    "sentence": "On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.",
                    "sentence_kor": "감독되지 않은 MT가 잘 수행되기 어려운 원거리 언어 쌍에서 AT는 4M 이상의 병렬 문장에서 훈련된 감독된 시만텍과 견줄 만한 성능을 달성하여 월등히 우수한 성능을 발휘한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "706",
            "abstractID": "SPA_abs-107",
            "text": [
                {
                    "index": "107-0",
                    "sentence": "Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models.",
                    "sentence_kor": "단어 정렬은 통계 기계 번역(MT) 모델을 훈련하는 데 필수적인 역할 때문에 한때 자연어 처리에서 비지도 학습의 핵심 과제였다.",
                    "tag": "1"
                },
                {
                    "index": "107-1",
                    "sentence": "Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection.",
                    "sentence_kor": "신경 MT 모델 훈련에 불필요하지만, 단어 정렬은 주석 전송 및 어휘 주입과 같은 신경 기계 번역의 대화형 적용에 여전히 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "107-2",
                    "sentence": "While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems.",
                    "sentence_kor": "통계 MT 방법이 우수한 성능의 신경 접근법으로 대체되었지만, 20년 된 GIZA++ 툴킷은 최첨단 단어 정렬 시스템의 핵심 구성 요소로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "107-3",
                    "sentence": "Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training.",
                    "sentence_kor": "신경 단어 정렬에 대한 이전 연구는 훈련 중에 GIZA++의 출력을 사용함으로써 GIZA++를 능가할 수 있었다.",
                    "tag": "1"
                },
                {
                    "index": "107-4",
                    "sentence": "We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets.",
                    "sentence_kor": "우리는 세 개의 데이터 세트에서 GIZA++를 지속적으로 능가하는 첫 번째 종단 간 신경 단어 정렬 방법을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "107-5",
                    "sentence": "Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.",
                    "sentence_kor": "우리의 접근 방식은 감독 번역용으로 훈련된 트랜스포머 모델을 긴밀하게 통합되고 번역 품질에 영향을 미치지 않는 방식으로 감독되지 않은 단어 정렬 모델로도 사용할 수 있도록 용도 변경한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "707",
            "abstractID": "SPA_abs-108",
            "text": [
                {
                    "index": "108-0",
                    "sentence": "Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism.",
                    "sentence_kor": "대부분의 신경 기계 번역 모델은 구문 정보가 주의 메커니즘에 의해 자동으로 학습된다고 가정하여 병렬 문장 쌍에만 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "108-1",
                    "sentence": "In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios.",
                    "sentence_kor": "본 연구에서는 트랜스포머 모델에 구문 지식을 통합하기 위한 다양한 접근 방식을 조사하고 특히 긴 문장 및 저자원 시나리오에서 번역 품질을 향상시키는 새로운 매개 변수가 없는 의존성 인식 자기 주의 메커니즘을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "108-2",
                    "sentence": "We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.",
                    "sentence_kor": "WMT 영어-독일어 및 영어-터키어 및 WAT 영어-일본어 번역 작업에 대한 각 접근 방식의 효과를 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "708",
            "abstractID": "SPA_abs-109",
            "text": [
                {
                    "index": "109-0",
                    "sentence": "Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations.",
                    "sentence_kor": "신경 기계 번역(NMT)을 위한 대규모 다국어 모델은 이론적으로는 매력적이지만 종종 이중 언어 모델의 성능이 떨어지고 제로샷 변환을 제대로 제공하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "109-1",
                    "sentence": "In this paper, we explore ways to improve them.",
                    "sentence_kor": "본 논문에서는 이를 개선하기 위한 방법을 알아본다.",
                    "tag": "2+3"
                },
                {
                    "index": "109-2",
                    "sentence": "We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures.",
                    "sentence_kor": "우리는 다국어 NMT가 다양한 유형학적 특성을 가진 언어 쌍을 지원하고 언어별 구성 요소와 NMT 아키텍처를 통해 이러한 병목 현상을 극복하기 위해 더 강력한 모델링 능력이 필요하다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "109-3",
                    "sentence": "We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs.",
                    "sentence_kor": "우리는 열등한 제로샷 성능의 주요 원천으로 목표 외 번역 문제(즉, 잘못된 대상 언어로 번역)를 식별하고 보이지 않는 훈련 언어 쌍의 번역을 강제하기 위해 무작위 온라인 역번역을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "109-4",
                    "sentence": "Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.",
                    "sentence_kor": "OPUS-100(100개 언어를 사용하는 새로운 다국어 데이터 세트)에 대한 실험은 우리의 접근 방식이 일대다 및 다대다 설정에서 이중 언어 모델과의 성능 격차를 상당히 좁히고 기존 피벗 기반 방법에 근접하여 최대 10 BLEU까지 제로샷 성능을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "709",
            "abstractID": "SPA_abs-110",
            "text": [
                {
                    "index": "110-0",
                    "sentence": "The performance of neural machine translation systems is commonly evaluated in terms of BLEU.",
                    "sentence_kor": "신경 기계 번역 시스템의 성능은 일반적으로 BLEU 측면에서 평가된다.",
                    "tag": "1"
                },
                {
                    "index": "110-1",
                    "sentence": "However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model.",
                    "sentence_kor": "그러나 대상 언어 속성 및 생성에 대한 의존성 때문에 BLEU 메트릭은 어떤 변환 방향을 모델링하기 더 어려운지에 대한 평가를 허용하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "110-2",
                    "sentence": "In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models.",
                    "sentence_kor": "본 논문에서, 우리는 대부분의 신경 기계 번역 모델의 확률론적 특성을 이용하는 비대칭 정보-기계 번역 난이도 메트릭인 교차 상호 정보(XMI)를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "110-3",
                    "sentence": "XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task.",
                    "sentence_kor": "XMI를 사용하면 번역 작업과 무관하게 대상 측 생성 구성요소의 난이도를 제어하면서 텍스트를 대상 언어로 변환하는 어려움을 더 잘 평가할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "110-4",
                    "sentence": "We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems.",
                    "sentence_kor": "그런 다음 현대 신경 번역 시스템을 사용한 언어 간 번역 어려움에 대한 체계적이고 통제된 첫 번째 연구를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "110-5",
                    "sentence": "Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.",
                    "sentence_kor": "실험을 복제하기 위한 코드는 https://github.com/e-bug/nmt-difficulty에서 온라인으로 확인할 수 있습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "710",
            "abstractID": "SPA_abs-111",
            "text": [
                {
                    "index": "111-0",
                    "sentence": "Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages.",
                    "sentence_kor": "다국어 신경 기계 번역(NMT)은 여러 언어 간에 공통 언어 정보를 공유하여 저자원 시나리오에서 뛰어난 정확성을 향상시켰다.",
                    "tag": "1"
                },
                {
                    "index": "111-1",
                    "sentence": "However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained.",
                    "sentence_kor": "그러나 기존의 다국어 모델은 다른 언어의 다양성과 특수성을 포착하지 못하여 충분히 훈련된 개별 모델에 비해 성능이 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "111-2",
                    "sentence": "In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture.",
                    "sentence_kor": "본 논문에서 우리는 언어 인식 인터링구아를 인코더-디코더 아키텍처에 통합한다.",
                    "tag": "1"
                },
                {
                    "index": "111-3",
                    "sentence": "The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair.",
                    "sentence_kor": "언어 간 네트워크는 모델이 다른 언어의 의미 공간으로부터 언어 독립적인 표현을 학습하는 동시에 특정 언어 쌍에 대한 언어 고유의 전문화를 가능하게 한다.",
                    "tag": "1"
                },
                {
                    "index": "111-4",
                    "sentence": "Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.",
                    "sentence_kor": "실험에 따르면 제안된 방법은 최첨단 다국어 NMT 기준선에 비해 현저한 개선을 달성하고 강력한 개별 모델과 유사한 성능을 낸다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "711",
            "abstractID": "SPA_abs-112",
            "text": [
                {
                    "index": "112-0",
                    "sentence": "Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity.",
                    "sentence_kor": "교차 언어 인코더의 평가는 일반적으로 감독 다운스트림 작업에서 제로샷 교차 언어 전송을 통해 수행되거나 감독되지 않은 교차 언어 텍스트 유사성을 통해 수행된다.",
                    "tag": "1"
                },
                {
                    "index": "112-1",
                    "sentence": "In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders.",
                    "sentence_kor": "본 논문에서 우리는 소스 텍스트를 다국어 인코더의 자연스러운 적대적 설정을 나타내는 (때로는 낮은 품질의) 시스템 번역과 직접 비교하는 MT(참조 없는 기계 번역) 평가에 관심이 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "112-2",
                    "sentence": "Reference-free evaluation holds the promise of web-scale comparison of MT systems.",
                    "sentence_kor": "참조가 없는 평가는 MT 시스템의 웹 스케일 비교를 약속한다.",
                    "tag": "1"
                },
                {
                    "index": "112-3",
                    "sentence": "We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER.",
                    "sentence_kor": "사전 훈련된 M-BERT와 LASER로 얻은 최첨단 언어 교차 의미 표현을 기반으로 다양한 측정 기준을 체계적으로 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "112-4",
                    "sentence": "We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations.",
                    "sentence_kor": "참조 없는 MT 평가를 위한 의미 인코더로서 성능이 떨어지고 (a) 상호 번역 표현 사이의 의미적 불일치, (b) \"번역어\", 즉 낮은 품질의 문자 번역을 처벌할 수 없는 두 가지 주요 한계를 식별한다.",
                    "tag": "4"
                },
                {
                    "index": "112-5",
                    "sentence": "We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling.",
                    "sentence_kor": "(1) 벡터 공간의 사후 정렬과 (2) 대상 측 언어 모델링과 의미 유사성 기반 메트릭의 결합이라는 두 가지 부분적인 해결책을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "112-6",
                    "sentence": "In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.",
                    "sentence_kor": "세그먼트 수준 MT 평가에서 최상의 메트릭은 참조 기반 BLEU를 5.7 상관점 차이로 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "712",
            "abstractID": "SPA_abs-113",
            "text": [
                {
                    "index": "113-0",
                    "sentence": "We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation.",
                    "sentence_kor": "신경 기계 번역을 사용하여 두 개의 단일 언어 말뭉치에서 병렬 문장을 추출하는 새로운 방법을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "113-1",
                    "sentence": "Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus.",
                    "sentence_kor": "우리의 방법은 한 말뭉치의 문장 번역에 의존하지만 다른 말뭉치에 구축된 접두사 트리에 의해 디코딩을 제한한다.",
                    "tag": "3"
                },
                {
                    "index": "113-2",
                    "sentence": "We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search.",
                    "sentence_kor": "우리는 신경 기계 번역 시스템이 그 자체로 문장 유사성 점수가 될 수 있으며 수정된 빔 검색과 쌍별 비교를 효율적으로 근사화한다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "113-3",
                    "sentence": "When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.",
                    "sentence_kor": "BUCC 공유 과제를 벤치마킹했을 때, 우리의 방법은 다른 제출물과 비슷한 결과를 달성한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "713",
            "abstractID": "SPA_abs-114",
            "text": [
                {
                    "index": "114-0",
                    "sentence": "Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences.",
                    "sentence_kor": "자기 주의 네트워크(SAN)의 필수적인 부분인 위치 인코딩(PE)은 입력 시퀀스에 대한 고정 위치 지수를 생성하여 자연어 처리 작업에 대한 워드 순서 정보를 보존하는 데 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "114-1",
                    "sentence": "However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently.",
                    "sentence_kor": "그러나 언어 간 시나리오, 기계 번역에서 소스 문장과 대상 문장의 PE는 독립적으로 모델링된다.",
                    "tag": "1"
                },
                {
                    "index": "114-2",
                    "sentence": "Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem.",
                    "sentence_kor": "언어마다 어순 차이가 있기 때문에 언어 간 위치 관계를 모델링하면 SAN이 이 문제를 해결하는 데 도움이 될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "114-3",
                    "sentence": "In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence.",
                    "sentence_kor": "본 논문에서 우리는 입력 문장에 대해 이중으로 인식하는 잠재 구조를 모델링하기 위해 교차 언어 위치 표현으로 SAN을 증강한다.",
                    "tag": "2+3"
                },
                {
                    "index": "114-4",
                    "sentence": "Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments.",
                    "sentence_kor": "특히, 우리는 SAN이 이중 언어 대각 정렬을 배우도록 장려하기 위해 브래킷 변환 문법(BTG) 기반 재정렬 정보를 활용한다.",
                    "tag": "3"
                },
                {
                    "index": "114-5",
                    "sentence": "Experimental results on WMT’14 English⇒German, WAT’17 Japanese⇒English, and WMT’17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines.",
                    "sentence_kor": "WMT'14 영어german독일어, WAT'17 일본어english영어 및 WMT'17 중국어english영어 번역 작업에 대한 실험 결과는 우리의 접근 방식이 강력한 기준선에 비해 번역 품질을 현저하고 지속적으로 향상시킨다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "114-6",
                    "sentence": "Extensive analyses confirm that the performance gains come from the cross-lingual information.",
                    "sentence_kor": "광범위한 분석에 따르면 성능 이득은 언어 간 정보에서 비롯된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "714",
            "abstractID": "SPA_abs-115",
            "text": [
                {
                    "index": "115-0",
                    "sentence": "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.",
                    "sentence_kor": "언어 분류, 필터링 및 청소를 통해 공통 크롤에서 추출한 다국어 OCAR 말뭉치를 사용하여 5개의 중간 자원 언어에 대한 단일 언어 상황별 단어 임베딩(ELMo)을 훈련한다.",
                    "tag": "1"
                },
                {
                    "index": "115-1",
                    "sentence": "We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks.",
                    "sentence_kor": "그런 다음 음성 태그 지정 및 구문 분석 작업에서 이러한 언어에 대한 OscAR 기반 및 Wikipedia 기반 ELMo 임베딩의 성능을 비교한다.",
                    "tag": "1"
                },
                {
                    "index": "115-2",
                    "sentence": "We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia.",
                    "sentence_kor": "공통 크롤 기반 오스카 데이터의 노이즈에도 불구하고 오스카에서 훈련된 임베딩이 Wikipedia에서 훈련된 단일 언어 임베딩보다 훨씬 더 우수하다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "115-3",
                    "sentence": "They actually equal or improve the current state of the art in tagging and parsing for all five languages.",
                    "sentence_kor": "실제로 5개 언어 모두에 대한 태그 지정 및 구문 분석에서 최신 기술과 동일하거나 개선됩니다.",
                    "tag": "1"
                },
                {
                    "index": "115-4",
                    "sentence": "In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
                    "sentence_kor": "특히, 그들은 또한 거의 항상 이전 기술 상태를 구성하는 다국어 위키피디아 기반 상황 임베딩(다언어 BERT)에 비해 개선되므로, 더 크고 더 다양한 말뭉치의 이점이 다국어 임베딩 아키텍처의 교차 언어 이점을 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "715",
            "abstractID": "SPA_abs-116",
            "text": [
                {
                    "index": "116-0",
                    "sentence": "While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge.",
                    "sentence_kor": "최첨단 신경망 모델은 언어 모델링 벤치마크에서 계속 낮은 복잡도 점수를 달성하고 있지만, 광범위한 예측 성능에 대한 최적화가 인간과 유사한 통사적 지식으로 이어지는지는 여전히 미지수이다.",
                    "tag": "1"
                },
                {
                    "index": "116-1",
                    "sentence": "Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations.",
                    "sentence_kor": "또한, 기존 연구는 적절한 구문 일반화를 생성하는 데 필요한 모델 특성에 대한 명확한 그림을 제공하지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "116-2",
                    "sentence": "We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites.",
                    "sentence_kor": "34개의 영어 구문 테스트 제품군에서 모델 유형 및 데이터 크기의 20가지 조합을 테스트하면서 신경 언어 모델의 구문 지식에 대한 체계적인 평가를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "116-3",
                    "sentence": "We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures.",
                    "sentence_kor": "순차적 모델이 다른 아키텍처를 능가하므로 모델 아키텍처별 구문 일반화 성능에서 상당한 차이를 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "116-4",
                    "sentence": "Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments.",
                    "sentence_kor": "모델 아키텍처 및 교육 데이터 세트 크기(1M-40M 단어)를 인수분해하면 구문 일반화 성능의 변동성이 실험에서 테스트한 말뭉치의 데이터 세트 크기보다 아키텍처에 의해 훨씬 더 크다는 것을 알 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "116-5",
                    "sentence": "Our results also reveal a dissociation between perplexity and syntactic generalization performance.",
                    "sentence_kor": "우리의 결과는 또한 난해함과 구문 일반화 성능 사이의 연관성을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "716",
            "abstractID": "SPA_abs-117",
            "text": [
                {
                    "index": "117-0",
                    "sentence": "With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful.",
                    "sentence_kor": "지난 몇 년 동안 강력한 신경 언어 모델의 출현과 함께, 연구 관심은 언어를 성공적으로 만드는 언어의 어떤 측면을 나타내는지에 점점 더 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "117-1",
                    "sentence": "Several testing methodologies have been developed to probe models’ syntactic representations.",
                    "sentence_kor": "모델의 구문 표현을 조사하기 위해 몇 가지 시험 방법론이 개발되었다.",
                    "tag": "1"
                },
                {
                    "index": "117-2",
                    "sentence": "One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with different syntax.",
                    "sentence_kor": "구문 구조를 유도하는 모델의 능력을 결정하는 한 가지 일반적인 방법은 템플릿에 따라 생성된 문자열에 대한 모델을 훈련시킨 다음 이러한 문자열을 다른 구문을 가진 표면적으로 유사한 문자열과 구별할 수 있는 모델의 능력을 테스트한다.",
                    "tag": "1"
                },
                {
                    "index": "117-3",
                    "sentence": "We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.",
                    "sentence_kor": "우리는 두 가지 비통사적 기준 언어 모델인 n그램 모델과 스크램블된 입력에 대해 훈련된 LSTM 모델을 사용하여 최근 논문의 긍정적인 결과를 재현함으로써 이 접근 방식의 근본적인 문제를 설명한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "717",
            "abstractID": "SPA_abs-118",
            "text": [
                {
                    "index": "118-0",
                    "sentence": "Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling.",
                    "sentence_kor": "서스펜스는 독자들을 사로잡고 이야기를 설득력 있게 만드는 이야기 소설의 중요한 요소이다.",
                    "tag": "1"
                },
                {
                    "index": "118-1",
                    "sentence": "While there is a vast theoretical literature on suspense, it is computationally not well understood.",
                    "sentence_kor": "서스펜스에 관한 방대한 이론 문헌이 있지만, 계산상으로는 잘 이해되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "118-2",
                    "sentence": "We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is.",
                    "sentence_kor": "우리는 서스펜스를 모델링하기 위한 두 가지 방법을 비교한다. 즉, 지금까지의 이야기가 얼마나 예상치 못한지에 대한 예측적 척도인 놀라움과 이야기의 지속이 얼마나 예상치 못한지에 대한 불확실성 감소이다.",
                    "tag": "1"
                },
                {
                    "index": "118-3",
                    "sentence": "Both can be computed either directly over story representations or over their probability distributions.",
                    "sentence_kor": "둘 다 스토리 표현에 대해 직접 계산하거나 확률 분포에 대해 계산할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "118-4",
                    "sentence": "We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction.",
                    "sentence_kor": "우리는 스토리를 인코딩하고 놀라움과 불확실성 감소를 계산하는 계층적 언어 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "118-5",
                    "sentence": "Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy.",
                    "sentence_kor": "인간 서스펜스 판단이 주석이 달린 단편 소설과 비교하여, 표현에 대한 불확실성 감소가 인간에 가까운 정확성에 가까운 최고의 예측 변수라는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "118-6",
                    "sentence": "We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.",
                    "sentence_kor": "우리는 또한 불확실성 감소가 영화 시놉스에서 서스펜스 있는 사건을 예측하는 데 사용될 수 있다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "718",
            "abstractID": "SPA_abs-119",
            "text": [
                {
                    "index": "119-0",
                    "sentence": "Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time.",
                    "sentence_kor": "독해 시간을 예측하는 것은 다른 단어들이 독해 시간에 의해 측정되는 인간 처리에 얼마나 영향을 미치는가에 초점을 맞춘 훨씬 이전 연구의 주제였다.",
                    "tag": "1"
                },
                {
                    "index": "119-1",
                    "sentence": "However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word).",
                    "sentence_kor": "그러나 이전 연구에서는 제한된 수의 참가자뿐만 아니라 단어 수준만 예측(즉, 단어 하나를 읽는 시간 예측)을 다루었다.",
                    "tag": "1"
                },
                {
                    "index": "119-2",
                    "sentence": "We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics.",
                    "sentence_kor": "주제, 글꼴 특성 및 가독성 지표와 같은 추가 정보를 감안할 때 문서 수준 예측이 효과적인지 여부를 검토하여 이러한 작업을 확장하려고 한다.",
                    "tag": "2"
                },
                {
                    "index": "119-3",
                    "sentence": "We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants.",
                    "sentence_kor": "우리는 수천 명이 넘는 참가자들의 데이터를 읽고, 배포하고, 수집하는 데 걸리는 시간에 텍스트의 다른 특징이 어떻게 기여하는지 조사하기 위해 새로운 실험을 수행한다.",
                    "tag": "2+3"
                },
                {
                    "index": "119-4",
                    "sentence": "We then employ a large number of machine learning methods to predict a user’s reading time.",
                    "sentence_kor": "그런 다음 사용자의 독서 시간을 예측하기 위해 많은 기계 학습 방법을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "119-5",
                    "sentence": "We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.",
                    "sentence_kor": "우리는 신경망에 의해 단어 수준 읽기 시간이 가장 효과적으로 예측될 수 있다는 것을 보여주는 광범위한 연구에도 불구하고, 더 큰 규모의 텍스트는 단어 수라는 한 가지 요인에 의해 쉽고 가장 정확하게 예측할 수 있다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "719",
            "abstractID": "SPA_abs-120",
            "text": [
                {
                    "index": "120-0",
                    "sentence": "Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse.",
                    "sentence_kor": "자연어 이해(NLU)와 자연어 생성(NLG)은 상반된 목표를 가진 과제 지향 대화 시스템을 구축하는 두 가지 기본적이고 관련된 과제이다. NLU는 자연어에서 형식 표현으로의 변환을 다루는 반면 NLG는 그 반대이다.",
                    "tag": "1"
                },
                {
                    "index": "120-1",
                    "sentence": "A key to success in either task is parallel training data which is expensive to obtain at a large scale.",
                    "sentence_kor": "두 작업 모두 성공의 열쇠는 대규모로 얻기에는 비용이 많이 드는 병렬 훈련 데이터이다.",
                    "tag": "1"
                },
                {
                    "index": "120-2",
                    "sentence": "In this work, we propose a generative model which couples NLU and NLG through a shared latent variable.",
                    "sentence_kor": "본 연구에서는 공유 잠재 변수를 통해 NLU와 NLG를 결합하는 생성 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "120-3",
                    "sentence": "This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG.",
                    "sentence_kor": "이 접근 방식을 통해 자연어와 공식 표현의 공간을 모두 탐색할 수 있으며, 잠재 공간을 통해 정보 공유를 촉진하여 결국 NLU와 NLG에 이익을 준다.",
                    "tag": "2+3"
                },
                {
                    "index": "120-4",
                    "sentence": "Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations.",
                    "sentence_kor": "우리 모델은 플랫 및 트리 구조 형식 표현 모두를 사용하여 두 개의 대화 데이터 세트에서 최첨단 성능을 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "120-5",
                    "sentence": "We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.",
                    "sentence_kor": "또한 레이블이 지정되지 않은 데이터를 사용하여 모델을 준지도 방식으로 훈련하여 성능을 향상시킬 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "720",
            "abstractID": "SPA_abs-121",
            "text": [
                {
                    "index": "121-0",
                    "sentence": "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains.",
                    "sentence_kor": "많은 다중 도메인 신경 기계 변환(NMT) 모델은 하나의 인코더가 도메인 간 공유 임베딩을 학습하도록 하여 지식 전송을 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "121-1",
                    "sentence": "However, this design lacks adaptation to individual domains.",
                    "sentence_kor": "그러나 이 설계는 개별 도메인에 대한 적응이 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "121-2",
                    "sentence": "To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing.",
                    "sentence_kor": "이러한 한계를 극복하기 위해 각 도메인에 대해 개별 모듈을 사용하는 새로운 다중 도메인 NMT 모델을 제안하며, 이 모델에 워드 레벨, 적응 및 계층별 도메인 혼합을 적용한다.",
                    "tag": "1"
                },
                {
                    "index": "121-3",
                    "sentence": "We first observe that words in a sentence are often related to multiple domains.",
                    "sentence_kor": "우리는 먼저 문장의 단어들이 종종 여러 영역과 관련이 있다는 것을 관찰한다.",
                    "tag": "1"
                },
                {
                    "index": "121-4",
                    "sentence": "Hence, we assume each word has a domain proportion, which indicates its domain preference.",
                    "sentence_kor": "따라서 각 단어는 도메인 선호도를 나타내는 도메인 비율을 가지고 있다고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "121-5",
                    "sentence": "Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions.",
                    "sentence_kor": "그런 다음 단어 표현은 도메인 비율을 기반으로 개별 도메인에 임베딩을 혼합하여 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "121-6",
                    "sentence": "We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions.",
                    "sentence_kor": "서로 다른 도메인에 대한 다중 헤드 닷 제품 주의 모듈을 주의 깊게 설계하고, 결국 단어 수준 계층별 도메인 비율에 따라 매개 변수의 가중 평균을 취함으로써 이것이 달성될 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "121-7",
                    "sentence": "Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well.",
                    "sentence_kor": "이를 통해 효과적인 도메인 지식 공유를 달성하고 세분화된 도메인별 지식도 수집할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "121-8",
                    "sentence": "Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
                    "sentence_kor": "우리의 실험은 제안된 모델이 몇 가지 NMT 작업에서 기존 모델을 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "721",
            "abstractID": "SPA_abs-122",
            "text": [
                {
                    "index": "122-0",
                    "sentence": "To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog.",
                    "sentence_kor": "개방형 도메인 멀티턴 대화에서 정책 학습의 과제를 해결하기 위해 대화 상자 전환에 대한 사전 정보를 그래프로 표시하고 보다 일관되고 제어 가능한 대화를 촉진하기 위한 그래프 기반 대화 정책을 학습할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "122-1",
                    "sentence": "To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent “what to say” and “how to say”, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response.",
                    "sentence_kor": "이를 위해 먼저 대화 말뭉치에서 대화 그래프(CG)를 구성하는데, 여기에는 \"말할 내용\"과 \"말하는 방법\"을 나타내는 꼭지점과 메시지(대화문맥의 마지막 발화)와 메시지 응답 사이의 자연스러운 전환을 나타내는 가장자리가 있다.",
                    "tag": "3"
                },
                {
                    "index": "122-2",
                    "sentence": "We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation.",
                    "sentence_kor": "그런 다음 그래프 트래버설로 대화 흐름 계획을 수행하는 새로운 CG 기반 정책 학습 프레임워크를 제시한다. 그래프 트래버설은 각 차례 CG에서 무엇을 나타내는 것인지와 어떻게 사용하는지를 식별하여 응답 생성을 안내하는 방법을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "122-3",
                    "sentence": "In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy.",
                    "sentence_kor": "이러한 방식으로, 우리는 효과적으로 CG를 활용하여 정책 학습을 촉진한다. (1) 보다 효과적인 장기 보상 설계를 가능하게 하고, (2) 고품질 후보 활동을 제공하며, (3) 정책을 더 많이 제어할 수 있게 한다.",
                    "tag": "3"
                },
                {
                    "index": "122-4",
                    "sentence": "Results on two benchmark corpora demonstrate the effectiveness of this framework.",
                    "sentence_kor": "두 개의 벤치마크 코퍼라에 대한 결과는 이 프레임워크의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "722",
            "abstractID": "SPA_abs-123",
            "text": [
                {
                    "index": "123-0",
                    "sentence": "Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs.",
                    "sentence_kor": "추상적 의미 표현(AMR)은 광범위한 문장 수준 의미 그래프이다.",
                    "tag": "1"
                },
                {
                    "index": "123-1",
                    "sentence": "Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only.",
                    "sentence_kor": "AMR에서 텍스트 생성에 대한 기존 접근법은 AMR 주석이 달린 데이터에 대해서만 시퀀스 대 시퀀스 또는 그래프 대 시퀀스 모델을 교육하는 데 초점을 맞추었다.",
                    "tag": "1"
                },
                {
                    "index": "123-2",
                    "sentence": "In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring.",
                    "sentence_kor": "본 논문에서, 우리는 강력한 사전 교육 언어 모델과 주기 일관성 기반 재점수를 결합하는 대체 접근 방식을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "123-3",
                    "sentence": "Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures.",
                    "sentence_kor": "접근 방식의 단순함에도 불구하고, 우리의 실험 결과는 이러한 모델이 변압기 아키텍처의 최근 사용을 포함하여 영어 LDC2017T10 데이터 세트에서 이전의 모든 기술을 능가한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "123-4",
                    "sentence": "In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.",
                    "sentence_kor": "표준 평가 지표 외에도, 우리는 접근 방식의 강도를 더욱 입증하는 인간 평가 실험을 제공한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "723",
            "abstractID": "SPA_abs-124",
            "text": [
                {
                    "index": "124-0",
                    "sentence": "We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies.",
                    "sentence_kor": "우리는 수반되는 코드 본문의 변화에 기초하여 기존의 자연어 주석을 자동으로 업데이트하는 새로운 과제를 공식화한다.",
                    "tag": "1"
                },
                {
                    "index": "124-1",
                    "sentence": "We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.",
                    "sentence_kor": "우리는 소스 코드 수정을 반영하기 위해 기존 주석에 적용되는 일련의 편집을 생성하기 위해 두 개의 서로 다른 언어 표현에 걸쳐 변경 사항을 상호 연관시키는 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "124-2",
                    "sentence": "We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment.",
                    "sentence_kor": "우리는 오픈 소스 소프트웨어 프로젝트의 커밋 이력에서 수집한 데이터 세트를 사용하여 모델을 교육하고 평가한다. 각 예는 메서드에 대한 동시 업데이트와 해당 코멘트로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "124-3",
                    "sentence": "We compare our approach against multiple baselines using both automatic metrics and human evaluation.",
                    "sentence_kor": "우리는 자동 메트릭과 인간 평가를 모두 사용하여 여러 기준선과 우리의 접근 방식을 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "124-4",
                    "sentence": "Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.",
                    "sentence_kor": "결과는 이 작업의 당면 과제를 반영하며, 우리 모델은 편집과 관련하여 기준선을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "724",
            "abstractID": "SPA_abs-125",
            "text": [
                {
                    "index": "125-0",
                    "sentence": "Subword segmentation is widely used to address the open vocabulary problem in machine translation.",
                    "sentence_kor": "하위 단어 세분화는 기계 번역에서 열린 어휘 문제를 해결하기 위해 널리 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "125-1",
                    "sentence": "The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens.",
                    "sentence_kor": "하위 단어 분할에 대한 가장 일반적인 접근 방식은 BPE(Byte Pair Encoding)로, 가장 빈도가 높은 단어를 그대로 유지하면서 희귀한 단어를 여러 토큰으로 분할한다.",
                    "tag": "1"
                },
                {
                    "index": "125-2",
                    "sentence": "While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors.",
                    "sentence_kor": "동일한 어휘로도 복수의 세분화가 가능하지만, BPE는 단어를 고유한 시퀀스로 분할한다. 이는 모델이 단어의 구성성을 더 잘 학습하고 분할 오류에 강하지 못하게 할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "125-3",
                    "sentence": "So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018).",
                    "sentence_kor": "지금까지 이러한 BPE 불완전성, 결정론적 특성을 극복하는 유일한 방법은 또 다른 하위 단어 분할 알고리즘을 만드는 것이었다(Kudo, 2018).",
                    "tag": "1"
                },
                {
                    "index": "125-4",
                    "sentence": "In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word.",
                    "sentence_kor": "대조적으로, 우리는 BPE 자체가 동일한 단어의 다중 분할을 생성하는 기능을 통합한다는 것을 보여준다.",
                    "tag": "1+2"
                },
                {
                    "index": "125-5",
                    "sentence": "We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE.",
                    "sentence_kor": "기존 BPE에 기반하고 호환 가능한 간단하고 효과적인 하위 단어 정규화 방법인 BPE-dropout을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "125-6",
                    "sentence": "It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework.",
                    "sentence_kor": "이는 BPE의 분할 절차를 확률적으로 손상시켜 동일한 고정 BPE 프레임워크 내에서 여러 분할을 생성하게 한다.",
                    "tag": "3"
                },
                {
                    "index": "125-7",
                    "sentence": "Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.",
                    "sentence_kor": "훈련 중 BPE-dropout과 추론 중 표준 BPE를 사용하면 이전 하위 단어 정규화에 비해 번역 품질이 2.3 BLEU까지 향상되고 0.9 BLEU까지 향상된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "725",
            "abstractID": "SPA_abs-126",
            "text": [
                {
                    "index": "126-0",
                    "sentence": "Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront.",
                    "sentence_kor": "대부분의 범용 추출 요약 모델은 짧고 모든 중요한 정보를 미리 제시하는 뉴스 기사에 대해 훈련된다.",
                    "tag": "1"
                },
                {
                    "index": "126-1",
                    "sentence": "As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document.",
                    "sentence_kor": "결과적으로, 그러한 모델은 위치에 편향되어 있으며 종종 문서 시작 부분부터 현명한 문장 선택을 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "126-2",
                    "sentence": "When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient.",
                    "sentence_kor": "복잡한 구조를 가지고 정보를 단편적으로 제시하는 긴 내러티브를 요약할 때, 단순한 위치 휴리스틱으로는 충분하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "126-3",
                    "sentence": "In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models.",
                    "sentence_kor": "본 논문에서, 우리는 내러티브의 기본 구조를 감독되지 않고 감독되는 일반적인 추출 요약 모델에 명시적으로 통합할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "126-4",
                    "sentence": "We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes).",
                    "sentence_kor": "우리는 주요 서술 사건(터닝 포인트)의 관점에서 서술 구조를 공식화하고 시나리오를 요약하기 위해 잠재된 것으로 취급한다(즉, 최적의 장면 시퀀스를 추출한다).",
                    "tag": "3"
                },
                {
                    "index": "126-5",
                    "sentence": "Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.",
                    "sentence_kor": "장면 수준 요약 라벨로 보강한 TV 시나리오의 CSI 말뭉치에 대한 실험 결과는 잠재 전환점이 CSI 에피소드의 중요한 측면과 상관관계가 있으며 일반 추출 알고리즘에 비해 요약 성능을 향상시켜 더 완전하고 다양한 요약을 이끌어 낸다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "726",
            "abstractID": "SPA_abs-127",
            "text": [
                {
                    "index": "127-0",
                    "sentence": "In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls.",
                    "sentence_kor": "최근 몇 년 동안 치매환자가 도출한 음성 샘플과 건강한 대조군 샘플을 구별하기 위한 계산 방법의 사용에 대한 관심이 급증하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "127-1",
                    "sentence": "The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.",
                    "sentence_kor": "두 개의 신경 언어 모델(LM)의 난해도 추정치(건강한 참가자가 생성하는 음성 기록물과 치매 환자 기록물에 대해 학습한 것)의 차이는 보이지 않는 기록물의 진단 분류를 위한 단일 기능으로 최첨단 성능을 산출하는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "127-2",
                    "sentence": "However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables.",
                    "sentence_kor": "그러나 이 접근법이 효과적인 이유에 대해서는 거의 알려져 있지 않으며, 가장 널리 사용되는 평가서 세트(DementiaBank)의 사례/제어 일치가 없기 때문에 이러한 접근법이 진정 진단적인지 또는 다른 변수에 민감한지는 불확실하다.",
                    "tag": "1"
                },
                {
                    "index": "127-3",
                    "sentence": "In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.",
                    "sentence_kor": "본 논문에서, 우리는 어휘 빈도를 조작하여 진행적 의미 치매를 시뮬레이션하기 위해 이전에 개발된 합성 내러티브를 사용하여 치매가 있거나 없는 참가자에 대해 훈련된 신경 LM을 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "127-4",
                    "sentence": "We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.",
                    "sentence_kor": "신경 LM의 난해성은 어휘적 빈도와 강력하고 차등적으로 연관되어 있으며, 보간 제어와 치매 LM의 결과로 발생하는 혼합 모델을 사용하는 것이 필사본 텍스트 전용으로 훈련된 모델의 현재 최첨단 모델에 따라 개선된다는 것을 발견했다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "727",
            "abstractID": "SPA_abs-128",
            "text": [
                {
                    "index": "128-0",
                    "sentence": "Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear.",
                    "sentence_kor": "최근 깊은 자연어 이해(NLU) 모델이 체계성을 보이는지에 대한 질문에 많은 관심이 있어 왔는데, 단어와 같은 단위가 문장의 의미에 일관되게 기여하도록 일반화했다.",
                    "tag": "1"
                },
                {
                    "index": "128-1",
                    "sentence": "There is accumulating evidence that neural models do not learn systematically.",
                    "sentence_kor": "신경 모델이 체계적으로 배우지 못한다는 증거가 축적되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "128-2",
                    "sentence": "We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour.",
                    "sentence_kor": "우리는 일련의 탐색 작업과 체계적인 행동을 측정하기 위한 메트릭 세트를 정의하면서 언어적 관점에서 체계성의 개념을 검토한다.",
                    "tag": "1"
                },
                {
                    "index": "128-3",
                    "sentence": "We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying.",
                    "sentence_kor": "우리는 또한 네트워크 아키텍처가 비체계적으로 일반화할 수 있는 방법을 식별하고 그러한 형태의 일반화가 만족스럽지 못할 수 있는 이유를 논의한다.",
                    "tag": "2+3"
                },
                {
                    "index": "128-4",
                    "sentence": "As a case study, we perform a series of experiments in the setting of natural language inference (NLI).",
                    "sentence_kor": "사례 연구로, 우리는 자연어 추론(NLI) 설정에서 일련의 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "128-5",
                    "sentence": "We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.",
                    "sentence_kor": "우리는 전반적인 고성능에도 불구하고 현재의 최첨단 NLU 시스템이 체계적으로 일반화되지 않는다는 증거를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "728",
            "abstractID": "SPA_abs-129",
            "text": [
                {
                    "index": "129-0",
                    "sentence": "Human conversations naturally evolve around related concepts and hop to distant concepts.",
                    "sentence_kor": "인간의 대화는 자연스럽게 관련 개념을 중심으로 발전하고 멀리 떨어진 개념으로 이동합니다.",
                    "tag": "1"
                },
                {
                    "index": "129-1",
                    "sentence": "This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows.",
                    "sentence_kor": "본 논문은 상식적인 지식 그래프를 활용하여 대화 흐름을 명시적으로 모델링하는 새로운 대화 생성 모델인 ConceptFlow를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "129-2",
                    "sentence": "By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations.",
                    "sentence_kor": "ConceptFlow는 대화를 개념 공간에 기초함으로써 상식적인 관계를 따라 개념 공간에서 횡단할 때 잠재적인 대화 흐름을 나타냅니다.",
                    "tag": "1"
                },
                {
                    "index": "129-3",
                    "sentence": "The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses.",
                    "sentence_kor": "트래버스는 개념 그래프의 그래프 어텐션으로 안내되며, 보다 의미적이고 유익한 응답을 생성하기 위해 개념 공간에서 보다 의미 있는 방향으로 이동한다.",
                    "tag": "3"
                },
                {
                    "index": "129-4",
                    "sentence": "Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures.",
                    "sentence_kor": "Reddit 대화에 대한 실험은 이전 지식 인식 대화 모델과 GPT-2 기반 모델에 비해 70% 적은 매개 변수를 사용하면서 ConceptFlow의 효과를 입증하여 명시적 모델링 대화 구조의 이점을 입증한다.",
                    "tag": "4+5"
                },
                {
                    "index": "129-5",
                    "sentence": "All source codes of this work are available at https://github.com/thunlp/ConceptFlow.",
                    "sentence_kor": "본 연구의 모든 소스 코드는 https://github.com/thunlp/ConceptFlow에서 확인할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "729",
            "abstractID": "SPA_abs-130",
            "text": [
                {
                    "index": "130-0",
                    "sentence": "The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST).",
                    "sentence_kor": "작업 지향 대화 상자 시스템의 NLU(Natural Language Under) 구성요소는 사용자의 요청을 처리하고 이를 DST(Dialog State Tracker)와 같은 다운스트림 구성요소에서 사용할 수 있는 구조화된 정보로 변환합니다.",
                    "tag": "1"
                },
                {
                    "index": "130-1",
                    "sentence": "This information is typically represented as a semantic frame that captures the and slot-labels provided by the user.",
                    "sentence_kor": "이 정보는 일반적으로 사용자가 제공하는 및 슬롯 레이블을 캡처하는 의미 프레임으로 표시됩니다.",
                    "tag": "1"
                },
                {
                    "index": "130-2",
                    "sentence": "We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains.",
                    "sentence_kor": "우리는 먼저 이러한 얕은 표현이 많은 도메인에 내재된 재귀적 특성을 포착하지 못하기 때문에 복잡한 대화 상자 시나리오에는 불충분하다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "130-3",
                    "sentence": "We propose a recursive, hierarchical frame-based representation and show how to learn it from data.",
                    "sentence_kor": "우리는 재귀적이고 계층적인 프레임 기반 표현을 제안하고 데이터에서 배우는 방법을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "130-4",
                    "sentence": "We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template.",
                    "sentence_kor": "프레임 생성 작업을 템플릿 기반 트리 디코딩 작업으로 공식화하며, 여기서 디코더는 템플릿을 반복적으로 생성한 다음 슬롯 값을 템플릿에 채운다.",
                    "tag": "3"
                },
                {
                    "index": "130-5",
                    "sentence": "We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end.",
                    "sentence_kor": "로컬 트리 기반 손실 기능을 글로벌 감독을 제공하는 용어로 확장하고 엔드 투 엔드 최적화 방법을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "130-6",
                    "sentence": "We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.",
                    "sentence_kor": "우리는 널리 사용되는 ATIS 데이터 세트에서 약간의 개선을 달성하고 여기서 설명하는 더 복잡한 데이터 세트에서 훨씬 더 큰 개선을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "730",
            "abstractID": "SPA_abs-131",
            "text": [
                {
                    "index": "131-0",
                    "sentence": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications.",
                    "sentence_kor": "자연어 처리(NLP) 애플리케이션에서 관심 있는 출력 실체에 대한 예측 신뢰도 보정 문제를 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "131-1",
                    "sentence": "It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare.",
                    "sentence_kor": "명명된 개체 인식 및 질문 답변과 같은 NLP 애플리케이션은 특히 애플리케이션이 의료와 같은 안전에 중요한 영역에 배치되는 경우 예측에 대해 보정된 신뢰 점수를 생성하는 것이 중요하다.",
                    "tag": "1+2"
                },
                {
                    "index": "131-2",
                    "sentence": "However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods.",
                    "sentence_kor": "그러나 이러한 구조화된 예측 모델의 출력 공간은 이진 또는 다중 클래스 보정 방법을 직접 적용하기에는 너무 큰 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "131-3",
                    "sentence": "In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models.",
                    "sentence_kor": "본 연구에서, 우리는 신경망 기반 구조화된 예측 모델에서 관심 있는 출력 실체에 대한 일반적인 보정 체계를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "131-4",
                    "sentence": "Our proposed method can be used with any binary class calibration scheme and a neural network model.",
                    "sentence_kor": "우리가 제안한 방법은 이진 클래스 보정 체계 및 신경망 모델과 함께 사용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "131-5",
                    "sentence": "Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements.",
                    "sentence_kor": "또한, 교정 방법이 추가 교육 비용이나 데이터 요구사항 없이 기본 모델의 성능을 향상시키기 위해 불확실성을 인식하는 엔티티별 디코딩 단계로 사용될 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "131-6",
                    "sentence": "We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems.",
                    "sentence_kor": "우리는 우리의 방법이 명명된 개체 인식, 음성 부분 태그 및 질문 응답 시스템에 대한 현재의 교정 기법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "131-7",
                    "sentence": "We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets.",
                    "sentence_kor": "또한 여러 작업 및 벤치마크 데이터 세트에 걸친 디코딩 단계에서 모델 성능이 향상되는 것을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "131-8",
                    "sentence": "Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
                    "sentence_kor": "우리의 방법은 도메인 밖 테스트 시나리오에서도 교정 및 모델 성능을 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "731",
            "abstractID": "SPA_abs-132",
            "text": [
                {
                    "index": "132-0",
                    "sentence": "Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies.",
                    "sentence_kor": "모방 학습 알고리즘은 최적에 가까운 검색 정책을 학습하여 많은 구조화된 예측 작업에 대한 최첨단 결과를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "132-1",
                    "sentence": "Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical.",
                    "sentence_kor": "이러한 알고리즘은 쿼리된 상태에서 최적의 조치를 제공할 수 있는 전문가에 대한 교육 시간 액세스를 가정한다. 불행히도 이러한 쿼리의 수는 종종 금지되어 이러한 접근 방식을 비현실적으로 만든다.",
                    "tag": "1"
                },
                {
                    "index": "132-2",
                    "sentence": "To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance.",
                    "sentence_kor": "이러한 쿼리 복잡성을 해결하기 위해 학습 알고리즘이 노이즈가 많은 지침을 제공하는 훨씬 저렴한 노이즈 휴리스틱에 추가로 액세스할 수 있는 능동적 학습 설정을 고려한다.",
                    "tag": "2+3"
                },
                {
                    "index": "132-3",
                    "sentence": "Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary.",
                    "sentence_kor": "우리의 알고리즘인 LEAQI는 전문가가 휴리스틱에 동의하지 않을 가능성이 있는 시기를 예측하는 차이 분류기를 학습하고 필요할 때만 전문가에게 질의한다.",
                    "tag": "3"
                },
                {
                    "index": "132-4",
                    "sentence": "We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.",
                    "sentence_kor": "우리는 세 가지 시퀀스 라벨링 작업에 LEAQI를 적용하여 전문가에게 훨씬 적은 질의와 수동적 접근법에 대한 유사(또는 더 나은) 정확도를 입증한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "732",
            "abstractID": "SPA_abs-133",
            "text": [
                {
                    "index": "133-0",
                    "sentence": "Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text.",
                    "sentence_kor": "우리가 결혼한 커플들이 텍스트에서 배우자 쌍을 추출하는 작업을 위해 일반적으로 허니문(honeymoon)에 가는 귀납적 편견을 지정하기를 원한다고 가정해보자.",
                    "tag": "1"
                },
                {
                    "index": "133-1",
                    "sentence": "In this paper, we allow model developers to specify these types of inductive biases as natural language explanations.",
                    "sentence_kor": "본 논문에서 우리는 모델 개발자들이 이러한 유형의 귀납적 편견을 자연어 설명으로 지정할 수 있도록 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "133-2",
                    "sentence": "We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input.",
                    "sentence_kor": "우리는 MultiNLI에서 미세 조정된 BERT를 사용하여 입력 문장과 관련하여 이러한 설명을 \"해석\"하여 입력에 대한 설명 안내 표현을 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "133-3",
                    "sentence": "Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.",
                    "sentence_kor": "세 가지 관계 추출 작업에서, 우리의 방법 ExpBERT는 BERT 기준선과 일치하지만 라벨링 데이터가 3-20배 적으며 베이스라인에서 라벨링 데이터의 양이 동일한 3-10 F1 포인트만큼 개선된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "733",
            "abstractID": "SPA_abs-134",
            "text": [
                {
                    "index": "134-0",
                    "sentence": "Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks.",
                    "sentence_kor": "BERT와 같은 최근의 트랜스포머 기반 아키텍처는 많은 자연어 처리 작업에서 인상적인 결과를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "134-1",
                    "sentence": "However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples.",
                    "sentence_kor": "그러나 채택된 벤치마크의 대부분은 수천 개의 예(때로는 수십 개)로 이루어져 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "134-2",
                    "sentence": "In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected.",
                    "sentence_kor": "많은 실제 시나리오에서 고품질 주석이 달린 데이터를 얻는 것은 비용이 많이 들고 시간이 많이 걸린다. 반대로, 대상 작업을 특징짓는 라벨이 부착되지 않은 예는 일반적으로 쉽게 수집될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "134-3",
                    "sentence": "One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks.",
                    "sentence_kor": "준지도 생성적 적대 네트워크를 기반으로 이미지 처리에서 준지도 학습을 가능하게 하는 유망한 방법이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "134-4",
                    "sentence": "In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting.",
                    "sentence_kor": "본 논문에서 우리는 GAN-BERT가 생성적 적대 설정에서 라벨이 없는 데이터로 BERT 유사 아키텍처를 미세 조정하는 경향이 있음을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "134-5",
                    "sentence": "Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.",
                    "sentence_kor": "실험 결과에 따르면 주석이 달린 예제에 대한 요구 사항을 대폭 줄일 수 있으며(주석된 예제는 최대 50-100개만 포함) 여러 문장 분류 작업에서 여전히 우수한 성능을 얻을 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "734",
            "abstractID": "SPA_abs-135",
            "text": [
                {
                    "index": "135-0",
                    "sentence": "Sequence labeling is a fundamental task for a range of natural language processing problems.",
                    "sentence_kor": "시퀀스 라벨링은 다양한 자연어 처리 문제에 대한 기본적인 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "135-1",
                    "sentence": "When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly.",
                    "sentence_kor": "실제로 사용할 때 성능은 주석 품질과 양에 의해 크게 영향을 받으며, 반면에 실측 자료 레이블을 얻는 것은 종종 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "135-2",
                    "sentence": "In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible.",
                    "sentence_kor": "많은 경우 실측 자료 라벨은 존재하지 않지만 잡음이 많은 주석이나 다른 도메인의 주석에 액세스할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "135-3",
                    "sentence": "In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data).",
                    "sentence_kor": "본 논문에서 우리는 여러 소스(예: 크라우드 주석, 교차 도메인 데이터)의 주석에 대해 훈련할 수 있는 새로운 프레임워크 컨센서스 네트워크(ConNet)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "135-4",
                    "sentence": "It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module.",
                    "sentence_kor": "모든 소스에 대한 개별 표현을 학습하고 상황 인식 주의 모듈을 통해 소스별 지식을 동적으로 집계합니다.",
                    "tag": "3"
                },
                {
                    "index": "135-5",
                    "sentence": "Finally, it leads to a model reflecting the agreement (consensus) among multiple sources.",
                    "sentence_kor": "마지막으로, 여러 출처 간의 합의(합의)를 반영하는 모델로 이어진다.",
                    "tag": "3"
                },
                {
                    "index": "135-6",
                    "sentence": "We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation.",
                    "sentence_kor": "제안된 프레임워크를 군중 주석을 사용한 학습과 감독되지 않은 교차 도메인 모델 적응이라는 두 가지 다중 소스 학습의 실제 설정에서 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "135-7",
                    "sentence": "Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.",
                    "sentence_kor": "광범위한 실험 결과는 우리 모델이 두 설정에서 기존 방법에 비해 크게 개선되었음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "135-8",
                    "sentence": "We also demonstrate that the method can apply to various tasks and cope with different encoders.",
                    "sentence_kor": "우리는 또한 이 방법이 다양한 작업에 적용되고 다른 인코더에 대처할 수 있음을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "735",
            "abstractID": "SPA_abs-136",
            "text": [
                {
                    "index": "136-0",
                    "sentence": "This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix.",
                    "sentence_kor": "본 논문은 텍스트 분류를 위한 준지도 학습 방법인 믹스텍스트(MixText)를 제시하며, 이 방법은 새롭게 설계된 TMIX라는 데이터 확대 방법을 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "136-1",
                    "sentence": "TMix creates a large amount of augmented training samples by interpolating text in hidden space.",
                    "sentence_kor": "TMix는 숨겨진 공간에 텍스트를 보간하여 대량의 증강 교육 샘플을 만듭니다.",
                    "tag": "3"
                },
                {
                    "index": "136-2",
                    "sentence": "Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.",
                    "sentence_kor": "또한, 우리는 라벨이 부착되지 않은 데이터에 대한 낮은 엔트로피 레이블을 추측하기 위해 데이터 증대의 최근 발전을 활용하여 라벨이 부착되지 않은 데이터만큼 사용이 용이하다.",
                    "tag": "3"
                },
                {
                    "index": "136-3",
                    "sentence": "By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks.",
                    "sentence_kor": "라벨링, 라벨링되지 않은 데이터와 증강된 데이터를 혼합함으로써 믹스텍스트는 몇 가지 텍스트 분류 벤치마크에서 기존의 사전 교육 및 미세 조정 모델 및 기타 최첨단 준지도 학습 방법을 크게 능가했다.",
                    "tag": "4"
                },
                {
                    "index": "136-4",
                    "sentence": "The improvement is especially prominent when supervision is extremely limited.",
                    "sentence_kor": "그 개선은 감독이 극도로 제한적일 때 특히 두드러진다.",
                    "tag": "4"
                },
                {
                    "index": "136-5",
                    "sentence": "We have publicly released our code at https://github.com/GT-SALT/MixText.",
                    "sentence_kor": "우리는 https://github.com/GT-SALT/MixText에서 코드를 공개했습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "736",
            "abstractID": "SPA_abs-137",
            "text": [
                {
                    "index": "137-0",
                    "sentence": "Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters.",
                    "sentence_kor": "NLP(Natural Language Processing)는 최근 수억 개의 매개 변수를 가진 대규모 사전 교육 모델을 사용함으로써 큰 성공을 거두고 있다.",
                    "tag": "1"
                },
                {
                    "index": "137-1",
                    "sentence": "However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices.",
                    "sentence_kor": "그러나 이러한 모델은 리소스가 제한된 모바일 장치에 배포할 수 없을 정도로 모델 크기가 크고 대기 시간이 길다.",
                    "tag": "1"
                },
                {
                    "index": "137-2",
                    "sentence": "In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.",
                    "sentence_kor": "본 논문에서 우리는 MobileB를 제안한다.널리 사용되는 BERT 모델을 압축하고 가속화하기 위한 ERT입니다.",
                    "tag": "2+3"
                },
                {
                    "index": "137-3",
                    "sentence": "Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.",
                    "sentence_kor": "원래의 BERT, MobileB와 마찬가지로ERT는 작업에 무관하다. 즉, 간단한 미세 조정을 통해 다양한 다운스트림 NLP 작업에 일반적으로 적용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "137-4",
                    "sentence": "Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.",
                    "sentence_kor": "기본적으로 MobileBERT는 BERT_LARGE의 얇은 버전이며 병목 구조와 자체 애착과 피드 포워드 네트워크 간의 신중하게 설계된 균형을 갖추고 있다.",
                    "tag": "3"
                },
                {
                    "index": "137-5",
                    "sentence": "To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model.",
                    "sentence_kor": "MobileB를 교육하려면ERT, 우리는 먼저 BERT_LARGE 모델을 통합한 역병목 모델인 특수 설계된 교사 모델을 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "137-6",
                    "sentence": "Then, we conduct knowledge transfer from this teacher to MobileBERT.",
                    "sentence_kor": "그리고 나서, 우리는 이 선생님으로부터 MobileB로 지식을 이전한다.ERT.",
                    "tag": "3"
                },
                {
                    "index": "137-7",
                    "sentence": "Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks.",
                    "sentence_kor": "경험적 연구에 따르면 MobileB는ERT는 BERT_BASE보다 4.3배 작고 5.5배 빠르며 잘 알려진 벤치마크에서 경쟁력 있는 결과를 얻을 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "137-8",
                    "sentence": "On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone.",
                    "sentence_kor": "GLUE, MobileB의 자연어 추론 과제ERT는 픽셀 4 전화기에서 GLUE 점수 77.7점(BERT_BASE보다 0.6점 낮음)과 지연 시간 62ms를 달성합니다.",
                    "tag": "4"
                },
                {
                    "index": "137-9",
                    "sentence": "On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).",
                    "sentence_kor": "SQuAD v1.1/v2.0 질문 응답 작업, MobileBERT는 90.0/79.2(BERT_BASE보다 1.5/2.1 높은)의 dev F1 점수를 획득합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "737",
            "abstractID": "SPA_abs-138",
            "text": [
                {
                    "index": "138-0",
                    "sentence": "Transfer learning has fundamentally changed the landscape of natural language processing (NLP).",
                    "sentence_kor": "전송 학습은 자연어 처리(NLP)의 환경을 근본적으로 변화시켰다.",
                    "tag": "1"
                },
                {
                    "index": "138-1",
                    "sentence": "Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.",
                    "sentence_kor": "많은 최첨단 모델은 처음에 대규모 텍스트 말뭉치에서 사전 교육된 다음 다운스트림 작업에서 미세 조정된다.",
                    "tag": "1"
                },
                {
                    "index": "138-2",
                    "sentence": "However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data.",
                    "sentence_kor": "그러나 다운스트림 작업의 제한된 데이터 리소스와 사전 교육 모델의 극도로 높은 복잡성 때문에 미세 조정으로 인해 미세 조정된 모델이 다운스트림 작업의 교육 데이터를 과도하게 적합시키고 보이지 않는 데이터로 일반화하지 못하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "138-3",
                    "sentence": "To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance.",
                    "sentence_kor": "이러한 문제를 원칙적인 방식으로 해결하기 위해 사전 훈련된 모델의 강력하고 효율적인 미세 조정을 위한 새로운 학습 프레임워크를 제안하여 더 나은 일반화 성능을 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "138-4",
                    "sentence": "The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating.",
                    "sentence_kor": "제안된 프레임워크는 두 가지 중요한 구성 요소인 1. 모델의 복잡성을 효과적으로 관리하는 평활도 유도 정규화, 2. 신뢰 영역 방법의 인스턴스이며 공격적인 업데이트를 방지할 수 있는 Bregman 근위점 최적화.",
                    "tag": "1"
                },
                {
                    "index": "138-5",
                    "sentence": "Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI.",
                    "sentence_kor": "우리의 실험은 제안된 프레임워크가 GLUE, SNLI, SciTail 및 ANLI를 포함한 여러 NLP 작업에서 새로운 최첨단 성능을 달성한다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "138-6",
                    "sentence": "Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.",
                    "sentence_kor": "또한 GLUE에서 110억 개의 매개 변수를 포함하는 가장 큰 사전 교육 모델인 최첨단 T5 모델을 능가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "738",
            "abstractID": "SPA_abs-139",
            "text": [
                {
                    "index": "139-0",
                    "sentence": "Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications.",
                    "sentence_kor": "분류 체계 구성이라고도 알려진 그래프 구조 분류학으로 어휘 의미론적 관계를 추출하는 것은 다양한 NLP 애플리케이션에서 유익했다.",
                    "tag": "1"
                },
                {
                    "index": "139-1",
                    "sentence": "Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks.",
                    "sentence_kor": "최근 GNN(Graph Neural Network)은 많은 작업을 성공적으로 처리하는 데 강력한 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "139-2",
                    "sentence": "However, there has been no attempt to exploit GNN to create taxonomies.",
                    "sentence_kor": "그러나 분류법을 만들기 위해 GNN을 이용하려는 시도는 없었다.",
                    "tag": "1"
                },
                {
                    "index": "139-3",
                    "sentence": "In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task.",
                    "sentence_kor": "본 논문에서, 우리는 분류법 구성 작업을 위한 GNN 기반 교차 도메인 전송 프레임워크인 Graph2Taxo를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "139-4",
                    "sentence": "Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain.",
                    "sentence_kor": "우리의 주된 기여는 기존 도메인에서 분류 체계 구성의 잠재적 특징을 학습하여 보이지 않는 도메인의 구조 학습을 안내하는 것이다.",
                    "tag": "5"
                },
                {
                    "index": "139-5",
                    "sentence": "We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction.",
                    "sentence_kor": "또한 분류 체계 구성을 위한 DAG(Directed 비순환 그래프) 생성의 새로운 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "139-6",
                    "sentence": "Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training.",
                    "sentence_kor": "구체적으로, 우리가 제안한 Graph2Taxo는 자동으로 추출된 저명칭 하이퍼넴 후보 쌍으로 구성된 노이즈 그래프와 훈련을 위해 알려진 일부 도메인에 대한 분류법 세트를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "139-7",
                    "sentence": "The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain.",
                    "sentence_kor": "그런 다음 학습된 모델을 사용하여 해당 도메인에 대한 용어 집합이 지정된 새로운 알 수 없는 도메인에 대한 분류법을 생성합니다.",
                    "tag": "3"
                },
                {
                    "index": "139-8",
                    "sentence": "Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.",
                    "sentence_kor": "과학과 환경 영역의 벤치마크 데이터 세트에 대한 실험은 우리의 접근 방식이 최첨단 기술에 비해 그에 상응하는 상당한 개선을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "739",
            "abstractID": "SPA_abs-140",
            "text": [
                {
                    "index": "140-0",
                    "sentence": "Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI).",
                    "sentence_kor": "교차 언어 단어 임베딩(CLWE)은 종종 이중 언어 어휘 유도(BLI)에서 평가된다.",
                    "tag": "1"
                },
                {
                    "index": "140-1",
                    "sentence": "Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI.",
                    "sentence_kor": "최근의 CLWE 방법은 BLI에서 일반화하기 위해 훈련 사전에 적합한 선형 투영을 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "140-2",
                    "sentence": "However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary.",
                    "sentence_kor": "그러나 적합도가 낮으면 훈련 사전의 단어에 의존하는 다른 다운스트림 작업에 대한 일반화를 방해할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "140-3",
                    "sentence": "We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary.",
                    "sentence_kor": "CLWE를 훈련 사전에 맞게 개조하여 이러한 한계를 해결하였다. 훈련 번역 쌍을 임베딩 공간에 더 가깝게 끌어당기고 훈련 사전에 과도하게 적합시킨다.",
                    "tag": "1"
                },
                {
                    "index": "140-4",
                    "sentence": "This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy.",
                    "sentence_kor": "이 간단한 후처리 단계는 BLI 테스트 정확도는 낮지만 두 개의 다운스트림 작업에서 정확도를 향상시키는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "140-5",
                    "sentence": "We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks.",
                    "sentence_kor": "또한 CLWE에서 유도된 훈련 사전과 합성 사전으로 모두 개조하여 다운스트림 작업에서 훨씬 더 잘 일반화하기도 한다.",
                    "tag": "1"
                },
                {
                    "index": "140-6",
                    "sentence": "Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.",
                    "sentence_kor": "우리의 결과는 다운스트림 작업에서 훈련 사전을 완전히 활용하는 것의 중요성을 확인하고 BLI가 결함이 있는 CLWE 평가인 이유를 설명한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "740",
            "abstractID": "SPA_abs-141",
            "text": [
                {
                    "index": "141-0",
                    "sentence": "Authorship attribution aims to identify the author of a text based on the stylometric analysis.",
                    "sentence_kor": "작성자 속성은 양식 분석에 기초하여 텍스트의 작성자를 식별하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "141-1",
                    "sentence": "Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style.",
                    "sentence_kor": "반면, 저작 난독화는 텍스트의 스타일을 수정함으로써 저작자의 귀속으로부터 보호하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "141-2",
                    "sentence": "In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model.",
                    "sentence_kor": "본 논문에서는 적대적 위협 모델에서 최첨단 저자 난독화 방법의 은밀성을 평가한다.",
                    "tag": "2"
                },
                {
                    "index": "141-3",
                    "sentence": "An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the adversary interested in authorship attribution.",
                    "sentence_kor": "난독화자는 상대방이 난독화에 의해 수정된 텍스트가 난독화되는지 여부를 탐지하기 어려울 정도로 은밀하다. 이는 저자 속성에 관심이 있는 상대에게 중요한 결정이다.",
                    "tag": "1"
                },
                {
                    "index": "141-4",
                    "sentence": "We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87.",
                    "sentence_kor": "기존의 작성 난독화 방법은 난독화 텍스트를 평균 F1 점수 0.87로 식별할 수 있기 때문에 은밀하지 않음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "141-5",
                    "sentence": "The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner.",
                    "sentence_kor": "은밀성이 없는 이유는 이러한 난독화기가 신경 언어 모델에 의해 확인된 것처럼 텍스트의 부드러움을 탐지 가능한 방식으로 저하시키기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "141-6",
                    "sentence": "Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.",
                    "sentence_kor": "우리의 결과는 익명성을 추구하는 작가의 정체성을 더 잘 보호할 수 있는 은밀한 저자 난독화 방법을 개발할 필요성을 강조한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "741",
            "abstractID": "SPA_abs-142",
            "text": [
                {
                    "index": "142-0",
                    "sentence": "Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications.",
                    "sentence_kor": "BERT와 같은 대규모 사전 교육 언어 모델은 NLP 애플리케이션에 상당한 개선을 가져왔다.",
                    "tag": "1"
                },
                {
                    "index": "142-1",
                    "sentence": "However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications.",
                    "sentence_kor": "그러나 실시간 애플리케이션에 배포하기 어려운 추론이 느린 것으로도 악명이 높다.",
                    "tag": "1"
                },
                {
                    "index": "142-2",
                    "sentence": "We propose a simple but effective method, DeeBERT, to accelerate BERT inference.",
                    "sentence_kor": "BERT 추론을 가속화하기 위해 간단하지만 효과적인 방법인 DeeBERT를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "142-3",
                    "sentence": "Our approach allows samples to exit earlier without passing through the entire model.",
                    "sentence_kor": "우리의 접근 방식은 샘플이 전체 모델을 통과하지 않고 더 일찍 종료되도록 한다.",
                    "tag": "3"
                },
                {
                    "index": "142-4",
                    "sentence": "Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality.",
                    "sentence_kor": "실험에 따르면 DeeBERT는 모델 품질 저하를 최소화하면서 추론 시간을 최대 40%까지 절약할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "142-5",
                    "sentence": "Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.",
                    "sentence_kor": "추가 분석에서는 BERT 변압기 계층에서 서로 다른 동작을 보여주고 중복성도 드러낸다.",
                    "tag": "4"
                },
                {
                    "index": "142-6",
                    "sentence": "Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks.",
                    "sentence_kor": "우리의 작업은 다운스트림 작업에 딥 트랜스포머 기반 모델을 효율적으로 적용하기 위한 새로운 아이디어를 제공한다.",
                    "tag": "4+5"
                },
                {
                    "index": "142-7",
                    "sentence": "Code is available at https://github.com/castorini/DeeBERT.",
                    "sentence_kor": "코드는 https://github.com/castorini/DeeBERT에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "742",
            "abstractID": "SPA_abs-143",
            "text": [
                {
                    "index": "143-0",
                    "sentence": "In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy.",
                    "sentence_kor": "계층적 텍스트 분류에서, 우리는 주어진 클래스 분류법의 위에서 아래로 문서의 범주를 예측하기 위해 일련의 추론 단계를 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "143-1",
                    "sentence": "Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model.",
                    "sentence_kor": "대부분의 연구는 계층 구조를 다루기 위한 새로운 신경망 아키텍처를 개발하는 데 초점을 맞추었지만, 우리는 기준 모델을 강화하는 효율적인 방법을 찾는 것을 선호한다.",
                    "tag": "1"
                },
                {
                    "index": "143-2",
                    "sentence": "We first define the task as a sequence-to-sequence problem.",
                    "sentence_kor": "우리는 먼저 작업을 시퀀스 투 시퀀스 문제로 정의한다.",
                    "tag": "1"
                },
                {
                    "index": "143-3",
                    "sentence": "Afterwards, we propose an auxiliary synthetic task of bottom-up-classification.",
                    "sentence_kor": "그 후 상향식 분류라는 보조 합성 작업을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "143-4",
                    "sentence": "Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space.",
                    "sentence_kor": "그런 다음 외부 사전에서 모든 계층 계층의 클래스에 대한 텍스트 정의를 검색하여 단어 벡터 공간에 매핑합니다.",
                    "tag": "3"
                },
                {
                    "index": "143-5",
                    "sentence": "We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search.",
                    "sentence_kor": "클래스 정의 임베딩을 추가 입력으로 사용하여 다음 레이어의 예측을 조건화하고 적응된 빔 검색을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "143-6",
                    "sentence": "Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy.",
                    "sentence_kor": "수정된 검색은 큰 이득을 제공하지 않았지만, 보조 작업과 클래스 정의의 추가 입력의 조합은 분류 정확도를 크게 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "143-7",
                    "sentence": "With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.",
                    "sentence_kor": "효율적인 접근 방식을 통해, 우리는 두 개의 잘 알려진 영어 데이터 세트에서 급격히 줄어든 매개 변수 수를 사용하여 이전 연구를 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "743",
            "abstractID": "SPA_abs-144",
            "text": [
                {
                    "index": "144-0",
                    "sentence": "We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts.",
                    "sentence_kor": "우리는 자동 음성 인식 기록의 텍스트 특징을 기반으로 자발적 발화의 언어 능력을 자동으로 채점하는 과제를 다룬다.",
                    "tag": "1"
                },
                {
                    "index": "144-1",
                    "sentence": "Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).",
                    "sentence_kor": "최근 멀티태스킹 학습의 발전에 힘입어, 우리는 주요 과제(그레이딩)와 보조 예측 과제(모포 구문 레이블링, 언어 모델링 및 네이티브) 사이의 유도적 이전을 이용하여 비원어민 영어 사용자의 숙련도 수준을 예측하는 방법을 배우는 멀티태스킹 방식으로 훈련된 신경망을 개발한다. 언어 식별(L1)입니다.",
                    "tag": "1"
                },
                {
                    "index": "144-2",
                    "sentence": "We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates.",
                    "sentence_kor": "양방향 반복 신경망과 변압기의 양방향 표현으로 전사를 인코딩하고, 기능이 풍부한 기준선과 비교하며, 다양한 숙련도 수준과 다양한 오류율의 전사로 성능을 분석한다.",
                    "tag": "1"
                },
                {
                    "index": "144-3",
                    "sentence": "Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task.",
                    "sentence_kor": "최고의 성능은 L1 예측을 보조 작업으로 하는 변압기 인코더에서 나온다.",
                    "tag": "1"
                },
                {
                    "index": "144-4",
                    "sentence": "We discuss areas for improvement and potential applications for text-only speech scoring.",
                    "sentence_kor": "우리는 텍스트 전용 음성 채점을 위한 개선 영역과 잠재적 응용 분야에 대해 논의한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "744",
            "abstractID": "SPA_abs-145",
            "text": [
                {
                    "index": "145-0",
                    "sentence": "Representation learning is a critical ingredient for natural language processing systems.",
                    "sentence_kor": "표현 학습은 자연어 처리 시스템의 중요한 요소이다.",
                    "tag": "1"
                },
                {
                    "index": "145-1",
                    "sentence": "Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.",
                    "sentence_kor": "BERT와 같은 최근의 트랜스포머 언어 모델은 강력한 텍스트 표현을 학습하지만, 이러한 모델은 토큰 및 문장 수준 훈련 목표를 목표로 하며 문서 간 관련성에 대한 정보를 활용하지 않으므로 문서 수준 표현력이 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "145-2",
                    "sentence": "For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity.",
                    "sentence_kor": "분류 및 권고와 같은 과학 문서에 대한 응용 프로그램의 경우 문서의 정확한 임베딩이 필수적이다.",
                    "tag": "1"
                },
                {
                    "index": "145-3",
                    "sentence": "We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.",
                    "sentence_kor": "문서 수준 관련성의 강력한 신호인 인용 그래프에서 트랜스포머 언어 모델을 사전 교육하여 과학 논문의 문서 수준 임베딩을 생성하는 새로운 방법인 SPECTER를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "145-4",
                    "sentence": "Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning.",
                    "sentence_kor": "사전 교육을 받은 기존 언어 모델과 달리 스펙터는 작업별 미세 조정 없이 다운스트림 애플리케이션에 쉽게 적용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "145-5",
                    "sentence": "Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.",
                    "sentence_kor": "또한 문서 수준 모델에 대한 추가 연구를 장려하기 위해 인용 예측에서 문서 분류 및 권장사항에 이르는 7가지 문서 수준 작업으로 구성된 새로운 평가 벤치마크인 SciDocs를 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "145-6",
                    "sentence": "We show that Specter outperforms a variety of competitive baselines on the benchmark.",
                    "sentence_kor": "스펙터가 벤치마크에서 다양한 경쟁 기준선을 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "745",
            "abstractID": "SPA_abs-146",
            "text": [
                {
                    "index": "146-0",
                    "sentence": "We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program.",
                    "sentence_kor": "우리는 프로그램의 높은 수준의 의미와 통사적 구성을 대표하는 경량 구조인 의미론적 지지체를 기반으로 한 프로그램 생성 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "146-1",
                    "sentence": "By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques.",
                    "sentence_kor": "먼저 그럴듯한 스캐폴드를 검색한 후 이를 프로그램에 대한 빔 검색의 제약조건으로 사용함으로써 기존 기법과 비교할 때 검색 공간의 커버리지를 높일 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "146-2",
                    "sentence": "We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases.",
                    "sentence_kor": "우리는 의사 코드 간 생성을 위해 SPoC 데이터 세트에 계층적 검색 방법을 적용한다. 여기서 우리는 라인 수준의 자연어 의사 코드 주석을 부여받고 실행 기반 테스트 사례를 만족하는 프로그램을 만드는 것을 목표로 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "146-3",
                    "sentence": "By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art.",
                    "sentence_kor": "추론 중에 의미론적 지지체를 사용함으로써, 우리는 이전의 최첨단 기술에 비해 상위 100대 정확도에서 10%의 절대적인 향상을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "146-4",
                    "sentence": "Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.",
                    "sentence_kor": "또한, 보이지 않는 문제에 대해 테스트했을 때 기존 최고 접근 방식의 상위 3,000개 성능에 도달하기 위해서는 11명의 후보자들만이 필요합니다. 이는 효율성이 크게 향상되었음을 보여줍니다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "746",
            "abstractID": "SPA_abs-147",
            "text": [
                {
                    "index": "147-0",
                    "sentence": "In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them.",
                    "sentence_kor": "본 논문에서 우리는 반구조화된 표로 된 텍스트가 어디에나 있다는 것을 관찰한다. 표로 구성된 텍스트를 이해하려면 텍스트 조각의 의미뿐만 아니라 텍스트 사이의 암묵적 관계도 이해해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "147-1",
                    "sentence": "We argue that such data can prove as a testing ground for understanding how we reason about information.",
                    "sentence_kor": "우리는 그러한 데이터가 우리가 정보에 대해 추론하는 방법을 이해하는 시험장이 될 수 있다고 주장한다.",
                    "tag": "1"
                },
                {
                    "index": "147-2",
                    "sentence": "To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.",
                    "sentence_kor": "이를 연구하기 위해, 우리는 위키피디아 정보 상자에서 추출한 표를 기반으로 한 인간이 작성한 텍스트 가설로 구성된 INFOTABS라는 새로운 데이터 세트를 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "147-3",
                    "sentence": "Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning.",
                    "sentence_kor": "우리의 분석은 구내의 반구조화, 다중 도메인 및 이질적인 특성이 복잡하고 다면적인 추론을 수용한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "147-4",
                    "sentence": "Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.",
                    "sentence_kor": "실험에 따르면 인간 주석자는 테이블-히포시스 쌍 간의 관계에 동의하지만, 몇 가지 표준 모델링 전략은 작업에서 성공하지 못하므로 테이블에 대한 추론이 어려운 모델링 과제를 제기할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "747",
            "abstractID": "SPA_abs-148",
            "text": [
                {
                    "index": "148-0",
                    "sentence": "Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA).",
                    "sentence_kor": "기존 기계 판독 이해(MRC) 모델은 웹 수준 정보 검색 및 질문 답변(QA)과 같은 실제 애플리케이션으로 효과적으로 확장되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "148-1",
                    "sentence": "We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed.",
                    "sentence_kor": "우리는 이것이 MRC 데이터셋의 특성에서 비롯되었다고 주장한다. 이 중 대부분은 지원 문서와 필요한 모든 정보가 완전히 관찰되는 정적 환경이다.",
                    "tag": "1"
                },
                {
                    "index": "148-2",
                    "sentence": "In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments.",
                    "sentence_kor": "본 논문에서는 기존 MRC 데이터 세트를 부분적으로 관찰 가능한 대화형 환경으로 재구성하는 간단한 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "148-3",
                    "sentence": "Specifically, we “occlude” the majority of a document’s text and add context-sensitive commands that reveal “glimpses” of the hidden text to a model.",
                    "sentence_kor": "특히, 우리는 문서의 텍스트 대부분을 \"교합\"하고 숨겨진 텍스트의 \"글림\"을 드러내는 상황에 맞는 명령을 모델에 추가합니다.",
                    "tag": "3"
                },
                {
                    "index": "148-4",
                    "sentence": "We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.",
                    "sentence_kor": "SQuAD 및 뉴스를 용도 변경초기 사례 연구로서 QA를 수행한 다음 순차적 의사결정을 통해 관련 정보를 찾는 모델을 교육하는 데 대화형 말뭉치를 사용할 수 있는 방법을 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "148-5",
                    "sentence": "We believe that this setting can contribute in scaling models to web-level QA scenarios.",
                    "sentence_kor": "우리는 이 설정이 모델을 웹 레벨 QA 시나리오로 확장하는 데 기여할 수 있다고 믿는다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "748",
            "abstractID": "SPA_abs-149",
            "text": [
                {
                    "index": "149-0",
                    "sentence": "Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets.",
                    "sentence_kor": "자연어 추론(NLI)을 수행하도록 미세 조정된 BERT와 같은 사전 훈련된 신경 모델은 표준 데이터 세트에서 높은 정확도를 보이는 경우가 많지만 제어된 도전 집합에서는 워드 순서에 대한 민감도가 놀랄 만큼 부족하다.",
                    "tag": "1"
                },
                {
                    "index": "149-1",
                    "sentence": "We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage.",
                    "sentence_kor": "우리는 이 문제가 주로 사전 훈련된 모델의 한계 때문이 아니라 미세 조정 단계에서 구문 구조의 중요성을 전달할 수 있는 크라우드소싱된 NLI 예가 부족하기 때문에 발생한다고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "149-2",
                    "sentence": "We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus.",
                    "sentence_kor": "우리는 MNLI 말뭉치의 문장에 구문 변환을 적용하여 생성된 구문학적으로 유용한 예를 사용하여 표준 훈련 세트를 강화하는 몇 가지 방법을 탐구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "149-3",
                    "sentence": "The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set.",
                    "sentence_kor": "가장 성능이 좋은 증강 방법인 주제/객체 반전에서는 MNLI 테스트 세트의 성능에 영향을 미치지 않고 워드 어순에 대한 민감도를 0.28에서 0.73으로 진단하는 제어된 예제에 대한 BERT의 정확도를 향상시켰다.",
                    "tag": "3+4"
                },
                {
                    "index": "149-4",
                    "sentence": "This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.",
                    "sentence_kor": "이러한 개선은 데이터 증대에 사용되는 특정 구조를 넘어 일반화되었으며, 증강으로 인해 BERT가 추상 구문 표현을 모집하게 되었다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "749",
            "abstractID": "SPA_abs-150",
            "text": [
                {
                    "index": "150-0",
                    "sentence": "Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech.",
                    "sentence_kor": "예측 코딩에 기초한 훈련 목표는 최근 라벨이 부착되지 않은 음성에서 의미 있는 표현을 학습하는 데 매우 효과적인 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "150-1",
                    "sentence": "One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames.",
                    "sentence_kor": "한 가지 예는 자기 회귀 예측 코딩(Chung et al., 2019)으로, 최근 과거 프레임과 같은 컨텍스트에서 보이지 않는 미래 프레임을 생성하기 위해 자기 회귀 RNN을 훈련시킨다.",
                    "tag": "1"
                },
                {
                    "index": "150-2",
                    "sentence": "The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks.",
                    "sentence_kor": "이러한 접근법의 기본 가설은 미래 프레임을 정확하게 예측할 수 있는 숨겨진 상태가 많은 다운스트림 작업에 유용한 표현이라는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "150-3",
                    "sentence": "In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions.",
                    "sentence_kor": "본 논문에서 우리는 이 가설을 확장하고 모델을 훈련시켜 숨겨진 상태로 인코딩된 정보를 풍부하게 하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "150-4",
                    "sentence": "We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task.",
                    "sentence_kor": "우리는 미래 프레임 예측 작업의 일반화를 개선하기 위한 정규화의 역할을 하는 보조 목표를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "150-5",
                    "sentence": "Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.",
                    "sentence_kor": "음성 분류, 음성 인식 및 음성 번역에 대한 실험 결과는 가설을 뒷받침할 뿐만 아니라 풍부한 음성 내용을 포함하는 표현을 학습하는 데 있어 접근 방식의 효과를 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "750",
            "abstractID": "SPA_abs-151",
            "text": [
                {
                    "index": "151-0",
                    "sentence": "Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP.",
                    "sentence_kor": "BERT 및 XLNet을 포함한 최근 트랜스포머 기반 상황별 단어 표현은 NLP 내의 여러 분야에서 최첨단 성능을 보여 주었다.",
                    "tag": "1"
                },
                {
                    "index": "151-1",
                    "sentence": "Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream.",
                    "sentence_kor": "작업별 데이터 세트에 대해 훈련된 상황별 모델을 미세 조정하는 것이 다운스트림에서 우수한 성능을 달성하는 핵심이었다.",
                    "tag": "1"
                },
                {
                    "index": "151-2",
                    "sentence": "While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication).",
                    "sentence_kor": "이러한 사전 교육 모델을 미세 조정하는 것은 어휘 애플리케이션(언어 양식만 있는 애플리케이션)에서는 간단하지만 다중 모드 언어(대면 커뮤니케이션 모델링에 초점을 맞춘 NLP의 성장 영역)에서는 간단하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "151-3",
                    "sentence": "More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic.",
                    "sentence_kor": "보다 구체적으로, 이것은 사전 교육된 모델들이 시각과 음향의 두 가지 추가 양식을 수용하는 데 필요한 구성 요소를 가지고 있지 않기 때문입니다.",
                    "tag": "1"
                },
                {
                    "index": "151-4",
                    "sentence": "In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).",
                    "sentence_kor": "본 논문에서 우리는 MAG(Multimodal Adaptation Gate)라는 BERT 및 XLNet에 대한 첨부 파일을 제안했다.",
                    "tag": "2"
                },
                {
                    "index": "151-5",
                    "sentence": "MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.",
                    "sentence_kor": "MAG를 사용하면 미세 조정 중에 BERT와 XLNet이 다중 모드 비언어 데이터를 수신할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "151-6",
                    "sentence": "It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities.",
                    "sentence_kor": "BERT 및 XLNet의 내부 표현으로의 이동을 생성함으로써 시각적 및 음향적 양상에 따라 좌우됩니다.",
                    "tag": "3"
                },
                {
                    "index": "151-7",
                    "sentence": "In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis.",
                    "sentence_kor": "실험에서 다중 모드 정서 분석을 위해 일반적으로 사용되는 CMU-MOSI 및 CMU-MOSEI 데이터 세트를 연구한다.",
                    "tag": "3"
                },
                {
                    "index": "151-8",
                    "sentence": "Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet.",
                    "sentence_kor": "MAG-BERT와 MAG-XLNet을 미세 조정하면 BERT와 XLNet의 언어 전용 미세 조정뿐만 아니라 이전 기준선에 비해 감성 분석 성능이 크게 향상된다.",
                    "tag": "3+4"
                },
                {
                    "index": "151-9",
                    "sentence": "On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.",
                    "sentence_kor": "CMU-MOSI 데이터 세트에서 MAG-XLNet은 NLP 커뮤니티에서 처음으로 인간 수준의 다중 모드 정서 분석 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "751",
            "abstractID": "SPA_abs-152",
            "text": [
                {
                    "index": "152-0",
                    "sentence": "We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers.",
                    "sentence_kor": "우리는 영어로 응급 의료 서비스에 전화를 거는 동안 실시간으로 질문을 음성으로 표시하는 도전적이고 실용적인 과제를 다루는데, 이 작업은 응급 전화 신청자를 위한 광범위한 의사 결정 지원 시스템에 포함되어 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "152-1",
                    "sentence": "We propose a novel multimodal approach to real-time sequence labeling in speech.",
                    "sentence_kor": "우리는 음성에서 실시간 시퀀스 라벨링에 대한 새로운 멀티모달 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "152-2",
                    "sentence": "Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition.",
                    "sentence_kor": "우리 모델은 자동 음성 인식을 통해 스트리밍된 오디오와 노이즈가 많은 전사로부터 텍스트로 공동으로 학습하기 때문에 음성 및 자체 텍스트 표현을 두 개의 개별 양식 또는 보기로 취급한다.",
                    "tag": "3"
                },
                {
                    "index": "152-3",
                    "sentence": "Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data.",
                    "sentence_kor": "우리의 결과는 불리한 소음과 제한된 양의 훈련 데이터 하에서 텍스트 또는 오디오와 비교할 때 두 가지 양식에서 공동 학습하는 상당한 이득을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "152-4",
                    "sentence": "The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.",
                    "sentence_kor": "결과는 의료 증상 감지로 일반화되며, 여기서 다중 모드 학습을 통해 유사한 개선 패턴을 관찰한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "752",
            "abstractID": "SPA_abs-153",
            "text": [
                {
                    "index": "153-0",
                    "sentence": "This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture.",
                    "sentence_kor": "본 논문은 트랜스포머 기반 아키텍처를 사용하는 오디오 시각적 자동 음성 인식(AV-ASR) 시스템을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "153-1",
                    "sentence": "We particularly focus on the scene context provided by the visual information, to ground the ASR.",
                    "sentence_kor": "우리는 특히 ASR을 접지하기 위해 시각 정보가 제공하는 장면 상황에 초점을 맞춘다.",
                    "tag": "3"
                },
                {
                    "index": "153-2",
                    "sentence": "We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer.",
                    "sentence_kor": "우리는 변압기의 인코더 레이어에서 오디오 기능에 대한 표현을 추출하고 추가 크로스모달 멀티헤드 주의 레이어를 사용하여 비디오 기능을 퓨전한다.",
                    "tag": "3"
                },
                {
                    "index": "153-3",
                    "sentence": "Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.",
                    "sentence_kor": "또한 다중 해상도 ASR에 대한 다중 작업 훈련 기준을 통합하여 문자 및 하위 단어 수준 전사를 생성하도록 모델을 훈련한다.",
                    "tag": "3"
                },
                {
                    "index": "153-4",
                    "sentence": "Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models.",
                    "sentence_kor": "How2 데이터 세트에 대한 실험 결과는 다중 해상도 훈련이 수렴 속도를 약 50% 높일 수 있고 하위 단어 예측 모델에 비해 워드 오류율(WER) 성능을 최대 18% 향상시킨다는 것을 나타낸다.",
                    "tag": "4"
                },
                {
                    "index": "153-5",
                    "sentence": "Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.",
                    "sentence_kor": "또한 시각 정보를 통합하면 오디오 전용 모델에 비해 성능이 최대 3.76% 향상되어 성능이 향상된다.",
                    "tag": "4"
                },
                {
                    "index": "153-6",
                    "sentence": "Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.",
                    "sentence_kor": "당사의 결과는 최첨단 듣기, 참석 및 맞춤법 기반 아키텍처와 유사합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "753",
            "abstractID": "SPA_abs-154",
            "text": [
                {
                    "index": "154-0",
                    "sentence": "End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation.",
                    "sentence_kor": "음성 변환(ST)을 위한 엔드 투 엔드 모델은 기존의 개별 ASR 및 MT 모델보다 음성 인식(ASR)과 기계 변환(MT)을 더 긴밀하게 결합하며, 단순한 모델 아키텍처와 오류 전파 감소 가능성을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "154-1",
                    "sentence": "Their performance is often assumed to be superior, though in many conditions this is not yet the case.",
                    "sentence_kor": "그들의 성능은 종종 더 우수하다고 가정되지만, 많은 조건에서는 아직 그렇지 않다.",
                    "tag": "1"
                },
                {
                    "index": "154-2",
                    "sentence": "We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines.",
                    "sentence_kor": "하이, 중형 및 저자원 조건에서 계단식 모델과 엔드 투 엔드 모델을 비교하고 계단식이 더 강력한 기준선을 유지함을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "154-3",
                    "sentence": "Further, we introduce two methods to incorporate phone features into ST models.",
                    "sentence_kor": "또한 전화 기능을 ST 모델에 통합하는 두 가지 방법을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "154-4",
                    "sentence": "We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting.",
                    "sentence_kor": "이러한 기능이 두 아키텍처를 모두 개선하여 엔드 투 엔드 모델과 계단식 모델 간의 격차를 좁히고 이전의 학술 연구를 능가하는 저자원 환경에서 최대 9 BLEU를 달성한다는 것을 보여줍니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "754",
            "abstractID": "SPA_abs-155",
            "text": [
                {
                    "index": "155-0",
                    "sentence": "Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people.",
                    "sentence_kor": "효과적인 대화는 사람들 사이의 의사소통에 필수적인 상호 지식을 확립하는 과정인 기초를 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "155-1",
                    "sentence": "Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication.",
                    "sentence_kor": "현대의 대화 시스템은 공통점을 구축하기 위해 명시적으로 훈련되지 않았기 때문에 의사소통의 중요한 측면을 간과한다.",
                    "tag": "1"
                },
                {
                    "index": "155-2",
                    "sentence": "Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality.",
                    "sentence_kor": "즉흥극(improv)은 본질적으로 공통의 토대를 구축하는 데 초점을 맞춘 대화의 높은 비율을 포함하고 있으며, 일관성과 실행 가능한 객관적 현실을 확립하기 위해 강력한 기초 연설 행위인 예스와 원칙을 이용한다.",
                    "tag": "1"
                },
                {
                    "index": "155-3",
                    "sentence": "We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.",
                    "sentence_kor": "우리는 26,000개 이상의 예/턴을 수집합니다. 즉흥대화에서 그것들을 옮겨서, 부츠랩된 분류기를 통해 더 크지만 더 인구가 적은 영화 대본대화 말뭉치에서 그것들을 추출합니다.",
                    "tag": "1"
                },
                {
                    "index": "155-4",
                    "sentence": "We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",
                    "sentence_kor": "우리는 좀 더 근거 있는 관련 대화를 장려하고 인간 평가를 통해 이러한 발견을 확인하기 위해 말뭉치와의 채팅 대화 시스템을 미세 조정한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "755",
            "abstractID": "SPA_abs-156",
            "text": [
                {
                    "index": "156-0",
                    "sentence": "To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners.",
                    "sentence_kor": "기계가 인간을 대화에 참여시킬 수 있다는 장기적인 목표를 달성하기 위해, 우리의 모델은 말하는 파트너의 관심을 사로잡아야 한다.",
                    "tag": "1"
                },
                {
                    "index": "156-1",
                    "sentence": "Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014).",
                    "sentence_kor": "주어진 사진을 기반으로 대화가 진행되는 이미지에 기반을 둔 통신은 인간에게 자연스럽게 어필하는 설정이다(Hu et al., 2014).",
                    "tag": "1"
                },
                {
                    "index": "156-2",
                    "sentence": "In this work we study large-scale architectures and datasets for this goal.",
                    "sentence_kor": "이 작업에서는 이 목표를 위한 대규모 아키텍처와 데이터 세트를 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "156-3",
                    "sentence": "We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components.",
                    "sentence_kor": "구성 요소를 융합하는 다양한 방법을 고려하여 최첨단 이미지 및 텍스트 표현을 사용하여 일련의 신경 아키텍처를 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "156-4",
                    "sentence": "To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019).",
                    "sentence_kor": "이러한 모델을 테스트하기 위해, 우리는 제공된 감정적 분위기나 스타일이 주어진 역할을 연기하도록 요청받는 근거 있는 인간-인간 대화의 데이터 세트를 수집한다. 이러한 특성들의 사용도 참여의 핵심 요소이기 때문이다(Guo et al., 2019).",
                    "tag": "3"
                },
                {
                    "index": "156-5",
                    "sentence": "Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits.",
                    "sentence_kor": "우리의 데이터 세트인 Image-Chat은 215개의 가능한 스타일 특성을 사용하여 202k개의 이미지에 걸쳐 202k개의 대화로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "156-6",
                    "sentence": "Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).",
                    "sentence_kor": "자동 메트릭스와 참여성에 대한 인간 평가는 접근 방식의 효과를 보여준다. 특히, 기존 IGC 작업에서 최첨단 성능을 얻었으며, 우리의 최고 성능 모델은 Image-Chat 테스트 세트의 인간과 거의 동등하다(당시 47.7% 선호).",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "756",
            "abstractID": "SPA_abs-157",
            "text": [
                {
                    "index": "157-0",
                    "sentence": "Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue.",
                    "sentence_kor": "두 에이전트 간의 대화 상호작용 품질을 평가하는 것은 특히 오픈 도메인 채팅 스타일 대화에서 어려운 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "157-1",
                    "sentence": "There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation.",
                    "sentence_kor": "최근 자동 대화 평가 지표를 개발하려는 노력이 있었지만, 대부분은 보이지 않는 데이터 세트로 일반화하지 않거나 추론 중에 인간이 생성한 참조 응답이 필요하므로 온라인 평가에 실행이 불가능하다.",
                    "tag": "1"
                },
                {
                    "index": "157-2",
                    "sentence": "Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them.",
                    "sentence_kor": "여기서는 사전 훈련된 대규모 언어 모델을 사용하여 발화의 잠재 표현을 추출하고 발화 사이에 존재하는 시간적 전환을 활용하는 참조되지 않은 자동 평가 지표를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "157-3",
                    "sentence": "We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.",
                    "sentence_kor": "우리는 우리 모델이 추론 중 비교를 위해 실제 응답을 요구하지 않으면서 온라인 환경에서 인간 주석과 더 높은 상관관계를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "757",
            "abstractID": "SPA_abs-158",
            "text": [
                {
                    "index": "158-0",
                    "sentence": "The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue.",
                    "sentence_kor": "인간 대화에서 음성 응답 오프셋의 타이밍은 대화의 상황적 요소에 따라 달라지는 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "158-1",
                    "sentence": "We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn.",
                    "sentence_kor": "우리는 반응 전환과 이전 전환을 고려하여 이러한 반응 오프셋의 분포를 시뮬레이션하는 신경 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "158-2",
                    "sentence": "The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS).",
                    "sentence_kor": "모델은 증분 음성 대화 시스템(SDS)의 파이프라인에 통합되도록 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "158-3",
                    "sentence": "We evaluate our models using offline experiments as well as human listening tests.",
                    "sentence_kor": "우리는 인간 듣기 테스트뿐만 아니라 오프라인 실험을 사용하여 모델을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "158-4",
                    "sentence": "We show that human listeners consider certain response timings to be more natural based on the dialogue context.",
                    "sentence_kor": "우리는 인간 청취자들이 대화의 맥락을 기반으로 특정 응답 타이밍을 더 자연스럽다고 생각한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "158-5",
                    "sentence": "The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.",
                    "sentence_kor": "이러한 모델을 SDS 파이프라인에 도입하면 상호 작용의 인지적 자연성이 증가할 수 있다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "758",
            "abstractID": "SPA_abs-159",
            "text": [
                {
                    "index": "159-0",
                    "sentence": "Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text.",
                    "sentence_kor": "부분적으로 입력 그래프와 출력 텍스트 사이의 구조적 차이 때문에 그래프 구조화된 데이터(예: 지식 그래프)에서 순차적 자연어 설명을 생성하는 것은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "159-1",
                    "sentence": "Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task.",
                    "sentence_kor": "따라서 직렬화된 입력이 필요한 대중적인 시퀀스 투 시퀀스 모델은 이 작업에 자연스럽게 적합하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "159-2",
                    "sentence": "Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult.",
                    "sentence_kor": "반면에 그래프 신경망은 입력 그래프를 더 잘 인코딩할 수 있지만 인코더와 디코더 사이의 구조적 격차를 넓혀 충실한 생성을 어렵게 한다.",
                    "tag": "1"
                },
                {
                    "index": "159-3",
                    "sentence": "To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text.",
                    "sentence_kor": "이 격차를 줄이기 위해 그래프 구조를 통합할 수 있을 뿐만 아니라 출력 텍스트의 선형 구조를 충족시킬 수 있는 듀얼 인코딩 모델인 듀얼엔크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "159-4",
                    "sentence": "Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.",
                    "sentence_kor": "강력한 단일 인코더 기준선을 사용한 경험적 비교는 이중 인코딩이 생성된 텍스트의 품질을 크게 향상시킬 수 있음을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "759",
            "abstractID": "SPA_abs-160",
            "text": [
                {
                    "index": "160-0",
                    "sentence": "We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.",
                    "sentence_kor": "문서의 모든 위치에서 텍스트 누락 범위를 예측하는 작업인 텍스트 채우기를 위한 간단한 접근 방식을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "160-1",
                    "sentence": "While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling—a special case of infilling where text is predicted at the end of a document.",
                    "sentence_kor": "채우기는 특히 쓰기 보조 도구에 대해 풍부한 기능을 가능하게 할 수 있지만, 문서 끝에 텍스트가 예측되는 특별한 채우기 사례인 언어 모델링에 더 많은 관심이 집중되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "160-2",
                    "sentence": "In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.",
                    "sentence_kor": "본 논문에서, 우리는 언어 모델(LM)의 기능을 더 일반적인 주입 작업으로 확장하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "160-3",
                    "sentence": "To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.",
                    "sentence_kor": "이를 위해 인위적으로 마스킹된 텍스트와 마스킹된 텍스트가 연결된 시퀀스에 대해 기성 LM을 교육(또는 미세 조정)한다.",
                    "tag": "3"
                },
                {
                    "index": "160-4",
                    "sentence": "We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.",
                    "sentence_kor": "우리가 언어 모델링에 의한 주입이라고 부르는 이 접근방식은 LM이 단편 소설, 과학적 추상화 및 가사의 세 가지 영역에서 전체 문장을 효과적으로 주입할 수 있도록 할 수 있다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "160-5",
                    "sentence": "Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.",
                    "sentence_kor": "게다가, 우리는 인간이 우리의 접근법에 의해 주입된 문장을 단편 소설 영역에서 기계로 생성된 것으로 식별하는 데 어려움을 겪고 있다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "760",
            "abstractID": "SPA_abs-161",
            "text": [
                {
                    "index": "161-0",
                    "sentence": "Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion.",
                    "sentence_kor": "누락된 문장 생성(또는 문장 채우기)은 문서 자동 완성 및 회의 노트 확장과 같은 자연어 생성에 광범위한 응용을 촉진한다.",
                    "tag": "1"
                },
                {
                    "index": "161-1",
                    "sentence": "This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context.",
                    "sentence_kor": "이 작업은 모델에게 구문론 및 의미론적으로 주변 컨텍스트를 연결할 수 있는 중간 누락 문장을 생성하도록 요청한다.",
                    "tag": "1"
                },
                {
                    "index": "161-2",
                    "sentence": "Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation.",
                    "sentence_kor": "문장 채우기 과제를 해결하려면 이해부터 담화 수준 계획, 생성에 이르는 자연어 처리 기술이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "161-3",
                    "sentence": "In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.",
                    "sentence_kor": "본 논문에서 우리는 BERT 및 GPT-2와 같은 기존의 대규모 사전 교육 모델의 힘을 활용하여 과제를 분리하고 이 세 가지 측면을 각각 해결하는 프레임워크를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "161-4",
                    "sentence": "We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.",
                    "sentence_kor": "우리는 생성에 대한 문장 표현을 학습하고 문맥에 맞는 누락된 문장을 추가로 생성할 때 모델의 효과를 경험적으로 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "761",
            "abstractID": "SPA_abs-162",
            "text": [
                {
                    "index": "162-0",
                    "sentence": "Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation.",
                    "sentence_kor": "자동 회귀 텍스트 생성 모델은 일반적으로 로컬 유창성에 초점을 맞추고 있으며, 긴 텍스트 생성에서 일관성 없는 의미론적 의미를 유발할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "162-1",
                    "sentence": "Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply.",
                    "sentence_kor": "또한, 유사한 의미를 가진 단어를 자동으로 생성하는 것은 어렵고 수작업으로 만들어진 언어 규칙을 적용하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "162-2",
                    "sentence": "We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues.",
                    "sentence_kor": "우리는 텍스트 계획 계획을 고려하고 앞에서 언급한 문제를 완화하기 위한 모델 기반 모방 학습 접근방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "162-3",
                    "sentence": "Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization.",
                    "sentence_kor": "특히, 우리는 다음 단어 예측을 지원하고 발전기 최적화에 대한 중간 보상을 제공할 수 있는 더 긴 범위에서 생성 프로세스에 초점을 맞추는 새로운 가이드 네트워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "162-4",
                    "sentence": "Extensive experiments demonstrate that the proposed method leads to improved performance.",
                    "sentence_kor": "광범위한 실험을 통해 제안된 방법이 성능 향상으로 이어진다는 것을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "762",
            "abstractID": "SPA_abs-163",
            "text": [
                {
                    "index": "163-0",
                    "sentence": "Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output.",
                    "sentence_kor": "seq2seq 방법 검색 및 편집은 일반적으로 교육 세트에서 출력을 검색하고 모델을 학습하여 최종 출력을 생성하도록 편집합니다.",
                    "tag": "1"
                },
                {
                    "index": "163-1",
                    "sentence": "We propose to extend this framework with a simple and effective post-generation ranking approach.",
                    "sentence_kor": "간단하고 효과적인 사후 생성 순위 접근방식으로 이 프레임워크를 확장할 것을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "163-2",
                    "sentence": "Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output.",
                    "sentence_kor": "우리의 프레임워크(i)는 각 입력에 대해 잠재적으로 관련된 몇 가지 출력을 검색하고, (iii) 각 후보를 독립적으로 편집하며, (iii) 최종 출력을 선택하기 위해 편집된 후보를 다시 검색한다.",
                    "tag": "3"
                },
                {
                    "index": "163-3",
                    "sentence": "We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies.",
                    "sentence_kor": "우리는 간단한 작업별 재순위화 접근법과 함께 표준 편집 모델을 사용하며, 이 접근법이 기존의 훨씬 더 복잡한 방법론을 능가한다는 것을 경험적으로 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "163-4",
                    "sentence": "Experiments on two machine translation (MT) datasets show new state-of-art results.",
                    "sentence_kor": "두 MT(기계 변환) 데이터 세트에 대한 실험은 새로운 최첨단 결과를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "163-5",
                    "sentence": "We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.",
                    "sentence_kor": "또한 Gigaword 요약 데이터 세트에서 거의 최첨단 성능을 달성하며, 분석 결과 향후 작업에서 더 나은 후보 출력 선택을 통해 성능을 개선할 수 있는 상당한 여지가 있는 것으로 나타났다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "763",
            "abstractID": "SPA_abs-164",
            "text": [
                {
                    "index": "164-0",
                    "sentence": "Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN).",
                    "sentence_kor": "지시사항을 따르는 것을 배우는 것은 비전 및 언어 탐색(VLN)을 위한 자율 에이전트에게 기본적으로 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "164-1",
                    "sentence": "In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones.",
                    "sentence_kor": "본 논문에서는 에이전트가 짧은 코퍼스에서 학습할 때 긴 경로를 탐색할 수 있는 방법을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "164-2",
                    "sentence": "We show that existing state-of-the-art agents do not generalize well.",
                    "sentence_kor": "기존 최첨단 에이전트가 잘 일반화되지 않는다는 것을 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "164-3",
                    "sentence": "To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.",
                    "sentence_kor": "이를 위해 긴 명령을 더 짧은 명령(BabySteps)으로 분해하고 순차적으로 완료하여 탐색하는 방법을 학습하는 새로운 VLN 에이전트인 BabyWalk를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "164-4",
                    "sentence": "A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps.",
                    "sentence_kor": "에이전트는 특수 설계 메모리 버퍼를 사용하여 과거 경험을 향후 단계를 위한 컨텍스트로 전환합니다.",
                    "tag": "3"
                },
                {
                    "index": "164-5",
                    "sentence": "The learning process is composed of two phases.",
                    "sentence_kor": "학습 과정은 두 단계로 구성됩니다.",
                    "tag": "3"
                },
                {
                    "index": "164-6",
                    "sentence": "In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps.",
                    "sentence_kor": "첫 번째 단계에서 에이전트는 시연을 통한 모방 학습을 사용하여 BabySteps를 수행합니다.",
                    "tag": "3"
                },
                {
                    "index": "164-7",
                    "sentence": "In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions.",
                    "sentence_kor": "두 번째 단계에서 에이전트는 커리큘럼 기반 강화 학습을 사용하여 점점 더 긴 지시로 탐색 작업에 대한 보상을 극대화한다.",
                    "tag": "3"
                },
                {
                    "index": "164-8",
                    "sentence": "We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability.",
                    "sentence_kor": "우리는 (긴 탐색 작업의) 두 개의 새로운 벤치마크 데이터 세트를 생성하고 기존 데이터 세트와 함께 사용하여 BabyWalk의 일반화 능력을 검토한다.",
                    "tag": "3"
                },
                {
                    "index": "164-9",
                    "sentence": "Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better.",
                    "sentence_kor": "경험적 결과에 따르면 BabyWalk는 특히 긴 지침을 더 잘 따를 수 있는 여러 메트릭에서 최첨단 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "164-10",
                    "sentence": "The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.",
                    "sentence_kor": "코드와 데이터 세트는 프로젝트 페이지인 https://github.com/Sha-Lab/babywalk에서 공개됩니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "764",
            "abstractID": "SPA_abs-165",
            "text": [
                {
                    "index": "165-0",
                    "sentence": "We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.",
                    "sentence_kor": "우리는 내레이션에 의해 안내되는 작업 구조의 생성 세분화 모델을 비디오의 액션 분할에 적용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "165-1",
                    "sentence": "We focus on unsupervised and weakly-supervised settings where no action labels are known during training.",
                    "sentence_kor": "우리는 훈련 중에 동작 라벨을 알 수 없는 감독되지 않고 약하게 감독되는 설정에 초점을 맞춘다.",
                    "tag": "3"
                },
                {
                    "index": "165-2",
                    "sentence": "Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos.",
                    "sentence_kor": "우리의 모델은 단순함에도 불구하고 자연주의적인 교육용 비디오 데이터 세트에 대한 이전 작업과 경쟁적으로 성능을 발휘한다.",
                    "tag": "4"
                },
                {
                    "index": "165-3",
                    "sentence": "Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.",
                    "sentence_kor": "우리의 모델을 통해 훈련에 사용되는 감독 소스를 변경할 수 있으며, 우리는 작업 구조와 서술 언어 모두 세분화 품질에서 큰 이점을 제공한다는 것을 발견했다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "765",
            "abstractID": "SPA_abs-166",
            "text": [
                {
                    "index": "166-0",
                    "sentence": "Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.",
                    "sentence_kor": "동영상에 대한 다중 문장 설명 생성은 문단의 문장 전반에 걸쳐 시각적 관련성뿐만 아니라 담화 기반 일관성에 대한 요구 사항이 높기 때문에 가장 어려운 캡션 작업 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "166-1",
                    "sentence": "Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture.",
                    "sentence_kor": "이 목표를 위해 메모리 모듈을 사용하여 변압기 아키텍처를 강화하는 MART(Memory-Augmented Recurrent Transformer)라는 새로운 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "166-2",
                    "sentence": "The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation.",
                    "sentence_kor": "기억 모듈은 비디오 세그먼트와 문장 이력에서 고도로 요약된 기억 상태를 생성하여 다음 문장(참조 및 반복 측면)의 더 나은 예측을 돕기 때문에 일관성 있는 단락 생성을 장려한다.",
                    "tag": "3"
                },
                {
                    "index": "166-3",
                    "sentence": "Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.",
                    "sentence_kor": "널리 사용되는 두 데이터 세트에 대한 광범위한 실험, 인적 평가 및 정성 분석 ActivityNet 캡션 및 YouCookII는 MART가 입력 비디오 이벤트와의 관련성을 유지하면서 기준 방법보다 더 일관되고 덜 반복적인 단락 캡션을 생성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "766",
            "abstractID": "SPA_abs-167",
            "text": [
                {
                    "index": "167-0",
                    "sentence": "Visual features are a promising signal for learning bootstrap textual models.",
                    "sentence_kor": "시각적 특징은 부트스트랩 텍스트 모델을 학습할 때 유망한 신호이다.",
                    "tag": "1"
                },
                {
                    "index": "167-1",
                    "sentence": "However, blackbox learning models make it difficult to isolate the specific contribution of visual components.",
                    "sentence_kor": "그러나 블랙박스 학습 모델은 시각적 구성요소의 특정 기여도를 분리하기 어렵게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "167-2",
                    "sentence": "In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal.",
                    "sentence_kor": "이 분석에서는 시각적 훈련 신호로부터 구문을 학습하기 위한 최근 접근 방식인 시각 기반 신경 구문 학습자(Si et al., 2019)의 사례 연구를 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "167-3",
                    "sentence": "By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance.",
                    "sentence_kor": "모델의 단순화된 버전을 구성함으로써 우리는 모델의 강력한 성능을 산출하는 핵심 요인을 분리한다.",
                    "tag": "3"
                },
                {
                    "index": "167-4",
                    "sentence": "Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better.",
                    "sentence_kor": "모델이 학습할 수 있는 것과는 반대로, 우리는 훨씬 덜 표현적인 버전이 유사한 예측을 만들어 내고 성능이 우수하거나 더 우수하다는 것을 발견한다.",
                    "tag": "5"
                },
                {
                    "index": "167-5",
                    "sentence": "We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.",
                    "sentence_kor": "우리는 또한 명사 구체성의 단순한 어휘 신호가 더 복잡한 구문적 추론과 반대로 모델 예측에서 주요 역할을 한다는 것을 발견했다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "767",
            "abstractID": "SPA_abs-168",
            "text": [
                {
                    "index": "168-0",
                    "sentence": "Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks.",
                    "sentence_kor": "VAE(Variative Autoencoder)는 상각된 변동 추론과 심층 신경망을 결합하여 잠재 변수에 대한 모델의 후방을 근사하는 생성 모델로 널리 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "168-1",
                    "sentence": "However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”.",
                    "sentence_kor": "그러나 강력한 자기 회귀 디코더와 쌍으로 구성될 때 VAE는 종종 \"뒤쪽 붕괴\"로 알려진 퇴화된 국소 최적값으로 수렴된다.",
                    "tag": "1"
                },
                {
                    "index": "168-2",
                    "sentence": "Previous approaches consider the Kullback–Leibler divergence (KL) individual for each datapoint.",
                    "sentence_kor": "이전 접근법은 각 데이터 포인트에 대한 Kullback-Leibler 발산(KL) 개인을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "168-3",
                    "sentence": "We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL’s distribution positive.",
                    "sentence_kor": "우리는 KL이 전체 데이터 세트에 걸친 분포를 따르도록 하고, KL 분포에 대한 기대를 긍정적으로 유지함으로써 사후 붕괴를 예방하기에 충분하다고 분석한다.",
                    "tag": "2"
                },
                {
                    "index": "168-4",
                    "sentence": "Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters.",
                    "sentence_kor": "그런 다음 대략적인 사후 매개 변수의 분포를 정규화하여 기대치의 하한을 설정하는 간단하지만 효과적인 접근 방식인 BN-VAE(Batch Normalized-VAE)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "168-5",
                    "sentence": "Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently.",
                    "sentence_kor": "새로운 모델 구성요소를 도입하거나 목표를 수정하지 않고도 우리의 접근 방식은 사후 붕괴를 효과적이고 효율적으로 피할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "168-6",
                    "sentence": "We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE).",
                    "sentence_kor": "우리는 또한 제안된 BN-VAE가 조건부 VAE(CVAE)로 확장될 수 있음을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "168-7",
                    "sentence": "Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.",
                    "sentence_kor": "경험적으로 우리의 접근 방식은 언어 모델링, 텍스트 분류 및 대화 생성에 대한 강력한 자기 회귀 기준을 능가하며 VAE와 거의 동일한 훈련 시간을 유지하면서 더 복잡한 접근법에 필적한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "768",
            "abstractID": "SPA_abs-169",
            "text": [
                {
                    "index": "169-0",
                    "sentence": "We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline—random word embeddings—focusing on the impact of the training set size and the linguistic properties of the task.",
                    "sentence_kor": "우리는 심층 상황별 임베딩(예: BERT)이 기존의 사전 훈련된 임베딩(예: GloVe)에 비해 성능이 크게 향상되는 설정과 훨씬 더 단순한 기준선인 무작위 단어 임베딩(교육 세트 크기 및 작업의 언어 속성)을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "169-1",
                    "sentence": "Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks.",
                    "sentence_kor": "놀랍게도, 우리는 이 두 가지 단순한 기준선이 모두 산업 규모 데이터의 상황별 임베딩과 일치할 수 있으며 벤치마크 작업에서 종종 5~10%의 정확도(절대) 내에서 수행된다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "169-2",
                    "sentence": "Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.",
                    "sentence_kor": "또한 컨텍스트 임베딩이 특히 큰 이득을 주는 데이터 속성, 즉 복잡한 구조를 포함하는 언어, 애매한 단어 사용 및 교육에서 보이지 않는 단어를 식별한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "769",
            "abstractID": "SPA_abs-170",
            "text": [
                {
                    "index": "170-0",
                    "sentence": "We study the potential for interaction in natural language classification.",
                    "sentence_kor": "우리는 자연어 분류에서 상호작용의 가능성을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "170-1",
                    "sentence": "We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions.",
                    "sentence_kor": "우리는 의도 분류를 위해 제한된 형태의 상호작용을 추가하는데, 여기서 사용자는 자연어를 사용하여 초기 쿼리를 제공하고, 시스템은 이진 또는 다중 선택 질문을 사용하여 추가 정보를 요구한다.",
                    "tag": "3"
                },
                {
                    "index": "170-2",
                    "sentence": "At each turn, our system decides between asking the most informative question or making the final classification pre-diction.",
                    "sentence_kor": "각 차례마다 시스템은 가장 유익한 질문을 하거나 최종 분류 사전 결정을 내린다.",
                    "tag": "3"
                },
                {
                    "index": "170-3",
                    "sentence": "The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks.",
                    "sentence_kor": "모델의 단순성으로 상호 작용 데이터 없이 시스템을 부트스트래핑할 수 있으며, 대신 간단한 크라우드소싱 작업에 의존합니다.",
                    "tag": "3"
                },
                {
                    "index": "170-4",
                    "sentence": "We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.",
                    "sentence_kor": "우리는 두 영역에 대한 접근 방식을 평가하여 상호작용의 이점과 추가 질문을 하는 것과 최종 예측을 하는 것 사이에서 균형을 맞추는 것을 배우는 이점을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "770",
            "abstractID": "SPA_abs-171",
            "text": [
                {
                    "index": "171-0",
                    "sentence": "Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications.",
                    "sentence_kor": "KG에서 실체와 관계의 연속 임베딩을 학습하는 지식 그래프(KG) 표현 학습 기술은 많은 AI 애플리케이션에서 인기를 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "171-1",
                    "sentence": "With a large KG, the embeddings consume a large amount of storage and memory.",
                    "sentence_kor": "대용량 KG의 경우 임베딩은 많은 양의 스토리지 및 메모리를 소비합니다.",
                    "tag": "1"
                },
                {
                    "index": "171-2",
                    "sentence": "This is problematic and prohibits the deployment of these techniques in many real world settings.",
                    "sentence_kor": "이는 문제가 있으며 많은 실제 환경에서 이러한 기술의 배치를 금지한다.",
                    "tag": "1"
                },
                {
                    "index": "171-3",
                    "sentence": "Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes.",
                    "sentence_kor": "따라서 KG의 각 실체를 이산 코드의 벡터로 표현하여 KG 임베딩 계층을 압축한 다음 이러한 코드의 임베딩을 구성하는 접근방식을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "171-4",
                    "sentence": "The approach can be trained end-to-end with simple modifications to any existing KG embedding technique.",
                    "sentence_kor": "이 접근법은 기존의 KG 내장 기법에 대한 간단한 수정으로 엔드 투 엔드로 훈련될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "171-5",
                    "sentence": "We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance.",
                    "sentence_kor": "다양한 표준 KG 임베딩 평가에 대한 접근 방식을 평가하고 성능 손실이 적은 임베딩의 50-1000배 압축을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "171-6",
                    "sentence": "The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.",
                    "sentence_kor": "또한 압축 임베딩은 KG 추론과 같은 다양한 추론 작업을 수행할 수 있는 능력을 가지고 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "771",
            "abstractID": "SPA_abs-172",
            "text": [
                {
                    "index": "172-0",
                    "sentence": "This work revisits the task of training sequence tagging models with limited resources using transfer learning.",
                    "sentence_kor": "이 작업은 전송 학습을 사용하여 제한된 리소스를 사용하여 시퀀스 태그 모델을 훈련하는 작업을 다시 살펴본다.",
                    "tag": "2+3"
                },
                {
                    "index": "172-1",
                    "sentence": "We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings.",
                    "sentence_kor": "우리는 최근 연구에서 소개된 몇 가지 제안된 접근 방식을 조사하고 정규화된 임베딩의 문장 재구성에 의존하는 새로운 손실을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "172-2",
                    "sentence": "Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines.",
                    "sentence_kor": "특히, 우리의 방법은 문장 재구성을 위한 디코딩 레이어를 추가하여 다양한 기준선의 성능을 향상시킬 수 있는 방법을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "172-3",
                    "sentence": "We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.",
                    "sentence_kor": "CoNLL02 NER 및 UD 1.2 POS 데이터 세트에서 개선된 결과를 보여주고, 단일 샘플만 사용하여 네덜란드어로 0.6 F1 점수를 달성하는 저리소스로 전송 학습의 힘을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "772",
            "abstractID": "SPA_abs-173",
            "text": [
                {
                    "index": "173-0",
                    "sentence": "Pretrained masked language models (MLMs) require finetuning for most NLP tasks.",
                    "sentence_kor": "사전 훈련된 마스킹 언어 모델(MLM)은 대부분의 NLP 작업에 대해 미세 조정이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "173-1",
                    "sentence": "Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one.",
                    "sentence_kor": "대신 토큰을 하나씩 마스킹하여 계산되는 유사 로그 우도 점수(PLL)를 통해 MLM을 즉시 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "173-2",
                    "sentence": "We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks.",
                    "sentence_kor": "우리는 PLL이 다양한 작업에서 GPT-2와 같은 자기 회귀 언어 모델의 점수를 능가한다는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "173-3",
                    "sentence": "By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.",
                    "sentence_kor": "RoBERTA는 ASR 및 NMT 가설을 재조정함으로써 엔드 투 엔드 LibriSpeech 모델의 WER을 30% 상대적인 수준으로 줄이고 저자원 변환 쌍을 위한 최첨단 기준선에서 +1.7 BLEU까지 합산하여 도메인 적응을 통한 추가 이득을 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "173-4",
                    "sentence": "We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP).",
                    "sentence_kor": "우리는 이러한 성공을 PLL이 GPT-2의 점수(+섬 효과에 대한 10점, BLiMP의 NPI 라이선스)를 크게 향상시키면서 좌우 편향 없이 언어 수용성을 감독 없이 표현했기 때문이라고 본다.",
                    "tag": "4"
                },
                {
                    "index": "173-5",
                    "sentence": "One can finetune MLMs to give scores without masking, enabling computation in a single inference pass.",
                    "sentence_kor": "마스킹 없이 점수를 부여하도록 MLM을 미세 조정할 수 있어 단일 추론 패스에서 계산이 가능하다.",
                    "tag": "4"
                },
                {
                    "index": "173-6",
                    "sentence": "In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages.",
                    "sentence_kor": "전반적으로, PLL과 관련 유사 퍼플렉시(PPPL)는 사전 교육을 받은 MLM의 수가 증가하는 것을 플러그 앤 플레이로 사용할 수 있게 한다. 예를 들어, 우리는 단일 언어 교차 모델을 사용하여 다국어로 번역을 다시 정렬한다.",
                    "tag": "4"
                },
                {
                    "index": "173-7",
                    "sentence": "We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.",
                    "sentence_kor": "언어 모델 채점을 위한 라이브러리는 https://github.com/awslabs/mlm-scoring에서 공개합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "773",
            "abstractID": "SPA_abs-174",
            "text": [
                {
                    "index": "174-0",
                    "sentence": "Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE.",
                    "sentence_kor": "거리 기반 지식 그래프 임베딩은 TransE에서 최신 RotatE에 이르기까지 지식 그래프 링크 예측 작업에서 상당한 개선을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "174-1",
                    "sentence": "However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict.",
                    "sentence_kor": "그러나 N 대 1, 1 대 N 및 N 대 N과 같은 복잡한 관계는 여전히 예측하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "174-2",
                    "sentence": "In this work, we propose a novel distance-based approach for knowledge graph link prediction.",
                    "sentence_kor": "본 연구에서는 지식 그래프 링크 예측을 위한 새로운 거리 기반 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "174-3",
                    "sentence": "First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations.",
                    "sentence_kor": "첫째, RotatE를 직교 변환에서 모델 관계로 2D 복합 도메인에서 고차원 공간으로 확장한다.",
                    "tag": "3"
                },
                {
                    "index": "174-4",
                    "sentence": "The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity.",
                    "sentence_kor": "관계를 위한 직교 변환 임베딩은 대칭/반대칭, 역대칭 및 구성 관계를 모델링하는 기능을 유지하면서 더 나은 모델링 용량을 달성합니다.",
                    "tag": "3+4"
                },
                {
                    "index": "174-5",
                    "sentence": "Second, the graph context is integrated into distance scoring functions directly.",
                    "sentence_kor": "둘째, 그래프 컨텍스트는 거리 점수 매기기 함수에 직접 통합된다.",
                    "tag": "3"
                },
                {
                    "index": "174-6",
                    "sentence": "Specifically, graph context is explicitly modeled via two directed context representations.",
                    "sentence_kor": "특히 그래프 컨텍스트는 두 가지 방향 컨텍스트 표현을 통해 명시적으로 모델링됩니다.",
                    "tag": "3"
                },
                {
                    "index": "174-7",
                    "sentence": "Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively.",
                    "sentence_kor": "지식 그래프에 포함된 각 노드는 각각 인접한 송신 노드 및 수신 노드/에지에서 계산되는 두 가지 컨텍스트 표현으로 증강된다.",
                    "tag": "4"
                },
                {
                    "index": "174-8",
                    "sentence": "The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases.",
                    "sentence_kor": "제안된 접근방식은 어려운 N-to-1, 1-to-N 및 N-to-N 사례에 대한 예측 정확도를 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "174-9",
                    "sentence": "Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.",
                    "sentence_kor": "우리의 실험 결과는 그것이 FB15k-237과 WNRR-18, 특히 높은 차수 노드를 많이 가진 FB15k-237에서 두 가지 공통 벤치마크에서 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "774",
            "abstractID": "SPA_abs-175",
            "text": [
                {
                    "index": "175-0",
                    "sentence": "Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability.",
                    "sentence_kor": "대부분의 분류 모델은 먼저 모든 클래스에 대한 사후 확률 분포를 예측한 다음 추정 확률이 가장 큰 클래스를 선택하는 방식으로 작동합니다.",
                    "tag": "1"
                },
                {
                    "index": "175-1",
                    "sentence": "In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone.",
                    "sentence_kor": "그러나 많은 설정에서 사후 확률 자체의 품질(예: 당뇨병에 걸릴 확률 65%)은 최종 예측 등급보다 더 신뢰할 수 있는 정보를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "175-2",
                    "sentence": "When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications.",
                    "sentence_kor": "이러한 방법이 교정이 미흡한 것으로 나타난 경우 현재까지 대부분의 수정사항은 예측 확률을 재조정하지만 최종 분류에는 거의 영향을 미치지 않는 사후 교정에 의존해왔다.",
                    "tag": "1"
                },
                {
                    "index": "175-3",
                    "sentence": "Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.",
                    "sentence_kor": "여기서는 예측된 후방 확률과 경험적 후방 확률 간의 차이를 최소화하면서 목표를 직접 최적화하는 후방 보정(PosCal) 훈련이라고 하는 종단 간 훈련 절차를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "175-4",
                    "sentence": "We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives.",
                    "sentence_kor": "우리는 PosCal이 교정 오류를 줄일 뿐만 아니라 두 목표의 성능 하락에 불이익을 주어 작업 성능을 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "175-5",
                    "sentence": "Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline.",
                    "sentence_kor": "당사의 PosCal은 기준선에 비해 GLUE에서 작업 성과 이득의 약 2.5%, 교정 오류 감소의 약 16.1%를 달성한다(Wang et al., 2018).",
                    "tag": "4"
                },
                {
                    "index": "175-6",
                    "sentence": "We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline.",
                    "sentence_kor": "xSLUE(Kang and Hovy, 2019)에서 13.2% 교정 오류 감소로 유사한 작업 성능을 달성했지만 2단계 교정 기준선을 능가하지는 못했다.",
                    "tag": "4"
                },
                {
                    "index": "175-7",
                    "sentence": "PosCal training can be easily extendable to any types of classification tasks as a form of regularization term.",
                    "sentence_kor": "PosCal 교육은 정규화 기간의 한 형태로 모든 유형의 분류 작업으로 쉽게 확장할 수 있습니다.",
                    "tag": "5"
                },
                {
                    "index": "175-8",
                    "sentence": "Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.",
                    "sentence_kor": "또한 PosCal은 대규모 교육 세트를 효율적으로 사용할 수 있도록 훈련 과정 중 교정 목표에 필요한 통계를 점진적으로 추적한다는 장점이 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "775",
            "abstractID": "SPA_abs-176",
            "text": [
                {
                    "index": "176-0",
                    "sentence": "Text generation often requires high-precision output that obeys task-specific rules.",
                    "sentence_kor": "텍스트 생성에는 작업별 규칙을 준수하는 고정밀 출력이 필요한 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "176-1",
                    "sentence": "This fine-grained control is difficult to enforce with off-the-shelf deep learning models.",
                    "sentence_kor": "이러한 세분화된 제어는 기성 딥 러닝 모델에서는 시행하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "176-2",
                    "sentence": "In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach.",
                    "sentence_kor": "본 연구에서는 구조적 잠재 변수 접근방식을 통해 학습된 이산 제어 상태로 신경 생성 모델을 증강하는 것을 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "176-3",
                    "sentence": "Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model.",
                    "sentence_kor": "이 공식에 따르면, 작업별 지식은 모델에 효과적으로 훈련되는 풍부한 후방 제약 조건을 통해 인코딩될 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "176-4",
                    "sentence": "This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models.",
                    "sentence_kor": "이 접근 방식을 통해 사용자는 신경 생성 모델의 대표력을 희생하지 않고 사전 지식을 기반으로 내부 모델 결정을 내릴 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "176-5",
                    "sentence": "Experiments consider applications of this approach for text generation.",
                    "sentence_kor": "실험에서는 텍스트 생성을 위해 이 접근법의 적용을 고려한다.",
                    "tag": "4"
                },
                {
                    "index": "176-6",
                    "sentence": "We find that this method improves over standard benchmarks, while also providing fine-grained control.",
                    "sentence_kor": "우리는 이 방법이 표준 벤치마크에 비해 개선되는 동시에 세밀한 제어 기능을 제공한다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "776",
            "abstractID": "SPA_abs-177",
            "text": [
                {
                    "index": "177-0",
                    "sentence": "Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.",
                    "sentence_kor": "많은 작업에서 뛰어난 성능에도 불구하고, NLP 시스템은 작은 적대적 입력 섭동에 쉽게 속는다.",
                    "tag": "1"
                },
                {
                    "index": "177-1",
                    "sentence": "Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.",
                    "sentence_kor": "그러한 동요를 방어하기 위한 기존 절차는 (i) 경험적 접근으로 더 강력한 공격에 취약하거나 (ii) 최악의 경우 공격에 대한 견고성을 보장하지만 BERT와 같은 최첨단 모델과는 호환되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "177-2",
                    "sentence": "In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.",
                    "sentence_kor": "이 작업에서는 모델 아키텍처를 손상시키지 않고 견고성을 보장하는 간단한 프레임워크인 강력한 인코딩(RobEn)을 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "177-3",
                    "sentence": "The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.",
                    "sentence_kor": "RobEn의 핵심 구성 요소는 인코딩 함수이며, 문장을 더 작고 분리된 인코딩 공간에 매핑합니다.",
                    "tag": "3"
                },
                {
                    "index": "177-4",
                    "sentence": "Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks.",
                    "sentence_kor": "이러한 인코딩을 병목 현상으로 사용하는 시스템은 표준 교육을 통해 견고성을 보장하며, 동일한 인코딩을 여러 작업에 걸쳐 사용할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "177-5",
                    "sentence": "We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).",
                    "sentence_kor": "강력한 인코딩 기능을 구성하기 위한 두 가지 요구 사항을 식별한다. 문장의 동요는 작은 인코딩 집합(안정성)에 매핑되어야 하며 인코딩을 사용하는 모델은 여전히 잘 수행되어야 한다(충실성).",
                    "tag": "3"
                },
                {
                    "index": "177-6",
                    "sentence": "We instantiate RobEn to defend against a large family of adversarial typos.",
                    "sentence_kor": "우리는 RobEn의 많은 적대적 오타로부터 방어하기 위해 예를 들어 설명한다.",
                    "tag": "4"
                },
                {
                    "index": "177-7",
                    "sentence": "Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.",
                    "sentence_kor": "GLUE의 6개 작업에서 BERT와 짝을 이룬 RobEn 인스턴스화는 고려 대상 제품군의 모든 적대적 오타에 대해 평균 71.3%의 강력한 정확도를 달성하는 반면, 오타 수정기를 사용한 이전 연구는 단순 그리디 공격에 대해서는 35.3%의 정확도만 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "777",
            "abstractID": "SPA_abs-178",
            "text": [
                {
                    "index": "178-0",
                    "sentence": "BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA).",
                    "sentence_kor": "BERT(트랜스포머의 양방향 인코더 표현)와 관련 사전 훈련된 트랜스포머는 많은 언어 이해 작업에서 큰 이점을 제공하여 새로운 최첨단(SOTA)을 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "178-1",
                    "sentence": "BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction.",
                    "sentence_kor": "BERT는 마스킹 언어 모델과 다음 문장 예측이라는 두 가지 보조 작업에 대해 사전 교육을 받는다.",
                    "tag": "1"
                },
                {
                    "index": "178-2",
                    "sentence": "In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding.",
                    "sentence_kor": "본 논문에서 우리는 암기부터 이해까지 사전 훈련을 더 잘 정렬하기 위해 독해에서 영감을 얻은 새로운 사전 훈련 과제를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "178-3",
                    "sentence": "Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage.",
                    "sentence_kor": "SSPT(Span Selection PreTraining)는 클로즈와 같은 훈련 인스턴스를 제시하지만, 모델의 매개 변수에서 답을 도출하기보다는 관련 구절에서 선택한다.",
                    "tag": "3"
                },
                {
                    "index": "178-4",
                    "sentence": "We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets.",
                    "sentence_kor": "다중 MRC(Machine Reading Comprehension) 데이터 세트에서 BERT-BASE와 BERT-LARGE 모두에 비해 중요하고 일관된 개선 사항을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "178-5",
                    "sentence": "Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction.",
                    "sentence_kor": "특히, 제안된 모델은 새로운 벤치마크 MRC 데이터 세트인 자연적 질문에 대한 SOTA 결과를 얻어 단답 예측에서 BERT-LARGE를 3 F1점 앞서는 강력한 경험적 증거를 가지고 있다.",
                    "tag": "4"
                },
                {
                    "index": "178-6",
                    "sentence": "We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system.",
                    "sentence_kor": "또한 HotpotQA에서 중요한 영향을 보여 응답 예측 F1을 4점 개선하고 사실 예측 F1을 1점 지원하고 이전의 최고 시스템을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "178-7",
                    "sentence": "Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.",
                    "sentence_kor": "또한, 우리는 사전 훈련 접근법이 훈련 데이터가 제한될 때 특히 효과적이어서 학습 곡선을 크게 개선한다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "778",
            "abstractID": "SPA_abs-179",
            "text": [
                {
                    "index": "179-0",
                    "sentence": "Recently, NLP has seen a surge in the usage of large pre-trained models.",
                    "sentence_kor": "최근 NLP는 대규모 사전 교육 모델의 사용이 급증하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "179-1",
                    "sentence": "Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice.",
                    "sentence_kor": "사용자는 대규모 데이터 세트에서 사전 훈련된 모델의 가중치를 다운로드한 다음 선택한 작업에 대한 가중치를 미세 조정한다.",
                    "tag": "1"
                },
                {
                    "index": "179-2",
                    "sentence": "This raises the question of whether downloading untrusted pre-trained weights can pose a security threat.",
                    "sentence_kor": "이로 인해 신뢰할 수 없는 사전 훈련된 가중치를 다운로드하면 보안 위협이 발생할 수 있는지 여부에 대한 의문이 제기된다.",
                    "tag": "1"
                },
                {
                    "index": "179-3",
                    "sentence": "In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.",
                    "sentence_kor": "본 논문에서 우리는 사전 훈련된 가중치에 미세 조정 후 \"백도어\"를 노출시키는 취약성이 주입되어 공격자가 임의 키워드를 주입하기만 하면 모델 예측을 조작할 수 있는 \"중량 중독\" 공격을 구성할 수 있음을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "179-4",
                    "sentence": "We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure.",
                    "sentence_kor": "우리는 우리가 RIPPLe라고 부르는 정규화 방법과 임베딩 수술이라고 부르는 초기화 절차를 적용함으로써 데이터 세트와 미세 조정 절차에 대한 제한된 지식으로도 그러한 공격이 가능하다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "179-5",
                    "sentence": "Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.",
                    "sentence_kor": "정서 분류, 독성 탐지 및 스팸 탐지에 대한 우리의 실험은 이 공격이 광범위하게 적용 가능하고 심각한 위협을 가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "179-6",
                    "sentence": "Finally, we outline practical defenses against such attacks.",
                    "sentence_kor": "마지막으로 이러한 공격에 대한 실질적인 방어를 간략히 설명합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "779",
            "abstractID": "SPA_abs-180",
            "text": [
                {
                    "index": "180-0",
                    "sentence": "Transformers have gradually become a key component for many state-of-the-art natural language representation models.",
                    "sentence_kor": "트랜스포머는 점차 많은 최첨단 자연어 표현 모델의 핵심 구성요소가 되었다.",
                    "tag": "1"
                },
                {
                    "index": "180-1",
                    "sentence": "A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0.",
                    "sentence_kor": "최근 트랜스포머 기반 모델인 BERTachieved는 GLUE, SQuAD v1.1 및 SQuAD v2.0을 포함한 다양한 자연어 처리 작업에 대한 최첨단 결과를 얻었다.",
                    "tag": "1"
                },
                {
                    "index": "180-2",
                    "sentence": "This model however is computationally prohibitive and has a huge number of parameters.",
                    "sentence_kor": "그러나 이 모델은 계산적으로 금지되어 있으며 엄청난 수의 매개 변수를 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "180-3",
                    "sentence": "In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.",
                    "sentence_kor": "본 연구에서는 보다 가벼운 모델을 얻기 위한 노력으로 BERT의 아키텍처 선택을 재검토한다.",
                    "tag": "2"
                },
                {
                    "index": "180-4",
                    "sentence": "We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency.",
                    "sentence_kor": "매개 변수의 수를 줄이는 데 중점을 두지만 우리의 방법은 FLOP 또는 대기 시간과 같은 다른 목표에 적용될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "180-5",
                    "sentence": "We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers.",
                    "sentence_kor": "트랜스포머 인코더 레이어의 수를 줄이는 대신 알고리즘으로 선택한 올바른 아키텍처 설계 치수를 줄임으로써 훨씬 효율적인 라이트 BERT 모델을 얻을 수 있음을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "180-6",
                    "sentence": "In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.",
                    "sentence_kor": "특히, suBERT는 동일한 수의 매개 변수를 가지면서도 세 개의 인코더 레이어를 가진 BERT에 비해 GLUE 및 SQuAD 데이터 세트에서 6.6% 높은 평균 정확도를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "780",
            "abstractID": "SPA_abs-181",
            "text": [
                {
                    "index": "181-0",
                    "sentence": "We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model.",
                    "sentence_kor": "사전 훈련된 자기 회귀 모델에 의해 정의된 에너지를 최소화하기 위해 비 자기 회귀 기계 변환 모델을 교육할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "181-1",
                    "sentence": "In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy.",
                    "sentence_kor": "특히, 우리는 비 자기 회귀 변환 시스템을 자기 회귀 교사 에너지를 최소화하도록 훈련된 추론 네트워크(Tu 및 Gimpel, 2018)로 본다.",
                    "tag": "3"
                },
                {
                    "index": "181-2",
                    "sentence": "This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model.",
                    "sentence_kor": "이는 그러한 교사 모델의 빔 검색 출력으로 구성된 증류 말뭉치에 대한 비 자기 회귀 모델을 훈련하는 일반적인 접근 방식과 대조된다.",
                    "tag": "4"
                },
                {
                    "index": "181-3",
                    "sentence": "Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.",
                    "sentence_kor": "엔진(ENERGy 기반 추론 NEtworks)이라고 부르는 우리의 접근 방식은 IWSLT 2014 DE-EN 및 WMT 2016 RO-EN 데이터 세트에서 최첨단 비 자기 회귀 결과를 달성하여 자기 회귀 모델의 성능에 접근한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "781",
            "abstractID": "SPA_abs-182",
            "text": [
                {
                    "index": "182-0",
                    "sentence": "Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged.",
                    "sentence_kor": "지난 몇 년 동안 저자원 신경 기계 번역(NMT)에 대한 두 가지 유망한 연구 방향이 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "182-1",
                    "sentence": "The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT.",
                    "sentence_kor": "첫 번째는 고자원 언어를 활용하여 다국어 NMT를 통해 저자원 언어의 품질을 향상시키는 데 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "182-2",
                    "sentence": "The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data.",
                    "sentence_kor": "두 번째 방향은 사전 훈련 변환 모델을 자체 감독하는 단일 언어 데이터를 사용한 후 소량의 감독 데이터에 대한 미세 조정이 뒤따른다.",
                    "tag": "1"
                },
                {
                    "index": "182-3",
                    "sentence": "In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT.",
                    "sentence_kor": "이 연구에서 우리는 이 두 가지 연구 라인에 참여하고 다국어 NMT에서 자체 감독을 통해 단일 언어 데이터의 효과를 입증한다.",
                    "tag": "2"
                },
                {
                    "index": "182-4",
                    "sentence": "We offer three major results:",
                    "sentence_kor": "다음과 같은 세 가지 주요 결과를 제공합니다.",
                    "tag": "4"
                },
                {
                    "index": "182-5",
                    "sentence": "(i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models.",
                    "sentence_kor": "(i) 단일 언어 데이터를 사용하면 다국어 모델에서 저자원 언어의 번역 품질을 크게 향상시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "182-6",
                    "sentence": "(ii) Self-supervision improves zero-shot translation quality in multilingual models.",
                    "sentence_kor": "(ii) 자기 감독은 다국어 모델에서 제로샷 번역 품질을 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "182-7",
                    "sentence": "(iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.",
                    "sentence_kor": "(iii) 단일 언어 데이터를 자체 감독과 함께 활용함으로써 다국어 모델에 새로운 언어를 추가하여 병렬 데이터나 역번역 없이 최대 33개의 BLEU를 번역할 수 있는 실행 가능한 경로를 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "782",
            "abstractID": "SPA_abs-183",
            "text": [
                {
                    "index": "183-0",
                    "sentence": "Back-translation is a widely used data augmentation technique which leverages target monolingual data.",
                    "sentence_kor": "역번역은 대상 단일 언어 데이터를 활용하는 널리 사용되는 데이터 확대 기법이다.",
                    "tag": "1"
                },
                {
                    "index": "183-1",
                    "sentence": "However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese.",
                    "sentence_kor": "그러나 BLEU와 같은 자동 메트릭스는 소스 자체가 변환 또는 번역어인 테스트 예제에 대해서만 상당한 개선을 나타내기 때문에 그 효율성에 문제가 있다.",
                    "tag": "1"
                },
                {
                    "index": "183-2",
                    "sentence": "This is believed to be due to translationese inputs better matching the back-translated training data.",
                    "sentence_kor": "이는 역번역된 훈련 데이터와 더 잘 일치하는 변환 입력 때문인 것으로 여겨진다.",
                    "tag": "1"
                },
                {
                    "index": "183-3",
                    "sentence": "In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators.",
                    "sentence_kor": "이 연구에서, 우리는 이 추측이 경험적으로 뒷받침되지 않으며 역번역이 전문 인간 번역가에 따르면 자연발생 텍스트와 번역어 모두의 번역 품질을 향상시킨다는 것을 보여준다.",
                    "tag": "1+2"
                },
                {
                    "index": "183-4",
                    "sentence": "We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs.",
                    "sentence_kor": "우리는 역번역이 더 유창한 출력을 내기 때문에 인간에 의해 선호된다는 견해를 뒷받침하는 경험적 증거를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "183-5",
                    "sentence": "BLEU cannot capture human preferences because references are translationese when source sentences are natural text.",
                    "sentence_kor": "BLEU는 원본 문장이 자연 텍스트일 때 참고문헌은 번역어이기 때문에 인간의 선호도를 포착할 수 없다.",
                    "tag": "4"
                },
                {
                    "index": "183-6",
                    "sentence": "We recommend complementing BLEU with a language model score to measure fluency.",
                    "sentence_kor": "BLEU를 언어 모델 점수로 보완하여 유창성을 측정하는 것이 좋습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "783",
            "abstractID": "SPA_abs-184",
            "text": [
                {
                    "index": "184-0",
                    "sentence": "Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information.",
                    "sentence_kor": "적응형 정책은 현재 컨텍스트 정보를 기반으로 변환 품질과 지연 시간 간의 균형을 유연하게 조정할 수 있기 때문에 동시 변환에 대한 고정 정책보다 낫다.",
                    "tag": "1"
                },
                {
                    "index": "184-1",
                    "sentence": "But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies.",
                    "sentence_kor": "그러나 적응형 정책을 얻기 위한 이전의 방법은 복잡한 교육 프로세스에 의존하거나 단순한 고정 정책을 저하시킵니다.",
                    "tag": "1"
                },
                {
                    "index": "184-2",
                    "sentence": "We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies.",
                    "sentence_kor": "우리는 일련의 고정 정책의 단순한 경험적 구성을 통해 적응 정책을 달성하기 위한 알고리즘을 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "184-3",
                    "sentence": "Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.",
                    "sentence_kor": "중국어 -> 영어와 독일어 -> 영어에 대한 실험 결과, 우리의 적응 정책은 동일한 대기 시간에 대해 고정 정책을 최대 4 BLEU 포인트까지 능가할 수 있으며, 더 놀라운 것은 그리디 모드(및 빔 모드에 매우 근접함)에서 전체 문장 변환의 BLEU 점수를 훨씬 능가한다는 것이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "784",
            "abstractID": "SPA_abs-185",
            "text": [
                {
                    "index": "185-0",
                    "sentence": "Neural architectures are the current state of the art in Word Sense Disambiguation (WSD).",
                    "sentence_kor": "신경 아키텍처는 WSD(Word Sense Disambigization)의 최신 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "185-1",
                    "sentence": "However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB).",
                    "sentence_kor": "그러나 어휘 지식 기반(LKB)에 인코딩된 방대한 양의 관계 정보를 제한적으로 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "185-2",
                    "sentence": "We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.",
                    "sentence_kor": "우리는 신경 아키텍처 내에 LKB 그래프의 정보를 내장하여 풍부한 지식을 활용할 수 있는 신경 지도 아키텍처인 향상된 WSD 통합 싱셋 임베딩 및 관계(EWISER)를 제시한다. 이 아키텍처는 네트워크가 훈련되지 않은 싱셋을 예측할 수 있도록 한다. 세트",
                    "tag": "2"
                },
                {
                    "index": "185-3",
                    "sentence": "As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks.",
                    "sentence_kor": "그 결과, 우리는 모든 표준 모든 단어 영어 WSD 평가 벤치마크의 연결에 대한 80% 한도를 처음으로 돌파한 거의 모든 평가 설정에 새로운 최첨단 기술을 설정했다.",
                    "tag": "4"
                },
                {
                    "index": "185-4",
                    "sentence": "On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.",
                    "sentence_kor": "다국어 전체 단어 WSD에서 우리는 영어만 교육하여 최첨단 결과를 보고한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "785",
            "abstractID": "SPA_abs-186",
            "text": [
                {
                    "index": "186-0",
                    "sentence": "Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus.",
                    "sentence_kor": "큰 텍스트에 의존하는 중국어 NLP 애플리케이션은 종종 말뭉치에 희박한 엄청난 양의 어휘를 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "186-1",
                    "sentence": "We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics.",
                    "sentence_kor": "우리는 이념적 언어로 쓰여진 문자의 형태인 글리프가 풍부한 의미를 가질 수 있다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "186-2",
                    "sentence": "We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem.",
                    "sentence_kor": "중국어 어휘 외 단어 임베딩 문제를 해결하기 위해 다중 모델인 Glyph2Vec를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "186-3",
                    "sentence": "Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios.",
                    "sentence_kor": "Glyph2Vec는 단어 글리프에서 시각적 특징을 추출하여 어휘 외 단어 임베딩을 위한 현재 단어 임베딩 공간을 확장할 필요가 있으며, 이는 특히 저자원 시나리오의 중국어 NLP 시스템을 개선하는 데 유용하다.",
                    "tag": "5"
                },
                {
                    "index": "186-4",
                    "sentence": "Experiments across different applications show the significant effectiveness of our model.",
                    "sentence_kor": "서로 다른 애플리케이션에 걸친 실험은 우리 모델의 상당한 효과를 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "786",
            "abstractID": "SPA_abs-187",
            "text": [
                {
                    "index": "187-0",
                    "sentence": "We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures.",
                    "sentence_kor": "우리는 SVO(Subject-Verb-Object) 구조에서 발견되는 것과 같은 상호 관련된 단어 그룹 간의 연관성을 학습하기 위한 신경 프레임워크를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "187-1",
                    "sentence": "Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.",
                    "sentence_kor": "우리의 모델은 예를 들어 그럴듯한 SVO 구성의 벡터가 서로 가깝게 놓여 있는 공동 함수별 단어 벡터 공간을 유도한다.",
                    "tag": "3"
                },
                {
                    "index": "187-2",
                    "sentence": "The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure.",
                    "sentence_kor": "모델은 공동 공간에서도 워드 그룹 멤버십에 대한 정보를 유지하므로 SVO 구조를 추론하는 여러 작업에 효과적으로 적용될 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "187-3",
                    "sentence": "We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity.",
                    "sentence_kor": "우리는 선택 선호도 및 이벤트 유사성 추정 작업에 대한 최첨단 결과를 보고함으로써 제안된 프레임워크의 견고성과 다용성을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "187-4",
                    "sentence": "The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.",
                    "sentence_kor": "결과는 우리의 작업 독립 모델을 통해 학습된 표현 조합이 이전 작업의 작업별 아키텍처를 능가하는 동시에 매개 변수의 수를 최대 95%까지 감소시킨다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "787",
            "abstractID": "SPA_abs-188",
            "text": [
                {
                    "index": "188-0",
                    "sentence": "While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied.",
                    "sentence_kor": "자동 용어 추출이 잘 연구된 영역이지만, 기술 수준을 구별하기 위한 계산 접근법은 여전히 연구가 미흡하다.",
                    "tag": "1"
                },
                {
                    "index": "188-1",
                    "sentence": "We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction.",
                    "sentence_kor": "우리는 4개 영역에 걸쳐 독일의 기술성 금본위제를 반자동으로 만들고 웹으로 작성된 일반 언어 말뭉치가 기술성 예측에 미치는 영향을 설명한다.",
                    "tag": "1"
                },
                {
                    "index": "188-2",
                    "sentence": "When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.",
                    "sentence_kor": "일반 언어와 도메인별 단어 임베딩을 결합한 분류 접근방식을 정의할 때, 우리는 이전 작업을 넘어 벡터 공간을 정렬하여 비교 임베딩을 얻는다.",
                    "tag": "1"
                },
                {
                    "index": "188-3",
                    "sentence": "We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.",
                    "sentence_kor": "우리는 일반 비교 대 도메인별 비교를 활용하기 위한 두 가지 새로운 모델, 즉 사전 계산된 비교 임베딩 정보를 입력으로 사용하는 단순한 신경망 모델과 내부적으로 비교를 계산하는 다채널 모델을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "188-4",
                    "sentence": "Both models outperform previous approaches, with the multi-channel model performing best.",
                    "sentence_kor": "두 모델 모두 이전 접근 방식보다 성능이 뛰어나며, 다채널 모델의 성능이 가장 우수합니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "788",
            "abstractID": "SPA_abs-189",
            "text": [
                {
                    "index": "189-0",
                    "sentence": "Metaphor is a linguistic device in which a concept is expressed by mentioning another.",
                    "sentence_kor": "은유는 개념이 다른 개념을 언급함으로써 표현되는 언어적 장치이다.",
                    "tag": "1"
                },
                {
                    "index": "189-1",
                    "sentence": "Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics.",
                    "sentence_kor": "그러므로 은유적 표현을 식별하려면 의미론에 대한 비구성적 이해가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "189-2",
                    "sentence": "Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models.",
                    "sentence_kor": "반면, 다중 단어 표현(MWE)은 의미적 불투명성의 정도가 다양한 언어 현상으로, 그 식별은 계산 모델에 난제를 제기한다.",
                    "tag": "1"
                },
                {
                    "index": "189-3",
                    "sentence": "This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.",
                    "sentence_kor": "이 연구는 MWE의 존재를 모델에 알려 은유의 분류가 강화된 신경 구조의 설계를 통해 은유와 MWE의 상호작용을 분석하려는 첫 번째 시도이다.",
                    "tag": "1"
                },
                {
                    "index": "189-4",
                    "sentence": "To the best of our knowledge, this is the first “MWE-aware” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena.",
                    "sentence_kor": "우리가 아는 한, 이것은 이러한 현상의 복잡한 상호작용에 대한 추가 실험을 위한 길을 닦는 최초의 \"MWE 인식\" 은유 식별 시스템이다.",
                    "tag": "1"
                },
                {
                    "index": "189-5",
                    "sentence": "The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.",
                    "sentence_kor": "결과와 분석에 따르면 제안된 아키텍처는 두 개의 서로 다른 확립된 은유 데이터 세트에서 최첨단 구조에 도달한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "789",
            "abstractID": "SPA_abs-190",
            "text": [
                {
                    "index": "190-0",
                    "sentence": "Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language.",
                    "sentence_kor": "다국어 표현은 여러 언어의 단어를 하나의 의미 공간에 포함시켜 언어에 관계없이 유사한 의미를 가진 단어가 서로 근접하도록 한다.",
                    "tag": "1"
                },
                {
                    "index": "190-1",
                    "sentence": "These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language.",
                    "sentence_kor": "이러한 임베딩은 한 언어로 훈련된 자연어 처리(NLP) 모델이 다른 언어로 배치되는 교차 언어 전송과 같은 다양한 설정에서 널리 사용되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "190-2",
                    "sentence": "While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages.",
                    "sentence_kor": "언어 간 전송 기술은 강력하지만, 소스 언어에서 대상 언어로 성별 편향을 전달한다.",
                    "tag": "1"
                },
                {
                    "index": "190-3",
                    "sentence": "In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications.",
                    "sentence_kor": "본 논문에서 우리는 다국어 임베딩의 성별 편중과 그것이 NLP 애플리케이션의 전송 학습에 어떤 영향을 미치는지 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "190-4",
                    "sentence": "We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives.",
                    "sentence_kor": "편향 분석을 위한 다국어 데이터 세트를 만들고 내적 및 외적 관점 모두에서 다국어 표현의 편견을 수량화하는 몇 가지 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "190-5",
                    "sentence": "Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning.",
                    "sentence_kor": "실험 결과는 임베딩을 다른 대상 공간에 정렬할 때 다국어 표현의 편향 크기가 다르게 변하고 정렬 방향이 전송 학습의 편향에도 영향을 미칠 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "190-6",
                    "sentence": "We further provide recommendations for using the multilingual word representations for downstream tasks.",
                    "sentence_kor": "또한 다운스트림 작업에 다국어 단어 표현을 사용하기 위한 권장사항을 제공한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "790",
            "abstractID": "SPA_abs-191",
            "text": [
                {
                    "index": "191-0",
                    "sentence": "Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity).",
                    "sentence_kor": "대부분의 NLP 데이터 세트는 성별과 같은 보호된 속성으로 주석을 달지 않으므로 표준 공정성 측정(예: 기회 균등)을 사용하여 분류 편향을 측정하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "191-1",
                    "sentence": "However, manually annotating a large dataset with a protected attribute is slow and expensive.",
                    "sentence_kor": "그러나 보호된 특성을 가진 대규모 데이터 세트에 수동으로 주석을 다는 것은 느리고 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "191-2",
                    "sentence": "Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?",
                    "sentence_kor": "모든 예제에 주석을 다는 대신, 그 부분집합에 주석을 달고 그 표본을 사용하여 치우침을 추정할 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "191-3",
                    "sentence": "While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias.",
                    "sentence_kor": "그렇게 하는 것이 가능하지만 주석이 달린 표본이 작을수록 추정치가 실제 편향에 가깝다는 확신이 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "191-4",
                    "sentence": "In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval.",
                    "sentence_kor": "본 연구에서는 바이어스 추정치에 대한 불확실성을 신뢰 구간으로 나타내기 위해 번스타인 한계를 사용할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "191-5",
                    "sentence": "We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias.",
                    "sentence_kor": "우리는 이 방법으로 도출된 95% 신뢰 구간이 실제 편향을 일관되게 제한한다는 경험적 증거를 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "191-6",
                    "sentence": "In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim.",
                    "sentence_kor": "이러한 불확실성을 수량화함에 있어, 우리가 Bernstein-bounded 불공정이라고 부르는 우리의 방법은 두 가지 중 하나를 주장할 증거가 충분하지 않을 때 분류자가 편향되거나 편견 없는 것으로 간주되는 것을 방지하는 데 도움이 된다.",
                    "tag": "5"
                },
                {
                    "index": "191-7",
                    "sentence": "Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases.",
                    "sentence_kor": "우리의 연구 결과는 특정 편견을 측정하는 데 현재 사용되는 데이터 세트가 너무 작아서 가장 지독한 경우를 제외하고 편견을 단정적으로 식별하지 못한다는 것을 암시한다.",
                    "tag": "5"
                },
                {
                    "index": "191-8",
                    "sentence": "For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences – to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.",
                    "sentence_kor": "예를 들어, 성별-편향형 문장에서 5% 더 정확한 공동 참조 해결 시스템을 고려하자. 95% 신뢰도로 편향되었다고 주장하려면 사용 가능한 가장 큰 WinoBias보다 3.8배 더 큰 편향 특정 데이터 세트가 필요하다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "791",
            "abstractID": "SPA_abs-192",
            "text": [
                {
                    "index": "192-0",
                    "sentence": "We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.",
                    "sentence_kor": "인쇄된 초기 현대 문서에서 글리프 모양을 분석하기 위해 깊고 해석 가능한 확률적 생성 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "192-1",
                    "sentence": "We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance.",
                    "sentence_kor": "우리는 다중 교란 분산 소스가 있는 경우 추출된 글리프 이미지를 기본 템플릿으로 클러스터링하는 데 초점을 맞춘다.",
                    "tag": "3"
                },
                {
                    "index": "192-2",
                    "sentence": "Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing.",
                    "sentence_kor": "우리의 접근 방식은 먼저 인터퍼테이블 잠재 변수를 통해 템플릿 매개 변수에서 공간 섭동과 같은 잘 이해된 인쇄 현상을 생성한 다음 잉크 변동, 지터, 보관 프로세스의 노이즈 및 ot를 담당하는 해석 불가능한 잠재 벡터를 생성하여 결과를 수정하는 신경 편집기 모델을 도입한다.초기 근대 인쇄와 관련된 그녀의 예기치 못한 현상",
                    "tag": "3"
                },
                {
                    "index": "192-3",
                    "sentence": "Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures.",
                    "sentence_kor": "비판적으로, 관측치와 해석 가능한 수정 템플릿 사이의 시각적 잔류로 입력이 제한된 추론 네트워크를 도입함으로써, 우리는 벡터 값 잠재 변수가 포착하는 것을 제어하고 분리할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "192-4",
                    "sentence": "We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.",
                    "sentence_kor": "혼합 글꼴 문서에서 서체를 완전히 감독하지 않는 작업에서 우리의 접근 방식이 견고한 해석 가능한 클러스터링 기준선(c.f. Ocular)과 지나치게 유연한 심층 생성 모델(VAE)을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "792",
            "abstractID": "SPA_abs-193",
            "text": [
                {
                    "index": "193-0",
                    "sentence": "Pooling is an important technique for learning text representations in many neural NLP models.",
                    "sentence_kor": "풀링은 많은 신경 NLP 모델에서 텍스트 표현을 학습하는 데 중요한 기법이다.",
                    "tag": "1"
                },
                {
                    "index": "193-1",
                    "sentence": "In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features.",
                    "sentence_kor": "평균, 최대 및 주의 풀링과 같은 기존 풀링 방법에서 텍스트 표현은 입력 기능의 L1 또는 L' 표준의 가중치 합이다.",
                    "tag": "1"
                },
                {
                    "index": "193-2",
                    "sentence": "However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks.",
                    "sentence_kor": "그러나 풀링 규범은 항상 고정되어 있으며 서로 다른 작업에서 정확한 텍스트 표현을 학습하는 데 최적이 아닐 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "193-3",
                    "sentence": "In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited.",
                    "sentence_kor": "또한 최대 및 주의 풀링과 같은 많은 인기 있는 풀링 방법에서 일부 기능은 지나치게 강조될 수 있지만 다른 유용한 기능은 완전히 활용되지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "193-4",
                    "sentence": "In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.",
                    "sentence_kor": "본 논문에서 우리는 텍스트 표현을 위한 학습 가능한 규범을 포함한 주의 깊은 풀링(APLN) 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "193-5",
                    "sentence": "Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks.",
                    "sentence_kor": "고정 풀링 규범을 사용하는 기존 풀링 방법과는 달리, 서로 다른 작업에서 텍스트 표현을 위한 최적의 표준을 자동으로 찾기 위한 엔드 투 엔드 방식으로 표준을 학습할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "193-6",
                    "sentence": "In addition, we propose two methods to ensure the numerical stability of the model training.",
                    "sentence_kor": "또한 모델 훈련의 수치 안정성을 보장하기 위한 두 가지 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "193-7",
                    "sentence": "The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion.",
                    "sentence_kor": "첫 번째는 스케일 리미팅으로, 부정성을 보장하고 지수 폭발 위험을 완화하기 위해 입력을 다시 확장합니다.",
                    "tag": "3"
                },
                {
                    "index": "193-8",
                    "sentence": "The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation.",
                    "sentence_kor": "두 번째 것은 재공식인데, 이는 입력의 실제 값 파워를 계산하지 않고 풀링 연산을 더욱 가속화하기 위해 지수 연산을 분해한다.",
                    "tag": "3"
                },
                {
                    "index": "193-9",
                    "sentence": "Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.",
                    "sentence_kor": "4개의 벤치마크 데이터 세트에 대한 실험 결과는 우리의 접근 방식이 주의깊은 풀링의 성능을 효과적으로 개선할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "793",
            "abstractID": "SPA_abs-194",
            "text": [
                {
                    "index": "194-0",
                    "sentence": "Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks.",
                    "sentence_kor": "다중 작업 학습(MTL)과 전송 학습(TL)은 최첨단 신경망을 훈련할 때 데이터 부족 문제를 극복하기 위한 기술이다.",
                    "tag": "1"
                },
                {
                    "index": "194-1",
                    "sentence": "However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach.",
                    "sentence_kor": "그러나 MTL 또는 TL에 대한 유익한 보조 데이터 세트를 찾는 것은 시간과 자원을 많이 소모하는 시행착오 접근법이다.",
                    "tag": "1"
                },
                {
                    "index": "194-2",
                    "sentence": "We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups.",
                    "sentence_kor": "MTL 또는 TL 설정에 대한 유익한 보조 데이터를 식별하기 위해 시퀀스 태깅 데이터 세트의 유사성을 자동으로 평가하는 새로운 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "194-3",
                    "sentence": "Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel.",
                    "sentence_kor": "우리의 방법은 두 시퀀스 태그 데이터 세트 간의 유사성을 계산할 수 있으며, 동일한 태그 세트 또는 여러 레이블을 병렬로 주석을 달 필요가 없다.",
                    "tag": "4"
                },
                {
                    "index": "194-4",
                    "sentence": "Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work.",
                    "sentence_kor": "또한, 우리의 방법은 토큰과 토큰 레이블을 고려하는데, 이는 이전 작업에서 수행한 것처럼 토큰 중 하나를 정보 소스로만 사용하는 것보다 더 강력하다.",
                    "tag": "4"
                },
                {
                    "index": "194-5",
                    "sentence": "We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance.",
                    "sentence_kor": "우리는 우리의 유사성 척도가 MTL의 보조 데이터 세트를 사용하여 주요 작업 성능을 향상시키는 신경망의 테스트 점수 변화와 상관관계가 있음을 경험적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "194-6",
                    "sentence": "We provide an efficient, open-source implementation.",
                    "sentence_kor": "효율적인 오픈 소스 구현을 제공합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "794",
            "abstractID": "SPA_abs-195",
            "text": [
                {
                    "index": "195-0",
                    "sentence": "Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words.",
                    "sentence_kor": "선택적 메커니즘을 가진 자기 주의 네트워크(SAN)는 입력 단어의 하위 집합에 집중함으로써 다양한 NLP 작업에서 상당한 개선을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "195-1",
                    "sentence": "However, the underlying reasons for their strong performance have not been well explained.",
                    "sentence_kor": "하지만, 그들의 실적에 대한 근본적인 이유는 잘 설명되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "195-2",
                    "sentence": "In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax.",
                    "sentence_kor": "본 논문에서는 유연하고 보편적인 Gumbel-Softmax로 구현되는 선택적 SAN(Solective SAN)의 강점을 평가하여 격차를 해소한다.",
                    "tag": "2+3"
                },
                {
                    "index": "195-3",
                    "sentence": "Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs.",
                    "sentence_kor": "자연어 추론, 의미적 역할 라벨링 및 기계 번역을 포함한 몇 가지 대표적인 NLP 작업에 대한 실험 결과는 SSAN이 표준 SAN을 지속적으로 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "195-4",
                    "sentence": "Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling.",
                    "sentence_kor": "잘 설계된 탐색 실험을 통해 SSAN의 개선이 부분적으로 SAN의 두 가지 공통적인 취약점인 워드 오더 인코딩과 구조 모델링의 완화에 기인할 수 있음을 경험적으로 검증한다.",
                    "tag": "4"
                },
                {
                    "index": "195-5",
                    "sentence": "Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.",
                    "sentence_kor": "특히 선택적 메커니즘은 문장의 의미에 기여하는 내용 단어에 더 많은 주의를 기울임으로써 SAN을 개선한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "795",
            "abstractID": "SPA_abs-196",
            "text": [
                {
                    "index": "196-0",
                    "sentence": "Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.",
                    "sentence_kor": "다층 변압기 네트워크는 인터리브 셀프 어텐션 및 피드포워드 서브레이어로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "196-1",
                    "sentence": "Could ordering the sublayers in a different pattern lead to better performance?",
                    "sentence_kor": "하위 계층을 다른 패턴으로 정렬하면 성능이 향상될 수 있습니까?",
                    "tag": "1"
                },
                {
                    "index": "196-2",
                    "sentence": "We generate randomly ordered transformers and train them with the language modeling objective.",
                    "sentence_kor": "우리는 무작위로 정렬된 변압기를 생성하고 언어 모델링 목표를 가지고 변압기를 훈련시킨다.",
                    "tag": "2+3"
                },
                {
                    "index": "196-3",
                    "sentence": "We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.",
                    "sentence_kor": "우리는 이러한 모델 중 일부는 인터리브 기준선보다 더 나은 성능을 달성할 수 있으며, 성공적인 변형은 하단에는 더 많은 자기 주의력을, 상단에는 더 많은 피드포워드 서브레이어를 갖는 경향이 있음을 관찰한다.",
                    "tag": "4"
                },
                {
                    "index": "196-4",
                    "sentence": "We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.",
                    "sentence_kor": "우리는 샌드위치 변압기라는 이 속성을 고수하는 새로운 변압기 패턴을 제안하고 그것이 매개 변수, 메모리 또는 훈련 시간에 비용 없이 여러 단어 수준 및 문자 수준 언어 모델링 벤치마크에서 복잡성을 개선한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "196-5",
                    "sentence": "However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.",
                    "sentence_kor": "그러나 샌드위치 재주문 패턴이 기계 번역 모델에서 증명하듯이 모든 작업에 걸쳐 성능 향상을 보장하지는 않는다.",
                    "tag": "4"
                },
                {
                    "index": "196-6",
                    "sentence": "Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.",
                    "sentence_kor": "대신 추가 이득을 실현하기 위해 작업별 하위 계층 재정렬에 대한 추가 조사가 필요하다고 제안한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "796",
            "abstractID": "SPA_abs-197",
            "text": [
                {
                    "index": "197-0",
                    "sentence": "Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort.",
                    "sentence_kor": "모델 앙상블 기법은 종종 신경 네트워크에서 작업 성능을 향상시키지만, 시간, 메모리 및 관리 노력을 증가시켜야 한다.",
                    "tag": "1"
                },
                {
                    "index": "197-1",
                    "sentence": "In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.",
                    "sentence_kor": "본 연구에서는 모델 앙상블의 효과를 단일 모델로 복제하는 새로운 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "197-2",
                    "sentence": "Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors.",
                    "sentence_kor": "우리의 접근 방식은 K-구분 의사 태그와 K-구분 벡터를 사용하여 단일 매개 변수 공간 내에 K-가상 모델을 만든다.",
                    "tag": "3"
                },
                {
                    "index": "197-3",
                    "sentence": "Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.",
                    "sentence_kor": "여러 데이터 세트의 텍스트 분류 및 시퀀스 레이블링 작업에 대한 실험은 우리의 방법이 매개 변수의 1/K배 적은 기존 모델 앙상블을 모방하거나 능가한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "797",
            "abstractID": "SPA_abs-198",
            "text": [
                {
                    "index": "198-0",
                    "sentence": "Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity.",
                    "sentence_kor": "제로샷 학습은 훈련 중 보이지 않는 수업, 특히 유사성이 낮은 수업에 사용할 수 있는 라벨링 데이터가 없기 때문에 어려운 문제였다.",
                    "tag": "1"
                },
                {
                    "index": "198-1",
                    "sentence": "In this situation, transferring from seen classes to unseen classes is extremely hard.",
                    "sentence_kor": "이런 상황에서, 보이는 수업에서 보이지 않는 수업으로 옮기는 것은 매우 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "198-2",
                    "sentence": "To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data.",
                    "sentence_kor": "이 문제를 해결하기 위해 본 논문에서는 레이블이 없는 데이터를 효율적으로 활용하는 자체 교육 기반 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "198-3",
                    "sentence": "Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets.",
                    "sentence_kor": "기존의 자체 교육 방법은 고정 휴리스틱을 사용하여 레이블이 지정되지 않은 데이터에서 인스턴스를 선택하며, 데이터 세트마다 성능이 다르다.",
                    "tag": "3"
                },
                {
                    "index": "198-4",
                    "sentence": "We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection.",
                    "sentence_kor": "우리는 데이터 선택 전략을 자동으로 학습하고 보다 신뢰할 수 있는 선택을 제공하기 위한 강화 학습 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "198-5",
                    "sentence": "Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification",
                    "sentence_kor": "벤치마크와 실제 전자 상거래 데이터 세트에 대한 실험 결과는 우리의 접근 방식이 제로샷 텍스트 분류에서 이전 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "798",
            "abstractID": "SPA_abs-199",
            "text": [
                {
                    "index": "199-0",
                    "sentence": "Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.",
                    "sentence_kor": "다중 모드 신경 기계 번역(NMT)은 원본 문장을 이미지와 쌍으로 구성된 대상 언어로 변환하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "199-1",
                    "sentence": "However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.",
                    "sentence_kor": "그러나 지배적인 다중 모달 NMT 모델은 다중 모달 표현 학습을 세분화할 가능성이 있는 서로 다른 모달의 의미 단위 사이의 세분화된 의미 대응을 완전히 이용하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "199-2",
                    "sentence": "To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.",
                    "sentence_kor": "이 문제를 다루기 위해 본 논문에서는 NMT를 위한 새로운 그래프 기반 다중 모드 퓨전 인코더를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "199-3",
                    "sentence": "Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).",
                    "sentence_kor": "구체적으로, 우리는 먼저 다중 모달 의미 단위(단어와 시각적 객체) 사이의 다양한 의미 관계를 포착하는 통일된 다중 모달 그래프를 사용하여 입력 문장과 이미지를 나타낸다.",
                    "tag": "3"
                },
                {
                    "index": "199-4",
                    "sentence": "We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.",
                    "sentence_kor": "그런 다음 노드 표현을 학습하기 위해 의미적 상호 작용을 반복적으로 수행하는 여러 그래프 기반 다중 모달 퓨전 계층을 쌓는다.",
                    "tag": "3"
                },
                {
                    "index": "199-5",
                    "sentence": "Finally, these representations provide an attention-based context vector for the decoder.",
                    "sentence_kor": "마지막으로, 이러한 표현은 디코더에 대한 주의 기반 컨텍스트 벡터를 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "199-6",
                    "sentence": "We evaluate our proposed encoder on the Multi30K datasets.",
                    "sentence_kor": "우리는 Multi30K 데이터 세트에서 제안된 인코더를 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "199-7",
                    "sentence": "Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.",
                    "sentence_kor": "실험 결과와 심층 분석은 다중 모드 NMT 모델의 우수성을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "799",
            "abstractID": "SPA_abs-200",
            "text": [
                {
                    "index": "200-0",
                    "sentence": "Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest.",
                    "sentence_kor": "최근 병렬 말뭉치가 없는 비감독 이중 언어 어휘 유도(BLI)는 많은 연구 관심을 끌었다.",
                    "tag": "1"
                },
                {
                    "index": "200-1",
                    "sentence": "One of the crucial parts in methods for the BLI task is the matching procedure.",
                    "sentence_kor": "BLI 작업의 방법에서 중요한 부분 중 하나는 일치 절차이다.",
                    "tag": "1"
                },
                {
                    "index": "200-2",
                    "sentence": "Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings.",
                    "sentence_kor": "이전 연구는 일치에 너무 강한 제약을 가하고 많은 반직관적 번역 쌍을 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "200-3",
                    "sentence": "Thus We propose a relaxed matching procedure to find a more precise matching between two languages.",
                    "sentence_kor": "따라서 우리는 두 언어 간의 보다 정확한 일치를 찾기 위해 완화된 일치 절차를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "200-4",
                    "sentence": "We also find that aligning source and target language embedding space bidirectionally will bring significant improvement.",
                    "sentence_kor": "또한 소스 및 대상 언어 임베딩 공간을 양방향으로 정렬하면 상당한 개선을 가져올 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "200-5",
                    "sentence": "We follow the previous iterative framework to conduct experiments.",
                    "sentence_kor": "우리는 실험을 수행하기 위해 이전의 반복적인 프레임워크를 따른다.",
                    "tag": "3"
                },
                {
                    "index": "200-6",
                    "sentence": "Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.",
                    "sentence_kor": "표준 벤치마크 결과는 제안된 방법의 효과를 입증하며, 이는 이전의 감독되지 않은 방법을 크게 능가한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "800",
            "abstractID": "SPA_abs-201",
            "text": [
                {
                    "index": "201-0",
                    "sentence": "This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units.",
                    "sentence_kor": "이 논문은 문장을 하위 워드 단위로 토큰화하는 새로운 분할 알고리즘인 동적 프로그래밍 인코딩(DPE)을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "201-1",
                    "sentence": "We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference.",
                    "sentence_kor": "우리는 출력 문장의 하위 단어 세분화를 학습과 추론을 위해 배제해야 하는 잠재 변수로 본다.",
                    "tag": "3"
                },
                {
                    "index": "201-2",
                    "sentence": "A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability.",
                    "sentence_kor": "정확한 로그 한계우도 추정과 정확한 MAP 추론을 통해 최대 사후 확률을 가진 표적 분할을 찾을 수 있는 혼합 문자-하위 단어 변환기가 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "201-3",
                    "sentence": "DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.",
                    "sentence_kor": "DPE는 동적 프로그래밍을 사용하여 출력 문장을 분할하기 위해 병렬 데이터를 사전 처리하는 수단으로 경량 혼합 문자-서브워드 변압기를 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "201-4",
                    "sentence": "Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences.",
                    "sentence_kor": "기계 번역에 대한 경험적 결과는 DPE가 출력 문장을 분할하는 데 효과적이며 소스 문장의 확률적 분할을 위해 BPE 드롭아웃과 결합될 수 있음을 시사한다.",
                    "tag": "4"
                },
                {
                    "index": "201-5",
                    "sentence": "DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).",
                    "sentence_kor": "DPE는 영어 <=(독일, 루마니아, 에스토니아, 핀란드, 헝가리)를 포함한 여러 WMT 데이터 세트에서 BPE 대비 평균 0.9 BLEU 개선(Sennrich 등, 2016) 및 BPE 중퇴자 대비 평균 0.55 BLEU 개선(Provilkov 등, 2019)을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "801",
            "abstractID": "SPA_abs-202",
            "text": [
                {
                    "index": "202-0",
                    "sentence": "We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages.",
                    "sentence_kor": "소스 언어와 대상 언어 간의 단어 임베딩의 감독되지 않은 정렬을 학습하기 위한 새로운 다양체 기반 기하학적 접근방식을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "202-1",
                    "sentence": "Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices.",
                    "sentence_kor": "우리의 접근 방식은 이중 확률 행렬의 다양성에 대한 도메인 적응 문제로 정렬 학습 문제를 공식화한다.",
                    "tag": "3"
                },
                {
                    "index": "202-2",
                    "sentence": "This viewpoint arises from the aim to align the second order information of the two language spaces.",
                    "sentence_kor": "이 관점은 두 언어 공간의 2차 정보를 정렬하려는 목적에서 비롯된다.",
                    "tag": "3"
                },
                {
                    "index": "202-3",
                    "sentence": "The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation.",
                    "sentence_kor": "이중 확률적 다양체의 풍부한 기하학은 제안된 공식에 효율적인 리만 켤레 그라데이션 알고리즘을 사용할 수 있게 한다.",
                    "tag": "3"
                },
                {
                    "index": "202-4",
                    "sentence": "Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs.",
                    "sentence_kor": "경험적으로, 제안된 접근법은 여러 언어 쌍에 걸친 이중 언어 어휘 유도 과제에서 최첨단 최적 전송 기반 접근 방식을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "202-5",
                    "sentence": "The performance improvement is more significant for distant language pairs.",
                    "sentence_kor": "성능 향상은 원거리 언어 쌍에서 더 중요하다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "802",
            "abstractID": "SPA_abs-203",
            "text": [
                {
                    "index": "203-0",
                    "sentence": "Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process.",
                    "sentence_kor": "비 자기 회귀 신경 기계 변환(NAT)은 전체 목표 시퀀스를 동시에 예측하고 추론 프로세스를 크게 가속화한다.",
                    "tag": "1"
                },
                {
                    "index": "203-1",
                    "sentence": "However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing.",
                    "sentence_kor": "그러나 NAT은 문장의 종속성 정보를 폐기하므로 불가피하게 다중 양식 문제를 겪습니다. 대상 토큰은 토큰 반복 또는 누락의 원인이 되는 다른 가능한 변환에 의해 제공될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "203-2",
                    "sentence": "To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments.",
                    "sentence_kor": "이 문제를 완화하기 위해 새로운 반 자기 회귀 모델 복구를 제안한다.이 작업에서 SAT는 일련의 세그먼트로 변환을 생성합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "203-3",
                    "sentence": "The segments are generated simultaneously while each segment is predicted token-by-token.",
                    "sentence_kor": "각 세그먼트가 토큰별로 예측되는 동안 세그먼트가 동시에 생성됩니다.",
                    "tag": "3"
                },
                {
                    "index": "203-4",
                    "sentence": "By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors.",
                    "sentence_kor": "세그먼트 길이를 동적으로 결정하고 반복 세그먼트를 삭제함으로써 복구SAT는 반복되는 토큰 오류와 누락된 토큰 오류를 복구할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "203-5",
                    "sentence": "Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.",
                    "sentence_kor": "널리 사용되는 세 가지 벤치마크 데이터 세트에 대한 실험 결과는 제안된 모델이 해당 자기 회귀 모델과 비교 가능한 성능을 유지하면서 4배 이상의 속도 향상을 달성한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "803",
            "abstractID": "SPA_abs-204",
            "text": [
                {
                    "index": "204-0",
                    "sentence": "Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output.",
                    "sentence_kor": "모델 예측을 실제 정확도 측정과 동일하게 하는 것을 목표로 하는 신뢰 보정은 생성된 출력에서 변환 오류의 유용한 지표를 제공할 수 있기 때문에 신경 기계 번역(NMT)에 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "204-1",
                    "sentence": "While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference.",
                    "sentence_kor": "이전 연구에 따르면 라벨 평활로 훈련된 NMT 모델은 지상 실측 훈련 데이터에서 잘 보정되어 있지만, 우리는 훈련과 추론 사이의 불일치로 인해 추론 중에 오보정이 여전히 NMT에 심각한 난제로 남아 있다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "204-2",
                    "sentence": "By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models.",
                    "sentence_kor": "세 가지 언어 쌍에 대한 실험을 세심하게 설계함으로써, 우리의 연구는 교정과 번역 성능 사이의 상관관계뿐만 아니라 오보정의 언어적 특성에 대한 심층 분석을 제공하고 인간이 NMT 모델을 더 잘 분석, 이해 및 개선하는 데 도움이 될 수 있는 많은 흥미로운 발견을 보고한다.",
                    "tag": "2+3"
                },
                {
                    "index": "204-3",
                    "sentence": "Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.",
                    "sentence_kor": "이러한 관찰을 바탕으로 추론 보정 및 변환 성능을 모두 개선할 수 있는 새로운 등급 레이블 평활 방법을 추가로 제안한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "804",
            "abstractID": "SPA_abs-205",
            "text": [
                {
                    "index": "205-0",
                    "sentence": "We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task.",
                    "sentence_kor": "중국어 텍스트 스팸 탐지 작업의 불균형, 효율성 및 텍스트 위장 문제를 해결하기 위해 SIGNAL(Semi-superVised GeNative Active Learning) 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "205-1",
                    "sentence": "A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation.",
                    "sentence_kor": "주석 후보자의 \"가치\"를 측정하기 위한 \"자기 다양성\" 기준이 제안된다.",
                    "tag": "4"
                },
                {
                    "index": "205-2",
                    "sentence": "A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation.",
                    "sentence_kor": "데이터 확대를 위해 마스킹된 주의 학습 접근 방식을 가진 준감독 변형 자동 인코더와 문자 변형 그래프 강화 증강 절차가 제안된다.",
                    "tag": "4"
                },
                {
                    "index": "205-3",
                    "sentence": "The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task.",
                    "sentence_kor": "예비 실험은 제안된 SIGNAL 모델이 스팸 샘플 선택에 민감할 뿐만 아니라 중국 스팸 탐지 작업에 대한 일련의 기존 활성 학습 모델의 성능을 향상시킬 수 있음을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "205-4",
                    "sentence": "To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.",
                    "sentence_kor": "우리가 아는 한, 이것은 텍스트 스팸 탐지를 위해 능동 학습과 준지도 생성 학습을 통합하는 첫 번째 작업이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "805",
            "abstractID": "SPA_abs-206",
            "text": [
                {
                    "index": "206-0",
                    "sentence": "Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public.",
                    "sentence_kor": "LJP(Legal Judgement Prediction)는 소송 사실을 설명하는 텍스트가 주어지면 자동으로 소송의 판결 결과를 예측하는 작업으로, 사법 지원 시스템과 대중을 위한 편리한 서비스에 큰 가능성을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "206-1",
                    "sentence": "In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged.",
                    "sentence_kor": "실제로, 유사한 법률 조항에 적용되는 법률 사건은 쉽게 잘못 판단되기 때문에 혼란스러운 혐의가 종종 제시된다.",
                    "tag": "1"
                },
                {
                    "index": "206-2",
                    "sentence": "To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems.",
                    "sentence_kor": "이 문제를 해결하기 위해 기존 작업은 도메인 전문가에 크게 의존하므로 서로 다른 법률 시스템에서의 적용을 방해한다.",
                    "tag": "1"
                },
                {
                    "index": "206-3",
                    "sentence": "In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.",
                    "sentence_kor": "본 논문에서 우리는 LJP의 과제를 해결하기 위한 엔드 투 엔드 모델인 LADAN을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "206-4",
                    "sentence": "To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions.",
                    "sentence_kor": "혼란스러운 요금을 구별하기 위해, 우리는 혼란스러운 법률 기사 사이의 미묘한 차이를 자동으로 학습하고, 사실 설명에서 효과적인 차별적 특징을 주의 깊게 추출하기 위해 학습된 차이를 완전히 활용하는 새로운 주의 메커니즘을 설계하는 새로운 그래프 신경망 GDL을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "206-5",
                    "sentence": "Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.",
                    "sentence_kor": "실제 데이터 세트에 대해 수행된 실험은 우리의 LADAN의 우수성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "806",
            "abstractID": "SPA_abs-207",
            "text": [
                {
                    "index": "207-0",
                    "sentence": "Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think.",
                    "sentence_kor": "채용 공고를 잘 작성하는 것은 채용 과정에서 중요한 단계이지만, 그 일은 많은 사람들이 생각하는 것보다 종종 더 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "207-1",
                    "sentence": "It is challenging to specify the level of education, experience, relevant skills per the company information and job description.",
                    "sentence_kor": "회사 정보 및 직무 기술별로 교육, 경험, 관련 기술 수준을 명시하는 것은 어렵습니다.",
                    "tag": "1"
                },
                {
                    "index": "207-2",
                    "sentence": "To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions.",
                    "sentence_kor": "이를 위해 직무 설명에 따라 직무 요건을 생성하기 위한 조건부 텍스트 생성 문제로 캐스팅되는 JPG(Job Posting Generation)의 새로운 과제를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "207-3",
                    "sentence": "To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA.",
                    "sentence_kor": "이 작업을 처리하기 위해 SAMA라는 데이터 기반 글로벌 기술 인식 멀티 어텐션 생성 모델을 고안한다.",
                    "tag": "3"
                },
                {
                    "index": "207-4",
                    "sentence": "Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels.",
                    "sentence_kor": "특히, 입력과 출력 사이의 복잡한 매핑 관계를 모델링하기 위해, 우리는 계층적 디코더를 설계하는데, 이 디코더는 먼저 여러 기술로 작업 설명에 레이블을 붙인 다음 기술 레이블에 의해 안내되는 전체 텍스트를 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "207-5",
                    "sentence": "At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results.",
                    "sentence_kor": "동시에, 기술에 대한 사전 지식을 활용하기 위해, 우리는 기술에 대한 글로벌 사전 지식을 포착하고 생성된 결과를 다듬기 위한 기술 지식 그래프를 추가로 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "207-6",
                    "sentence": "The proposed approach is evaluated on real-world job posting data.",
                    "sentence_kor": "제안된 접근 방식은 실제 채용 공고 데이터에 대해 평가된다.",
                    "tag": "4"
                },
                {
                    "index": "207-7",
                    "sentence": "Experimental results clearly demonstrate the effectiveness of the proposed method.",
                    "sentence_kor": "실험 결과는 제안된 방법의 효과를 명확하게 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "807",
            "abstractID": "SPA_abs-208",
            "text": [
                {
                    "index": "208-0",
                    "sentence": "The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code.",
                    "sentence_kor": "국제질병분류(ICD)는 질병을 분류하는 표준화된 방법을 제공하며, 이 방법은 각 질병에 고유한 코드를 부여한다.",
                    "tag": "1"
                },
                {
                    "index": "208-1",
                    "sentence": "ICD coding aims to assign proper ICD codes to a medical record.",
                    "sentence_kor": "ICD 코딩은 의료 기록에 적절한 ICD 코드를 지정하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "208-2",
                    "sentence": "Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.",
                    "sentence_kor": "수동 코딩은 매우 힘들고 오류가 발생하기 쉽기 때문에 자동 ICD 코딩 작업에 많은 방법이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "208-3",
                    "sentence": "However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence.",
                    "sentence_kor": "그러나 대부분의 기존 방법은 두 가지 중요한 특성을 무시하고 각 코드를 독립적으로 예측한다. 코드 계층 및 코드 동시 발생.",
                    "tag": "1"
                },
                {
                    "index": "208-4",
                    "sentence": "In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem.",
                    "sentence_kor": "본 논문에서 우리는 위의 문제를 해결하기 위해 쌍곡선 및 공동 그래프 표현 방법(HyperCore)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "208-5",
                    "sentence": "Specifically, we propose a hyperbolic representation method to leverage the code hierarchy.",
                    "sentence_kor": "특히 코드 계층을 활용하기 위한 쌍곡 표현 방법을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "208-6",
                    "sentence": "Moreover, we propose a graph convolutional network to utilize the code co-occurrence.",
                    "sentence_kor": "또한 코드 동시 발생을 활용하기 위한 그래프 컨볼루션 네트워크를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "208-7",
                    "sentence": "Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.",
                    "sentence_kor": "널리 사용되는 두 데이터 세트에 대한 실험 결과는 제안된 모델이 이전의 최첨단 방법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "808",
            "abstractID": "SPA_abs-209",
            "text": [
                {
                    "index": "209-0",
                    "sentence": "Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling).",
                    "sentence_kor": "심층 신경망은 높은 수준의 특징을 추출하는 데 효과적이지만, 분류 방법은 일반적으로 단순한 특징 집계 작업(예: 풀링)을 통해 입력을 벡터 표현으로 인코딩한다.",
                    "tag": "1"
                },
                {
                    "index": "209-1",
                    "sentence": "Such operations limit the performance.",
                    "sentence_kor": "이러한 작업은 성능을 제한한다.",
                    "tag": "1"
                },
                {
                    "index": "209-2",
                    "sentence": "For instance, a multi-label document may contain several concepts.",
                    "sentence_kor": "예를 들어, 다중 레이블 문서에는 몇 가지 개념이 포함될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "209-3",
                    "sentence": "In this case, one vector can not sufficiently capture its salient and discriminative content.",
                    "sentence_kor": "이 경우, 하나의 벡터가 그 두드러진 차별적 내용을 충분히 포착할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "209-4",
                    "sentence": "Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.",
                    "sentence_kor": "따라서, 우리는 두 가지 장점을 가진 다중 라벨 분류(MLC)를 위한 쌍곡 캡슐 네트워크(HyperCaps)를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "209-5",
                    "sentence": "First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents.",
                    "sentence_kor": "첫째, 쌍곡 캡슐은 각 레이블에 대해 세분화된 문서 정보를 캡처하도록 설계되었으며, 레이블과 문서 간의 복잡한 구조를 특성화할 수 있는 기능을 가지고 있다.",
                    "tag": "4"
                },
                {
                    "index": "209-6",
                    "sentence": "Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks.",
                    "sentence_kor": "둘째, 쌍곡 동적 라우팅(HDR)은 레이블 인식 방식으로 쌍곡 캡슐을 집계하기 위해 도입되어 레이블 수준의 차별 정보가 신경망의 깊이를 따라 보존될 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "209-7",
                    "sentence": "To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing.",
                    "sentence_kor": "대규모 MLC 데이터 세트를 효율적으로 처리하기 위해 라우팅 중에 캡슐 번호를 적응적으로 조정하는 새로운 라우팅 방법을 추가로 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "209-8",
                    "sentence": "Extensive experiments are conducted on four benchmark datasets.",
                    "sentence_kor": "4개의 벤치마크 데이터 세트에 대해 광범위한 실험이 수행된다.",
                    "tag": "4"
                },
                {
                    "index": "209-9",
                    "sentence": "Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.",
                    "sentence_kor": "최신 방법과 비교하여 HyperCaps는 특히 꼬리 레이블에서 MLC의 성능을 크게 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "809",
            "abstractID": "SPA_abs-210",
            "text": [
                {
                    "index": "210-0",
                    "sentence": "They typically contain user descriptions of the problem, the setup, and steps for attempted resolution.",
                    "sentence_kor": "여기에는 일반적으로 문제에 대한 사용자 설명, 설정 및 해결 시도 단계가 포함됩니다.",
                    "tag": "1"
                },
                {
                    "index": "210-1",
                    "sentence": "Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces.",
                    "sentence_kor": "또한 명령 출력, 코드 조각, 오류 메시지 또는 스택 추적과 같은 다양한 비자연어 텍스트 요소를 포함합니다.",
                    "tag": "1"
                },
                {
                    "index": "210-2",
                    "sentence": "These elements contain potentially crucial information for problem resolution.",
                    "sentence_kor": "이러한 요소에는 문제 해결에 잠재적으로 중요한 정보가 포함되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "210-3",
                    "sentence": "However, they cannot be correctly parsed by tools designed for natural language.",
                    "sentence_kor": "그러나 자연어용으로 설계된 도구로는 올바르게 구문 분석할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "210-4",
                    "sentence": "In this paper, we address the problem of segmentation for technical support questions.",
                    "sentence_kor": "본 논문에서는 기술 지원 질문에 대한 세분화 문제를 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "210-5",
                    "sentence": "We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches.",
                    "sentence_kor": "우리는 이 문제를 시퀀스 라벨링 과제로 공식화하고 최첨단 접근법의 성능을 연구한다.",
                    "tag": "3"
                },
                {
                    "index": "210-6",
                    "sentence": "We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach.",
                    "sentence_kor": "우리는 이것을 직관적인 상황별 문장 수준 분류 기준선과 최첨단 감독 텍스트 세분화 접근법과 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "210-7",
                    "sentence": "We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model.",
                    "sentence_kor": "또한 서로 다른 데이터 소스에서 사전 훈련된 여러 언어 모델의 상황별 임베딩을 결합하는 새로운 구성 요소를 도입하여 사전 훈련된 단일 언어 모델의 임베딩 사용에 비해 현저한 개선을 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "210-8",
                    "sentence": "Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.",
                    "sentence_kor": "마지막으로, 우리는 또한 응답 검색의 다운스트림 작업에 대한 개선을 통해 이러한 세분화의 유용성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "810",
            "abstractID": "SPA_abs-211",
            "text": [
                {
                    "index": "211-0",
                    "sentence": "The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability.",
                    "sentence_kor": "자동 텍스트 기반 진단은 정확성과 해석성 사이의 적절한 균형을 요구하기 때문에 임상 사용에 있어 여전히 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "211-1",
                    "sentence": "In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system.",
                    "sentence_kor": "본 논문에서, 우리는 정확하지만 해석 가능한 진단 시스템을 구축하기 위해 베이지안 네트워크 앙상블을 엔티티 인식 컨볼루션 신경망(CNN) 위에 쌓아 올리는 새로운 프레임워크를 도입하여 해결책을 제안하려고 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "211-2",
                    "sentence": "The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare.",
                    "sentence_kor": "제안된 프레임워크는 AI 기반 의료에 중요한 베이지안 네트워크의 해석 가능성뿐만 아니라 심층 신경망의 높은 정확성과 일반성을 활용한다.",
                    "tag": "4"
                },
                {
                    "index": "211-3",
                    "sentence": "The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.",
                    "sentence_kor": "병원의 실제 전자 의료 기록(EMR) 문서에 대해 수행되고 전문 의사가 주석을 단 평가는 제안된 프레임워크가 정확도 성능에서 이전의 자동 진단 방법을 능가하며 프레임워크의 진단 설명이 합리적임을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "811",
            "abstractID": "SPA_abs-212",
            "text": [
                {
                    "index": "212-0",
                    "sentence": "In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis.",
                    "sentence_kor": "최근 몇 년 동안 텍스트 감정 분석 분야에서 감정 원인 쌍 추출(ECPE)이라는 새로운 흥미로운 작업이 등장했다.",
                    "tag": "1"
                },
                {
                    "index": "212-1",
                    "sentence": "It aims at extracting the potential pairs of emotions and their corresponding causes in a document.",
                    "sentence_kor": "그것은 문서에서 잠재적인 감정 쌍과 그에 상응하는 원인을 추출하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "212-2",
                    "sentence": "To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes.",
                    "sentence_kor": "이 과제를 해결하기 위해 기존 연구는 2단계 프레임워크를 채택했는데, 먼저 개인의 감정 세트와 원인 세트를 추출한 후 그에 상응하는 감정과 원인을 짝을 지어준다.",
                    "tag": "1"
                },
                {
                    "index": "212-3",
                    "sentence": "However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step.",
                    "sentence_kor": "그러나 이러한 두 단계의 파이프라인에는 몇 가지 고유한 결함이 있다. 1) 모델링은 최종 감정 원인 쌍을 직접 추출하는 것을 목표로 하지 않는다. 2) 첫 번째 단계의 오류는 두 번째 단계의 성능에 영향을 미칠 것이다.",
                    "tag": "1"
                },
                {
                    "index": "212-4",
                    "sentence": "To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.",
                    "sentence_kor": "이러한 단점을 해결하기 위해 본 논문에서 우리는 감정 원인 쌍을 2D 표현 체계로 나타내는 ECPE-Two-Dimensional(ECPE-2D)이라는 새로운 엔드 투 엔드 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "212-5",
                    "sentence": "A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs.",
                    "sentence_kor": "다른 감정 원인 쌍의 상호작용을 모델링하기 위해 2D 변압기 모듈과 두 가지 변형인 창문 제한 및 교차로 2D 변압기가 추가로 제안된다.",
                    "tag": "3"
                },
                {
                    "index": "212-6",
                    "sentence": "The 2D representation, interaction, and prediction are integrated into a joint framework.",
                    "sentence_kor": "2D 표현, 상호작용 및 예측은 공동 프레임워크로 통합됩니다.",
                    "tag": "3"
                },
                {
                    "index": "212-7",
                    "sentence": "In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.",
                    "sentence_kor": "공동 모델링의 장점 외에도 벤치마크 감정 원인 말뭉치에 대한 실험 결과는 우리의 접근방식이 최첨단 기술의 F1 점수를 61.28%에서 68.89%로 향상시킨다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "812",
            "abstractID": "SPA_abs-213",
            "text": [
                {
                    "index": "213-0",
                    "sentence": "Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document.",
                    "sentence_kor": "감정 원인 쌍 추출은 주어진 문서에서 원인 절과 결합된 모든 감정 절을 추출하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "213-1",
                    "sentence": "Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.",
                    "sentence_kor": "이전 연구는 첫 번째 단계가 감정 절을 추출하고 절을 개별적으로 유발하고 두 번째 단계는 분류기를 훈련시켜 부정적인 쌍을 걸러내는 2단계 접근법을 채택한다.",
                    "tag": "1"
                },
                {
                    "index": "213-2",
                    "sentence": "However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well.",
                    "sentence_kor": "그러나 감정 원인 쌍 추출을 위한 파이프라인 스타일 시스템은 오류 전파를 겪고 두 단계가 서로 잘 적응하지 못할 수 있기 때문에 차선책이다.",
                    "tag": "1"
                },
                {
                    "index": "213-3",
                    "sentence": "In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction.",
                    "sentence_kor": "본 논문에서는 순위 지정 관점, 즉 문서에서 절 쌍 후보 순위를 매기는 관점에서 감정 원인 쌍 추출을 다루고 종단 간 추출을 수행하기 위한 절간 모델링을 강조하는 1단계 신경 접근법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "213-4",
                    "sentence": "It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking.",
                    "sentence_kor": "그래프 주의로 절 표현을 학습하기 위해 문서의 절 사이의 상호 관계를 모델링하고 효과적인 순위를 위한 커널 기반 상대 위치 임베딩으로 절 쌍 표현을 강화한다.",
                    "tag": "3"
                },
                {
                    "index": "213-5",
                    "sentence": "Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.",
                    "sentence_kor": "실험 결과는 우리의 접근 방식이 특히 한 문서에서 여러 쌍을 추출하는 조건에서 현재의 2단계 시스템을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "813",
            "abstractID": "SPA_abs-214",
            "text": [
                {
                    "index": "214-0",
                    "sentence": "We present a simple but effective method for aspect identification in sentiment analysis.",
                    "sentence_kor": "우리는 감정 분석에서 측면 식별을 위한 간단하지만 효과적인 방법을 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "214-1",
                    "sentence": "Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages.",
                    "sentence_kor": "우리의 감독되지 않은 방법은 단어 임베딩과 POS 태그만 필요로 하므로 새로운 도메인 및 언어에 적용하기 쉽다.",
                    "tag": "3"
                },
                {
                    "index": "214-2",
                    "sentence": "We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable.",
                    "sentence_kor": "우리는 RBF 커널을 기반으로 하는 새로운 단일 헤드 주의 메커니즘인 대비 주의(CAT)를 도입하여 성능을 크게 향상시키고 모델을 해석할 수 있게 한다.",
                    "tag": "3"
                },
                {
                    "index": "214-3",
                    "sentence": "Previous work relied on syntactic features and complex neural models.",
                    "sentence_kor": "이전 연구는 통사적 특징과 복잡한 신경 모델에 의존했다.",
                    "tag": "3"
                },
                {
                    "index": "214-4",
                    "sentence": "We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed.",
                    "sentence_kor": "우리는 측면 추출을 위한 현재 벤치마크 데이터 세트의 단순성을 고려할 때, 그러한 복잡한 모델이 필요하지 않음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "214-5",
                    "sentence": "The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.",
                    "sentence_kor": "본 논문에서 보고된 실험 재현 코드는 https://github.com/clips/cat에서 확인할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "814",
            "abstractID": "SPA_abs-215",
            "text": [
                {
                    "index": "215-0",
                    "sentence": "Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target.",
                    "sentence_kor": "자세 감지는 주어진 대상에 대한 의견 있는 텍스트의 태도를 분류하는 것을 목표로 하는 중요한 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "215-1",
                    "sentence": "Remarkable success has been achieved when sufficient labeled training data is available.",
                    "sentence_kor": "라벨이 부착된 교육 데이터가 충분할 때 주목할 만한 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "215-2",
                    "sentence": "However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets.",
                    "sentence_kor": "그러나, 충분한 데이터에 주석을 다는 것은 노동 집약적이어서, 새로운 목표를 가진 데이터에 대한 자세 분류기를 일반화하는 데 상당한 장벽이 된다.",
                    "tag": "1"
                },
                {
                    "index": "215-3",
                    "sentence": "In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets.",
                    "sentence_kor": "본 논문에서 우리는 외부 지식(의미적 및 감정적 사전)을 가교로 사용하여 서로 다른 대상에 대한 지식 전달을 가능하게 하는 교차 대상 자세 탐지를 위한 의미론적-감정적 지식 전달(SEKT) 모델을 제안했다.",
                    "tag": "1+2"
                },
                {
                    "index": "215-4",
                    "sentence": "Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags.",
                    "sentence_kor": "특히, 시맨틱-감정 이종 그래프는 외부 시맨틱 및 감정 어휘로 구성되며, 그런 다음 그래프 컨볼루션 네트워크에 공급되어 단어와 감정 태그 사이의 다중 홉 의미 연결을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "215-5",
                    "sentence": "Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell.",
                    "sentence_kor": "그런 다음, 소스 도메인과 대상 도메인 간의 격차를 해소하는 사전 지식 역할을 하는 학습된 의미-감정 그래프 표현은 새로운 지식 인식 메모리 유닛을 BiLSTM 셀에 추가하여 양방향 단기 메모리(BiLSTM) 자세 분류기에 완전히 통합된다.",
                    "tag": "3"
                },
                {
                    "index": "215-6",
                    "sentence": "Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.",
                    "sentence_kor": "대규모 실제 데이터 세트에 대한 광범위한 실험은 최첨단 기준 방법에 대한 SEKT의 우수성을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "815",
            "abstractID": "SPA_abs-216",
            "text": [
                {
                    "index": "216-0",
                    "sentence": "Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis.",
                    "sentence_kor": "교차 도메인 정서 분석은 감정 분석을 사용하는 서로 다른 애플리케이션 간의 도메인 격차를 해소해야 할 필요성 때문에 최근 몇 년 동안 상당한 관심을 받아왔다.",
                    "tag": "1"
                },
                {
                    "index": "216-1",
                    "sentence": "In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge.",
                    "sentence_kor": "본 논문에서, 우리는 외부 상식 지식의 역할을 탐구함으로써 이 과제에 대한 새로운 관점을 취한다.",
                    "tag": "2"
                },
                {
                    "index": "216-2",
                    "sentence": "We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts.",
                    "sentence_kor": "우리는 ConceptNet 지식 그래프를 활용하여 도메인별 및 도메인 일반 배경 개념을 모두 제공하여 문서의 의미를 풍부하게 하는 새로운 프레임워크 KinGDOM을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "216-3",
                    "sentence": "These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner.",
                    "sentence_kor": "이러한 개념은 도메인 불변 방식으로 도메인 간 개념을 활용하는 그래프 컨볼루션 자동 인코더를 교육함으로써 학습된다.",
                    "tag": "3"
                },
                {
                    "index": "216-4",
                    "sentence": "Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.",
                    "sentence_kor": "이러한 학습된 개념으로 인기 있는 도메인-대수 기준 방법을 조건화하면 최첨단 접근 방식에 비해 성능을 향상시켜 제안된 프레임워크의 효과를 입증하는 데 도움이 된다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "816",
            "abstractID": "SPA_abs-217",
            "text": [
                {
                    "index": "217-0",
                    "sentence": "The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification.",
                    "sentence_kor": "양상 기반 정서 분석(ABSA)은 양상 추출과 양상 정서 분류라는 두 가지 개념 작업으로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "217-1",
                    "sentence": "Rather than considering the tasks separately, we build an end-to-end ABSA solution.",
                    "sentence_kor": "과제를 별도로 고려하는 대신, 우리는 엔드 투 엔드 ABSA 솔루션을 구축한다.",
                    "tag": "1+2"
                },
                {
                    "index": "217-2",
                    "sentence": "Previous works in ABSA tasks did not fully leverage the importance of syntactical information.",
                    "sentence_kor": "ABSA 과제의 이전 연구들은 구문 정보의 중요성을 완전히 활용하지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "217-3",
                    "sentence": "Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms.",
                    "sentence_kor": "따라서 측면 추출 모델은 종종 다중 단어 측면 용어의 경계를 감지하지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "217-4",
                    "sentence": "On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words.",
                    "sentence_kor": "반면, 양상 정서 분류기는 양상 용어와 문맥 단어 사이의 구문적 상관관계를 설명할 수 없었다.",
                    "tag": "1"
                },
                {
                    "index": "217-5",
                    "sentence": "This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning.",
                    "sentence_kor": "이 논문은 문장의 문법적 측면을 탐구하고 구문론적 학습을 위한 자기 주의 메커니즘을 채택한다.",
                    "tag": "1+2"
                },
                {
                    "index": "217-6",
                    "sentence": "We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor.",
                    "sentence_kor": "음성 부분 임베딩, 의존성 기반 임베딩 및 상황별 임베딩(예: BERT, RoBERTa)을 결합하여 측면 추출기의 성능을 향상시킨다.",
                    "tag": "3"
                },
                {
                    "index": "217-7",
                    "sentence": "We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms.",
                    "sentence_kor": "우리는 또한 측면 용어와 약한 구문 연결을 가지면서 관련이 없는 단어의 역효과를 강조하기 위한 구문 상대 거리를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "217-8",
                    "sentence": "This increases the accuracy of the aspect sentiment classifier.",
                    "sentence_kor": "이것은 측면 감정 분류기의 정확도를 증가시킨다.",
                    "tag": "4"
                },
                {
                    "index": "217-9",
                    "sentence": "Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.",
                    "sentence_kor": "당사의 솔루션은 두 하위 작업 모두에서 SemEval-2014 데이터 세트의 최첨단 모델을 능가합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "817",
            "abstractID": "SPA_abs-218",
            "text": [
                {
                    "index": "218-0",
                    "sentence": "Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews.",
                    "sentence_kor": "측면 기반 정서 분석은 온라인 리뷰에서 특정 측면에 대한 정서 극성을 결정하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "218-1",
                    "sentence": "Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words.",
                    "sentence_kor": "대부분의 최근 노력은 측면을 의견 단어와 암시적으로 연결하기 위해 주의 기반 신경망 모델을 채택한다.",
                    "tag": "1"
                },
                {
                    "index": "218-2",
                    "sentence": "However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections.",
                    "sentence_kor": "그러나 언어의 복잡성과 한 문장의 여러 측면의 존재로 인해, 이러한 모델들은 종종 연결을 혼동한다.",
                    "tag": "1"
                },
                {
                    "index": "218-3",
                    "sentence": "In this paper, we address this problem by means of effective encoding of syntax information.",
                    "sentence_kor": "본 논문에서는 구문 정보의 효과적인 인코딩을 통해 이 문제를 다룬다.",
                    "tag": "2+3"
                },
                {
                    "index": "218-4",
                    "sentence": "Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree.",
                    "sentence_kor": "첫째, 일반적인 종속성 구문 분석 트리를 재구성하고 정리하여 대상 측면에 뿌리를 둔 통일된 측면 지향 종속성 트리 구조를 정의한다.",
                    "tag": "3"
                },
                {
                    "index": "218-5",
                    "sentence": "Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.",
                    "sentence_kor": "그런 다음 감정 예측을 위한 새로운 트리 구조를 인코딩하기 위한 관계형 그래프 주의 네트워크(R-GAT)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "218-6",
                    "sentence": "Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.",
                    "sentence_kor": "SemEval 2014 및 Twitter 데이터 세트에 대한 광범위한 실험이 수행되며, 실험 결과는 측면과 의견 단어 사이의 연결이 우리의 접근 방식으로 더 잘 확립될 수 있음을 확인시켜 주며, 그 결과 그래프 주의 네트워크(GAT)의 성능이 크게 향상되었다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "818",
            "abstractID": "SPA_abs-219",
            "text": [
                {
                    "index": "219-0",
                    "sentence": "Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA).",
                    "sentence_kor": "측면 용어 추출과 의견 용어 추출은 세분화된 측면 기반 정서 분석(ABSA)의 두 가지 핵심 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "219-1",
                    "sentence": "The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems.",
                    "sentence_kor": "측면-의견 쌍은 소비자 및 의견 마이닝 시스템을 위한 제품 또는 서비스에 대한 글로벌 프로파일을 제공할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "219-2",
                    "sentence": "However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms.",
                    "sentence_kor": "그러나 기존 방법은 주어진 측면 용어 또는 의견 용어 없이 측면-의견 쌍을 직접 출력할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "219-3",
                    "sentence": "Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs.",
                    "sentence_kor": "두 항을 공동으로 추출하기 위해 최근의 일부 공동 추출 방법이 제안되었지만 쌍으로 추출하지는 못한다.",
                    "tag": "1"
                },
                {
                    "index": "219-4",
                    "sentence": "To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE).",
                    "sentence_kor": "이를 위해 본 논문은 쌍방향 및 의견 용어 추출(PAOTE) 과제를 해결하기 위한 엔드 투 엔드 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "219-5",
                    "sentence": "Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works.",
                    "sentence_kor": "또한 본 논문은 대부분의 이전 연구에서 수행한 시퀀스 태그 공식보다는 공동 용어 및 관계 추출의 관점에서 문제를 다룬다.",
                    "tag": "3"
                },
                {
                    "index": "219-6",
                    "sentence": "We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries.",
                    "sentence_kor": "용어들이 범위 경계의 감독 하에 추출되는 공유 범위를 기반으로 하는 다중 작업 학습 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "219-7",
                    "sentence": "Meanwhile, the pair-wise relations are jointly identified using the span representations.",
                    "sentence_kor": "한편, 쌍별 관계는 범위 표현을 사용하여 공동으로 식별된다.",
                    "tag": "4"
                },
                {
                    "index": "219-8",
                    "sentence": "Extensive experiments show that our model consistently outperforms state-of-the-art methods.",
                    "sentence_kor": "광범위한 실험에 따르면 우리 모델은 지속적으로 최첨단 방법을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "819",
            "abstractID": "SPA_abs-220",
            "text": [
                {
                    "index": "220-0",
                    "sentence": "Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”.",
                    "sentence_kor": "의견 역할 라벨링(ORL)은 세분화된 의견 분석 과제이며 \"누가 어떤 종류의 감정을 표현했는가?\"에 답하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "220-1",
                    "sentence": "Due to the scarcity of labeled data, ORL remains challenging for data-driven methods.",
                    "sentence_kor": "레이블링된 데이터가 부족하기 때문에 ORL은 데이터 기반 방법에서 여전히 어려운 과제입니다.",
                    "tag": "1"
                },
                {
                    "index": "220-2",
                    "sentence": "In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations.",
                    "sentence_kor": "본 연구에서는 서로 다른 표현을 비교하고 통합함으로써 통사적 지식으로 신경 ORL 모델을 강화하려고 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "220-3",
                    "sentence": "We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels.",
                    "sentence_kor": "또한 서로 다른 처리 수준에서 파서 정보를 인코딩하기 위해 종속 그래프 컨볼루션 네트워크(DEPGCN)를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "220-4",
                    "sentence": "In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously.",
                    "sentence_kor": "파서 부정확성을 보완하고 오류 전파를 줄이기 위해 파서와 ORL 모델을 동시에 교육하는 MTL(Multi-Task Learning)을 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "220-5",
                    "sentence": "We verify our methods on the benchmark MPQA corpus.",
                    "sentence_kor": "벤치마크 MPQA 말뭉치에서 방법을 검증한다.",
                    "tag": "3"
                },
                {
                    "index": "220-6",
                    "sentence": "The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline.",
                    "sentence_kor": "실험 결과는 구문 정보가 ORL에 매우 유용하다는 것을 보여주며, 우리의 최종 MTL 모델은 구문 애그노스틱 기준선보다 F1 점수를 9.29 효과적으로 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "220-7",
                    "sentence": "In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT).",
                    "sentence_kor": "또한 통사적 지식의 기여가 상황별 단어 표현(BERT)과 완전히 겹치지 않는다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "220-8",
                    "sentence": "Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.",
                    "sentence_kor": "우리의 최고 모델은 현재의 최첨단 모델보다 4.34 더 높은 F1 점수를 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "820",
            "abstractID": "SPA_abs-221",
            "text": [
                {
                    "index": "221-0",
                    "sentence": "State-of-the-art argument mining studies have advanced the techniques for predicting argument structures.",
                    "sentence_kor": "최첨단 인수 마이닝 연구는 인수 구조를 예측하는 기술을 발전시켰다.",
                    "tag": "1"
                },
                {
                    "index": "221-1",
                    "sentence": "However, the technology for capturing non-tree-structured arguments is still in its infancy.",
                    "sentence_kor": "그러나 트리 구조화되지 않은 인수를 포착하는 기술은 아직 초기 단계에 있다.",
                    "tag": "1"
                },
                {
                    "index": "221-2",
                    "sentence": "In this paper, we focus on non-tree argument mining with a neural model.",
                    "sentence_kor": "본 논문에서 우리는 신경 모델을 사용한 비 트리 인수 마이닝에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "221-3",
                    "sentence": "We jointly predict proposition types and edges between propositions.",
                    "sentence_kor": "우리는 제안 간의 제안 유형과 가장자리를 공동으로 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "221-4",
                    "sentence": "Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges.",
                    "sentence_kor": "제안된 모델은 (i) 일련의 제안을 효과적으로 인코딩하는 작업별 매개 변수화(TSP)와 (ii) 가장자리로 구성된 비 트리 인수를 예측할 수 있는 제안 수준 바이아핀 주의(PLBA)를 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "221-5",
                    "sentence": "Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.",
                    "sentence_kor": "실험 결과에 따르면 TSP와 PLBA 부스트 에지 예측 성능은 기준선과 비교된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "821",
            "abstractID": "SPA_abs-222",
            "text": [
                {
                    "index": "222-0",
                    "sentence": "We propose a novel linearization of a constituent tree, together with a new locally normalized model.",
                    "sentence_kor": "우리는 새로운 국소 정규화 모델과 함께 구성 트리의 새로운 선형화를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "222-1",
                    "sentence": "For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them.",
                    "sentence_kor": "문장의 각 분할점에 대해 모델은 해당 분할점으로 끝나는 모든 범위의 정규화기를 계산한 다음 해당 분할점으로부터 트리 범위를 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "222-2",
                    "sentence": "Compared with global models, our model is fast and parallelizable.",
                    "sentence_kor": "글로벌 모델과 비교하여 당사의 모델은 빠르고 병렬화가 가능합니다.",
                    "tag": "4"
                },
                {
                    "index": "222-3",
                    "sentence": "Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.",
                    "sentence_kor": "이전의 로컬 모델과 달리 선형화 방법은 스팬에 직접 연결되며 스팬 예측을 수행할 때 더 많은 로컬 특징을 고려하므로 더 해석 가능하고 효과적이다.",
                    "tag": "4"
                },
                {
                    "index": "222-4",
                    "sentence": "Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.",
                    "sentence_kor": "PTB(95.8 F1)와 CTB(92.4 F1)에 대한 실험에 따르면 우리 모델은 기존 로컬 모델을 크게 능가하고 글로벌 모델과 경쟁적인 결과를 효율적으로 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "822",
            "abstractID": "SPA_abs-223",
            "text": [
                {
                    "index": "223-0",
                    "sentence": "We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.",
                    "sentence_kor": "우리는 구문 분석 문제를 일련의 포인팅 작업으로 캐스팅하는 새로운 구성 구문 분석 모델을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "223-1",
                    "sentence": "Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span.",
                    "sentence_kor": "특히, 우리 모델은 범위의 경계 단어에 해당하는 포인팅 점수를 통해 범위가 합법적인 트리 구성 요소일 가능성을 추정한다.",
                    "tag": "3"
                },
                {
                    "index": "223-2",
                    "sentence": "Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference.",
                    "sentence_kor": "구문 분석 모델은 효율적인 하향식 디코딩을 지원하며 학습 목표는 값비싼 CCY 추론에 의존하지 않고 구조적 일관성을 적용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "223-3",
                    "sentence": "The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity.",
                    "sentence_kor": "표준 English Penn Treebank 구문 분석 작업에 대한 실험을 통해 사전 훈련된 모델을 사용하지 않고도 92.78 F1을 달성했으며, 이는 유사한 시간 복잡성을 가진 기존의 모든 방법보다 높은 수치이다.",
                    "tag": "4"
                },
                {
                    "index": "223-4",
                    "sentence": "Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.",
                    "sentence_kor": "사전 교육된 BERT를 사용하여 모델은 95.48 F1을 달성하여 최신 기술과 경쟁하면서도 속도가 더 빠릅니다. 또한 우리의 접근 방식은 다국어 선거구 구문 분석에 대한 SPMRL 공유 작업에서 바스크어와 스웨덴어의 새로운 최첨단 기술을 확립한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "823",
            "abstractID": "SPA_abs-224",
            "text": [
                {
                    "index": "224-0",
                    "sentence": "In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.",
                    "sentence_kor": "딥 러닝(DL) 시대에 구문 분석 모델은 상황에 맞는 다중 계층 BiLSTM의 놀라운 기능 덕분에 성능에 거의 영향을 미치지 않고 매우 단순하다.",
                    "tag": "1"
                },
                {
                    "index": "224-1",
                    "sentence": "As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.",
                    "sentence_kor": "높은 효율성과 성능으로 인해 가장 널리 사용되는 그래프 기반 의존성 분석기로서, 바이아핀 분석기는 아크 인자화 가정 하에서 단일 의존성을 직접 채점하고 매우 단순한 로컬 토큰별 교차 엔트로피 훈련 손실을 채택한다.",
                    "tag": "1"
                },
                {
                    "index": "224-2",
                    "sentence": "This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.",
                    "sentence_kor": "본 논문은 바이아핀 파서에 대한 2차 트리CRF 확장을 처음으로 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "224-3",
                    "sentence": "For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.",
                    "sentence_kor": "오랫동안 내부 외부 알고리즘의 복잡성과 비효율성은 TreeCRF의 인기를 방해한다.",
                    "tag": "3"
                },
                {
                    "index": "224-4",
                    "sentence": "To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.",
                    "sentence_kor": "이 문제를 해결하기 위해 GPU에서 직접 대형 매트릭스 연산을 위한 내부 및 비테르비 알고리즘을 일괄화하고 효율적인 역전파를 통해 복잡한 외부 알고리즘을 피하는 효과적인 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "224-5",
                    "sentence": "Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.",
                    "sentence_kor": "13개 언어의 27개 데이터 세트에 대한 실험과 분석은 구조적 학습(글로벌 TreeCRF 손실) 및 고차 모델링과 같은 DL 시대 이전에 개발된 기술이 여전히 유용하며, 특히 부분적으로 주석이 달린 훈련 데이터의 경우 최첨단 비아핀 파서보다 구문 분석 성능을 더욱 향상시킬 수 있음을 분명히 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "224-6",
                    "sentence": "We release our code at https://github.com/yzhangcs/crfpar.",
                    "sentence_kor": "https://github.com/yzhangcs/crfpar에서 코드를 공개합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "824",
            "abstractID": "SPA_abs-225",
            "text": [
                {
                    "index": "225-0",
                    "sentence": "Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks.",
                    "sentence_kor": "시퀀스 기반 신경망은 구문 구조에 상당한 민감도를 보이지만 여전히 트리 기반 네트워크보다 구문 작업에서 성능이 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "225-1",
                    "sentence": "Such tree-based networks can be provided with a constituency parse, a dependency parse, or both.",
                    "sentence_kor": "이러한 트리 기반 네트워크는 구성 요소 구문 분석, 종속성 구문 분석 또는 둘 다와 함께 제공될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "225-2",
                    "sentence": "We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task.",
                    "sentence_kor": "우리는 이 두 가지 표현 체계 중 어떤 것이 주체-동사 합의 예측 과제의 성과를 향상시키는 구문 구조에 대한 편견을 더 효과적으로 도입하는지 평가한다.",
                    "tag": "2"
                },
                {
                    "index": "225-3",
                    "sentence": "We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement.",
                    "sentence_kor": "우리는 구성 기반 네트워크가 의존성 기반 네트워크보다 더 강력하게 일반화되며, 두 가지 유형의 구조를 결합해도 추가적인 개선을 산출하지 못한다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "225-4",
                    "sentence": "Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.",
                    "sentence_kor": "마지막으로, 우리는 생성된 데이터의 양을 미세 조정함으로써 순차 모델의 구문 강건성을 상당히 개선할 수 있음을 보여주며, 순차 모델이 결여된 구문 편향을 전달하기 위해 데이터 증가가 명시적 구성 구조에 대한 실행 가능한 대안임을 시사한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "825",
            "abstractID": "SPA_abs-226",
            "text": [
                {
                    "index": "226-0",
                    "sentence": "Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages.",
                    "sentence_kor": "다국어 시퀀스 라벨링은 여러 언어에 대해 단일 통일 모델을 사용하여 레이블 시퀀스를 예측하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "226-1",
                    "sentence": "Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages.",
                    "sentence_kor": "다중 단일 언어 모델에 의존하는 것에 비해 다국어 모델을 사용하는 것은 모델 크기가 작고 온라인 서비스가 더 용이하며 저자원 언어에 대한 일반화 가능성이 있다는 장점이 있다.",
                    "tag": "1"
                },
                {
                    "index": "226-2",
                    "sentence": "However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations.",
                    "sentence_kor": "그러나 현재 다국어 모델은 모델 용량 제한으로 인해 개별 단일 언어 모델의 성능이 여전히 크게 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "226-3",
                    "sentence": "In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).",
                    "sentence_kor": "본 논문에서 우리는 여러 단일 언어 모델(교사)의 구조적 지식을 통일된 다국어 모델(학생)에 증설하여 단일 언어 모델과 통합 다국어 모델 사이의 격차를 줄일 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "226-4",
                    "sentence": "We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions.",
                    "sentence_kor": "우리는 구조 수준 정보를 기반으로 하는 두 가지 새로운 KD 방법을 제안한다. (1) 학생과 교사의 구조 수준 확률 분포 사이의 거리를 대략 최소화하고, (2) 구조 수준 지식을 지역 분포로 집계하고, 두 지역 확률 분포 사이의 거리를 최소화한다.",
                    "tag": "3"
                },
                {
                    "index": "226-5",
                    "sentence": "Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.",
                    "sentence_kor": "25개의 데이터 세트가 있는 4개의 다국어 작업에 대한 우리의 실험은 우리의 접근 방식이 몇 가지 강력한 기준선을 능가하고 기준 모델과 교사 모델보다 더 강력한 제로샷 일반화 가능성을 가지고 있다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "826",
            "abstractID": "SPA_abs-227",
            "text": [
                {
                    "index": "227-0",
                    "sentence": "Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner.",
                    "sentence_kor": "소셜 미디어 콘텐츠의 트렌드 주제는 시간이 지남에 따라 진화하며, 따라서 소셜 미디어 사용자와 소셜 미디어 사용자의 대인관계 커뮤니케이션을 역동적으로 이해하는 것이 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "227-1",
                    "sentence": "Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests.",
                    "sentence_kor": "여기서는 사용자가 진화하는 관심사를 충족하는 대화에 참여할 수 있도록 동적 온라인 대화 권장 사항을 연구합니다.",
                    "tag": "2"
                },
                {
                    "index": "227-2",
                    "sentence": "While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time.",
                    "sentence_kor": "대부분의 이전 작업은 정적 사용자 이익을 가정하지만, 우리의 모델은 사용자 관심의 시간적 측면을 포착하고 훈련 시간 동안 보이지 않는 미래 대화를 추가로 처리할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "227-3",
                    "sentence": "Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter.",
                    "sentence_kor": "구체적으로, 우리는 시간이 지남에 따라 사용자 상호 작용과 관심사의 변화를 이용하여 그들이 어떤 논의를 시작할지 예측하는 신경 아키텍처를 제안한다.",
                    "tag": "3+4"
                },
                {
                    "index": "227-4",
                    "sentence": "We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests.",
                    "sentence_kor": "우리는 대규모 Reddit 대화 모음에 대한 실험을 수행하며, 세 개의 하위 레딧에 대한 결과는 우리 모델이 사용자의 관심사를 정적으로 가정하는 최첨단 모델을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "227-5",
                    "sentence": "We further evaluate on handling “cold start”, and observe consistently better performance by our model when considering various degrees of sparsity of user’s chatting history and conversation contexts.",
                    "sentence_kor": "우리는 \"콜드 스타트\" 처리에 대해 추가로 평가하고 사용자의 채팅 내역과 대화 맥락의 다양한 희소성을 고려할 때 모델에 의해 지속적으로 더 나은 성능을 관찰한다.",
                    "tag": "3+4"
                },
                {
                    "index": "227-6",
                    "sentence": "Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.",
                    "sentence_kor": "마지막으로, 모델 출력에 대한 분석은 사용자 관심 변화를 나타내며, 접근 방식의 장점과 효과를 설명한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "827",
            "abstractID": "SPA_abs-228",
            "text": [
                {
                    "index": "228-0",
                    "sentence": "In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.",
                    "sentence_kor": "본 논문에서는 소셜 미디어 게시물에 대한 다중 모드 명명 개체 인식(MNER)을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "228-1",
                    "sentence": "Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context.",
                    "sentence_kor": "MNER에 대한 기존 접근법은 주로 두 가지 단점을 겪는다. (1) 단어 인식 시각적 표현을 생성함에도 불구하고 단어 표현은 시각적 맥락에 둔감하다. (2) 이들 대부분은 시각적 맥락에 의해 초래되는 편견을 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "228-2",
                    "sentence": "To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations.",
                    "sentence_kor": "첫 번째 문제를 해결하기 위해 이미지 인식 단어 표현과 단어 인식 시각적 표현을 모두 얻기 위한 다중 모드 상호 작용 모듈을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "228-3",
                    "sentence": "To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions.",
                    "sentence_kor": "시각적 편향을 완화하기 위해 순수 텍스트 기반 엔티티 스팬 감지를 보조 모듈로 활용하고, 엔티티 스팬 예측과 함께 최종 예측을 안내하는 통합 멀티모달 트랜스포머를 설계할 것을 추가로 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "228-4",
                    "sentence": "Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.",
                    "sentence_kor": "실험에 따르면 우리의 통합 접근 방식은 두 개의 벤치마크 데이터 세트에서 새로운 최첨단 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "828",
            "abstractID": "SPA_abs-229",
            "text": [
                {
                    "index": "229-0",
                    "sentence": "Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements.",
                    "sentence_kor": "주가를 더 잘 처리하기 위해 뉴스 기사를 통합했던 이전 연구들은 가격 변동을 예측하기 위해 다양한 신경망을 사용했다.",
                    "tag": "1"
                },
                {
                    "index": "229-1",
                    "sentence": "The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction.",
                    "sentence_kor": "텍스트와 가격 정보는 모두 신경망에 암호화되어 있으므로, 가격 예측이라는 악명 높은 어려운 문제의 원래 프레임워크가 아닌 상황에서는 이 접근방식을 적용하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "229-2",
                    "sentence": "In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding.",
                    "sentence_kor": "대조적으로, 본 논문은 주식 임베딩이라고 불리는 주식의 벡터 표현을 통해 뉴스 기사의 영향을 인코딩하는 방법을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "229-3",
                    "sentence": "The stock embedding is acquired with a deep learning framework using both news articles and price history.",
                    "sentence_kor": "주식 임베딩은 뉴스 기사와 가격 이력을 모두 사용하여 딥 러닝 프레임워크로 획득된다.",
                    "tag": "3"
                },
                {
                    "index": "229-4",
                    "sentence": "Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction.",
                    "sentence_kor": "임베딩은 벡터의 운영 형태를 취하기 때문에 가격 예측 외에 다른 재정 문제에 적용할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "229-5",
                    "sentence": "As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data.",
                    "sentence_kor": "한 가지 예로, 우리는 로이터 & 블룸버그 헤드라인을 사용하여 포트폴리오 최적화 결과를 보여주는데, 이는 주가 데이터만을 사용하는 기준 방법으로 얻은 자본 이득보다 2.8배나 큰 것이다.",
                    "tag": "4"
                },
                {
                    "index": "229-6",
                    "sentence": "This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.",
                    "sentence_kor": "이는 제안된 주식 임베딩이 텍스트 재무 의미론을 활용하여 재무 예측 문제를 해결할 수 있음을 시사한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "829",
            "abstractID": "SPA_abs-230",
            "text": [
                {
                    "index": "230-0",
                    "sentence": "Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction.",
                    "sentence_kor": "전체 뉴스 매체의 정치적 편견과 보도의 사실성을 예측하는 것은 미디어 프로파일링의 중요한 요소이며, 미디어 프로파일링은 연구가 부족하지만 점점 더 중요한 연구 방향이다.",
                    "tag": "1"
                },
                {
                    "index": "230-1",
                    "sentence": "The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically.",
                    "sentence_kor": "온라인 상에서 가짜, 편향 및 선전성 콘텐츠의 현재 확산 수준으로 인해 의심스러운 모든 주장을 수동 또는 자동으로 사실 확인하기가 불가능해졌다.",
                    "tag": "1"
                },
                {
                    "index": "230-2",
                    "sentence": "Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content.",
                    "sentence_kor": "따라서, 전체 뉴스 매체를 프로파일링하고 가짜 또는 편향된 내용을 게재할 가능성이 있는 매체를 찾는 것이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "230-3",
                    "sentence": "This makes it possible to detect likely “fake news” the moment they are published, by simply checking the reliability of their source.",
                    "sentence_kor": "이것은 단순히 출처에 대한 신뢰성을 확인함으로써 그들이 발행되는 순간 \"가짜 뉴스\"를 탐지하는 것을 가능하게 한다.",
                    "tag": "1"
                },
                {
                    "index": "230-4",
                    "sentence": "From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context.",
                    "sentence_kor": "실질적인 관점에서, 정치적 편견과 보도의 사실성은 언어적 측면뿐만 아니라 사회적 맥락도 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "230-5",
                    "sentence": "Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium’s audience on social media).",
                    "sentence_kor": "여기서는 (i) 작성된 내용(즉, 대상 매체에 의해 게시된 내용 및 트위터에서 자신을 설명하는 방법)과 (ii) 읽는 사람의 영향(즉, 대상 매체의 청중을 소셜 미디어에 분석하는 것)을 모두 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "230-6",
                    "sentence": "We further study (iii) what was written about the target medium (in Wikipedia).",
                    "sentence_kor": "(iii) 목표 매체에 대해 작성된 내용을 추가로 연구합니다(위키피디아).",
                    "tag": "2"
                },
                {
                    "index": "230-7",
                    "sentence": "The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.",
                    "sentence_kor": "평가 결과는 작성된 내용이 가장 중요하다는 것을 보여주며, 우리는 또한 모든 정보 소스를 결합하는 것이 현재의 최신 기술에 비해 큰 향상을 가져온다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "830",
            "abstractID": "SPA_abs-231",
            "text": [
                {
                    "index": "231-0",
                    "sentence": "We explore the utilities of explicit negative examples in training neural language models.",
                    "sentence_kor": "우리는 신경 언어 모델을 훈련할 때 명백한 부정적인 예제의 효용성을 탐구한다.",
                    "tag": "1"
                },
                {
                    "index": "231-1",
                    "sentence": "Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks.",
                    "sentence_kor": "여기서 부정적인 예들은 *개가 짖는 것과 같은 문장의 잘못된 단어들이다.",
                    "tag": "1"
                },
                {
                    "index": "231-2",
                    "sentence": "Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.",
                    "sentence_kor": "신경 언어 모델은 일반적으로 훈련 데이터의 문장 집합인 긍정적인 예에 대해서만 훈련되지만, 최근 연구에 따르면 이러한 방식으로 훈련된 모델은 장거리 합의와 같은 복잡한 구문 구조를 강력하게 처리할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "231-3",
                    "sentence": "In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity.",
                    "sentence_kor": "이 논문에서 우리는 먼저 특정 구성(예: 주어-동사 합의)에 대한 부정적 예제를 적절히 사용하면 영어에서 모델의 견고성이 향상되고 복잡성이 무시해도 된다는 것을 입증한다.",
                    "tag": "2"
                },
                {
                    "index": "231-4",
                    "sentence": "The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word.",
                    "sentence_kor": "우리의 성공의 열쇠는 정확한 단어와 틀린 단어 사이의 추가적인 여백 손실이다.",
                    "tag": "3"
                },
                {
                    "index": "231-5",
                    "sentence": "We then provide a detailed analysis of the trained models.",
                    "sentence_kor": "그런 다음 훈련된 모델에 대한 자세한 분석을 제공합니다.",
                    "tag": "3"
                },
                {
                    "index": "231-6",
                    "sentence": "One of our findings is the difficulty of object-relative clauses for RNNs.",
                    "sentence_kor": "우리의 연구 결과 중 하나는 RNN에 대한 객체 상대 조항의 어려움이다.",
                    "tag": "4"
                },
                {
                    "index": "231-7",
                    "sentence": "We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause.",
                    "sentence_kor": "우리는 우리의 직접 학습 신호에도 불구하고 모델이 여전히 객체-상대적 조항에 대한 합의를 해결하는데 어려움을 겪고 있다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "231-8",
                    "sentence": "Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses.",
                    "sentence_kor": "구성을 포함하는 훈련 문장의 확대가 어느 정도 도움이 되지만, 정확도는 여전히 주제-상대적 조항의 수준에 도달하지 못한다.",
                    "tag": "4"
                },
                {
                    "index": "231-9",
                    "sentence": "Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.",
                    "sentence_kor": "직접적으로 인지적으로 호소력은 없지만, 우리의 방법은 어려운 언어 구조에 대한 신경 모델의 진정한 구조적 한계를 분석하는 도구가 될 수 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "831",
            "abstractID": "SPA_abs-232",
            "text": [
                {
                    "index": "232-0",
                    "sentence": "We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors.",
                    "sentence_kor": "우리는 자연스러운 문법 오류에 직면했을 때 사전 훈련된 언어 인코더(ELMo, BERT 및 RoBERTa)의 동작을 진단하기 위한 철저한 연구를 수행한다.",
                    "tag": "2"
                },
                {
                    "index": "232-1",
                    "sentence": "Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data.",
                    "sentence_kor": "특히, 우리는 비원어민으로부터 실제 문법 오류를 수집하고 클린 텍스트 데이터에서 이러한 오류를 시뮬레이션하기 위해 적대적 공격을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "232-2",
                    "sentence": "We use this approach to facilitate debugging models on downstream applications.",
                    "sentence_kor": "이 접근 방식을 사용하여 다운스트림 애플리케이션에서 디버깅 모델을 용이하게 합니다.",
                    "tag": "4"
                },
                {
                    "index": "232-3",
                    "sentence": "Results confirm that the performance of all tested models is affected but the degree of impact varies.",
                    "sentence_kor": "결과는 시험한 모든 모델의 성능에 영향을 미치지만 충격의 정도는 다르다는 것을 확인시켜 줍니다.",
                    "tag": "4"
                },
                {
                    "index": "232-4",
                    "sentence": "To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors.",
                    "sentence_kor": "모델 동작을 해석하기 위해 문법적이지 않은 문장을 식별하는 능력과 오류 위치를 밝히는 언어적 허용성 작업을 추가로 설계한다.",
                    "tag": "4"
                },
                {
                    "index": "232-5",
                    "sentence": "We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions.",
                    "sentence_kor": "우리는 문장 정확성 예측에 대해 훈련된 간단한 분류기가 있는 고정 상황별 인코더가 오류 위치를 찾을 수 있다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "232-6",
                    "sentence": "We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context.",
                    "sentence_kor": "우리는 또한 BERT에 대한 클로즈 테스트를 설계하고 BERT가 컨텍스트에서 오류와 특정 토큰 간의 상호 작용을 포착한다는 것을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "232-7",
                    "sentence": "Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.",
                    "sentence_kor": "우리의 결과는 문법적 오류에 대한 언어 인코더의 견고성과 행동을 이해하는 것을 보여주었다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "832",
            "abstractID": "SPA_abs-233",
            "text": [
                {
                    "index": "233-0",
                    "sentence": "Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks.",
                    "sentence_kor": "변압기 아키텍처를 기반으로 한 문장 인코더는 다양한 자연어 작업에서 유망한 결과를 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "233-1",
                    "sentence": "The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture.",
                    "sentence_kor": "주된 자극은 아키텍처에서 고유한 다중 머리의 주의 때문에 단어 간의 장거리 의존성을 포착하는 사전 훈련된 신경 언어 모델에 있다.",
                    "tag": "1"
                },
                {
                    "index": "233-2",
                    "sentence": "However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model.",
                    "sentence_kor": "그러나 사전 훈련된 변압기 기반 모델 내에서 수백 개의 주의 헤드 중 언어 속성이 처리, 표현 및 다운스트림 작업에 사용되는 방법에 대해서는 알려진 바가 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "233-3",
                    "sentence": "For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA).",
                    "sentence_kor": "일련의 언어적 특징을 처리하는 데 있어 주의 헤드의 역할을 조사하는 초기 목표를 위해 사전 훈련된 네 개의 변압기 제품군(GPT, GPT2, BERT 및 ELCTRA)에 대해 10개의 탐색 작업과 3개의 다운스트림 작업으로 일련의 실험을 수행했다.",
                    "tag": "2+3"
                },
                {
                    "index": "233-4",
                    "sentence": "Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",
                    "sentence_kor": "의미 있는 통찰력은 열 지도 시각화의 렌즈를 통해 보여지며, 가장 영향력 있는 주의 헤드를 활용하는 비교적 간단한 문장 표현 방법을 제안하는데 활용되어 다운스트림 작업에 대한 추가적인 성능 향상을 초래한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "833",
            "abstractID": "SPA_abs-234",
            "text": [
                {
                    "index": "234-0",
                    "sentence": "Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems.",
                    "sentence_kor": "지식 그래프 임베딩 방법은 종종 트리플 분류 및 검색 개인화 문제에 대한 새로운 것을 예측하기 위해 유효한 트리플을 암기하는 한계를 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "234-1",
                    "sentence": "To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples.",
                    "sentence_kor": "이를 위해 관계 메모리 네트워크를 탐색하여 관계 3배의 잠재적 의존성을 인코딩하는 R-MeN이라는 새로운 내장 모델을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "234-2",
                    "sentence": "R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism.",
                    "sentence_kor": "R-MeN은 각 트리플을 변압기 자기 주의 메커니즘을 사용하여 메모리와 동시에 상호작용하는 3개의 입력 벡터의 시퀀스로 간주한다.",
                    "tag": "3"
                },
                {
                    "index": "234-3",
                    "sentence": "Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector.",
                    "sentence_kor": "따라서 R-MeN은 해당 벡터를 반환하기 위해 메모리와 각 입력 벡터 사이의 상호작용으로부터 새로운 정보를 인코딩한다.",
                    "tag": "3"
                },
                {
                    "index": "234-4",
                    "sentence": "Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple.",
                    "sentence_kor": "결과적으로, R-MeN은 이러한 3개의 반환된 벡터를 컨볼루션 신경망 기반 디코더에 공급하여 트리플에 대한 스칼라 점수를 생성한다.",
                    "tag": "3"
                },
                {
                    "index": "234-5",
                    "sentence": "Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.",
                    "sentence_kor": "실험 결과에 따르면 제안된 R-MeN은 검색 개인화 작업의 경우 SEARCH17에서, 삼중 분류 작업의 경우 WN11 및 FB13에서 최첨단 결과를 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "834",
            "abstractID": "SPA_abs-235",
            "text": [
                {
                    "index": "235-0",
                    "sentence": "It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.",
                    "sentence_kor": "대규모 말뭉치에서 언어 모델을 사전 교육하고 작업별 데이터에서 이를 미세 조정하는 것이 일반적인 접근 방식이었다.",
                    "tag": "1"
                },
                {
                    "index": "235-1",
                    "sentence": "In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem.",
                    "sentence_kor": "실제로 소규모 데이터 세트에서 사전 훈련된 모델을 미세하게 조정하면 초과 및/또는 과소 추정 문제가 발생할 수 있음을 관찰한다.",
                    "tag": "1"
                },
                {
                    "index": "235-2",
                    "sentence": "In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones.",
                    "sentence_kor": "본 논문에서, 우리는 확률 질량을 과대 추정된 영역에서 과소 추정된 영역으로 자르고 전송하여 텍스트 생성 작업에서 위의 문제를 완화하는 새로운 방법인 MC-Tailor를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "235-3",
                    "sentence": "Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.",
                    "sentence_kor": "다양한 텍스트 생성 데이터 세트에 대한 실험에 따르면 MC-Tailor는 일관되고 상당히 미세 조정 접근 방식을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "835",
            "abstractID": "SPA_abs-236",
            "text": [
                {
                    "index": "236-0",
                    "sentence": "Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese.",
                    "sentence_kor": "대부분의 중국어 사전 교육 모델은 중국어로 가장 작은 의미 있는 발화인 단어에 표현된 의미론을 무시하고 문자의 외부 맥락에 따라 문자를 기본 단위로 삼고 있다.",
                    "tag": "1"
                },
                {
                    "index": "236-1",
                    "sentence": "Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models.",
                    "sentence_kor": "따라서 다양한 문자 기반 중국어 사전 교육 언어 모델을 보완하는 명시적 단어 정보를 활용하기 위한 새로운 단어 정렬 주의를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "236-2",
                    "sentence": "Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion.",
                    "sentence_kor": "특히, 문자 수준의 주의를 단어 수준에 맞추는 풀링 메커니즘을 고안하고 다중 소스 정보 융합에 의한 분할 오류 전파의 잠재적 문제를 완화할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "236-3",
                    "sentence": "As a result, word and character information are explicitly integrated at the fine-tuning procedure.",
                    "sentence_kor": "결과적으로 단어 및 문자 정보는 미세 조정 절차에서 명시적으로 통합된다.",
                    "tag": "3"
                },
                {
                    "index": "236-4",
                    "sentence": "Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.",
                    "sentence_kor": "5개의 중국 NLP 벤치마크 작업에 대한 실험 결과는 우리 방법이 BERT, ERNIE 및 BERT-wm에 비해 크게 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "836",
            "abstractID": "SPA_abs-237",
            "text": [
                {
                    "index": "237-0",
                    "sentence": "Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling.",
                    "sentence_kor": "VAE(Variative Autoencoder)는 잠재 변수를 상각된 변동 추론과 결합하며, 최적화는 대개 특히 텍스트 모델링에서 사후 붕괴라는 사소한 국소 최적값으로 수렴된다.",
                    "tag": "1"
                },
                {
                    "index": "237-1",
                    "sentence": "By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold.",
                    "sentence_kor": "최적화 역학을 추적하여 데이터 매니폴드의 매개 변수화가 제대로 되지 않는 인코더-디코더 비호환성을 관찰한다.",
                    "tag": "1"
                },
                {
                    "index": "237-2",
                    "sentence": "We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them.",
                    "sentence_kor": "우리는 후방 네트워크가 인코더와 디코더 매개 변수화를 개선함으로써 사소한 로컬 최적화를 피할 수 있다고 주장한다.",
                    "tag": "1+2"
                },
                {
                    "index": "237-3",
                    "sentence": "To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.",
                    "sentence_kor": "이를 위해 VAE 모델을 동일한 구조의 결정론적 자동 인코더와 결합하고 인코더 가중치 공유 및 디코더 신호 일치를 통해 인코더 및 디코더 매개 변수화를 개선하는 Coupled-VAE를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "237-4",
                    "sentence": "We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy.",
                    "sentence_kor": "제안된 커플링-VAE 접근방식을 정규화, 사후 제품군, 디코더 구조 및 최적화 전략이 다른 다양한 VAE 모델에 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "237-5",
                    "sentence": "Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space.",
                    "sentence_kor": "벤치마크 데이터 세트(즉, PTB, Yelp 및 Yahoo)에 대한 실험은 잠재 공간의 확률 추정 및 풍부성 측면에서 지속적으로 개선된 결과를 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "237-6",
                    "sentence": "We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.",
                    "sentence_kor": "또한 조건부 언어 모델링으로 방법을 일반화하고 스위치보드 데이터 세트에서 대화 생성의 다양성을 크게 향상시키는 커플링-CVAE를 제안한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "837",
            "abstractID": "SPA_abs-238",
            "text": [
                {
                    "index": "238-0",
                    "sentence": "State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution.",
                    "sentence_kor": "최첨단 NLP 모델은 종종 동의어 대체와 같은 인간-유나웨어 변환에 속아 넘어갈 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "238-1",
                    "sentence": "For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution.",
                    "sentence_kor": "보안상의 이유로, 가능한 동의어 대체에 의해 예측이 변경될 수 없음을 입증할 수 있는 인증된 견고성을 가진 모델을 개발하는 것이 매우 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "238-2",
                    "sentence": "In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness.",
                    "sentence_kor": "본 연구에서는 입력 문장에 무작위 단어 대체를 적용하여 확률적 앙상블을 구성하고 앙상블의 통계적 속성을 활용하여 견고성을 입증하는 새로운 무작위 스무딩 기법을 기반으로 인증된 강력한 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "238-3",
                    "sentence": "Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level).",
                    "sentence_kor": "우리의 방법은 모델 출력의 블랙박스 쿼리만 필요하므로 사전 훈련된 모델(BERT 등)과 모든 유형의 모델(세계 수준 또는 하위 단어 수준)에 적용할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "238-4",
                    "sentence": "Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks.",
                    "sentence_kor": "우리의 방법은 IMDB 및 Amazon 텍스트 분류 작업 모두에서 인증된 견고성에 대한 최신 방법을 크게 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "238-5",
                    "sentence": "To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.",
                    "sentence_kor": "우리가 아는 한, 우리는 BERT와 같은 대형 시스템에서 실질적으로 의미 있는 인증 정확도로 인증된 견고성을 달성한 첫 번째 작업이다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "838",
            "abstractID": "SPA_abs-239",
            "text": [
                {
                    "index": "239-0",
                    "sentence": "Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages.",
                    "sentence_kor": "감독되지 않은 이중 언어 어휘 사전 유도는 두 언어의 단일 언어 말뭉치에서 단어 번역을 유도하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "239-1",
                    "sentence": "Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages.",
                    "sentence_kor": "최근의 방법은 대부분 비지도 교차 언어 단어 임베딩을 기반으로 하며, 그 핵심은 단어 번역의 초기 솔루션을 찾는 것이고, 그 다음에는 두 언어의 임베딩 공간 사이의 매핑 학습과 정교함이 뒤따른다.",
                    "tag": "1"
                },
                {
                    "index": "239-2",
                    "sentence": "However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words.",
                    "sentence_kor": "그러나 이전 방법은 (1) 제한적이고 부정확할 수 있으며 (2) 일부 단어의 사전 훈련되지 않은 임베딩에 의해 유입되는 일부 노이즈를 포함하는 경향이 있는 단어 수준 정보에 기초하여 초기 솔루션을 찾는다.",
                    "tag": "1"
                },
                {
                    "index": "239-3",
                    "sentence": "To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way.",
                    "sentence_kor": "이러한 문제를 다루기 위해 본 논문에서 우리는 이중 언어 어휘를 거칠고 미세한 방법으로 유도하는 새로운 그래프 기반 패러다임을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "239-4",
                    "sentence": "We first build a graph for each language with its vertices representing different words.",
                    "sentence_kor": "우리는 먼저 각 언어에 대해 다른 단어를 나타내는 꼭지점으로 그래프를 작성한다.",
                    "tag": "3"
                },
                {
                    "index": "239-5",
                    "sentence": "Then we extract word cliques from the graphs and map the cliques of two languages.",
                    "sentence_kor": "그런 다음 그래프에서 단어 클리크를 추출하고 두 언어의 클리크를 매핑합니다.",
                    "tag": "3"
                },
                {
                    "index": "239-6",
                    "sentence": "Based on that, we induce the initial word translation solution with the central words of the aligned cliques.",
                    "sentence_kor": "이를 바탕으로 정렬된 패밀리의 중심 단어로 초기 단어 번역 솔루션을 유도한다.",
                    "tag": "3"
                },
                {
                    "index": "239-7",
                    "sentence": "This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings.",
                    "sentence_kor": "이 대략적인 접근 방식은 더 풍부하고 정확한 클리크 수준 정보를 활용할 뿐만 아니라 사전 훈련된 임베딩에서 노이즈의 나쁜 영향을 효과적으로 감소시킨다.",
                    "tag": "4"
                },
                {
                    "index": "239-8",
                    "sentence": "Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons.",
                    "sentence_kor": "마지막으로, 초기 솔루션을 교차 언어 임베딩을 학습하기 위한 씨앗으로 취하여 이중 언어 어휘 사전을 유도한다.",
                    "tag": "4"
                },
                {
                    "index": "239-9",
                    "sentence": "Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.",
                    "sentence_kor": "실험에 따르면 우리의 접근 방식은 이전 방법에 비해 이중 언어 어휘 유도의 성능을 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "839",
            "abstractID": "SPA_abs-240",
            "text": [
                {
                    "index": "240-0",
                    "sentence": "Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance.",
                    "sentence_kor": "신경 기계 번역 시스템은 상당한 효과에도 불구하고 덜 적절한 입력에서 고장나는 경향이 있으며, 이는 이러한 경우 신경 기반 시스템이 고장나는 방법과 시기를 만족시키는 데 도움이 된다.",
                    "tag": "1"
                },
                {
                    "index": "240-1",
                    "sentence": "Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning.",
                    "sentence_kor": "여기서는 제한된 수작업 오류 기능을 사용하여 불량 사례를 수집하고 분석하는 대신 강화 학습을 기반으로 한 새로운 패러다임을 통해 적대적 사례를 생성하여 이 문제를 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "240-2",
                    "sentence": "Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture.",
                    "sentence_kor": "우리의 패러다임은 주어진 성능 메트릭(예: BLEU)에 대한 함정을 노출할 수 있고 주어진 신경 기계 변환 아키텍처를 목표로 할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "240-3",
                    "sentence": "We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer.",
                    "sentence_kor": "우리는 두 가지 주요 신경 기계 번역 아키텍처인 RNN-검색 및 트랜스포머에 대한 적대적 공격 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "240-4",
                    "sentence": "The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples.",
                    "sentence_kor": "그 결과는 우리의 방법이 의미를 보존하는 적대적 예를 통해 효율적으로 안정적인 공격을 생성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "240-5",
                    "sentence": "We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.",
                    "sentence_kor": "또한 공격의 선호 패턴에 대한 정성적 및 정량적 분석을 제시하여 함몰 노출 능력을 입증한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "840",
            "abstractID": "SPA_abs-241",
            "text": [
                {
                    "index": "241-0",
                    "sentence": "The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance.",
                    "sentence_kor": "감독되지 않은 기계 번역에 일반적으로 사용되는 프레임워크는 두 번역 방향의 초기 번역 모델을 구축한 다음 반복적인 역번역을 수행하여 번역 성능을 공동으로 향상시킨다.",
                    "tag": "1"
                },
                {
                    "index": "241-1",
                    "sentence": "The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance.",
                    "sentence_kor": "초기화가 잘못되면 검색 공간이 잘못 압축되고 이 단계에서 너무 많은 노이즈가 유입되면 최종 성능이 저하될 수 있기 때문에 초기화 단계가 매우 중요합니다.",
                    "tag": "1"
                },
                {
                    "index": "241-2",
                    "sentence": "In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.",
                    "sentence_kor": "본 논문에서 우리는 감독되지 않은 번역 모델을 더 잘 초기화하기 위한 새로운 검색 및 재작성 기반 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "241-3",
                    "sentence": "We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model.",
                    "sentence_kor": "먼저 두 언어의 단일 언어 말뭉치에서 의미론적으로 비교 가능한 문장을 검색한 다음 대상 측면을 다시 작성하여 설계된 재작성 모델을 사용하여 소스와 검색 대상 사이의 의미 격차를 최소화한다.",
                    "tag": "3"
                },
                {
                    "index": "241-4",
                    "sentence": "The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation.",
                    "sentence_kor": "다시 작성된 문장 쌍은 두 NMT 모델에 대한 유사 데이터를 생성하는 데 사용되는 시만텍 모델을 초기화한 후 반복 역번역하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "241-5",
                    "sentence": "Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.",
                    "sentence_kor": "실험에 따르면 우리의 방법은 더 나은 초기 비지도 변환 모델을 구축하고 최종 번역 성능을 4 BLEU 점수 이상으로 향상시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "241-6",
                    "sentence": "Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.",
                    "sentence_kor": "코드는 https://github.com/Imagist-Shuo/RRforUNMT.git에서 공개됩니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "841",
            "abstractID": "SPA_abs-242",
            "text": [
                {
                    "index": "242-0",
                    "sentence": "Open-domain dialogue generation has gained increasing attention in Natural Language Processing.",
                    "sentence_kor": "개방형 도메인 대화 생성은 자연어 처리 분야에서 점점 더 많은 관심을 받고 있다.",
                    "tag": "1"
                },
                {
                    "index": "242-1",
                    "sentence": "Its evaluation requires a holistic means.",
                    "sentence_kor": "그것의 평가에는 전체적인 수단이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "242-2",
                    "sentence": "Human ratings are deemed as the gold standard.",
                    "sentence_kor": "인간평가는 금본위제로 간주된다.",
                    "tag": "1"
                },
                {
                    "index": "242-3",
                    "sentence": "As human evaluation is inefficient and costly, an automated substitute is highly desirable.",
                    "sentence_kor": "인간의 평가는 비효율적이고 비용이 많이 들기 때문에, 자동화된 대체품이 매우 바람직하다.",
                    "tag": "1"
                },
                {
                    "index": "242-4",
                    "sentence": "In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.",
                    "sentence_kor": "본 논문에서, 우리는 개방형 도메인 대화의 다양한 측면을 포착하는 전체론적 평가 지표를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "242-5",
                    "sentence": "Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency.",
                    "sentence_kor": "우리의 측정 기준은 (1) GPT-2 기반의 대화 문장 간 맥락 일관성, (2) 표현에 있어서의 GPT-2 기반의 유창성, (3) 증강 쿼리에 대한 응답에 있어서의 다양성, (4) 논리적 자기 일관성에 기초한 텍스트-입력-참조성으로 구성된다.",
                    "tag": "3"
                },
                {
                    "index": "242-6",
                    "sentence": "The empirical validity of our metrics is demonstrated by strong correlations with human judgments.",
                    "sentence_kor": "측정기준의 경험적 타당성은 인간의 판단과 강한 상관관계로 입증된다.",
                    "tag": "4"
                },
                {
                    "index": "242-7",
                    "sentence": "We open source the code and relevant materials.",
                    "sentence_kor": "코드와 관련 자료를 공개합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "842",
            "abstractID": "SPA_abs-243",
            "text": [
                {
                    "index": "243-0",
                    "sentence": "The hypernymy detection task has been addressed under various frameworks.",
                    "sentence_kor": "하이퍼나이미 감지 작업은 다양한 프레임워크에서 해결되었다.",
                    "tag": "1"
                },
                {
                    "index": "243-1",
                    "sentence": "Previously, the design of unsupervised hypernymy scores has been extensively studied.",
                    "sentence_kor": "이전에는 비지도 초자연적 점수의 설계가 광범위하게 연구되었다.",
                    "tag": "1"
                },
                {
                    "index": "243-2",
                    "sentence": "In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”.",
                    "sentence_kor": "이와는 대조적으로, 감독 분류자, 특히 분포 모델은 예측하기 위해 용어의 글로벌 컨텍스트를 활용하지만, \"어휘 암기\"를 겪을 가능성이 더 높다.",
                    "tag": "1"
                },
                {
                    "index": "243-3",
                    "sentence": "In this work, we revisit supervised distributional models for hypernymy detection.",
                    "sentence_kor": "이 작업에서는 하이퍼나이미 탐지를 위해 감독된 분포 모델을 다시 검토한다.",
                    "tag": "1+2"
                },
                {
                    "index": "243-4",
                    "sentence": "Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE).",
                    "sentence_kor": "두 용어의 임베딩을 분류 입력으로 채택하기보다는 양방향 잔류 관계 임베딩(BiRRE)이라는 표현 학습 프레임워크를 도입한다.",
                    "tag": "3"
                },
                {
                    "index": "243-5",
                    "sentence": "In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations.",
                    "sentence_kor": "이 모델에서 용어 쌍은 하이퍼나이미 분류의 특징으로 BiRRE 벡터로 표현되며, 하이퍼나이미 관계에 의해 항이 임베딩 공간의 다른 항에 매핑될 가능성을 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "243-6",
                    "sentence": "A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections.",
                    "sentence_kor": "신경 언어 모델에 의해 하이퍼넴과 하이파이언이 생성되는 방법을 시뮬레이션하고 투영의 양방향 잔차를 기반으로 BiRRE 벡터를 생성하기 위해 음의 정규화를 사용한 잠재 투영 모델(LPMNR)을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "243-7",
                    "sentence": "Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.",
                    "sentence_kor": "실험에 따르면 BiRRE는 다양한 평가 프레임워크에 비해 강력한 기준선을 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "843",
            "abstractID": "SPA_abs-244",
            "text": [
                {
                    "index": "244-0",
                    "sentence": "Biomedical named entities often play important roles in many biomedical text mining tools.",
                    "sentence_kor": "생물의학 명명 단체는 많은 생물의학 텍스트 마이닝 도구에서 중요한 역할을 하는 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "244-1",
                    "sentence": "However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging.",
                    "sentence_kor": "그러나 제공된 동의어의 불완전성과 표면 형태의 수많은 변형으로 인해 생물의학 실체의 정규화는 매우 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "244-2",
                    "sentence": "In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities.",
                    "sentence_kor": "본 논문에서 우리는 실체의 동의어만을 기반으로 하는 생물의학 실체의 표현 학습에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "244-3",
                    "sentence": "To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.",
                    "sentence_kor": "불완전한 동의어에서 배우기 위해 모델 기반 후보 선택을 사용하고 상위 후보자에 있는 동의어의 한계 가능성을 최대화한다.",
                    "tag": "3"
                },
                {
                    "index": "244-4",
                    "sentence": "Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves.",
                    "sentence_kor": "모델 기반 후보들은 모델이 진화함에 따라 더 어려운 마이너스 샘플을 포함하도록 반복적으로 업데이트된다.",
                    "tag": "3"
                },
                {
                    "index": "244-5",
                    "sentence": "In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates.",
                    "sentence_kor": "이러한 방식으로, 우리는 400,000명 이상의 후보에서 마이너스 샘플을 명시적으로 사전 선택하는 것을 피한다.",
                    "tag": "3"
                },
                {
                    "index": "244-6",
                    "sentence": "On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.",
                    "sentence_kor": "세 가지 서로 다른 엔티티 유형(질병, 화학, 부작용)을 가진 네 개의 생물 의학 엔티티 표준화 데이터 세트에서, 우리의 모델 BioSyn은 각 데이터 세트에서 거의 상한에 도달한 이전의 최첨단 모델을 지속적으로 능가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "844",
            "abstractID": "SPA_abs-245",
            "text": [
                {
                    "index": "245-0",
                    "sentence": "Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks.",
                    "sentence_kor": "어휘적 수반인 하이퍼니미 탐지는 많은 자연어 이해 작업의 기본적인 하위 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "245-1",
                    "sentence": "Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios.",
                    "sentence_kor": "이전의 탐색은 대부분 고자원 언어(예: 영어)에 대한 단일 언어 하이퍼니미 탐지에 초점을 맞추고 있지만 저자원 시나리오를 조사하는 경우는 거의 없다.",
                    "tag": "1"
                },
                {
                    "index": "245-2",
                    "sentence": "This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.",
                    "sentence_kor": "이 논문은 고자원 언어를 결합하여 저자원 하이퍼니미 감지 문제를 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "245-3",
                    "sentence": "We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.",
                    "sentence_kor": "우리는 세 가지 공동 훈련 패러다임을 광범위하게 비교하고 처음으로 저자원 문제를 해결하기 위해 메타 학습을 적용할 것을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "245-4",
                    "sentence": "Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.",
                    "sentence_kor": "실험은 세 가지 설정 중 우리 방법의 우수성을 입증하며, 이는 작은 데이터 세트에 대한 과적합을 방지하여 극도로 낮은 리소스 언어의 성능을 크게 향상시킨다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "845",
            "abstractID": "SPA_abs-246",
            "text": [
                {
                    "index": "246-0",
                    "sentence": "This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space.",
                    "sentence_kor": "이 논문은 사전 훈련된 단어 벡터 공간에서 특정 단어 클래스에 속하는 단어 벡터의 분포에 대한 조사를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "246-1",
                    "sentence": "To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model.",
                    "sentence_kor": "이를 위해 분포에 대한 몇 가지 가정을 만들고 그에 따라 분포를 모형화했으며 각 모델의 우수성을 비교하여 각 가정을 검증했습니다.",
                    "tag": "3"
                },
                {
                    "index": "246-2",
                    "sentence": "Specifically, we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class.",
                    "sentence_kor": "구체적으로, 우리는 동사의 직접 객체의 의미 클래스와 유의어에서의 의미 클래스라는 두 가지 유형의 단어 클래스를 고려했고 벡터 공간의 단어가 주어진 단어 클래스의 멤버일 가능성을 적절하게 추정하는 모델을 구축하려고 노력했다.",
                    "tag": "3"
                },
                {
                    "index": "246-3",
                    "sentence": "Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution.",
                    "sentence_kor": "선택 선호도 및 WordNet 데이터 세트에 대한 우리의 결과는 중심 기반 모델이 충분한 성능을 달성하지 못하고, 분포의 기하학적 구조와 하위 그룹의 존재에 제한적인 영향을 미치며, 분포의 적절한 모델링을 위해 부정적인 인스턴스도 고려해야 한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "246-4",
                    "sentence": "We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.",
                    "sentence_kor": "우리는 각 모델에 의해 계산된 점수와 멤버십의 정도 사이의 관계를 추가로 조사했고 차별적 학습 기반 모델이 클래스의 경계를 찾는 데 가장 좋은 반면 긍정적인 인스턴스와 부정적인 인스턴스 사이의 오프셋을 기반으로 한 모델은 멤버십의 정도를 결정하는 데 가장 잘 수행한다는 것을 발견했다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "846",
            "abstractID": "SPA_abs-247",
            "text": [
                {
                    "index": "247-0",
                    "sentence": "In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC.",
                    "sentence_kor": "문헌에서 기존 연구는 항상 측면별 독립 문장 수준 분류 문제 측면으로 간주하며, 이러한 정보는 명백히 ASC의 정보 결핍 문제를 완화하는데 중요하지만 측면별 문서 수준 정서 선호 정보를 대부분 무시한다.",
                    "tag": "1"
                },
                {
                    "index": "247-1",
                    "sentence": "In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency).",
                    "sentence_kor": "본 논문에서 우리는 문서 내부의 두 가지 종류의 정서 선호도 정보, 즉 동일한 측면(즉, 가슴 내 정서 일관성)과 모든 관련 측면(명칭 가슴 간 정서 경향)을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "247-2",
                    "sentence": "On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation.",
                    "sentence_kor": "기본적으로 측면 관련 문장 표현을 공동으로 학습하기 위한 CoGAN(Cooperative Graph Attention Networks) 접근방식을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "247-3",
                    "sentence": "Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference.",
                    "sentence_kor": "특히, 두 개의 그래프 주의 네트워크를 활용하여 각각 두 종류의 문서 수준 정서 선호도 정보를 모델링한 다음, 이중 선호도를 통합하는 대화형 메커니즘이 뒤따른다.",
                    "tag": "3"
                },
                {
                    "index": "247-4",
                    "sentence": "Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines.",
                    "sentence_kor": "상세한 평가는 제안된 ASC 접근 방식이 최첨단 기준선에 비해 큰 이점을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "247-5",
                    "sentence": "This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.",
                    "sentence_kor": "이는 ASC에 대한 문서 수준의 정서 선호도 정보의 중요성과 이러한 정보를 캡처하는 접근 방식의 효과를 정당화한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "847",
            "abstractID": "SPA_abs-248",
            "text": [
                {
                    "index": "248-0",
                    "sentence": "The current aspect extraction methods suffer from boundary errors.",
                    "sentence_kor": "현재 측면 추출 방법은 경계 오류로 인해 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "248-1",
                    "sentence": "In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth.",
                    "sentence_kor": "일반적으로 이러한 오류는 추출된 측면과 실제 사실 사이의 비교적 사소한 차이를 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "248-2",
                    "sentence": "However, they hurt the performance severely.",
                    "sentence_kor": "하지만, 그들은 공연에 심각한 타격을 입혔다.",
                    "tag": "1"
                },
                {
                    "index": "248-3",
                    "sentence": "In this paper, we propose to utilize a pointer network for repositioning the boundaries.",
                    "sentence_kor": "본 논문에서 우리는 경계 위치 변경을 위해 포인터 네트워크를 활용할 것을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "248-4",
                    "sentence": "Recycling mechanism is used, which enables the training data to be collected without manual intervention.",
                    "sentence_kor": "재활용 메커니즘이 사용되므로 수동 개입 없이 교육 데이터를 수집할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "248-5",
                    "sentence": "We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant.",
                    "sentence_kor": "노트북의 벤치마크 데이터 세트 SE14와 레스토랑의 SE14-16에 대한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "248-6",
                    "sentence": "Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.",
                    "sentence_kor": "실험 결과는 우리의 방법이 기준선에 비해 상당히 개선되었으며 최첨단 방법을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "848",
            "abstractID": "SPA_abs-249",
            "text": [
                {
                    "index": "249-0",
                    "sentence": "Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification.",
                    "sentence_kor": "측면 기반 정서 분석(ABSA)은 측면 용어 추출, 의견 용어 추출 및 측면 수준 정서 분류와 같은 세 가지 하위 작업을 포함한다.",
                    "tag": "1"
                },
                {
                    "index": "249-1",
                    "sentence": "Most existing studies focused on one of these subtasks only.",
                    "sentence_kor": "대부분의 기존 연구는 이러한 하위 작업 중 하나에 초점을 맞췄습니다.",
                    "tag": "1"
                },
                {
                    "index": "249-2",
                    "sentence": "Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework.",
                    "sentence_kor": "최근의 몇몇 연구는 통합된 프레임워크로 완전한 ABSA 문제를 해결하려는 성공적인 시도를 했다.",
                    "tag": "1"
                },
                {
                    "index": "249-3",
                    "sentence": "However, the interactive relations among three subtasks are still under-exploited.",
                    "sentence_kor": "그러나 세 개의 하위 작업 간의 대화형 관계는 여전히 활용도가 낮습니다.",
                    "tag": "1"
                },
                {
                    "index": "249-4",
                    "sentence": "We argue that such relations encode collaborative signals between different subtasks.",
                    "sentence_kor": "우리는 그러한 관계가 서로 다른 하위 작업 간의 협력 신호를 인코딩한다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "249-5",
                    "sentence": "For example, when the opinion term is “delicious”, the aspect term must be “food” rather than “place”.",
                    "sentence_kor": "예를 들어, 의견 용어가 \"맛있다\"고 할 때, 측면 용어는 \"장소\"가 아니라 \"음식\"이어야 한다.",
                    "tag": "3"
                },
                {
                    "index": "249-6",
                    "sentence": "In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network.",
                    "sentence_kor": "이러한 관계를 완전히 활용하기 위해, 우리는 쌓인 다중 계층 네트워크의 다중 작업 학습 및 관계 전파 메커니즘을 통해 하위 작업이 조정적으로 작동할 수 있는 RACL(Relation-Aware Collaborative Learning) 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "249-7",
                    "sentence": "Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task",
                    "sentence_kor": "세 개의 실제 데이터 세트에 대한 광범위한 실험은 RACL이 완전한 ABSA 작업에 대한 최첨단 방법을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "849",
            "abstractID": "SPA_abs-250",
            "text": [
                {
                    "index": "250-0",
                    "sentence": "We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.",
                    "sentence_kor": "구성 정서 의미를 효과적으로 포착하는 BERT의 변형인 SentiBERT를 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "250-1",
                    "sentence": "The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition.",
                    "sentence_kor": "이 모델은 의미 구성을 캡처하기 위해 이진 구성 구문 구문 분석 트리와 상황별 표현을 통합한다.",
                    "tag": "3"
                },
                {
                    "index": "250-2",
                    "sentence": "Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification.",
                    "sentence_kor": "포괄적인 실험은 SentiBERT가 구문 수준의 감정 분류에서 경쟁력 있는 성능을 달성한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "250-3",
                    "sentence": "We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks.",
                    "sentence_kor": "또한 SST의 구문 수준 주석에서 학습한 정서 구성이 감정 분류 작업과 같은 관련 과제뿐만 아니라 다른 정서 분석 작업으로 전달될 수 있음을 입증한다.",
                    "tag": "5"
                },
                {
                    "index": "250-4",
                    "sentence": "Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT.",
                    "sentence_kor": "또한 SentiBERT를 이해하기 위해 절제 연구를 수행하고 시각화 방법을 설계한다.",
                    "tag": "5"
                },
                {
                    "index": "250-5",
                    "sentence": "We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.",
                    "sentence_kor": "우리는 SentiBERT가 부정과 대조 관계를 포착하는 데 있어 기준선 접근법보다 낫다는 것을 보여주고 구성 정서 의미를 모델링한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "850",
            "abstractID": "SPA_abs-251",
            "text": [
                {
                    "index": "251-0",
                    "sentence": "Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text.",
                    "sentence_kor": "감정 원인 쌍 추출은 주석 없는 감정 텍스트에서 모든 잠재적 감정 쌍과 해당 원인을 추출하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "251-1",
                    "sentence": "Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation.",
                    "sentence_kor": "대부분의 기존 방법은 감정과 원인을 개별적으로 식별하는 파이프라인 프레임워크로 오류 전파의 단점을 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "251-2",
                    "sentence": "Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction.",
                    "sentence_kor": "이 문제에 대해, 우리는 작업을 구문 분석 같은 지시된 그래프 구성 절차로 변환하기 위한 전환 기반 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "251-3",
                    "sentence": "The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently.",
                    "sentence_kor": "제안된 모델은 일련의 작용에 기초하여 라벨링된 가장자리가 있는 방향 그래프를 점진적으로 생성하며, 여기서부터 우리는 해당 원인과 동시에 감정을 인식할 수 있으므로 개별 하위 작업을 공동으로 최적화하고 상호 의존적으로 작업의 상호 이익을 극대화한다.",
                    "tag": "3+4"
                },
                {
                    "index": "251-4",
                    "sentence": "Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.",
                    "sentence_kor": "실험 결과에 따르면 우리의 접근방식은 F1 측정에서 최첨단 방법을 6.71%(p<0.01) 능가하여 최고의 성능을 달성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "851",
            "abstractID": "SPA_abs-252",
            "text": [
                {
                    "index": "252-0",
                    "sentence": "End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously.",
                    "sentence_kor": "종단 간 음성 변환은 교차 언어 의미를 동시에 전사, 이해 및 학습해야 하기 때문에 인코더에 큰 부담을 준다.",
                    "tag": "1"
                },
                {
                    "index": "252-1",
                    "sentence": "To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features.",
                    "sentence_kor": "강력한 인코더를 얻기 위해 기존 방법은 이를 ASR 데이터에 사전 교육하여 음성 기능을 캡처한다.",
                    "tag": "1"
                },
                {
                    "index": "252-2",
                    "sentence": "However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered.",
                    "sentence_kor": "그러나 간단한 음성 인식을 통해서만 인코더를 사전 교육하는 것은 충분하지 않으며 높은 수준의 언어 지식을 고려해야 한다고 주장한다.",
                    "tag": "1+2"
                },
                {
                    "index": "252-3",
                    "sentence": "Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.",
                    "sentence_kor": "이에 영감을 받아, 우리는 전사 학습을 위한 기초 과정과 두 언어의 발화 이해 및 단어 매핑에 대한 두 가지 고급 과정을 포함하는 커리큘럼 사전 훈련 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "252-4",
                    "sentence": "The difficulty of these courses is gradually increasing.",
                    "sentence_kor": "이 과목의 난이도가 점차 높아지고 있다",
                    "tag": "4"
                },
                {
                    "index": "252-5",
                    "sentence": "Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.",
                    "sentence_kor": "실험에 따르면 커리큘럼 사전 교육 방법은 En-De 및 En-Fr 음성 번역 벤치마크를 크게 향상시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "852",
            "abstractID": "SPA_abs-253",
            "text": [
                {
                    "index": "253-0",
                    "sentence": "In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system.",
                    "sentence_kor": "본 연구에서는 액센트 정보가 종단 간 자동 음성 인식(ASR) 시스템의 음성 내부 표현에 어떻게 반영되는지에 대한 자세한 분석을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "253-1",
                    "sentence": "We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents.",
                    "sentence_kor": "우리는 컨볼루션 및 반복 레이어로 구성된 최첨단 엔드 투 엔드 ASR 시스템을 사용하며, 이는 다량의 미국 어센트 영어 스피치에 대해 훈련되고 7가지 영어 억양의 음성 샘플에 대한 모델을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "253-2",
                    "sentence": "We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers.",
                    "sentence_kor": "우리는 세 가지 주요 탐색 기법인 a) 기울기 기반 설명 방법, b) 정보 이론적 측정 및 c) 액센트 및 전화 분류기의 출력을 사용하여 액센트가 내부 표현에 미치는 영향을 조사한다.",
                    "tag": "3"
                },
                {
                    "index": "253-3",
                    "sentence": "We find different accents exhibiting similar trends irrespective of the probing technique used.",
                    "sentence_kor": "우리는 사용된 탐색 기법에 관계 없이 유사한 경향을 나타내는 다른 억양을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "253-4",
                    "sentence": "We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.",
                    "sentence_kor": "또한 대부분의 액센트 정보가 첫 번째 반복 계층 내에서 인코딩된다는 것을 발견했는데, 이는 액센트에 불변하는 표현을 학습하기 위해 이러한 엔드 투 엔드 모델을 어떻게 조정할 수 있는지 시사한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "853",
            "abstractID": "SPA_abs-254",
            "text": [
                {
                    "index": "254-0",
                    "sentence": "Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts.",
                    "sentence_kor": "상황에 맞는 단어 임베딩(예: ELMo 또는 BERT)을 사용하는 자기 주의 신경 구문 분석기는 현재 음성 기록에서 공동 구문 분석 및 불용성 검출에서 최첨단 결과를 산출한다.",
                    "tag": "1"
                },
                {
                    "index": "254-1",
                    "sentence": "Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant.",
                    "sentence_kor": "상황에 맞는 단어 임베딩은 라벨이 부착되지 않은 대량의 데이터에 대해 사전 교육되기 때문에, 신경 모델을 훈련하기 위해 라벨이 부착되지 않은 추가 데이터를 사용하는 것은 중복되는 것처럼 보일 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "254-2",
                    "sentence": "However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations.",
                    "sentence_kor": "그러나 라벨이 부착되지 않은 데이터를 통합하기 위한 준지도 기술인 자가 훈련이 불능 검출에 대한 자가 주의 파서에 대한 새로운 최첨단 기술을 설정한다는 것을 보여줌으로써 자가 훈련이 사전 훈련된 상황별 단어 표현과 직교하는 이점을 제공한다는 것을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "254-3",
                    "sentence": "We also show that ensembling self-trained parsers provides further gains for disfluency detection.",
                    "sentence_kor": "우리는 또한 앙상블 자체 훈련 파서가 불능 감지를 위한 추가적인 이득을 제공한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "854",
            "abstractID": "SPA_abs-255",
            "text": [
                {
                    "index": "255-0",
                    "sentence": "Pre-trained language models have achieved huge improvement on many NLP tasks.",
                    "sentence_kor": "사전 훈련된 언어 모델은 많은 NLP 작업에서 큰 향상을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "255-1",
                    "sentence": "However, these methods are usually designed for written text, so they do not consider the properties of spoken language.",
                    "sentence_kor": "그러나 이러한 방법들은 대개 필기 텍스트용으로 설계되므로 구어의 속성을 고려하지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "255-2",
                    "sentence": "Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems.",
                    "sentence_kor": "따라서 본 논문은 인식 시스템에 의해 생성된 격자에 사전 훈련 언어 모델의 아이디어를 일반화하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "255-3",
                    "sentence": "We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks.",
                    "sentence_kor": "우리는 구어 이해 작업에 상황별 표현을 제공하기 위해 신경 격자 언어 모델을 훈련시키는 프레임워크를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "255-4",
                    "sentence": "The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency.",
                    "sentence_kor": "제안된 2단계 사전 훈련 접근방식은 음성 데이터의 요구를 줄이고 효율성을 향상시킨다.",
                    "tag": "3+4"
                },
                {
                    "index": "255-5",
                    "sentence": "Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.",
                    "sentence_kor": "의도 감지 및 대화 행위 인식 데이터 세트에 대한 실험은 제안된 방법이 음성 입력에서 평가할 때 지속적으로 강력한 기준선을 능가한다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "255-6",
                    "sentence": "The code is available at https://github.com/MiuLab/Lattice-ELMo.",
                    "sentence_kor": "코드는 https://github.com/MiuLab/Lattice-ELMo에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "855",
            "abstractID": "SPA_abs-256",
            "text": [
                {
                    "index": "256-0",
                    "sentence": "An increasing number of people in the world today speak a mixed-language as a result of being multilingual.",
                    "sentence_kor": "오늘날 세계의 점점 더 많은 사람들이 다국어를 구사하는 결과로 혼합 언어를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "256-1",
                    "sentence": "However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data.",
                    "sentence_kor": "그러나 제한된 자원의 가용성과 혼합 언어 데이터 수집에 필요한 비용과 상당한 노력 때문에 코드 전환을 위한 음성 인식 시스템을 구축하는 것은 여전히 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "256-2",
                    "sentence": "We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.",
                    "sentence_kor": "따라서 우리는 고자원 단일 언어 데이터 세트에서 정보를 신중하게 추출하여 저자원 환경에서 코드 교환 음성 인식 시스템에 대한 학습을 전송하는 새로운 학습 방법인 메타 전송 학습을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "256-3",
                    "sentence": "Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data.",
                    "sentence_kor": "우리 모델은 코드 전환 데이터의 최적화를 조정하여 혼합 언어 음성을 더 잘 인식하도록 개별 언어를 인식하는 방법을 배운다.",
                    "tag": "3"
                },
                {
                    "index": "256-4",
                    "sentence": "Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.",
                    "sentence_kor": "실험 결과를 바탕으로, 우리 모델은 음성 인식 및 언어 모델링 작업에 대한 기존 기준을 능가하며 수렴이 더 빠르다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "856",
            "abstractID": "SPA_abs-257",
            "text": [
                {
                    "index": "257-0",
                    "sentence": "Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means.",
                    "sentence_kor": "비아냥은 사람의 진짜 의미와 정반대를 표현하기 위한 정교한 언어 현상이다.",
                    "tag": "1"
                },
                {
                    "index": "257-1",
                    "sentence": "With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms.",
                    "sentence_kor": "소셜 미디어의 급속한 성장과 함께, 다양한 소셜 플랫폼에 다양한 비꼬는 트윗이 널리 게시되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "257-2",
                    "sentence": "In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions.",
                    "sentence_kor": "다중 모드 맥락에서 빈정거림은 더 이상 순수한 언어적 현상이 아니며, 소셜 미디어 짧은 텍스트의 특성 때문에, 그 반대는 종종 교차 양식 표현을 통해 나타난다.",
                    "tag": "1"
                },
                {
                    "index": "257-3",
                    "sentence": "Thus traditional text-based methods are insufficient to detect multimodal sarcasm.",
                    "sentence_kor": "따라서 기존의 텍스트 기반 방법은 다중 모드 빈정거림을 탐지하기에 충분하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "257-4",
                    "sentence": "To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context.",
                    "sentence_kor": "다중 모드 비꼬는 트윗을 추론하기 위해 본 논문에서 우리는 관련 맥락에서 교차 양식 대비를 모델링하는 새로운 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "257-5",
                    "sentence": "Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net).",
                    "sentence_kor": "우리의 방법은 분해 및 관계 네트워크(D&R Net)를 구성하여 교차 양식 대비와 의미적 연관성을 모두 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "257-6",
                    "sentence": "The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context.",
                    "sentence_kor": "분해 네트워크는 이미지와 텍스트 사이의 공통성과 불일치를 나타내며, 관계 네트워크는 교차 양식 맥락에서 의미적 연관성을 모델링한다.",
                    "tag": "3"
                },
                {
                    "index": "257-7",
                    "sentence": "Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.",
                    "sentence_kor": "공개 데이터 세트에 대한 실험 결과는 다중 모드 빈정거림 감지에서 모델의 효과를 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "857",
            "abstractID": "SPA_abs-258",
            "text": [
                {
                    "index": "258-0",
                    "sentence": "In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently.",
                    "sentence_kor": "본 연구에서는 소스 언어의 음성을 대상 언어의 텍스트로 동시에 번역하는 엔드 투 엔드 동시 음성-텍스트 번역 시스템인 SimulSpeech를 개발하였다.",
                    "tag": "1+2"
                },
                {
                    "index": "258-1",
                    "sentence": "SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation.",
                    "sentence_kor": "SimulSpeech는 음성 인코더, 음성 세그먼트기 및 텍스트 디코더로 구성되며, 여기서 1) 세그먼트기는 인코더에 구축되고 연결주의 시간 분류(CTC) 손실을 활용하여 입력 스트리밍 음성을 실시간으로 분할한다. 2) 인코더-디코더 주의는 동시 번역을 위한 대기 전략을 채택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "258-2",
                    "sentence": "SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)).",
                    "sentence_kor": "SimulSpeech는 (동시 자동 음성 인식(ASR) 및 동시 신경 기계 번역(NMT)을 갖춘) 이전의 계단식 시스템보다 더 어렵다.",
                    "tag": "3"
                },
                {
                    "index": "258-3",
                    "sentence": "We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech.",
                    "sentence_kor": "성능을 보장하기 위해 두 가지 새로운 지식 증류 방법을 소개한다. 1) 주의 수준 지식 증류는 동시 NMT 및 ASR 모델의 주의 행렬의 곱으로부터 지식을 전송하여 SimulSpeech의 주의 메커니즘 훈련을 돕는다. 2) 데이터 수준 지식 증류법은 지식을 전송한다.완전한 문장 NMT 모델에서 벗어나 SimulSpeech의 최적화를 돕기 위해 데이터 배포의 복잡성을 줄인다.",
                    "tag": "2+3"
                },
                {
                    "index": "258-4",
                    "sentence": "Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.",
                    "sentence_kor": "MuST-C 영어-스페인어 및 영어-독일어 구어 번역 데이터 세트에 대한 실험에 따르면 SimulSpeech는 (동시 번역 없이) 완전한 문장의 단대단 음성에서 텍스트 번역에 비해 합리적인 BLEU 점수를 달성하고 지연 시간을 줄였으며, t의 2단계 단계 단계 동시 번역 모델보다 성능이 우수한 것으로 나타났다.BLEU 점수와 번역 지연이 겹치는 어름들",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "858",
            "abstractID": "SPA_abs-259",
            "text": [
                {
                    "index": "259-0",
                    "sentence": "Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR).",
                    "sentence_kor": "구어 이해 작업은 일반적으로 음성 활동 감지, 스피커 분화 및 자동 음성 인식(ASR)과 같은 복잡한 처리 블록을 포함하는 파이프라인에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "259-1",
                    "sentence": "We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding.",
                    "sentence_kor": "음성 기능에서 직접 발화 수준 레이블을 예측하여 1차 생성 대화록에 대한 의존성과 전사 자유 행동 코딩에 대한 의존성을 제거하기 위한 새로운 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "259-2",
                    "sentence": "Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features.",
                    "sentence_kor": "우리의 분류기는 사전 훈련된 음성-2-벡터 인코더를 병목 현상으로 사용하여 음성 기능에서 단어 수준 표현을 생성한다.",
                    "tag": "2"
                },
                {
                    "index": "259-3",
                    "sentence": "This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec.",
                    "sentence_kor": "사전 훈련된 이 인코더는 Word2Vec와 유사한 목표를 사용하여 단어의 음성 기능을 인코딩하는 방법을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "259-4",
                    "sentence": "Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels.",
                    "sentence_kor": "우리가 제안한 접근 방식은 음성 기능과 단어 세분화 정보를 사용하여 음성 발화 수준 목표 레이블을 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "259-5",
                    "sentence": "We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.",
                    "sentence_kor": "우리는 우리의 모델이 심리치료 관련 행동 코드를 예측하는 작업에 전사된 텍스트를 사용하는 다른 최첨단 접근법에 대해 경쟁적인 결과를 달성한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "859",
            "abstractID": "SPA_abs-260",
            "text": [
                {
                    "index": "260-0",
                    "sentence": "Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context.",
                    "sentence_kor": "트위터의 의견 예측은 트윗 콘텐츠와 이웃 컨텍스트의 일시적인 특성 때문에 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "260-1",
                    "sentence": "In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours.",
                    "sentence_kor": "본 논문에서 우리는 사용자의 트윗 게시 동작을 시간적 포인트 프로세스로 모델링하여 사용자의 과거 트윗 시퀀스와 이웃이 게시한 트윗을 감안할 때 다음 트윗의 게시 시간과 입장 레이블을 공동으로 예측한다.",
                    "tag": "2+3"
                },
                {
                    "index": "260-2",
                    "sentence": "We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context.",
                    "sentence_kor": "우리는 이웃 맥락에서 동적 주제 변화를 포착하기 위해 주제 중심 주의 메커니즘을 설계한다.",
                    "tag": "2+3"
                },
                {
                    "index": "260-3",
                    "sentence": "Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.",
                    "sentence_kor": "실험 결과에 따르면 제안된 모델은 여러 경쟁 기준선에 비해 미래 트윗의 게시 시간과 자세 레이블을 더 정확하게 예측한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "860",
            "abstractID": "SPA_abs-261",
            "text": [
                {
                    "index": "261-0",
                    "sentence": "Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support.",
                    "sentence_kor": "새로운 친목을 쌓거나 기술 지원을 요청하는 등 많은 온라인 텍스트 대화에는 신뢰가 내재되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "261-1",
                    "sentence": "But trust can be betrayed through deception.",
                    "sentence_kor": "그러나 신뢰는 속임수를 통해 배신될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "261-2",
                    "sentence": "We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other.",
                    "sentence_kor": "우리는 7명의 플레이어가 서로 동맹을 맺고 깨면서 세계 지배를 위해 경쟁하는 협상 기반의 게임 '외교'에서 기만의 언어와 역학을 연구합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "261-3",
                    "sentence": "Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.",
                    "sentence_kor": "외교 커뮤니티 참가자들과의 연구는 발신자가 의도한 진실성과 수신자가 인지한 진실성에 대해 주석을 단 17,289개의 메시지를 수집한다.",
                    "tag": "3"
                },
                {
                    "index": "261-4",
                    "sentence": "Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives.",
                    "sentence_kor": "기존 데이터 세트와 달리, 이는 대화자가 목표를 진전시키기 위해 전략적으로 진실과 거짓을 결합하는 오랜 관계에서 속임수를 포착한다.",
                    "tag": "2+3"
                },
                {
                    "index": "261-5",
                    "sentence": "A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.",
                    "sentence_kor": "권력 역학과 대화 맥락을 사용하는 모델은 거짓말이 거의 인간 선수들만큼 일어날 때를 예측할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "861",
            "abstractID": "SPA_abs-262",
            "text": [
                {
                    "index": "262-0",
                    "sentence": "Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks.",
                    "sentence_kor": "생성 기능 일치 네트워크(GFMN)는 사전 훈련된 신경망의 기능에 모멘트 일치를 수행하여 이미지에 대한 최첨단 암시적 생성 모델을 훈련하기 위한 접근법이다.",
                    "tag": "1"
                },
                {
                    "index": "262-1",
                    "sentence": "In this paper, we present new GFMN formulations that are effective for sequential data.",
                    "sentence_kor": "본 논문에서 우리는 순차 데이터에 효과적인 새로운 GFMN 공식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "262-2",
                    "sentence": "Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer.",
                    "sentence_kor": "우리의 실험 결과는 제안된 방법 SeqGFMN이 무조건적인 텍스트 생성, 클래스 조건부 텍스트 생성 및 비지도 텍스트 스타일 전송의 세 가지 뚜렷한 생성 작업에 대한 효과를 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "262-3",
                    "sentence": "SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.",
                    "sentence_kor": "SeqGFMN은 텍스트 생성 및 텍스트 스타일 전송을 위한 다양한 적대적 접근 방식을 교육하기에 안정적이며 능가한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "862",
            "abstractID": "SPA_abs-263",
            "text": [
                {
                    "index": "263-0",
                    "sentence": "A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures.",
                    "sentence_kor": "최근 많은 연구자들이 점점 더 복잡해지는 신경 네트워크(NN) 아키텍처의 필요성에 의문을 제기하고 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "263-1",
                    "sentence": "In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks.",
                    "sentence_kor": "특히, 최근 몇몇 논문은 더 단순하고 적절하게 조정된 모델이 적어도 여러 NLP 작업에 걸쳐 경쟁력이 있다는 것을 보여주었다.",
                    "tag": "1+2"
                },
                {
                    "index": "263-2",
                    "sentence": "In this work, we show that this is also the case for text generation from structured and unstructured data.",
                    "sentence_kor": "본 연구에서는 정형 및 비정형 데이터에서 텍스트를 생성하는 경우에도 마찬가지임을 보여 줍니다.",
                    "tag": "2"
                },
                {
                    "index": "263-3",
                    "sentence": "We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.",
                    "sentence_kor": "우리는 정형 데이터와 비정형 데이터에서 각각 텍스트 생성을 위한 신경 테이블-텍스트 생성 및 신경 질문 생성(NQG) 작업을 고려한다.",
                    "tag": "2+3"
                },
                {
                    "index": "263-4",
                    "sentence": "Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models.",
                    "sentence_kor": "표 대 텍스트 생성은 주어진 표를 기반으로 설명을 생성하는 것을 목표로 하며, NQG는 NN 모델을 사용하여 생성된 질문이 해당 구절의 특정 하위 스팬에 의해 답변될 수 있는 주어진 구절에서 질문을 생성하는 작업이다.",
                    "tag": "4"
                },
                {
                    "index": "263-5",
                    "sentence": "Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks.",
                    "sentence_kor": "실험 결과는 지수 이동 평균 기술로 훈련된 기본 주의 기반 seq2seq 모델이 두 작업 모두에서 최첨단 기술을 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "263-6",
                    "sentence": "Code is available at https://github.com/h-shahidi/2birds-gen.",
                    "sentence_kor": "코드는 https://github.com/h-shahidi/2birds-gen에서 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "863",
            "abstractID": "SPA_abs-264",
            "text": [
                {
                    "index": "264-0",
                    "sentence": "This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.",
                    "sentence_kor": "본 논문은 베이지안 계층적 단어 표현(BHWR) 학습 알고리즘을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "264-1",
                    "sentence": "BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors.",
                    "sentence_kor": "BHWR은 계층적 이전을 통한 의미 분류학 모델링과 결합된 변이 베이즈 단어 표현 학습을 용이하게 한다.",
                    "tag": "4"
                },
                {
                    "index": "264-2",
                    "sentence": "By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations.",
                    "sentence_kor": "BHWR은 관련 단어 사이에 관련 정보를 전파함으로써 분류법을 활용하여 해당 표현의 품질을 개선한다.",
                    "tag": "4"
                },
                {
                    "index": "264-3",
                    "sentence": "Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.",
                    "sentence_kor": "여러 언어 데이터 세트의 평가는 의미론적 우선 순위를 포함하거나 포함하지 않고 베이지안 모델링을 용이하게 하는 적절한 대안보다 BHWR의 장점을 입증한다.",
                    "tag": "4+5"
                },
                {
                    "index": "264-4",
                    "sentence": "Finally, we further show that BHWR produces better representations for rare words.",
                    "sentence_kor": "마지막으로, 우리는 BHWR이 희귀한 단어에 대해 더 나은 표현을 만들어 낸다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "864",
            "abstractID": "SPA_abs-265",
            "text": [
                {
                    "index": "265-0",
                    "sentence": "Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.",
                    "sentence_kor": "사전 훈련된 변압기 모델의 미세 조정은 일반적인 NLP 작업을 해결하기 위한 표준 접근 방식이 되었다.",
                    "tag": "1"
                },
                {
                    "index": "265-1",
                    "sentence": "Most of the existing approaches rely on a randomly initialized classifier on top of such networks.",
                    "sentence_kor": "대부분의 기존 접근법은 이러한 네트워크 위에 무작위로 초기화된 분류기에 의존한다.",
                    "tag": "1+2"
                },
                {
                    "index": "265-2",
                    "sentence": "We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.",
                    "sentence_kor": "사전 훈련된 모델에는 특정 분류기 라벨에 대한 사전이 없기 때문에 이 미세 조정 절차는 차선책이지만 작업의 고유한 텍스트 표현을 이미 학습했을 수 있다고 주장한다.",
                    "tag": "1+2"
                },
                {
                    "index": "265-3",
                    "sentence": "In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.",
                    "sentence_kor": "본 논문에서는 신뢰성 순위 과제를 전체 텍스트 형식으로 캐스팅하고 사전 교육 단계에서 조정된 마스킹 언어 모델링 헤드를 활용하는 새로운 점수 방법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "265-4",
                    "sentence": "We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.",
                    "sentence_kor": "우리는 모델이 COPA, Swag, HellaSwag 및 Commonsense에 초점을 맞춰 전제가 주어진 일련의 가설에 순위를 매겨야 하는 상식 추론 작업을 연구한다.QA 데이터 세트.",
                    "tag": "2+3"
                },
                {
                    "index": "265-5",
                    "sentence": "By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches.",
                    "sentence_kor": "미세 조정 없이 채점 방법을 활용함으로써 감독 접근 방식과 유사한 강력한 기준선(예: COPA의 80% 테스트 정확도)을 생성할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "265-6",
                    "sentence": "Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.",
                    "sentence_kor": "또한 제안된 점수 함수에서 직접 미세 조정할 때, 우리는 우리의 방법이 무작위 재시작(예: COPA 테스트 정확도의 x10 표준 편차 감소)에 걸쳐 훨씬 더 안정적인 훈련 단계를 제공하며 동등한 성능에 도달하기 위해 표준 분류기 접근법보다 주석이 적은 데이터가 필요하다는 것을 보여준다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "865",
            "abstractID": "SPA_abs-266",
            "text": [
                {
                    "index": "266-0",
                    "sentence": "In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.",
                    "sentence_kor": "최근 몇 년 동안 지식 그래프 임베딩은 인공지능의 매우 뜨거운 연구 주제가 되었으며 추천 및 질문 답변과 같은 다양한 다운스트림 애플리케이션에서 점점 더 중요한 역할을 하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "266-1",
                    "sentence": "However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.",
                    "sentence_kor": "그러나 지식 그래프 임베딩을 위한 기존 방법은 모델 복잡성과 모델 표현성 사이에서 적절한 절충을 할 수 없어 여전히 만족스럽지 못하다.",
                    "tag": "1"
                },
                {
                    "index": "266-2",
                    "sentence": "To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.",
                    "sentence_kor": "이 문제를 완화하기 위해 모델 복잡성을 증가시키지 않고도 매우 경쟁력 있는 관계 표현성을 달성할 수 있는 경량 모델링 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "266-3",
                    "sentence": "Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations.",
                    "sentence_kor": "우리의 프레임워크는 점수 함수 설계에 초점을 맞추고 1) 충분한 특징 상호작용 촉진, 2) 관계의 대칭 및 반대칭 특성 보존 등 두 가지 중요한 특성을 강조한다.",
                    "tag": "2"
                },
                {
                    "index": "266-4",
                    "sentence": "It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.",
                    "sentence_kor": "주목할 점은 점수 기능의 일반적이고 우아한 디자인 때문에 우리의 프레임워크가 많은 유명한 기존 방법을 특별한 경우로 통합할 수 있다는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "266-5",
                    "sentence": "Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.",
                    "sentence_kor": "또한 공개 벤치마크에 대한 광범위한 실험은 프레임워크의 효율성과 효과를 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "266-6",
                    "sentence": "Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.",
                    "sentence_kor": "소스 코드와 데이터는 https://github.com/Wentao-Xu/SEEK에서 확인할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "866",
            "abstractID": "SPA_abs-267",
            "text": [
                {
                    "index": "267-0",
                    "sentence": "Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs.",
                    "sentence_kor": "희소 모델은 저장에 필요한 메모리가 적고 필요한 FLOP 수를 줄임으로써 더 빠른 추론을 가능하게 한다.",
                    "tag": "1"
                },
                {
                    "index": "267-1",
                    "sentence": "This is relevant both for time-critical and on-device computations using neural networks.",
                    "sentence_kor": "이는 신경망을 사용하는 시간 중요 및 장치 내 계산에 모두 관련이 있다.",
                    "tag": "1"
                },
                {
                    "index": "267-2",
                    "sentence": "The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model.",
                    "sentence_kor": "안정화된 복권 가설은 네트워크가 정리되지 않은 통합 모델을 기반으로 계산된 마스크를 사용하여 훈련 반복이 없거나 적은 후에 가지치기를 할 수 있다고 명시한다.",
                    "tag": "1"
                },
                {
                    "index": "267-3",
                    "sentence": "On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.",
                    "sentence_kor": "변압기 아키텍처와 WMT 2014 영어-독일어 및 영어-프랑스어 작업에서, 우리는 안정된 복권 가지치기가 최대 85%의 희소성 수준에 대해 크기 가지치기와 유사한 성능을 발휘한다는 것을 보여주고, 훨씬 더 높은 희소성 수준에 대해 다른 모든 기법을 능가하는 새로운 가지치기 기술의 조합을 제안한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "867",
            "abstractID": "SPA_abs-268",
            "text": [
                {
                    "index": "268-0",
                    "sentence": "Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor.",
                    "sentence_kor": "신경 모델은 기계 판독 이해(MRC)에서 큰 성공을 거두었으며, 그 중 많은 부분이 일반적으로 증거 추출기와 답변 예측기의 두 가지 요소로 구성된다.",
                    "tag": "2"
                },
                {
                    "index": "268-1",
                    "sentence": "The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence.",
                    "sentence_kor": "전자는 참조 텍스트에서 가장 관련성이 높은 정보를 찾는 반면 후자는 추출된 증거에서 답을 찾거나 생성하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "268-2",
                    "sentence": "Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.",
                    "sentence_kor": "증거 추출기 훈련을 위한 증거 라벨의 중요성에도 불구하고, 특히 YES/NO 질문 답변 및 다중 선택 MRC와 같은 많은 비 추출적 MRC 작업에서 증거 라벨에 저렴하게 접근할 수 없다.",
                    "tag": "3"
                },
                {
                    "index": "268-3",
                    "sentence": "To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process.",
                    "sentence_kor": "이 문제를 해결하기 위해 반복 프로세스에서 자동 생성된 증거 라벨을 사용하여 증거 추출기를 감독하는 자체 훈련 방법(STM)을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "268-4",
                    "sentence": "At each iteration, a base MRC model is trained with golden answers and noisy evidence labels.",
                    "sentence_kor": "각 반복에서 기본 MRC 모델은 황금 답변과 노이즈가 많은 증거 레이블로 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "268-5",
                    "sentence": "The trained model will predict pseudo evidence labels as extra supervision in the next iteration.",
                    "sentence_kor": "훈련된 모델은 다음 반복 시 추가 감독으로서 유사 증거 레이블을 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "268-6",
                    "sentence": "We evaluate STM on seven datasets over three MRC tasks.",
                    "sentence_kor": "우리는 3개의 MRC 작업에 걸쳐 7개의 데이터 세트에 대해 STM을 평가한다.",
                    "tag": "3+4"
                },
                {
                    "index": "268-7",
                    "sentence": "Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.",
                    "sentence_kor": "실험 결과는 기존 MRC 모델의 개선을 입증하며, 우리는 또한 그러한 자체 훈련 방법이 MRC에서 어떻게 그리고 왜 작동하는지 분석한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "868",
            "abstractID": "SPA_abs-269",
            "text": [
                {
                    "index": "269-0",
                    "sentence": "While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well.",
                    "sentence_kor": "최근의 트리 기반 신경 모델은 수학 단어 문제(MWP)에 대한 솔루션 표현 생성에서 유망한 결과를 보여주었지만, 이러한 모델의 대부분은 수량 간의 관계와 순서 정보를 잘 포착하지 못한다.",
                    "tag": "1"
                },
                {
                    "index": "269-1",
                    "sentence": "This results in poor quantity representations and incorrect solution expressions.",
                    "sentence_kor": "이로 인해 수량 표현이 잘못되고 솔루션 표현이 잘못됩니다.",
                    "tag": "1"
                },
                {
                    "index": "269-2",
                    "sentence": "In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions.",
                    "sentence_kor": "본 논문에서 우리는 그래프 기반 인코더와 트리 기반 디코더의 장점을 결합하여 더 나은 솔루션 표현을 생성하는 새로운 딥 러닝 아키텍처인 Graph2Tree를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "269-3",
                    "sentence": "Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs.",
                    "sentence_kor": "Graph2Tree 프레임워크에는 수량 셀 그래프와 수량 비교 그래프라는 두 개의 그래프가 포함되어 있으며, MWP 단위의 수량 간의 관계와 순서 정보를 효과적으로 표현함으로써 기존 방법의 한계를 해결하도록 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "269-4",
                    "sentence": "We conduct extensive experiments on two available datasets.",
                    "sentence_kor": "우리는 두 개의 사용 가능한 데이터 세트에 대해 광범위한 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "269-5",
                    "sentence": "Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly.",
                    "sentence_kor": "우리의 실험 결과는 Graph2Tree가 두 벤치마크 데이터 세트에서 최첨단 기준선을 크게 능가한다는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "269-6",
                    "sentence": "We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.",
                    "sentence_kor": "또한 사례 연구를 논의하고 MWP 텍스트를 솔루션 표현식으로 변환하는 데 있어 Graph2Tree의 효과를 경험적으로 검토한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "869",
            "abstractID": "SPA_abs-270",
            "text": [
                {
                    "index": "270-0",
                    "sentence": "In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis.",
                    "sentence_kor": "순서 분류 작업에서 항목은 감정 분석에서 \"긍정적\", \"중립적\", \"부정적\"과 같이 상대적인 순서를 가진 클래스에 할당되어야 한다.",
                    "tag": "1"
                },
                {
                    "index": "270-1",
                    "sentence": "Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes).",
                    "sentence_kor": "놀랍게도 순서 분류 작업에 대한 가장 인기 있는 평가 지표는 관련 정보를 무시하거나(예: 각 등급의 정밀도/호출은 상대 순서를 무시한다) 추가 정보를 가정한다(예: 평균 오차는 등급 간 절대 거리를 가정한다).",
                    "tag": "2+3"
                },
                {
                    "index": "270-2",
                    "sentence": "In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory.",
                    "sentence_kor": "본 논문에서 우리는 측정 이론과 정보 이론에 뿌리를 둔 순서 분류, 근접도 평가 측정에 대한 새로운 측정 기준을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "270-3",
                    "sentence": "Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously.",
                    "sentence_kor": "NLP 공유 작업의 합성 데이터와 데이터에 대한 이론적 분석과 실험 결과는 제안된 메트릭이 서로 다른 기존 작업에서 품질 측면을 동시에 포착한다는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "270-4",
                    "sentence": "In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.",
                    "sentence_kor": "또한 인스턴스화된 측정 척도에 따라 몇 가지 일반적인 분류(공칭 척도) 및 오류 최소화(간격 척도) 메트릭을 일반화합니다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "870",
            "abstractID": "SPA_abs-271",
            "text": [
                {
                    "index": "271-0",
                    "sentence": "Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks.",
                    "sentence_kor": "단어의 분산 표현은 자연어 처리(NLP) 작업에 필수적인 구성 요소였다.",
                    "tag": "1"
                },
                {
                    "index": "271-1",
                    "sentence": "However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices).",
                    "sentence_kor": "그러나 단어 임베딩의 메모리 공간이 크기 때문에 NLP 모델을 메모리 제약 장치(예: 자율 주행 자동차, 모바일 장치)에 배치하는 것이 어렵다.",
                    "tag": "1+2"
                },
                {
                    "index": "271-2",
                    "sentence": "In this paper, we propose a novel method to adaptively compress word embeddings.",
                    "sentence_kor": "본 논문에서, 우리는 단어 임베딩을 적응적으로 압축하는 새로운 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "271-3",
                    "sentence": "We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4).",
                    "sentence_kor": "우리는 기본적으로 (8, 5, 2, 4)와 같은 개별 코드로 단어를 나타내는 코드북 접근법을 따른다.",
                    "tag": "2+3"
                },
                {
                    "index": "271-4",
                    "sentence": "However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks.",
                    "sentence_kor": "그러나 모든 단어에 동일한 길이의 코드를 할당한 이전 작업과 달리, 우리는 다운스트림 작업을 학습하여 각 단어에 다른 길이의 코드를 적응적으로 할당한다.",
                    "tag": "3+4"
                },
                {
                    "index": "271-5",
                    "sentence": "The proposed method works in two steps.",
                    "sentence_kor": "제안된 방법은 두 단계로 작동합니다.",
                    "tag": "3"
                },
                {
                    "index": "271-6",
                    "sentence": "First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks.",
                    "sentence_kor": "첫째, 각 단어는 검벨-소프트맥스 트릭을 적용하여 엔드 투 엔드 방식으로 코드 길이를 선택하는 방법을 직접 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "271-7",
                    "sentence": "After selecting the code length, each word learns discrete codes through a neural network with a binary constraint.",
                    "sentence_kor": "코드 길이를 선택한 후, 각 단어는 이진 제약 조건이 있는 신경망을 통해 이산 코드를 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "271-8",
                    "sentence": "To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks.",
                    "sentence_kor": "제안된 방법의 일반적인 적용 가능성을 보여주기 위해, 우리는 네 가지 다른 다운스트림 작업에 대한 성과를 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "271-9",
                    "sentence": "Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy.",
                    "sentence_kor": "종합적인 평가 결과는 우리의 방법이 효과적이며 작업 정확도를 해치지 않고 고도로 압축된 단어 임베딩을 만든다는 것을 분명히 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "271-10",
                    "sentence": "Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.",
                    "sentence_kor": "또한, 우리는 우리의 모델이 작업의 중요성을 고려하여 각 코드북에 단어를 할당한다는 것을 보여준다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "871",
            "abstractID": "SPA_abs-272",
            "text": [
                {
                    "index": "272-0",
                    "sentence": "Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global).",
                    "sentence_kor": "문서 클러스터링은 긴 텍스트의 복잡한 구조, 특히 문장 내(로컬) 및 문장 간 특징(글로벌)에 대한 깊은 이해가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "272-1",
                    "sentence": "Existing representation learning models do not fully capture these features.",
                    "sentence_kor": "기존 표현 학습 모델은 이러한 특징을 완전히 포착하지 못한다.",
                    "tag": "1+2"
                },
                {
                    "index": "272-2",
                    "sentence": "To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph.",
                    "sentence_kor": "이를 해결하기 위해 키워드 상관 그래프에 그래프 자동 인코더(GAE)를 구축하는 문서 클러스터링을 위한 새로운 그래프 기반 표현을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "272-3",
                    "sentence": "The graph is constructed with topical keywords as nodes and multiple local and global features as edges.",
                    "sentence_kor": "그래프는 노드로 주제 키워드를 구성하고 가장자리로 여러 로컬 및 글로벌 형상을 사용하여 구성된다.",
                    "tag": "4"
                },
                {
                    "index": "272-4",
                    "sentence": "A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them.",
                    "sentence_kor": "GAE는 두 가지 특징을 공동으로 재구성할 수 있는 잠재 표현을 학습하여 두 가지 특징을 집계하기 위해 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "272-5",
                    "sentence": "Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes.",
                    "sentence_kor": "그런 다음 문서 클래스를 유도하기 위한 특징으로 벡터 치수를 사용하여 학습된 표현에 대해 클러스터링을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "272-6",
                    "sentence": "Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.",
                    "sentence_kor": "두 데이터 세트에 대한 광범위한 실험을 통해 우리의 접근 방식으로 학습한 기능이 용어 빈도 역 문서 빈도 및 평균 임베딩을 포함한 다른 기존 기능보다 더 나은 클러스터링 성능을 달성할 수 있다는 것을 알 수 있다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "872",
            "abstractID": "SPA_abs-273",
            "text": [
                {
                    "index": "273-0",
                    "sentence": "Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector.",
                    "sentence_kor": "함수 분포 의미론은 단어의 의미를 벡터 대신 함수(이진 분류자)로 표현함으로써 분포 의미론에 대해 언어학적으로 해석 가능한 프레임워크를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "273-1",
                    "sentence": "However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge.",
                    "sentence_kor": "그러나 잠재 변수의 수가 많다는 것은 추론이 계산 비용이 많이 들고, 따라서 모델을 훈련하는 것은 수렴하는 속도가 느리다는 것을 의미한다.",
                    "tag": "1"
                },
                {
                    "index": "273-2",
                    "sentence": "In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.",
                    "sentence_kor": "본 논문에서, 나는 상각후 변동 추론을 수행하기 위해 그래프-컨볼루션 신경망을 사용하여 함수 분포 의미론의 생성 모델을 증강하는 픽시 자동 인코더를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "273-3",
                    "sentence": "This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.",
                    "sentence_kor": "이를 통해 모델을 보다 효과적으로 훈련하여 두 가지 작업(문맥 및 의미 구성의 의미적 유사성)에서 더 나은 결과를 달성하고 사전 훈련된 대규모 언어 모델인 BERT를 능가할 수 있다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "873",
            "abstractID": "SPA_abs-274",
            "text": [
                {
                    "index": "274-0",
                    "sentence": "Pretraining deep language models has led to large performance gains in NLP.",
                    "sentence_kor": "심층 언어 모델을 사전 교육함으로써 NLP에서 큰 성능 향상을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "274-1",
                    "sentence": "Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words.",
                    "sentence_kor": "이러한 성공에도 불구하고 쉬크와 쉬체 (2020)는 최근 이 모델들이 희귀한 단어들을 이해하는데 어려움을 겪고 있다는 것을 보여주었다.",
                    "tag": "1+2"
                },
                {
                    "index": "274-2",
                    "sentence": "For static word embeddings, this problem has been addressed by separately learning representations for rare words.",
                    "sentence_kor": "정적 단어 임베딩의 경우 이 문제는 희귀 단어에 대한 표현을 별도로 학습하여 해결되었다.",
                    "tag": "1+2"
                },
                {
                    "index": "274-3",
                    "sentence": "In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models.",
                    "sentence_kor": "이 작업에서는 이 아이디어를 사전 교육된 언어 모델로 변환합니다. 우리는 심층 언어 모델의 입력 표현으로 적합한 희귀한 단어에 대해 고품질 임베딩을 추론할 수 있는 BERT 기반의 강력한 아키텍처인 BERTRAM을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "274-4",
                    "sentence": "This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture.",
                    "sentence_kor": "이것은 단어의 표면 형태와 맥락이 깊은 아키텍처에서 서로 상호작용할 수 있게 함으로써 달성된다.",
                    "tag": "2+3"
                },
                {
                    "index": "274-5",
                    "sentence": "Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.",
                    "sentence_kor": "BERTRAM을 BERT에 통합하면 희귀한 단어 검색 작업과 세 가지 다운스트림 작업 모두에서 희귀 및 중간 주파수 단어의 표현이 개선되어 성능이 크게 향상된다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "874",
            "abstractID": "SPA_abs-275",
            "text": [
                {
                    "index": "275-0",
                    "sentence": "Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly.",
                    "sentence_kor": "단어의 MFS(Most Frequency Sense)를 알면 WSD(Word Sense Disambigization) 모델에 큰 도움이 된다는 것이 입증되었다.",
                    "tag": "1+2"
                },
                {
                    "index": "275-1",
                    "sentence": "However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary.",
                    "sentence_kor": "그러나 의미 주석 데이터가 부족하기 때문에 언어 어휘에서 의미에 대한 신뢰할 수 있고 커버리지가 높은 분포를 유도하기가 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "275-2",
                    "sentence": "To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences.",
                    "sentence_kor": "이 문제를 해결하기 위해 본 논문에서 우리는 원시 문장의 말뭉치에서 단어 감각의 분포를 유도하기 위한 자동 다국어 접근 방식인 CluBERT를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "275-3",
                    "sentence": "Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches.",
                    "sentence_kor": "우리의 실험은 CluBERT가 대체 접근법에 의해 추출된 것보다 더 높은 품질의 영국 감각에 대한 분포를 학습한다는 것을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "275-4",
                    "sentence": "When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models.",
                    "sentence_kor": "보조 도구의 MFS를 유도하는 데 사용할 경우, CluBERT는 영어 워드 센스 디셈빅화 작업에서 최첨단 결과를 얻고 두 가지 기성 WSD 모델의 모호화 성능을 개선하는 데 도움이 된다.",
                    "tag": "3"
                },
                {
                    "index": "275-5",
                    "sentence": "Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks.",
                    "sentence_kor": "또한, 우리의 배포판은 다국어 WSD 작업에서 MFS를 계산하는 모든 대안을 능가하는 다른 언어에서도 효과적이라는 것이 입증되었다.",
                    "tag": "2+3"
                },
                {
                    "index": "275-6",
                    "sentence": "We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.",
                    "sentence_kor": "우리의 감각 분포는 https://github.com/SapienzaNLP/clubert에서 5개 언어로 배포된다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "875",
            "abstractID": "SPA_abs-276",
            "text": [
                {
                    "index": "276-0",
                    "sentence": "Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data.",
                    "sentence_kor": "교차 도메인 정서 분류는 대량의 레이블링 데이터의 부족을 해결하는 것을 목표로 한다.",
                    "tag": "1+2"
                },
                {
                    "index": "276-1",
                    "sentence": "It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain.",
                    "sentence_kor": "소스 도메인에서 학습한 분류기를 사용하여 대상 도메인의 정서 극성을 예측할 것을 요구한다.",
                    "tag": "1"
                },
                {
                    "index": "276-2",
                    "sentence": "In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation.",
                    "sentence_kor": "본 논문에서는 사전 교육 언어 모델 BERT를 비지도 도메인 적응에 효율적으로 적용하는 방법을 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "276-3",
                    "sentence": "Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge.",
                    "sentence_kor": "사전 훈련 작업과 말뭉치 때문에 BERT는 작업에 무관하며, 도메인 인식이 부족하고 지식을 전송할 때 소스 도메인과 대상 도메인의 특성을 구별할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "276-4",
                    "sentence": "To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task.",
                    "sentence_kor": "이러한 문제를 해결하기 위해, 우리는 대상 도메인 마스킹 언어 모델 작업과 새로운 도메인 구별 사전 훈련 과제를 포함하는 사후 훈련 절차를 설계한다.",
                    "tag": "2"
                },
                {
                    "index": "276-5",
                    "sentence": "The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way.",
                    "sentence_kor": "교육 후 절차는 BERT가 도메인을 인식하도록 장려하고 자체 감독 방식으로 도메인별 기능을 증류한다.",
                    "tag": "2+3"
                },
                {
                    "index": "276-6",
                    "sentence": "Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features.",
                    "sentence_kor": "그런 다음 이를 기반으로 향상된 도메인 불변 기능을 도출하기 위한 적대적 훈련을 수행할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "276-7",
                    "sentence": "Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin.",
                    "sentence_kor": "아마존 데이터 세트에 대한 광범위한 실험은 우리 모델이 최첨단 방법을 큰 폭으로 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "276-8",
                    "sentence": "The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.",
                    "sentence_kor": "절제 연구는 BERT뿐만 아니라 우리의 방법에서도 괄목할 만한 개선이 있었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "876",
            "abstractID": "SPA_abs-277",
            "text": [
                {
                    "index": "277-0",
                    "sentence": "Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches.",
                    "sentence_kor": "최근, 감정 분석은 사전 훈련 접근법의 도움으로 현저한 진전을 보았다.",
                    "tag": "1+2"
                },
                {
                    "index": "277-1",
                    "sentence": "However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches.",
                    "sentence_kor": "그러나 감정어와 양상-감정 쌍과 같은 감정 지식은 전통적인 감정 분석 접근법에 널리 사용되고 있음에도 불구하고 사전 훈련 과정에서 무시된다.",
                    "tag": "1"
                },
                {
                    "index": "277-2",
                    "sentence": "In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks.",
                    "sentence_kor": "본 논문에서는 다중 감정 분석 작업에 대한 통일된 감정 표현을 배우기 위해 정서 지식 강화 사전 훈련(SKEP)을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "277-3",
                    "sentence": "With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation.",
                    "sentence_kor": "SKEP는 자동으로 마이닝된 지식의 도움으로 정서 마스킹을 수행하고 단어, 극성 및 측면 수준의 정서 정보를 사전 훈련된 정서 표현에 포함시키기 위해 세 가지 정서 지식 예측 목표를 구성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "277-4",
                    "sentence": "In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.",
                    "sentence_kor": "특히, 측면-감정 쌍의 예측은 한 쌍의 단어 사이의 의존성을 포착하기 위해 다중 레이블 분류로 변환된다.",
                    "tag": "3"
                },
                {
                    "index": "277-5",
                    "sentence": "Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets.",
                    "sentence_kor": "세 가지 종류의 정서 작업에 대한 실험에 따르면 SKEP는 강력한 사전 훈련 기준을 크게 능가하며 대부분의 테스트 데이터 세트에서 새로운 최첨단 결과를 달성한다.",
                    "tag": "3"
                },
                {
                    "index": "277-6",
                    "sentence": "We release our code at https://github.com/baidu/Senta.",
                    "sentence_kor": "https://github.com/baidu/Senta에서 코드를 공개합니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "877",
            "abstractID": "SPA_abs-278",
            "text": [
                {
                    "index": "278-0",
                    "sentence": "Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces.",
                    "sentence_kor": "심층 신경 언어 모델의 해석성에 대한 최근 연구는 자연어 구문의 많은 특성이 표현 공간에 암호화되어 있다는 결론을 내렸다.",
                    "tag": "1"
                },
                {
                    "index": "278-1",
                    "sentence": "However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism.",
                    "sentence_kor": "그러나 그러한 연구는 종종 단일 언어와 단일 언어 형식주의에 초점을 맞춤으로써 제한된 범위로 어려움을 겪는다.",
                    "tag": "1+2"
                },
                {
                    "index": "278-2",
                    "sentence": "In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages.",
                    "sentence_kor": "본 연구에서는 언어 모델에 의해 포착된 구문 구조의 외형이 표면 구문 또는 심층 구문 분석 스타일을 고수하는 정도와 패턴이 서로 다른 언어에서 일관되는지 여부를 조사하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "278-3",
                    "sentence": "We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.",
                    "sentence_kor": "13개 언어로 훈련된 BERT 및 ELMo 모델에 지시된 종속성 트리를 추출하기 위한 시도를 적용하여 두 가지 구문 주석 스타일을 탐색한다. UD(Universal Dependencies), 심층 구문 관계 우선 순위 지정 및 표면 구조에 초점을 맞춘 SUD(Surface-Syntactic Universal Dependencies)입니다.",
                    "tag": "2+3"
                },
                {
                    "index": "278-4",
                    "sentence": "We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.",
                    "sentence_kor": "우리는 두 모델 모두 언어와 계층에 따라 흥미로운 변화로 SUD보다 UD에 대한 선호도를 나타내며, 이 선호도의 강도는 나무 모양의 차이와 관련이 있다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "878",
            "abstractID": "SPA_abs-279",
            "text": [
                {
                    "index": "279-0",
                    "sentence": "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences.",
                    "sentence_kor": "시퀀스 대 시퀀스 구성 요소 구문 분석에는 트리를 시퀀스로 나타내려면 선형화가 필요합니다.",
                    "tag": "1+2"
                },
                {
                    "index": "279-1",
                    "sentence": "Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date.",
                    "sentence_kor": "괄호 또는 시프트 감소 동작을 기반으로 할 수 있는 하향식 트리 선형화는 현재까지 최고의 정확도를 달성했다.",
                    "tag": "1"
                },
                {
                    "index": "279-2",
                    "sentence": "In this paper, we show that these results can be improved by using an in-order linearization instead.",
                    "sentence_kor": "본 논문에서 우리는 대신 순서 내 선형화를 사용하여 이러한 결과를 개선할 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "279-3",
                    "sentence": "Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers.",
                    "sentence_kor": "이러한 관찰을 바탕으로 Vinyals 등(2015)의 접근 방식에서 영감을 얻은 풍부한 순서 내 이동 감소 선형화를 구현하여 완전한 감독을 받는 단일 모델 시퀀스 투 시퀀스 구성 파서 중 영어 PTB 데이터 세트에 대해 현재까지 최고의 정확도를 달성하였다.",
                    "tag": "3+4"
                },
                {
                    "index": "279-4",
                    "sentence": "Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",
                    "sentence_kor": "마지막으로, 우리는 결정론적 주의 메커니즘을 적용하여 최첨단 전환 기반 파서의 속도를 일치시켜 시퀀스 투 시퀀스 모델이 정확도뿐만 아니라 속도에서도 일치시킬 수 있음을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "879",
            "abstractID": "SPA_abs-280",
            "text": [
                {
                    "index": "280-0",
                    "sentence": "A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar.",
                    "sentence_kor": "그래프 기반 의미 표현을 처리할 때 중요한 문제는 그래프 구문 분석이다. 즉, (능력) 문법에 따라 주어진 그래프의 가능한 모든 파생을 계산하는 것이다.",
                    "tag": "1+2"
                },
                {
                    "index": "280-1",
                    "sentence": "We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs).",
                    "sentence_kor": "우리는 대형 그래프와 대형 하이퍼에지 교체 문법(HRG)을 사용하여 정확한 그래프 파싱이 효율적일 수 있음을 처음으로 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "280-2",
                    "sentence": "The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules.",
                    "sentence_kor": "발전은 HRG 규칙의 말단 에지 인접성으로 지역성을 활용하여 달성된다.",
                    "tag": "2+3"
                },
                {
                    "index": "280-3",
                    "sentence": "In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.",
                    "sentence_kor": "특히 1) 말단 에지 우선 구문 분석 전략, 2) HRG 하위 클래스의 범주화, 즉 우리가 약하게 정규 그래프 문법이라고 부르는 것, 3) 어휘 및 구어 규칙 모두에 인수 구조를 배포하는 것의 중요성을 강조한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "880",
            "abstractID": "SPA_abs-281",
            "text": [
                {
                    "index": "281-0",
                    "sentence": "Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT.",
                    "sentence_kor": "증분 구문 분석은 인간 문장 처리를 모델링하려는 인지 과학자와 ASR 및 MT용 언어 모델링과 증분 구문 분석을 결합하려는 NLP 연구원 모두에게 활발한 연구 영역이었다.",
                    "tag": "1"
                },
                {
                    "index": "281-1",
                    "sentence": "Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like.",
                    "sentence_kor": "대부분의 노력은 올바른 전환 메커니즘을 설계하는 데 집중되었지만 이러한 전환 파서에 대한 확률적 모델이 어떤 모습이어야 하는지에 대한 질문에 답하는 작업은 거의 수행되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "281-2",
                    "sentence": "A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank.",
                    "sentence_kor": "최근 제안된 CCG 파서의 매우 점진적인 전환 메커니즘은 간단한 국소 정규화 차별적 방식으로 훈련되었을 때 영국 CCG뱅크에서 매우 나쁜 결과를 초래한다.",
                    "tag": "1"
                },
                {
                    "index": "281-3",
                    "sentence": "We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias.",
                    "sentence_kor": "우리는 이 문제의 원인으로 세 가지 편견, 즉 라벨 편향, 노출 편향 및 불균형 확률 편향을 식별한다.",
                    "tag": "2"
                },
                {
                    "index": "281-4",
                    "sentence": "While known techniques for tackling these biases improve results, they still do not make the parser state of the art.",
                    "sentence_kor": "이러한 편견을 해결하기 위한 알려진 기술은 결과를 개선하지만, 여전히 기술의 파서 상태를 만들지는 못한다.",
                    "tag": "2+3"
                },
                {
                    "index": "281-5",
                    "sentence": "Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation.",
                    "sentence_kor": "대신, 우리는 가장 큰 위반만 최소화하는 대신 모든 빔 검색 위반을 최소화하는 개선된 빔 검색 최적화 버전을 사용하여 이 세 가지 편견을 동시에 해결한다.",
                    "tag": "4"
                },
                {
                    "index": "281-6",
                    "sentence": "The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.",
                    "sentence_kor": "새로운 증분 파서는 이전에 발표된 모든 증분 CCG 파서보다 더 나은 결과를 제공하며 널리 사용되는 일부 비증분 CCG 파서보다 성능이 우수하다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "881",
            "abstractID": "SPA_abs-282",
            "text": [
                {
                    "index": "282-0",
                    "sentence": "Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser.",
                    "sentence_kor": "최근 연구에서 신경 재배열기는 베이스 파서가 생성하는 상위 k 트리에 대한 의존성 구문 분석 결과를 개선할 수 있다는 것을 보여주었다.",
                    "tag": "1+2"
                },
                {
                    "index": "282-1",
                    "sentence": "However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology.",
                    "sentence_kor": "그러나 지금까지 모든 신경 재배열자는 구성 어순과 서투른 형태를 가진 두 언어 모두 영어와 중국어에 대해서만 평가되었다.",
                    "tag": "1+2"
                },
                {
                    "index": "282-2",
                    "sentence": "In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech.",
                    "sentence_kor": "이 논문에서 우리는 영어와 형태학적으로 풍부한 두 언어인 독일어와 체코어에서 성공적인 신경 재순위 모델의 가능성을 재평가한다.",
                    "tag": "2"
                },
                {
                    "index": "282-3",
                    "sentence": "In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs).",
                    "sentence_kor": "또한 그래프 컨볼루션 네트워크(GCN)를 기반으로 한 차별적 리랭커의 새로운 변형을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "282-4",
                    "sentence": "We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech.",
                    "sentence_kor": "우리는 GCN이 영어에서 이전 모델을 능가할 뿐만 아니라 독일어 및 체코어 기준보다 결과를 개선할 수 있는 유일한 모델임을 보여준다.",
                    "tag": "3"
                },
                {
                    "index": "282-5",
                    "sentence": "We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.",
                    "sentence_kor": "우리는 a) 금나무 비율과 b) k-best 리스트의 다양성에 대한 분석을 바탕으로 실적의 순위를 재조정하는 차이에 대해 설명한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "882",
            "abstractID": "SPA_abs-283",
            "text": [
                {
                    "index": "283-0",
                    "sentence": "To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.",
                    "sentence_kor": "인공지능 시스템에 대한 신뢰를 높이기 위해, 유망한 연구 방향은 예측에 대한 자연어 설명을 생성할 수 있는 신경 모델을 설계하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "283-1",
                    "sentence": "In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.",
                    "sentence_kor": "이 연구에서, 우리는 그러한 모델이 그럼에도 불구하고 \"이미지에 개가 있기 때문\" 및 \"같은 이미지에 개가 없기 때문에\"와 같은 상호 불일치 설명을 생성하는 경향이 있음을 보여준다. 모델의 의사결정 과정 또는 설명 생성의 결함을 노출시킨다.",
                    "tag": "2+3"
                },
                {
                    "index": "283-2",
                    "sentence": "We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.",
                    "sentence_kor": "우리는 일관성이 없는 자연어 설명의 생성에 대항하여 온전성 검사 모델을 위한 간단하지만 효과적인 적대적 프레임워크를 도입한다.",
                    "tag": "1+2"
                },
                {
                    "index": "283-3",
                    "sentence": "Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.",
                    "sentence_kor": "또한 프레임워크의 일부로, 우리는 시퀀스 투 시퀀스 공격에서 이전에는 다루지 않았던 시나리오인 전체 표적 시퀀스로 적대적 공격 문제를 다룬다.",
                    "tag": "3"
                },
                {
                    "index": "283-4",
                    "sentence": "Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.",
                    "sentence_kor": "마지막으로, 우리는 예측에 대한 자연어 설명을 제공하는 최첨단 신경 자연어 추론 모델에 우리의 프레임워크를 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "283-5",
                    "sentence": "Our framework shows that this model is capable of generating a significant number of inconsistent explanations.",
                    "sentence_kor": "우리의 프레임워크는 이 모델이 상당한 수의 일관성 없는 설명을 생성할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "883",
            "abstractID": "SPA_abs-284",
            "text": [
                {
                    "index": "284-0",
                    "sentence": "By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).",
                    "sentence_kor": "작은 추가 매개변수 세트를 도입함으로써, 시도는 특징 표현(예: 상황별 임베딩)을 사용하여 감독된 방식으로 특정 언어 작업(예: 종속성 구문 분석)을 해결하는 방법을 학습한다.",
                    "tag": "1+2"
                },
                {
                    "index": "284-1",
                    "sentence": "The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.",
                    "sentence_kor": "이러한 조사 작업의 효과는 사전 훈련된 모델이 언어 지식을 인코딩한다는 증거로 간주된다.",
                    "tag": "2"
                },
                {
                    "index": "284-2",
                    "sentence": "However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.",
                    "sentence_kor": "그러나, 언어 모델을 평가하는 이 접근법은 조사 자체에 의해 학습되는 지식의 양의 불확실성에 의해 약화된다.",
                    "tag": "3"
                },
                {
                    "index": "284-3",
                    "sentence": "Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).",
                    "sentence_kor": "이러한 작업을 보완하여 사전 훈련된 언어 모델(예: BERT)을 분석하기 위한 매개 변수 없는 탐색 기법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "284-4",
                    "sentence": "Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.",
                    "sentence_kor": "우리의 방법은 프로빙 작업의 직접적인 감독을 필요로 하지 않으며 프로빙 프로세스에 추가 매개 변수를 도입하지도 않는다.",
                    "tag": "3"
                },
                {
                    "index": "284-5",
                    "sentence": "Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.",
                    "sentence_kor": "BERT에 대한 우리의 실험은 우리의 방법을 사용하여 BERT에서 복구된 구문 트리가 언어학적으로 형성되지 않은 기준선보다 훨씬 더 낫다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "284-6",
                    "sentence": "We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.",
                    "sentence_kor": "우리는 또한 경험적으로 유도된 의존성 구조를 다운스트림 정서 분류 작업에 공급하고 그 개선이 인간이 설계한 의존성 스키마와 호환되거나 심지어 우월하다는 것을 발견한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "884",
            "abstractID": "SPA_abs-285",
            "text": [
                {
                    "index": "285-0",
                    "sentence": "Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence.",
                    "sentence_kor": "언어 모델은 문장의 구문 관계를 포함하여 앞의 문맥에 대한 복잡한 정보를 추적합니다.",
                    "tag": "1+2"
                },
                {
                    "index": "285-1",
                    "sentence": "We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.",
                    "sentence_kor": "우리는 그들이 영어로 프로노미날 아나포라를 해결하는 데 도움이 되는 정보도 캡처하는지 조사한다.",
                    "tag": "1"
                },
                {
                    "index": "285-2",
                    "sentence": "We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.",
                    "sentence_kor": "우리는 탐사 작업과 코어 참조 주석이 달린 말뭉치에 대한 분석을 통해 LSTM 및 트랜스포머 아키텍처를 사용하여 두 가지 최신 모델을 분석한다.",
                    "tag": "2+3"
                },
                {
                    "index": "285-3",
                    "sentence": "The Transformer outperforms the LSTM in all analyses.",
                    "sentence_kor": "트랜스포머는 모든 분석에서 LSTM을 능가한다.",
                    "tag": "1+2"
                },
                {
                    "index": "285-4",
                    "sentence": "Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.",
                    "sentence_kor": "우리의 결과는 언어 모델이 진정한 참조 정보를 학습하는 것보다 문법적 제약을 학습하는 데 더 성공적이라는 것을 암시한다. 우리가 세계의 실체를 지칭하기 위해 언어를 사용한다는 점에서 말이다.",
                    "tag": "4+5"
                },
                {
                    "index": "285-5",
                    "sentence": "However, we find traces of the latter aspect, too.",
                    "sentence_kor": "그러나 우리는 후자의 흔적도 발견한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "885",
            "abstractID": "SPA_abs-286",
            "text": [
                {
                    "index": "286-0",
                    "sentence": "In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer.",
                    "sentence_kor": "트랜스포머 모델에서 \"자기 주의\"는 참석한 임베딩의 정보를 다음 계층의 초점 임베딩의 표현으로 결합한다.",
                    "tag": "1"
                },
                {
                    "index": "286-1",
                    "sentence": "Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.",
                    "sentence_kor": "따라서 트랜스포머의 여러 계층에서 서로 다른 토큰에서 비롯된 정보가 점점 더 섞이게 됩니다.",
                    "tag": "1+2"
                },
                {
                    "index": "286-2",
                    "sentence": "This makes attention weights unreliable as explanations probes.",
                    "sentence_kor": "따라서 설명 탐침으로 주의 가중치를 신뢰할 수 없습니다.",
                    "tag": "1"
                },
                {
                    "index": "286-3",
                    "sentence": "In this paper, we consider the problem of quantifying this flow of information through self-attention.",
                    "sentence_kor": "본 논문에서 우리는 자기 주의를 통해 이러한 정보 흐름을 수량화하는 문제를 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "286-4",
                    "sentence": "We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.",
                    "sentence_kor": "우리는 입력 토큰의 상대적인 관련성으로 주의 가중치를 사용할 때 사후적 방법으로 주의 가중치, 주의 롤아웃 및 주의 흐름을 주어진 입력 토큰에 주의를 근사화하는 두 가지 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "286-5",
                    "sentence": "We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
                    "sentence_kor": "우리는 이러한 방법이 정보의 흐름에 대한 보완적 시각을 제공한다는 것을 보여주며, 원시 주의와 비교하여 둘 다 절제 방법 및 입력 그라데이션으로 얻은 입력 토큰의 중요도 점수와 더 높은 상관관계를 산출한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "886",
            "abstractID": "SPA_abs-287",
            "text": [
                {
                    "index": "287-0",
                    "sentence": "Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions.",
                    "sentence_kor": "주의 분포의 해석 가능성에 대한 최근의 연구는 모델의 예측에 대한 충실하고 그럴듯한 설명의 개념을 이끌어냈다.",
                    "tag": "1+2"
                },
                {
                    "index": "287-1",
                    "sentence": "Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction.",
                    "sentence_kor": "주의 가중치가 높을수록 모델 예측에 미치는 영향이 클 경우 주의 분포는 충실한 설명으로 간주할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "287-2",
                    "sentence": "They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions.",
                    "sentence_kor": "모델 예측에 대해 사람이 이해할 수 있는 정당성을 제공할 경우 그럴듯한 설명으로 간주할 수 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "287-3",
                    "sentence": "In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions.",
                    "sentence_kor": "이 연구에서는 먼저 LSTM 기반 인코더의 현재 주의 메커니즘이 모델 예측에 대한 충실하거나 그럴듯한 설명을 제공할 수 없는 이유를 설명한다.",
                    "tag": "1+2"
                },
                {
                    "index": "287-4",
                    "sentence": "We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions.",
                    "sentence_kor": "LSTM 기반 인코더에서 서로 다른 시간 단계의 숨겨진 표현은 서로 매우 유사하며(높은 원뿔성) 이러한 상황에서 주의 가중치는 임의 순열도 모델의 예측에 영향을 미치지 않기 때문에 큰 의미를 갖지 않는다는 것을 관찰한다.",
                    "tag": "3"
                },
                {
                    "index": "287-5",
                    "sentence": "Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.",
                    "sentence_kor": "광범위한 작업 및 데이터 세트에 대한 실험을 바탕으로 주의 분포를 관찰하여 모델의 예측을 구두점과 같은 중요하지 않은 단어로 돌리고 예측에 대한 그럴듯한 설명을 제공하지 못하는 경우가 많다.",
                    "tag": "3+4"
                },
                {
                    "index": "287-6",
                    "sentence": "To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse.",
                    "sentence_kor": "주의 메커니즘을 보다 충실하고 그럴듯하게 만들기 위해, 우리는 다양한 시간 단계에서 학습된 숨겨진 표현이 다양하도록 보장하는 다양성 중심 훈련 목표를 가진 수정된 LSTM 셀을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "287-7",
                    "sentence": "We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.",
                    "sentence_kor": "우리는 결과 주의 분포가 (i) 은닉 상태의 보다 정확한 중요도 순위를 제공하므로 (iii) 모델의 예측에 중요한 단어들을 더 잘 나타낸다(iii) 경사도 기반 속성 방법과 더 잘 상관된다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "287-8",
                    "sentence": "Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions.",
                    "sentence_kor": "인간 평가는 우리 모델에 의해 학습된 주의 분포가 모델의 예측에 대한 그럴듯한 설명을 제공한다는 것을 보여준다.",
                    "tag": "4+5"
                },
                {
                    "index": "287-9",
                    "sentence": "Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention",
                    "sentence_kor": "우리의 코드는 https://github.com/akashkm99/Interpretable-Attention에서 공개되었습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "887",
            "abstractID": "SPA_abs-288",
            "text": [
                {
                    "index": "288-0",
                    "sentence": "Multi-task Learning methods have achieved great progress in text classification.",
                    "sentence_kor": "다중 작업 학습 방법은 텍스트 분류에서 큰 진전을 이루었다.",
                    "tag": "1"
                },
                {
                    "index": "288-1",
                    "sentence": "However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.",
                    "sentence_kor": "그러나 기존 방법은 다중 작업 텍스트 분류 문제가 볼록한 다중 객체 최적화 문제이며, 이는 실제 애플리케이션에서 비현실적이라고 가정한다.",
                    "tag": "1+2"
                },
                {
                    "index": "288-2",
                    "sentence": "To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.",
                    "sentence_kor": "이 문제를 해결하기 위해 본 논문은 볼록한 가정 없이 다중 작업 분류 문제를 최적화하는 새로운 Tchebycheff 절차를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "288-3",
                    "sentence": "The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.",
                    "sentence_kor": "광범위한 실험은 우리의 이론적 분석을 뒷받침하고 우리 제안의 우수성을 입증한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "888",
            "abstractID": "SPA_abs-289",
            "text": [
                {
                    "index": "289-0",
                    "sentence": "This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.",
                    "sentence_kor": "이 논문은 풍부한 언어 정보를 사용하여 NMT에서 단어 형성을 모델링하는 전략, 즉 퓨전 형태학을 고려하여 하위 문자열로 분할하는 것을 넘어서는 단어 세분화 접근 방식을 연구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "289-1",
                    "sentence": "Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.",
                    "sentence_kor": "언어학적으로 건전한 세분화는 모델링 단어 형성을 수용하기 위해 대상 측 변곡 방법과 결합된다.",
                    "tag": "3"
                },
                {
                    "index": "289-2",
                    "sentence": "The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.",
                    "sentence_kor": "최상의 시스템 변형은 소스 측 형태학적 분석과 복잡한 대상 측 단어를 모델링하여 표준 시스템보다 개선한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "889",
            "abstractID": "SPA_abs-290",
            "text": [
                {
                    "index": "290-0",
                    "sentence": "Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.",
                    "sentence_kor": "능동 학습에 대한 기존 접근 방식은 가장 효율적인 교육을 산출하는 주석을 위해 레이블이 없는 인스턴스를 샘플링하여 시스템 성능을 극대화한다.",
                    "tag": "1"
                },
                {
                    "index": "290-1",
                    "sentence": "However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.",
                    "sentence_kor": "그러나 능동적 학습이 최종 사용자 애플리케이션과 통합될 경우 참여 사용자가 그렇지 않으면 읽기에 관심이 없을 인스턴스 라벨링에 시간을 소비하기 때문에 이로 인해 좌절감을 느낄 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "290-2",
                    "sentence": "In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).",
                    "sentence_kor": "본 논문에서, 우리는 능동 학습 시스템(효율적으로 훈련)과 사용자(유용한 인스턴스 수신)의 겉보기에 대응되는 목표를 공동으로 최적화하는 새로운 능동 학습 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "290-3",
                    "sentence": "We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.",
                    "sentence_kor": "우리는 교육 애플리케이션에서 우리의 접근방식을 연구하는데, 이 기법은 시스템이 특정 사용자에 대한 연습의 적절성을 신속하게 예측하는 방법을 배워야 하는 반면, 사용자는 자신의 기술과 일치하는 연습만 받아야 하기 때문에 특히 이 기술의 이점을 얻는다.",
                    "tag": "2+3"
                },
                {
                    "index": "290-4",
                    "sentence": "We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.",
                    "sentence_kor": "우리는 실제 사용자의 데이터로 여러 학습 전략과 사용자 유형을 평가하고 대체 방법이 최종 사용자에게 많은 부적합한 연습으로 이어질 때 우리의 공동 접근 방식이 두 가지 목표를 더 잘 만족시킨다는 것을 발견한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "890",
            "abstractID": "SPA_abs-291",
            "text": [
                {
                    "index": "291-0",
                    "sentence": "This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).",
                    "sentence_kor": "본 논문은 BERT와 같은 사전 훈련된 마스킹 언어 모델(MLM)을 문법 오류 수정(GEC)을 위한 인코더-디코더(EncDec) 모델에 효과적으로 통합하는 방법을 조사한다.",
                    "tag": "1+2"
                },
                {
                    "index": "291-1",
                    "sentence": "The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.",
                    "sentence_kor": "EncDec 모델에 MLM을 통합하는 이전의 일반적인 방법에는 GEC에 적용할 때 잠재적인 단점이 있기 때문에 이 질문에 대한 답은 예상만큼 간단하지 않다.",
                    "tag": "2+3"
                },
                {
                    "index": "291-2",
                    "sentence": "For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.",
                    "sentence_kor": "예를 들어 GEC 모델에 대한 입력 분포는 사전 훈련 MLM에 사용된 말뭉치의 분포와 상당히 다를 수 있다(오류, 어설픈 등). 그러나 이 문제는 이전 방법에서는 다루지 않았다.",
                    "tag": "3"
                },
                {
                    "index": "291-3",
                    "sentence": "Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM.",
                    "sentence_kor": "우리의 실험은 우리가 먼저 주어진 GEC 말뭉치로 MLM을 미세 조정한 다음 미세 조정된 MLM의 출력을 GEC 모델의 추가 기능으로 사용하는 제안된 방법이 MLM의 이점을 극대화한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "291-4",
                    "sentence": "The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.",
                    "sentence_kor": "가장 성능이 좋은 모델은 BEA-2019 및 CoNLL-2014 벤치마크에서 최첨단 성능을 달성합니다.",
                    "tag": "4+5"
                },
                {
                    "index": "291-5",
                    "sentence": "Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.",
                    "sentence_kor": "우리의 코드는 https://github.com/kanekomasahiro/bert-gec에서 공개적으로 이용할 수 있다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "891",
            "abstractID": "SPA_abs-292",
            "text": [
                {
                    "index": "292-0",
                    "sentence": "With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.",
                    "sentence_kor": "뉴스 정보의 폭발과 함께, 개인화된 뉴스 추천은 사용자들이 그들의 관심 있는 콘텐츠를 빠르게 찾는 데 매우 중요해졌다.",
                    "tag": "1"
                },
                {
                    "index": "292-1",
                    "sentence": "Most existing methods usually learn the representations of users and news from news contents for recommendation.",
                    "sentence_kor": "대부분의 기존 방법은 일반적으로 추천을 위해 뉴스 콘텐츠에서 사용자와 뉴스를 학습합니다.",
                    "tag": "1"
                },
                {
                    "index": "292-2",
                    "sentence": "However, they seldom consider high-order connectivity underlying the user-news interactions.",
                    "sentence_kor": "그러나 사용자-뉴스 상호 작용의 기본이 되는 고차 연결성은 거의 고려하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "292-3",
                    "sentence": "Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news.",
                    "sentence_kor": "또한 기존 방법은 사용자가 다른 뉴스를 클릭하게 하는 잠재적인 선호 요인을 파악하지 못했다.",
                    "tag": "1"
                },
                {
                    "index": "292-4",
                    "sentence": "In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.",
                    "sentence_kor": "본 논문에서는 사용자-뉴스 상호작용을 이분 그래프로 모델링하고 GNUD라는 비감독 선호도 해제를 포함한 새로운 그래프 신경 뉴스 권장 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "292-5",
                    "sentence": "Our model can encode high-order relationships into user and news representations by information propagation along the graph.",
                    "sentence_kor": "우리 모델은 그래프를 따라 정보 전파를 통해 고차 관계를 사용자와 뉴스 표현으로 인코딩할 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "292-6",
                    "sentence": "Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.",
                    "sentence_kor": "또한, 학습된 표현은 이웃 라우팅 알고리즘에 의해 잠재적 선호 요소와 얽혀 있으며, 이는 표현성과 해석성을 향상시킬 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "292-7",
                    "sentence": "A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.",
                    "sentence_kor": "기본 설정 정규화기는 또한 분리된 기본 설정을 독립적으로 반영하여 분리된 표현의 품질을 향상시키도록 각 분리된 하위 공간을 강제하도록 설계되었다.",
                    "tag": "3+4"
                },
                {
                    "index": "292-8",
                    "sentence": "Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.",
                    "sentence_kor": "실제 뉴스 데이터 세트에 대한 실험 결과는 제안된 모델이 뉴스 추천의 성능을 효과적으로 개선하고 최첨단 뉴스 추천 방법을 능가할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "892",
            "abstractID": "SPA_abs-293",
            "text": [
                {
                    "index": "293-0",
                    "sentence": "In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.",
                    "sentence_kor": "본 논문에서 우리는 형사 사건에서 여러 피고를 대상으로 한 사실 설명에서 원칙과 부속품을 식별하는 문제를 연구한다.",
                    "tag": "1"
                },
                {
                    "index": "293-1",
                    "sentence": "We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.",
                    "sentence_kor": "우리는 사실 묘사를 서술 텍스트로, 피고들을 서술 이야기에 대한 역할로 취급한다.",
                    "tag": "2"
                },
                {
                    "index": "293-2",
                    "sentence": "We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework.",
                    "sentence_kor": "우리는 행동 의미 정보와 통계적 특성을 가진 피고를 모델링한 다음 순위 학습 프레임워크 내에서 피고의 중요성을 학습할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "293-3",
                    "sentence": "Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.",
                    "sentence_kor": "실제 데이터 세트에 대한 실험 결과는 행동 분석이 복잡한 경우 피고의 영향을 효과적으로 모델링할 수 있음을 보여준다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "893",
            "abstractID": "SPA_abs-294",
            "text": [
                {
                    "index": "294-0",
                    "sentence": "The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.",
                    "sentence_kor": "온라인 통신 플랫폼의 상승은 온라인상에서 공격적이고 학대적인 행동이 확산되는 것과 같은 바람직하지 않은 영향을 동반하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "294-1",
                    "sentence": "Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.",
                    "sentence_kor": "이 문제를 해결하기 위해 자연어 처리(NLP) 커뮤니티는 남용 탐지를 위한 다양한 기술을 실험했다.",
                    "tag": "1"
                },
                {
                    "index": "294-2",
                    "sentence": "While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.",
                    "sentence_kor": "이러한 방법은 상당한 성공을 거두었지만, 지금까지 사용자의 감정 상태와 이것이 언어에 어떤 영향을 미칠 수 있는지는 무시하고 논평과 사용자의 온라인 커뮤니티의 언어 속성을 모델링하는 데만 초점을 맞추었다.",
                    "tag": "2+3"
                },
                {
                    "index": "294-3",
                    "sentence": "The latter is, however, inextricably linked to abusive behaviour.",
                    "sentence_kor": "그러나 후자는 학대 행위와 불가분의 관계에 있다.",
                    "tag": "3"
                },
                {
                    "index": "294-4",
                    "sentence": "In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.",
                    "sentence_kor": "본 논문에서, 우리는 한 과제가 다른 과제에게 알릴 수 있도록 하는 다중 작업 학습 프레임워크에서 실험하는 감정과 욕설 탐지의 첫 번째 공동 모델을 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "294-5",
                    "sentence": "Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.",
                    "sentence_kor": "우리의 결과는 정서적 기능을 통합하면 데이터셋 전체의 남용 탐지 성능이 크게 향상된다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "894",
            "abstractID": "SPA_abs-295",
            "text": [
                {
                    "index": "295-0",
                    "sentence": "The key to effortless end-user programming is natural language.",
                    "sentence_kor": "쉬운 최종 사용자 프로그래밍의 핵심은 자연어입니다.",
                    "tag": "1+2"
                },
                {
                    "index": "295-1",
                    "sentence": "We examine how to teach intelligent systems new functions, expressed in natural language.",
                    "sentence_kor": "우리는 자연어로 표현된 지능형 시스템에 새로운 기능을 가르치는 방법을 검토한다.",
                    "tag": "1+2"
                },
                {
                    "index": "295-2",
                    "sentence": "As a first step, we collected 3168 samples of teaching efforts in plain English.",
                    "sentence_kor": "첫 단계로, 우리는 쉬운 영어로 된 3168개의 교육 노력의 샘플을 수집했다.",
                    "tag": "3"
                },
                {
                    "index": "295-3",
                    "sentence": "Then we built fuSE, a novel system that translates English function descriptions into code.",
                    "sentence_kor": "그리고 나서 우리는 영어 기능 설명을 코드로 변환하는 새로운 시스템인 fuSE를 만들었다.",
                    "tag": "3+4"
                },
                {
                    "index": "295-4",
                    "sentence": "Our approach is three-tiered and each task is evaluated separately.",
                    "sentence_kor": "우리의 접근 방식은 3계층이며 각 작업은 별도로 평가된다.",
                    "tag": "4"
                },
                {
                    "index": "295-5",
                    "sentence": "We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).",
                    "sentence_kor": "먼저 발화에 새로운 기능을 가르치려는 의도가 있는지 여부를 분류한다(정확도: 97.7% BERT 사용).",
                    "tag": "4"
                },
                {
                    "index": "295-6",
                    "sentence": "Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).",
                    "sentence_kor": "그런 다음 언어 구조를 분석하고 의미론적 모델(정확도: 97.6%)을 구성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "295-7",
                    "sentence": "Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).",
                    "sentence_kor": "마지막으로 방법의 서명을 합성하고 중간 단계(방법 본문의 지침)를 API 호출에 매핑하고 제어 구조(F1: 67.0%)를 주입한다.",
                    "tag": "3+4"
                },
                {
                    "index": "295-8",
                    "sentence": "In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.",
                    "sentence_kor": "보이지 않는 데이터 세트에 대한 엔드 투 엔드 평가에서 fuSE는 방법 서명의 84.6%, API 호출의 79.2%를 정확하게 합성했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "895",
            "abstractID": "SPA_abs-296",
            "text": [
                {
                    "index": "296-0",
                    "sentence": "Moderation is crucial to promoting healthy online discussions.",
                    "sentence_kor": "건전한 온라인 토론을 촉진하기 위해서는 절제가 중요하다.",
                    "tag": "1"
                },
                {
                    "index": "296-1",
                    "sentence": "Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.",
                    "sentence_kor": "비록 몇몇 '독성' 탐지 데이터 세트와 모델이 발표되었지만, 그들 대부분은 댓글이 독립적으로 판단될 수 있다고 암시적으로 가정하면서 게시물의 맥락을 무시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "296-2",
                    "sentence": "We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?",
                    "sentence_kor": "우리는 두 가지 질문에 초점을 맞춰 이 가정을 조사한다. (a) 맥락이 인간의 판단에 영향을 미치는가? (b) 맥락에 대한 조건이 독성 탐지 시스템의 성능을 향상시키는가?",
                    "tag": "3"
                },
                {
                    "index": "296-3",
                    "sentence": "We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.",
                    "sentence_kor": "우리는 위키피디아의 대화를 실험합니다. 맥락에 대한 개념을 줄과 토론 제목에 있는 이전 게시물로 제한하면서요.",
                    "tag": "3"
                },
                {
                    "index": "296-4",
                    "sentence": "We find that context can both amplify or mitigate the perceived toxicity of posts.",
                    "sentence_kor": "우리는 문맥이 포스트의 인지된 독성을 증폭시키거나 완화시킬 수 있다는 것을 발견했다.",
                    "tag": "3"
                },
                {
                    "index": "296-5",
                    "sentence": "Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.",
                    "sentence_kor": "또한 수동으로 라벨링된 게시물의 작지만 중요한 부분 집합(우리 실험 중 하나에서 5%)은 주석자에게 맥락을 제공하지 않을 경우 반대 독성 라벨을 갖게 된다.",
                    "tag": "4"
                },
                {
                    "index": "296-6",
                    "sentence": "Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.",
                    "sentence_kor": "놀랍게도, 우리는 또한 문맥이 문맥을 인식하기 위해 다양한 분류기와 메커니즘을 시도했기 때문에 문맥이 실제로 독성 분류기의 성능을 향상시킨다는 증거를 발견하지 못했다.",
                    "tag": "4"
                },
                {
                    "index": "296-7",
                    "sentence": "This points to the need for larger datasets of comments annotated in context.",
                    "sentence_kor": "이는 컨텍스트에 주석이 달린 더 큰 주석 데이터 세트가 필요하다는 것을 의미한다.",
                    "tag": "5"
                },
                {
                    "index": "296-8",
                    "sentence": "We make our code and data publicly available.",
                    "sentence_kor": "우리는 코드와 데이터를 공개적으로 사용할 수 있도록 합니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "896",
            "abstractID": "SPA_abs-297",
            "text": [
                {
                    "index": "297-0",
                    "sentence": "Answering natural language questions over tables is usually seen as a semantic parsing task.",
                    "sentence_kor": "표를 통해 자연어 질문에 답하는 것은 일반적으로 의미 구문 분석 작업으로 간주된다.",
                    "tag": "1"
                },
                {
                    "index": "297-1",
                    "sentence": "To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.",
                    "sentence_kor": "완전한 논리 형식의 수집 비용을 줄이기 위해, 한 가지 일반적인 접근법은 논리 형식 대신 표시로 구성된 약한 감독에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "297-2",
                    "sentence": "However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.",
                    "sentence_kor": "그러나 약한 감독에서 의미론적 파서를 훈련하면 어려움이 따르며, 또한 생성된 논리 형식은 의미를 검색하기 전의 중간 단계로만 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "297-3",
                    "sentence": "In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.",
                    "sentence_kor": "본 논문에서 우리는 논리 형식을 생성하지 않고 표에 대한 답변을 질문하는 접근 방식인 TaPas를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "297-4",
                    "sentence": "TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.",
                    "sentence_kor": "TaPas는 약한 감독으로부터 훈련하며, 표 셀을 선택하고 선택적으로 해당 집계 연산자를 그러한 선택에 적용함으로써 표기를 예측한다.",
                    "tag": "3"
                },
                {
                    "index": "297-5",
                    "sentence": "TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.",
                    "sentence_kor": "TaPas는 BERT 아키텍처를 확장하여 테이블을 입력으로 인코딩하고, Wikipedia에서 탐색한 텍스트 세그먼트와 테이블의 효과적인 공동 사전 교육에서 초기화하며, 엔드 투 엔드 교육을 받는다.",
                    "tag": "3+4"
                },
                {
                    "index": "297-6",
                    "sentence": "We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.",
                    "sentence_kor": "우리는 세 가지 서로 다른 의미 구문 분석 데이터 세트로 실험하고, TaPas가 55.1에서 67.2로 SQA의 최첨단 정확도를 개선하고 WikiSQL 및 WikiTQ의 최첨단 기능과 동등하지만 더 단순한 모델 아키텍처로 수행함으로써 의미 구문 분석 모델을 능가하거나 이에 필적하는 것을 발견했다.",
                    "tag": "3"
                },
                {
                    "index": "297-7",
                    "sentence": "We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.",
                    "sentence_kor": "또한 WikiSQL에서 WikiTQ로의 전송 학습은 최신 기술보다 4.2점 높은 48.7의 정확도를 제공합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "897",
            "abstractID": "SPA_abs-298",
            "text": [
                {
                    "index": "298-0",
                    "sentence": "In argumentation, people state premises to reason towards a conclusion.",
                    "sentence_kor": "논쟁에서, 사람들은 결론을 도출하기 위한 전제를 진술한다.",
                    "tag": "1"
                },
                {
                    "index": "298-1",
                    "sentence": "The conclusion conveys a stance towards some target, such as a concept or statement.",
                    "sentence_kor": "결론은 개념이나 진술과 같은 일부 목표에 대한 입장을 전달한다.",
                    "tag": "1"
                },
                {
                    "index": "298-2",
                    "sentence": "Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.",
                    "sentence_kor": "그러나, 종종 결론은 암묵적으로 남아있는데, 그것은 토론에서 자명하거나 수사적인 이유로 배제되기 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "298-3",
                    "sentence": "However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.",
                    "sentence_kor": "그러나 결론은 주장을 이해하고 따라서 주장을 처리하는 모든 응용 프로그램의 핵심입니다.",
                    "tag": "1+2"
                },
                {
                    "index": "298-4",
                    "sentence": "We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises.",
                    "sentence_kor": "따라서 우리는 주장의 결론이 그 전제로부터 어느 정도 재구성될 수 있는가에 대한 질문을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "298-5",
                    "sentence": "In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets.",
                    "sentence_kor": "특히, 우리는 여기서 결정적인 단계는 결론의 목표를 추론하는 것이고, 우리는 이 목표가 전제의 목표와 관련이 있다고 가정한다.",
                    "tag": "2+3"
                },
                {
                    "index": "298-6",
                    "sentence": "We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.",
                    "sentence_kor": "우리는 두 가지 보완 목표 추론 접근 방식을 개발한다. 하나는 전제 대상의 순위를 매기고 최상위 대상을 결론 대상으로 선택하고, 다른 하나는 삼중 신경망을 사용하여 학습된 임베딩 공간에서 새로운 결론 대상을 찾는다.",
                    "tag": "3"
                },
                {
                    "index": "298-7",
                    "sentence": "Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.",
                    "sentence_kor": "두 도메인의 말뭉치에 대한 우리의 평가는 두 접근 방식의 혼합이 여러 강력한 기준을 능가하는 가장 좋은 것으로 나타났다.",
                    "tag": "4"
                },
                {
                    "index": "298-8",
                    "sentence": "According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.",
                    "sentence_kor": "인간 주석자들에 따르면, 우리는 사례의 89%에서 합리적으로 적절한 결론 목표를 추론한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "898",
            "abstractID": "SPA_abs-299",
            "text": [
                {
                    "index": "299-0",
                    "sentence": "Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.",
                    "sentence_kor": "MMT(Multimodal Machine Translation)는 번역 품질을 향상시키기 위해 다른 양식, 일반적으로 정적 이미지의 정보를 도입하는 것을 목표로 한다.",
                    "tag": "1+2"
                },
                {
                    "index": "299-1",
                    "sentence": "Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.",
                    "sentence_kor": "이전 연구에서는 다양한 통합 방법을 제안했지만, 대부분은 여러 양식의 상대적 중요성을 고려하지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "299-2",
                    "sentence": "Equally treating all modalities may encode too much useless information from less important modalities.",
                    "sentence_kor": "모든 양식을 동등하게 취급하면 덜 중요한 양식의 쓸데없는 정보가 너무 많이 인코딩될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "299-3",
                    "sentence": "In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.",
                    "sentence_kor": "본 논문에서는 MMT에서 위의 문제를 해결하기 위해 Transformer의 멀티모달 자기 주의를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "299-4",
                    "sentence": "The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.",
                    "sentence_kor": "제안된 방법은 텍스트를 기반으로 이미지의 표현을 학습하므로 이미지의 관련 없는 정보를 인코딩하지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "299-5",
                    "sentence": "Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.",
                    "sentence_kor": "실험과 시각화 분석을 통해 우리 모델이 시각 정보의 이점을 얻고 다양한 메트릭 측면에서 이전 작업과 경쟁 기준선을 크게 능가한다는 것을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "899",
            "abstractID": "SPA_abs-300",
            "text": [
                {
                    "index": "300-0",
                    "sentence": "In this paper, we present a simple yet effective padding scheme that can be used as a drop-in module for existing convolutional neural networks.",
                    "sentence_kor": "본 논문에서는 기존 컨볼루션 신경망의 드롭인 모듈로 사용할 수 있는 간단하면서도 효과적인 패딩 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "300-1",
                    "sentence": "We call it partial convolution based padding, with the intuition that the padded region can be treated as holes and the original input as non-holes.",
                    "sentence_kor": "패딩된 영역은 구멍으로 처리될 수 있고 원래 입력은 구멍이 아닌 것으로 처리될 수 있다는 직관을 가진 부분 컨볼루션 기반 패딩이라고 부른다.",
                    "tag": "2+3"
                },
                {
                    "index": "300-2",
                    "sentence": "Specifically, during the convolution operation, the convolution results are re-weighted near image borders based on the ratios between the padded area and the convolution sliding window area.",
                    "sentence_kor": "특히, 컨볼루션 작업 중에 컨볼루션 결과는 패딩된 영역과 컨볼루션 슬라이딩 윈도우 영역 사이의 비율을 기준으로 이미지 경계 부근에서 다시 가중치를 부여한다.",
                    "tag": "1"
                },
                {
                    "index": "300-3",
                    "sentence": "Extensive experiments with various deep network models on ImageNet classification and semantic segmentation demonstrate that the proposed padding scheme consistently outperforms standard zero padding with better accuracy.",
                    "sentence_kor": "ImageNet 분류 및 의미 분할에 대한 다양한 심층 네트워크 모델을 사용한 광범위한 실험은 제안된 패딩 방식이 더 나은 정확도로 표준 제로 패딩을 일관되게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "900",
            "abstractID": "SPA_abs-301",
            "text": [
                {
                    "index": "301-0",
                    "sentence": "We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions.",
                    "sentence_kor": "판별기를 낮은 에너지는 데이터 다양체 근처 영역에, 높은 에너지는 다른 영역에 귀속시키는 에너지 함수로 보는 \"에너지 기반 생성적 적대 네트워크\" 모델(EBGAN)을 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "301-1",
                    "sentence": "Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples.",
                    "sentence_kor": "확률론적 GAN과 유사하게 발전기는 최소한의 에너지로 대조 표본을 생산하도록 훈련되는 반면 판별기는 이러한 생성된 표본에 높은 에너지를 할당하도록 훈련되는 것으로 간주된다.",
                    "tag": "1+2"
                },
                {
                    "index": "301-2",
                    "sentence": "Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output.",
                    "sentence_kor": "판별기를 에너지 함수로 보는 것은 로지스틱 출력을 가진 일반적인 이진 분류기에 더하여 다양한 아키텍처와 손실 함수를 사용할 수 있게 한다.",
                    "tag": "3"
                },
                {
                    "index": "301-3",
                    "sentence": "Among them, we show one instantiation of EBGAN framework as using an auto encoder architecture, with the energy being the reconstruction error, in place of the discriminator.",
                    "sentence_kor": "그 중에서, 우리는 자동 인코더 아키텍처를 사용할 때 판별기 대신 에너지가 재구성 오류인 EBGAN 프레임워크의 하나의 인스턴스화를 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "301-4",
                    "sentence": "We show that this form of EBGAN exhibits more stable behavior than regular GANs during training.",
                    "sentence_kor": "우리는 이러한 형태의 EBGAN이 훈련 중 일반 GAN보다 더 안정적인 동작을 보인다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "301-5",
                    "sentence": "We also show that a single-scale architecture can be trained to generate high-resolution images.",
                    "sentence_kor": "또한 단일 스케일 아키텍처가 고해상도 이미지를 생성하도록 훈련될 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "901",
            "abstractID": "SPA_abs-302",
            "text": [
                {
                    "index": "302-0",
                    "sentence": "Training modern deep learning models requires large amounts of computation, often provided by GPUs.",
                    "sentence_kor": "현대의 딥 러닝 모델을 훈련하려면 GPU에 의해 제공되는 많은 양의 계산이 필요하다.",
                    "tag": "1+2"
                },
                {
                    "index": "302-1",
                    "sentence": "Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications.",
                    "sentence_kor": "하나의 GPU에서 다수의 GPU로 연산을 확장하면 훨씬 더 빠른 훈련과 연구 진행이 가능하지만 두 가지 복잡성이 수반된다.",
                    "tag": "1+2"
                },
                {
                    "index": "302-2",
                    "sentence": "First, the training library must support inter-GPU communication.",
                    "sentence_kor": "첫째, 교육 라이브러리는 GPU 간 통신을 지원해야 합니다.",
                    "tag": "3"
                },
                {
                    "index": "302-3",
                    "sentence": "Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead.",
                    "sentence_kor": "사용된 특정 방법에 따라, 이 통신에는 무시할 수 있는 것부터 상당한 오버헤드가 수반될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "302-4",
                    "sentence": "Second, the user must modify his or her training code to take advantage of inter-GPU communication.",
                    "sentence_kor": "둘째, 사용자는 GPU 간 통신을 이용하기 위해 교육 코드를 수정해야 한다.",
                    "tag": "3"
                },
                {
                    "index": "302-5",
                    "sentence": "Depending on the training library's API, the modification required may be either significant or minimal.",
                    "sentence_kor": "교육 라이브러리의 API에 따라, 필요한 수정은 중요하거나 적을 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "302-6",
                    "sentence": "Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training.",
                    "sentence_kor": "TensorFlow 라이브러리에서 다중 GPU 훈련을 가능하게 하는 기존 방법은 무시할 수 없는 통신 오버헤드를 수반하며 사용자가 모델 구축 코드를 크게 수정해야 하므로 많은 연구자들이 전체 혼란을 피하고 더 느린 단일 GPU 훈련을 고수하도록 유도한다.",
                    "tag": "3"
                },
                {
                    "index": "302-7",
                    "sentence": "In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow.",
                    "sentence_kor": "본 논문에서 우리는 두 가지 스케일링 장애물을 모두 개선하는 오픈 소스 라이브러리인 Horovod를 소개한다. 이 라이브러리는 링 감소를 통한 효율적인 GPU 간 통신을 채택하고 사용자 코드를 몇 줄만 수정하면 텐서플로에서 더 빠르고 쉬운 분산 교육이 가능하다.",
                    "tag": "4"
                },
                {
                    "index": "302-8",
                    "sentence": "Horovod is available under the Apache 2.0 license at this https URL",
                    "sentence_kor": "Horovod는 이 https URL의 Apache 2.0 라이센스로 사용할 수 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "902",
            "abstractID": "SPA_abs-303",
            "text": [
                {
                    "index": "303-0",
                    "sentence": "Conditional GANs are at the forefront of natural image synthesis.",
                    "sentence_kor": "조건부 GAN은 자연 이미지 합성의 최전선에 있다.",
                    "tag": "1"
                },
                {
                    "index": "303-1",
                    "sentence": "The main drawback of such models is the necessity for labeled data.",
                    "sentence_kor": "이러한 모델의 주요 단점은 라벨이 붙은 데이터가 필요하다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "303-2",
                    "sentence": "In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs.",
                    "sentence_kor": "이 작업에서 우리는 두 가지 인기 있는 비지도 학습 기술인 적대적 훈련과 자기 감독을 활용하고 조건부 GAN과 무조건적인 GAN 사이의 격차를 해소하기 위한 단계를 밟는다.",
                    "tag": "2"
                },
                {
                    "index": "303-3",
                    "sentence": "In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game.",
                    "sentence_kor": "특히, 우리는 네트워크가 고전적인 GAN 게임에 대해 적대적이면서도 표현 학습 작업에 대해 협력할 수 있도록 한다.",
                    "tag": "2"
                },
                {
                    "index": "303-4",
                    "sentence": "The role of self supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training.",
                    "sentence_kor": "자기 감시의 역할은 판별자가 훈련 중에 잊지 않는 의미 있는 특징 표현을 배우도록 장려하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "303-5",
                    "sentence": "We test empirically both the quality of the learned image representations, and the quality of the synthesized images.",
                    "sentence_kor": "학습된 이미지 표현의 품질과 합성 이미지의 품질을 모두 경험적으로 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "303-6",
                    "sentence": "Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts.",
                    "sentence_kor": "동일한 조건에서 자체 감독 GAN은 최첨단 조건부 대응물과 유사한 성능을 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "303-7",
                    "sentence": "Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.",
                    "sentence_kor": "마지막으로, 우리는 완전히 감독되지 않은 학습에 대한 이 접근법이 무조건적인 ImageNet 생성에서 23.4의 FID를 달성하도록 확장될 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "903",
            "abstractID": "SPA_abs-304",
            "text": [
                {
                    "index": "304-0",
                    "sentence": "One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural networks.",
                    "sentence_kor": "임베디드 시스템에 신경망을 배치하는 주요 장벽 중 하나는 기존 신경망의 대용량 메모리와 전력 소비였다.",
                    "tag": "1"
                },
                {
                    "index": "304-1",
                    "sentence": "In this work, we introduce SqueezeNext, a new family of neural network architectures whose design was guided by considering previous architectures such as SqueezeNet, as well as by simulation results on a neural network accelerator.",
                    "sentence_kor": "본 연구에서는 스퀴즈넷과 같은 이전 아키텍처와 신경망 가속기의 시뮬레이션 결과를 고려하여 설계를 안내한 새로운 신경망 아키텍처 제품군인 스퀴즈넥스트를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "304-2",
                    "sentence": "This new network is able to match AlexNet's accuracy on the ImageNet benchmark with 112× fewer parameters, and one of its deeper variants is able to achieve VGG-19 accuracy with only 4.4 Million parameters, (31× smaller than VGG-19).",
                    "sentence_kor": "이 새로운 네트워크는 112배 적은 매개 변수로 ImageNet 벤치마크에서 AlexNet의 정확도와 일치할 수 있으며, 그 보다 깊은 변형 중 하나는 440만 개의 매개 변수(VGG-19보다 31배 작음)만으로 VGG-19 정확도를 달성할 수 있습니다.",
                    "tag": "2"
                },
                {
                    "index": "304-3",
                    "sentence": "SqueezeNext also achieves better top-5 classification accuracy with 1.3× fewer parameters as compared to MobileNet, but avoids using depthwise-separable convolutions that are inefficient on some mobile processor platforms.",
                    "sentence_kor": "스퀴즈넥스트는 또한 MobileNet에 비해 1.3배 적은 매개 변수로 상위 5개 분류 정확도를 달성하지만 일부 모바일 프로세서 플랫폼에서는 비효율적인 깊이 분리형 컨볼루션을 사용하지 않습니다.",
                    "tag": "2+3"
                },
                {
                    "index": "304-4",
                    "sentence": "This wide range of accuracy gives the user the ability to make speed accuracy tradeoffs, depending on the available resources on the target hardware.",
                    "sentence_kor": "이러한 광범위한 정확도는 사용자가 대상 하드웨어의 가용 자원에 따라 속도 정확도 균형을 이룰 수 있는 능력을 제공한다.",
                    "tag": "3"
                },
                {
                    "index": "304-5",
                    "sentence": "Using hardware simulation results for power and inference speed on an embedded system has guided us to design variations of the baseline model that are 2.59×/8.26× faster and 2.25×/7.5× more energy efficient as compared to SqueezeNet/AlexNet without any accuracy degradation.",
                    "sentence_kor": "임베디드 시스템의 전력 및 추론 속도에 하드웨어 시뮬레이션 결과를 사용하여 정확도 저하 없이 스퀴즈넷/알렉스넷에 비해 2.59배/8.26배 더 빠르고 2.25배/7.5배 더 높은 에너지 효율의 기본 모델을 설계할 수 있었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "904",
            "abstractID": "SPA_abs-305",
            "text": [
                {
                    "index": "305-0",
                    "sentence": "We consider the problem of anomaly detection in images, and present a new detection technique.",
                    "sentence_kor": "우리는 이미지에서 이상 탐지 문제를 고려하고 새로운 탐지 기술을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "305-1",
                    "sentence": "Given a sample of images, all known to belong to a \"normal\" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects).",
                    "sentence_kor": "모두 \"정상\" 등급(예: 개)에 속하는 것으로 알려진 이미지 샘플이 주어지면 분포 범위를 벗어난 이미지(예: 개가 아닌 개체)를 탐지할 수 있는 심층 신경 모델을 훈련하는 방법을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "305-2",
                    "sentence": "The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images.",
                    "sentence_kor": "우리 계획의 주된 아이디어는 주어진 모든 이미지에 적용되는 수십 개의 기하학적 변환을 구별하기 위해 다중 클래스 모델을 훈련시키는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "305-3",
                    "sentence": "The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images.",
                    "sentence_kor": "모델에 의해 학습된 보조 전문지식은 변환된 이미지에 적용될 때 모델의 소프트맥스 활성화 통계를 기반으로 비정상적인 이미지를 효과적으로 식별하는 형상 감지기를 테스트 시간에 생성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "305-4",
                    "sentence": "We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.",
                    "sentence_kor": "우리는 우리의 알고리즘이 최첨단 방법을 큰 폭으로 향상시킨다는 것을 나타내는 제안된 검출기를 사용한 광범위한 실험을 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "905",
            "abstractID": "SPA_abs-306",
            "text": [
                {
                    "index": "306-0",
                    "sentence": "Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.",
                    "sentence_kor": "Convolutional Neural Networks(ConvNets)는 일반적으로 고정된 리소스 예산으로 개발된 다음 더 많은 리소스를 사용할 수 있는 경우 정확도를 높이기 위해 확장된다.",
                    "tag": "1"
                },
                {
                    "index": "306-1",
                    "sentence": "In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.",
                    "sentence_kor": "본 논문에서는 모델 확장을 체계적으로 연구하여 네트워크 깊이, 너비 및 해상도의 균형을 세심하게 유지하면 성능이 향상될 수 있음을 확인한다.",
                    "tag": "2"
                },
                {
                    "index": "306-2",
                    "sentence": "Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.",
                    "sentence_kor": "이러한 관찰을 바탕으로 간단하지만 매우 효과적인 복합 계수를 사용하여 깊이/폭/해상도 모든 차원을 균일하게 확장하는 새로운 스케일링 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "306-3",
                    "sentence": "We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.",
                    "sentence_kor": "우리는 MobileNet과 ResNet의 확장에 대한 이 방법의 효과를 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "306-4",
                    "sentence": "To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.",
                    "sentence_kor": "더 나아가, 신경 아키텍처 검색을 사용하여 새로운 기본 네트워크를 설계하고 이를 확장하여 이전 ConvNets보다 훨씬 더 높은 정확도와 효율성을 달성하는 EfficientNets라는 모델군을 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "306-5",
                    "sentence": "In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.",
                    "sentence_kor": "특히, 당사의 EfficientNet-B7은 ImageNet에서 최첨단 84.4% TOP-1/97.1% TOP-5 정확도를 달성하며 기존 ConvNet보다 추론 시 8.4배 작고 6.1배 빠르다.",
                    "tag": "4"
                },
                {
                    "index": "306-6",
                    "sentence": "Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.",
                    "sentence_kor": "또한 EfficientNets는 전송이 잘되며 CIFAR-100(91.7%), 꽃(98.8%), 기타 3가지 전송 학습 데이터 세트에서 최첨단 정확도를 달성하며 매개 변수는 훨씬 적다.",
                    "tag": "4"
                },
                {
                    "index": "306-7",
                    "sentence": "Source code is at this https URL.",
                    "sentence_kor": "소스 코드는 이 https URL에 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "906",
            "abstractID": "SPA_abs-307",
            "text": [
                {
                    "index": "307-0",
                    "sentence": "Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks.",
                    "sentence_kor": "IoU(Intersection over Union)는 물체 탐지 벤치마크에 사용되는 가장 일반적인 평가 지표이다.",
                    "tag": "1"
                },
                {
                    "index": "307-1",
                    "sentence": "However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value.",
                    "sentence_kor": "그러나 경계 상자의 매개 변수를 회귀하기 위해 일반적으로 사용되는 거리 손실을 최적화하는 것과 이 메트릭 값을 최대화하는 것 사이에는 차이가 있다.",
                    "tag": "1"
                },
                {
                    "index": "307-2",
                    "sentence": "The optimal objective for a metric is the metric itself.",
                    "sentence_kor": "메트릭의 최적 목표는 메트릭 자체입니다.",
                    "tag": "2"
                },
                {
                    "index": "307-3",
                    "sentence": "In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss.",
                    "sentence_kor": "축 정렬 2D 경계 상자의 경우 IoU를 회귀 손실로 직접 사용할 수 있음을 보여줄 수 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "307-4",
                    "sentence": "However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes.",
                    "sentence_kor": "그러나 IoU에는 플래토가 있어 겹치지 않는 경계 상자의 경우 최적화할 수 없다.",
                    "tag": "3+4"
                },
                {
                    "index": "307-5",
                    "sentence": "In this paper, we address the weaknesses of IoU by introducing a generalized version as both a new loss and a new metric.",
                    "sentence_kor": "본 논문에서 우리는 일반화된 버전을 새로운 손실과 새로운 메트릭으로 도입하여 IOU의 약점을 해결한다.",
                    "tag": "2"
                },
                {
                    "index": "307-6",
                    "sentence": "By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.",
                    "sentence_kor": "이러한 일반화된 IOU(Generalized IoU)를 최첨단 객체 감지 프레임워크에 손실로 통합함으로써, 우리는 PASCAL VOC 및 MS COCO와 같은 인기 객체 감지 벤치마크에 대한 표준, IOU 기반 및 새로운 GIOU 기반 성능 측정을 모두 사용하여 성능이 지속적으로 향상되었음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "907",
            "abstractID": "SPA_abs-308",
            "text": [
                {
                    "index": "308-0",
                    "sentence": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.",
                    "sentence_kor": "질문 답변, 기계 번역, 독해 및 요약과 같은 자연어 처리 작업은 일반적으로 작업별 데이터 세트에 대한 지도 학습을 통해 접근한다.",
                    "tag": "1"
                },
                {
                    "index": "308-1",
                    "sentence": "We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.",
                    "sentence_kor": "우리는 언어 모델이 WebText라고 하는 수백만 개의 웹 페이지의 새로운 데이터 세트에 대해 훈련할 때 어떠한 명시적 감독 없이 이러한 작업을 배우기 시작한다는 것을 입증한다.",
                    "tag": "1+2"
                },
                {
                    "index": "308-2",
                    "sentence": "When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.",
                    "sentence_kor": "문서와 질문을 조건으로 할 때 언어 모델에 의해 생성된 답변은 CoQA 데이터 세트에서 55 F1에 도달한다. 127,000개 이상의 교육 예제를 사용하지 않고 기준 시스템 4개 중 3개의 성능과 일치하거나 그 이상이다.",
                    "tag": "2+3"
                },
                {
                    "index": "308-3",
                    "sentence": "The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.",
                    "sentence_kor": "언어 모델의 용량은 제로샷 작업 전송의 성공에 필수적이며 이를 증가시키면 작업 전반에 걸쳐 로그 선형 방식으로 성능이 향상된다.",
                    "tag": "3+4"
                },
                {
                    "index": "308-4",
                    "sentence": "Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebTex",
                    "sentence_kor": "우리의 가장 큰 모델인 GPT-2는 1.5B 매개 변수 Transformer로 제로샷 설정에서 테스트된 언어 모델링 데이터 세트 8개 중 7개에서 최첨단 결과를 달성하지만 WebTex에는 여전히 적합하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "308-5",
                    "sentence": "Samples from the model reflect these improvements and contain coherent paragraphs of text.",
                    "sentence_kor": "모델에서 추출한 샘플은 이러한 개선 사항을 반영하고 일관된 텍스트 단락을 포함하고 있다.",
                    "tag": "4"
                },
                {
                    "index": "308-6",
                    "sentence": "These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.",
                    "sentence_kor": "이러한 연구 결과는 자연적으로 발생하는 시연으로부터 작업 수행 방법을 배우는 언어 처리 시스템을 구축하기 위한 유망한 경로를 제시한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "908",
            "abstractID": "SPA_abs-309",
            "text": [
                {
                    "index": "309-0",
                    "sentence": "A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.",
                    "sentence_kor": "최근 자연어 처리(NLP)의 많은 성공은 많은 양의 텍스트에 대해 훈련된 단어의 분산 벡터 표현에 의해 주도되었다.",
                    "tag": "1"
                },
                {
                    "index": "309-1",
                    "sentence": "These representations are typically used as general purpose features for words across a range of NLP problems.",
                    "sentence_kor": "이러한 표현은 일반적으로 다양한 NLP 문제에 걸쳐 단어의 범용 기능으로 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "309-2",
                    "sentence": "However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem.",
                    "sentence_kor": "그러나, 이러한 성공을 문장과 같은 단어 순서의 표현 학습으로 확장하는 것은 여전히 미해결 문제로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "309-3",
                    "sentence": "Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations.",
                    "sentence_kor": "최근 연구는 범용 고정 길이 문장 표현을 학습하기 위해 서로 다른 훈련 목표를 가진 감독되지 않은 학습 기법 및 지도 학습 기법을 탐구했다.",
                    "tag": "1+2"
                },
                {
                    "index": "309-4",
                    "sentence": "In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model.",
                    "sentence_kor": "본 연구에서, 우리는 단일 모델에서 다양한 훈련 목표의 귀납적 편향을 결합하는 문장 표현을 위한 간단하고 효과적인 다중 작업 학습 프레임워크를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "309-5",
                    "sentence": "We train this model on several data sources with multiple training objectives on over 100 million sentences.",
                    "sentence_kor": "우리는 이 모델을 1억 개 이상의 문장에 대한 여러 훈련 목표를 가진 여러 데이터 소스에 대해 교육한다.",
                    "tag": "3"
                },
                {
                    "index": "309-6",
                    "sentence": "Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods.",
                    "sentence_kor": "광범위한 실험을 통해 약하게 관련된 작업에 걸쳐 단일 반복 문장 인코더를 공유하면 이전 방법에 비해 일관된 개선이 가능하다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "309-7",
                    "sentence": "We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.",
                    "sentence_kor": "학습된 범용 표현을 사용하여 전송 학습 및 저자원 설정의 맥락에서 상당한 개선을 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "909",
            "abstractID": "SPA_abs-310",
            "text": [
                {
                    "index": "310-0",
                    "sentence": "We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models.",
                    "sentence_kor": "숨겨진 마르코프 모델의 맥락에서 격자 없는 최대 상호 정보(LF-MMI) 목표 함수를 사용하여 음향 모델의 엔드 투 엔드 훈련에 대한 우리의 연구를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "310-1",
                    "sentence": "By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees.",
                    "sentence_kor": "엔드 투 엔드 훈련이란 이전에 훈련된 모델, 강제 정렬 또는 상태-타이 결정 트리 구축 없이 한 단계에서 단일 DNN의 플랫 스타트 훈련을 의미한다.",
                    "tag": "2"
                },
                {
                    "index": "310-2",
                    "sentence": "We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks.",
                    "sentence_kor": "우리는 트리 없이 상황에 따라 모델링을 가능하게 하기 위해 완전한 바이폰을 사용하며, 우리의 엔드 투 엔드 LF-MMI 접근 방식이 잘 알려진 대형 어휘 작업에서 일반 LF-MMI와 유사한 결과를 얻을 수 있다는 것을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "310-3",
                    "sentence": "We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models.",
                    "sentence_kor": "또한 문자 기반 및 어휘가 없는 설정에서 CTC와 같은 다른 엔드 투 엔드 방법과 비교하고 훨씬 더 작은 모델을 사용하면서 서로 다른 대규모 어휘 작업에서 단어 오류율이 5~25% 감소했음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "910",
            "abstractID": "SPA_abs-311",
            "text": [
                {
                    "index": "311-0",
                    "sentence": "Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase.",
                    "sentence_kor": "현재 성공한 소스 분리 기법의 대부분은 크기 스펙트로그램을 입력으로 사용하므로 기본적으로 신호의 일부인 위상이 생략됩니다.",
                    "tag": "1"
                },
                {
                    "index": "311-1",
                    "sentence": "To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation --- which take into account all the information available in the raw audio signal, including the phase.",
                    "sentence_kor": "잠재적으로 유용한 정보를 누락하지 않기 위해 단계를 포함하여 원시 오디오 신호에서 사용할 수 있는 모든 정보를 고려한 음악 소스 분리를 위한 엔드 투 엔드 모델의 사용 가능성을 연구한다.",
                    "tag": "1+2"
                },
                {
                    "index": "311-2",
                    "sentence": "Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model.",
                    "sentence_kor": "지난 수십 년 동안 엔드 투 엔드 음악 소스 분리는 거의 도달할 수 없는 것으로 간주되었지만, 우리의 결과는 파형 기반 모델이 스펙트로그램 기반 딥 러닝 모델과 유사하게(더 낫지는 않더라도) 수행할 수 있다는 것을 확인시켜 준다.",
                    "tag": "2+3"
                },
                {
                    "index": "311-3",
                    "sentence": "Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.",
                    "sentence_kor": "즉, 우리가 제안하는 Wavenet 기반 모델과 Wave-U-Net은 최근의 스펙트로그램 기반 딥러닝 모델인 DeepConvSep을 능가할 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "911",
            "abstractID": "SPA_abs-312",
            "text": [
                {
                    "index": "312-0",
                    "sentence": "We introduce a convolutional recurrent neural network (CRNN) for music tagging.",
                    "sentence_kor": "음악 태깅을 위한 컨볼루션 반복 신경망(CRNN)을 소개한다.",
                    "tag": "1"
                },
                {
                    "index": "312-1",
                    "sentence": "CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features.",
                    "sentence_kor": "CRNN은 로컬 형상 추출을 위한 컨볼루션 신경망(CNN)과 추출된 형상의 시간적 요약을 위한 반복 신경망을 활용한다.",
                    "tag": "1+2"
                },
                {
                    "index": "312-2",
                    "sentence": "We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample.",
                    "sentence_kor": "CRNN을 샘플당 성능 및 교육 시간과 관련하여 매개 변수의 수를 제어하면서 음악 태그 지정에 사용된 세 개의 CNN 구조와 비교한다.",
                    "tag": "2+3"
                },
                {
                    "index": "312-3",
                    "sentence": "Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.",
                    "sentence_kor": "전반적으로 CRNN은 매개 변수 및 훈련 시간과 관련하여 강력한 성능을 보여 음악 기능 추출 및 기능 요약에서 하이브리드 구조의 효과를 나타낸다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "912",
            "abstractID": "SPA_abs-313",
            "text": [
                {
                    "index": "313-0",
                    "sentence": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error.",
                    "sentence_kor": "우리는 근사치와 추정 오차 사이에 거의 최적에 가깝고 계산적으로 효율적인 거래를 가능하게 하는 분산의 볼록한 대리자를 제공하는 위험 최소화 및 확률적 최적화에 대한 접근 방식을 개발한다.",
                    "tag": "1+2"
                },
                {
                    "index": "313-1",
                    "sentence": "Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator.",
                    "sentence_kor": "우리의 접근 방식은 분포적으로 강력한 최적화 기술과 오웬의 경험적 우도를 기반으로 하며, 추정기의 이론적 성능을 특징짓는 유한 표본 및 점근적 결과를 제공한다.",
                    "tag": "2+3"
                },
                {
                    "index": "313-2",
                    "sentence": "In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance.",
                    "sentence_kor": "특히, 우리는 우리의 절차가 최적성 증명서와 함께 제공되어 편향과 분산의 자동 균형을 통해 경험적 위험 최소화보다 (일부 시나리오에서) 더 빠른 수렴 속도를 달성한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "313-3",
                    "sentence": "We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.",
                    "sentence_kor": "우리는 실제로 추정기가 훈련 샘플의 분산과 절대 성능을 교환하여 여러 분류 문제에 대한 표준 경험적 위험 최소화보다 표본 외(테스트) 성능을 향상시킨다는 것을 입증하는 경험적 증거를 제공한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "913",
            "abstractID": "SPA_abs-314",
            "text": [
                {
                    "index": "314-0",
                    "sentence": "Numerical studies on race car aerodynamics at wing in ground effect have been carried out using a steady 3d, double precision, pressure-based, and standard k-epsilon turbulence model.",
                    "sentence_kor": "지상 효과의 날개에서 경주용 자동차 공기 역학에 대한 수치 연구는 꾸준한 3d, 이중 정밀도, 압력 기반 및 표준 k-엡실론 난류 모델을 사용하여 수행되었다.",
                    "tag": "1"
                },
                {
                    "index": "314-1",
                    "sentence": "Through various parametric analytical studies we have observed that at a particular speed and ground clearance of the wings a favorable negative lift was found high at a particular angle of attack for all the physical models considered in this paper.",
                    "sentence_kor": "다양한 파라메트릭 분석 연구를 통해 우리는 날개의 특정 속도와 지상 간극에서 본 논문에서 고려된 모든 물리적 모델에 대해 특정 공격 각도에서 유리한 마이너스 리프트가 높게 발견되었다는 것을 관찰했다.",
                    "tag": "2+3"
                },
                {
                    "index": "314-2",
                    "sentence": "The fact is that if the ground clearance height to chord length (h/c) is too small, the developing boundary layers from either side (the ground and the lower surface of the wing) can interact, leading to an altered variation of the aerodynamic characteristics at wing in ground effect.",
                    "sentence_kor": "사실은 지상고에서 현 길이까지(h/c)가 너무 작을 경우 어느 한쪽(지면과 날개 하단 표면)에서 발달하는 경계층이 상호작용하여 지면 효과에서 날개 공기역학적 특성의 변화를 초래할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "314-3",
                    "sentence": "Therefore a suitable ground clearance must be predicted throughout the racing for a better performance of the race car, which obviously depends upon the coupled effects of the topography, wing orientation with respect to the ground, the incoming flow features and/or the race car speed.",
                    "sentence_kor": "따라서 경주용 자동차의 더 나은 성능을 위해 레이싱 내내 적절한 지상 공간을 예측해야 하며, 이는 지형, 지면에 대한 날개 방향, 들어오는 흐름 특징 및/또는 경주용 차의 속도에 따라 분명히 달라진다.",
                    "tag": "3"
                },
                {
                    "index": "314-4",
                    "sentence": "We have concluded that for the design of high performance and high speed race cars the adjustable wings capable to alter the ground clearance and the angles of attack is the best design option for any race car for racing safely with variable speeds.",
                    "sentence_kor": "우리는 고성능 및 고속 경주용 자동차의 설계를 위해 지상고와 공격 각도를 변경할 수 있는 조절식 날개가 가변속도로 안전하게 경주할 수 있는 모든 경주용 자동차에 최고의 설계 옵션이라는 결론을 내렸다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "914",
            "abstractID": "SPA_abs-315",
            "text": [
                {
                    "index": "315-0",
                    "sentence": "In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning.",
                    "sentence_kor": "이 작업에서는 정책 외, 반환 기반 강화 학습을 위한 몇 가지 오래된 새로운 알고리즘을 새롭게 살펴봅니다.",
                    "tag": "1"
                },
                {
                    "index": "315-1",
                    "sentence": "Expressing these in a common form, we derive a novel algorithm, Retrace(λ), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of \"off-policyness\"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies.",
                    "sentence_kor": "이를 공통 형태로 표현하면 (1) 분산이 낮으며 (2) \"정책 외\"의 정도에 관계없이 모든 행동 정책에서 수집된 샘플을 안전하게 사용하며 (3) 정책 외 행동 정책에서 수집된 샘플을 최대한 활용하므로 효율적이라는 세 가지 속성을 가진 새로운 알고리즘인 리트레이스(),)를 도출한다.",
                    "tag": "1+2"
                },
                {
                    "index": "315-2",
                    "sentence": "We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms.",
                    "sentence_kor": "정책 외 정책 평가 및 제어 설정 모두에서 관련 운영자의 수축 특성을 분석하고 온라인 샘플 기반 알고리즘을 도출한다.",
                    "tag": "3"
                },
                {
                    "index": "315-3",
                    "sentence": "We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration).",
                    "sentence_kor": "우리는 이것이 GLIE 가정 없이 a.s.를 Q'로 수렴하는 첫 번째 반환 기반 오프 폴리시 제어 알고리즘이라고 믿는다(무한 탐사가 있는 한계에서의 자유).",
                    "tag": "3"
                },
                {
                    "index": "315-4",
                    "sentence": "As a corollary, we prove the convergence of Watkins' Q(λ), which was an open problem since 1989.",
                    "sentence_kor": "결과적으로, 우리는 1989년 이래로 미해결 문제였던 왓킨스의 Q(),)의 수렴을 증명하였다.",
                    "tag": "4"
                },
                {
                    "index": "315-5",
                    "sentence": "We illustrate the benefits of Retrace(λ) on a standard suite of Atari 2600 games.",
                    "sentence_kor": "저희는 아타리 2600 게임 세트에서의 Retrace())의 이점을 설명합니다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "915",
            "abstractID": "SPA_abs-316",
            "text": [
                {
                    "index": "316-0",
                    "sentence": "Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms.",
                    "sentence_kor": "원시 관찰 및 보상에 따라 강력한 가치 함수를 학습하는 것은 이제 모델이 없는 모델 기반 심층 강화 학습 알고리즘으로 가능하다.",
                    "tag": "1"
                },
                {
                    "index": "316-1",
                    "sentence": "There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map.",
                    "sentence_kor": "후속 표현(SR)이라고 불리는 세 번째 대안이 있는데, 이는 값 함수를 보상 예측 변수와 후속 맵의 두 가지 구성 요소로 분해합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "316-2",
                    "sentence": "The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards.",
                    "sentence_kor": "후속 맵은 주어진 상태에서 예상되는 미래 상태 점유율을 나타내며 보상 예측 변수는 상태를 스칼라 보상에 매핑한다.",
                    "tag": "3"
                },
                {
                    "index": "316-3",
                    "sentence": "The value function of a state can be computed as the inner product between the successor map and the reward weights.",
                    "sentence_kor": "상태의 값 함수는 후속 맵과 보상 가중치 사이의 내부 곱으로 계산될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "316-4",
                    "sentence": "In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework.",
                    "sentence_kor": "본 논문에서는 엔드 투 엔드 심층 강화 학습 프레임워크 내에서 SR을 일반화하는 DSR을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "316-5",
                    "sentence": "DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy.",
                    "sentence_kor": "DSR에는 보상 및 세계 역학의 인수분해로 인한 원위부 보상 변화에 대한 민감도 증가, 무작위 정책에 따라 훈련된 후속 맵이 주어진 경우 병목 상태(하위 목표)를 추출할 수 있는 능력 등 몇 가지 매력적인 속성이 있다.",
                    "tag": "4"
                },
                {
                    "index": "316-6",
                    "sentence": "We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.",
                    "sentence_kor": "단순한 그리드 월드 도메인(MazeBase)과 둠 게임 엔진이라는 원시 픽셀 관찰이 주어진 두 가지 다양한 환경에 대한 접근 방식의 효과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "916",
            "abstractID": "SPA_abs-317",
            "text": [
                {
                    "index": "317-0",
                    "sentence": "We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations.",
                    "sentence_kor": "우리는 그 환경에 대한 에이전트의 불확실성과 관찰 전반에 걸쳐 이 불확실성을 일반화하는 문제를 고려한다.",
                    "tag": "1+2"
                },
                {
                    "index": "317-1",
                    "sentence": "Specifically, we focus on the problem of exploration in non-tabular reinforcement learning.",
                    "sentence_kor": "특히, 우리는 비표형 강화 학습의 탐구 문제에 초점을 맞춘다.",
                    "tag": "3"
                },
                {
                    "index": "317-2",
                    "sentence": "Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model.",
                    "sentence_kor": "내재 동기 문헌에서 영감을 얻어 밀도 모델을 사용하여 불확실성을 측정하고 임의 밀도 모델에서 의사 카운트를 도출하기 위한 새로운 알고리즘을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "317-3",
                    "sentence": "This technique enables us to generalize count-based exploration algorithms to the non-tabular case.",
                    "sentence_kor": "이 기술을 사용하면 카운트 기반 탐색 알고리즘을 표가 아닌 경우로 일반화할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "317-4",
                    "sentence": "We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels.",
                    "sentence_kor": "우리는 우리의 아이디어를 Atari 2600 게임에 적용하여 원시 픽셀에서 합리적인 의사 카운트를 제공한다.",
                    "tag": "3+4"
                },
                {
                    "index": "317-5",
                    "sentence": "We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.",
                    "sentence_kor": "우리는 이러한 사이비 카운트를 본질적인 보상으로 변환하고 불명예스럽게도 어려운 몬테주마의 역습을 포함한 여러 하드 게임에서 상당히 개선된 탐색을 얻는다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "917",
            "abstractID": "SPA_abs-318",
            "text": [
                {
                    "index": "318-0",
                    "sentence": "Partially observed control problems are a challenging aspect of reinforcement learning.",
                    "sentence_kor": "부분적으로 관찰된 제어 문제는 강화 학습의 어려운 측면이다.",
                    "tag": "1"
                },
                {
                    "index": "318-1",
                    "sentence": "We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time.",
                    "sentence_kor": "우리는 시간 경과에 따라 역전파로 훈련된 반복 신경망을 사용하여 부분적으로 관찰된 도메인을 해결하기 위해 결정론적 정책 기울기와 확률적 값 기울기라는 두 가지 관련 모델이 없는 알고리즘을 확장한다.",
                    "tag": "1+2"
                },
                {
                    "index": "318-2",
                    "sentence": "We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements.",
                    "sentence_kor": "우리는 이 접근 방식이 장기 기억과 결합되어 다양한 메모리 요구사항을 나타내는 다양한 물리적 제어 문제를 해결할 수 있음을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "318-3",
                    "sentence": "These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps.",
                    "sentence_kor": "여기에는 잡음이 많은 센서의 정보를 단기적으로 통합하고 시스템 매개변수를 식별하는 것은 물론 여러 단계에 걸쳐 정보를 보존해야 하는 장기 기억 문제가 포함된다.",
                    "tag": "3"
                },
                {
                    "index": "318-4",
                    "sentence": "We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task.",
                    "sentence_kor": "우리는 또한 잘 알려진 모리스 물 미로 작업의 단순화된 형태의 탐색과 기억 문제의 결합에 대한 성공을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "318-5",
                    "sentence": "Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels.",
                    "sentence_kor": "마지막으로, 우리의 접근 방식이 픽셀에서 직접 학습하여 고차원 관찰을 처리할 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "318-6",
                    "sentence": "We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.",
                    "sentence_kor": "반복적인 결정론적 및 확률적 정책은 에이전트가 효과적인 검색 전략을 배워야 하는 물 미로를 포함하여 이러한 작업에 대해 유사하게 좋은 솔루션을 배울 수 있다는 것을 발견했다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "918",
            "abstractID": "SPA_abs-319",
            "text": [
                {
                    "index": "319-0",
                    "sentence": "In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world).",
                    "sentence_kor": "본 논문에서는 마인크래프트(유연한 3D 세계)의 새로운 강화 학습(RL) 과제를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "319-1",
                    "sentence": "We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures.",
                    "sentence_kor": "그런 다음 이러한 작업을 사용하여 기존의 심층 강화 학습(DRL) 아키텍처를 새로운 메모리 기반 DRL 아키텍처와 체계적으로 비교 및 대조한다.",
                    "tag": "3"
                },
                {
                    "index": "319-2",
                    "sentence": "These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks.",
                    "sentence_kor": "이러한 과제는 부분적인 관찰 가능성(1인칭 시각적 관찰로 인한) 지연 보상 고차원 시각적 관찰 및 작업을 잘 수행하기 위해 올바른 방법으로 능동적 지각 사용의 필요성을 포함하여 RL 방법에 도전 과제를 제기하는 문제를 제어 가능한 방식으로 강조하도록 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "319-3",
                    "sentence": "While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures.",
                    "sentence_kor": "이러한 작업은 개념적으로 설명하기 쉽지만, 이러한 모든 과제를 동시에 가지고 있기 때문에 현재의 DRL 아키텍처에서는 어렵다.",
                    "tag": "3"
                },
                {
                    "index": "319-4",
                    "sentence": "Additionally, we evaluate the generalization performance of the architectures on environments not used during training.",
                    "sentence_kor": "또한, 우리는 교육 중에 사용되지 않는 환경에서 아키텍처의 일반화 성능을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "319-5",
                    "sentence": "The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.",
                    "sentence_kor": "실험 결과는 새로운 아키텍처가 기존 DRL 아키텍처보다 보이지 않는 환경에 더 잘 일반화된다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "919",
            "abstractID": "SPA_abs-320",
            "text": [
                {
                    "index": "320-0",
                    "sentence": "Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain.",
                    "sentence_kor": "심층 강화 학습 방법은 아타리 2600 도메인의 게임에 대한 학습 제어 정책에서 최첨단 성능을 달성했습니다.",
                    "tag": "1"
                },
                {
                    "index": "320-1",
                    "sentence": "One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate.",
                    "sentence_kor": "오락실 학습 환경(ALE)에서 중요한 매개변수 중 하나는 프레임 스킵 속도이다.",
                    "tag": "1"
                },
                {
                    "index": "320-2",
                    "sentence": "It decides the granularity at which agents can control game play.",
                    "sentence_kor": "에이전트가 게임 플레이를 제어할 수 있는 세분성을 결정합니다.",
                    "tag": "1"
                },
                {
                    "index": "320-3",
                    "sentence": "A frame skip value of k allows the agent to repeat a selected action k number of times.",
                    "sentence_kor": "프레임 건너뛰기 값 k를 사용하면 에이전트가 선택한 작업을 k번 반복할 수 있습니다.",
                    "tag": "1+2"
                },
                {
                    "index": "320-4",
                    "sentence": "The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state.",
                    "sentence_kor": "심층 Q-네트워크(DQN) 및 결투 네트워크 아키텍처(DuDQN)와 같은 최신 아키텍처의 현재 상태는 정적 프레임 스킵 속도를 가진 프레임워크로 구성되며, 여기서 네트워크의 작업 출력이 현재 상태에 관계없이 고정된 수의 프레임에 대해 반복됩니다.",
                    "tag": "3"
                },
                {
                    "index": "320-5",
                    "sentence": "In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter.",
                    "sentence_kor": "본 논문에서는 프레임 스킵 비율을 동적 학습 가능 매개 변수로 만드는 새로운 아키텍처인 DFDQN(Dynamic Frame Skip Deep Q-Network)을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "320-6",
                    "sentence": "This allows us to choose the number of times an action is to be repeated based on the current state.",
                    "sentence_kor": "이를 통해 현재 상태를 기준으로 작업이 반복되는 횟수를 선택할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "320-7",
                    "sentence": "We show empirically that such a setting improves the performance on relatively harder games like Seaquest.",
                    "sentence_kor": "우리는 그러한 설정이 Seaquest와 같은 상대적으로 어려운 게임에서 성능을 향상시킨다는 것을 경험적으로 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "920",
            "abstractID": "SPA_abs-321",
            "text": [
                {
                    "index": "321-0",
                    "sentence": "This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states.",
                    "sentence_kor": "본 논문은 추상 상태 및 추상 상태 간의 확장된 조치 측면에서 주어진 과제에 대한 계층적 설명을 식별하는 것을 포함하는 강화 학습에 자동화된 기술 습득 프레임워크를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "321-1",
                    "sentence": "Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms.",
                    "sentence_kor": "이 과제에 존재하는 그러한 구조를 식별하면 강화 학습 알고리즘을 단순화하고 속도를 높일 수 있는 방법이 제공된다.",
                    "tag": "3"
                },
                {
                    "index": "321-2",
                    "sentence": "These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch.",
                    "sentence_kor": "이러한 구조는 또한 처음부터 재학습 정책 없이 여러 작업에 걸쳐 이러한 알고리즘을 일반화하는 데 도움이 된다.",
                    "tag": "2+3"
                },
                {
                    "index": "321-3",
                    "sentence": "We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states.",
                    "sentence_kor": "동적 시스템의 아이디어를 사용하여 상태 공간에서 전이 가능한 영역을 찾아 추상 상태와 연결합니다.",
                    "tag": "3"
                },
                {
                    "index": "321-4",
                    "sentence": "The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure.",
                    "sentence_kor": "스펙트럼 클러스터링 알고리즘 PCCA+는 기본 구조에 정렬된 적절한 추상화를 식별하는 데 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "321-5",
                    "sentence": "Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states.",
                    "sentence_kor": "기술은 그러한 추상적인 상태들 사이의 전환을 이끄는 행동의 순서로 정의된다.",
                    "tag": "4"
                },
                {
                    "index": "321-6",
                    "sentence": "The connectivity information from PCCA+ is used to generate these skills or options.",
                    "sentence_kor": "PCCA+의 연결 정보는 이러한 기술이나 옵션을 생성하는 데 사용됩니다.",
                    "tag": "4"
                },
                {
                    "index": "321-7",
                    "sentence": "These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model.",
                    "sentence_kor": "이러한 기술은 학습 과제와 독립적이며 동일한 모델에 대해 정의된 다양한 과제에서 효율적으로 재사용될 수 있습니다.",
                    "tag": "4+5"
                },
                {
                    "index": "321-8",
                    "sentence": "This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate.",
                    "sentence_kor": "이 접근법은 대략적인 추정치를 구성하기 위해 샘플 궤적을 사용함으로써 환경의 정확한 모델이 없어도 잘 작동한다.",
                    "tag": "4+5"
                },
                {
                    "index": "321-9",
                    "sentence": "We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.",
                    "sentence_kor": "우리는 또한 행동 조건부 비디오 예측 네트워크에서 학습한 표현을 사용하여 상태 집계를 수행하고 집계된 상태 공간에서 기술 습득 프레임워크를 사용하는 대규모 상태 공간을 가진 복잡한 작업으로 기술 습득 프레임워크를 확장하는 접근 방식을 제시한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "921",
            "abstractID": "SPA_abs-322",
            "text": [
                {
                    "index": "322-0",
                    "sentence": "Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms.",
                    "sentence_kor": "피드백이 희박한 환경에서 목표 지향적 행동을 학습하는 것은 강화 학습 알고리즘의 주요 과제이다.",
                    "tag": "2"
                },
                {
                    "index": "322-1",
                    "sentence": "The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions.",
                    "sentence_kor": "탐색이 불충분하여 에이전트가 강력한 가치 기능을 학습할 수 없게 되어 일차적인 어려움이 발생한다.",
                    "tag": "1"
                },
                {
                    "index": "322-2",
                    "sentence": "Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems.",
                    "sentence_kor": "본질적으로 동기 부여 에이전트는 문제를 직접 해결하기보다는 그 자체를 위해 새로운 행동을 탐구할 수 있다.",
                    "tag": "1+2"
                },
                {
                    "index": "322-3",
                    "sentence": "Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment.",
                    "sentence_kor": "이러한 고유한 동작은 결국 에이전트가 환경에서 발생하는 작업을 해결하는 데 도움이 될 수 있습니다.",
                    "tag": "1+2"
                },
                {
                    "index": "322-4",
                    "sentence": "We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning.",
                    "sentence_kor": "우리는 계층적 가치 기능을 서로 다른 시간적 규모로 작동하는 프레임워크인 계층적-DQN(H-DQN)을 내재적으로 동기화된 심층 강화 학습과 함께 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "322-5",
                    "sentence": "A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals.",
                    "sentence_kor": "최상위 가치 함수는 고유 목표보다 정책을 배우고 하위 수준 함수는 주어진 목표를 만족시키기 위해 원자 작용보다 정책을 학습한다.",
                    "tag": "2+3"
                },
                {
                    "index": "322-6",
                    "sentence": "h-DQN allows for flexible goal specifications, such as functions over entities and relations.",
                    "sentence_kor": "H-DQN은 실체 및 관계에 대한 기능과 같은 유연한 목표 사양을 허용합니다.",
                    "tag": "3"
                },
                {
                    "index": "322-7",
                    "sentence": "This provides an efficient space for exploration in complicated environments.",
                    "sentence_kor": "이것은 복잡한 환경에서 탐험을 위한 효율적인 공간을 제공합니다.",
                    "tag": "3+4"
                },
                {
                    "index": "322-8",
                    "sentence": "We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.",
                    "sentence_kor": "우리는 (1) 복잡한 이산 확률적 의사결정 과정과 (2) 고전적인 ATARI 게임 '몬테주마의 복수'라는 매우 희박하고 지연된 피드백으로 두 가지 문제에 대한 접근 방식의 강점을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "922",
            "abstractID": "SPA_abs-323",
            "text": [
                {
                    "index": "323-0",
                    "sentence": "We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images.",
                    "sentence_kor": "단안 이미지에서 로봇 그립을 위한 손과 눈의 조정에 대한 학습 기반 접근 방식을 설명한다.",
                    "tag": "1"
                },
                {
                    "index": "323-1",
                    "sentence": "To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose.",
                    "sentence_kor": "잡기 위한 손과 눈의 조정을 배우기 위해, 우리는 단안 카메라 이미지만 사용하고 카메라 보정 또는 현재 로봇 포즈와 독립적으로 그리퍼의 작업 공간 모션이 성공적으로 포착될 확률을 예측하기 위해 대형 컨볼루션 신경망을 훈련시켰다.",
                    "tag": "2+3"
                },
                {
                    "index": "323-2",
                    "sentence": "This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination.",
                    "sentence_kor": "이를 위해서는 네트워크가 그리퍼와 장면의 물체 사이의 공간 관계를 관찰하여 손과 눈의 조정을 학습해야 한다.",
                    "tag": "3"
                },
                {
                    "index": "323-3",
                    "sentence": "We then use this network to servo the gripper in real time to achieve successful grasps.",
                    "sentence_kor": "그런 다음 이 네트워크를 사용하여 그리퍼를 실시간으로 서보하여 성공적인 파악을 달성한다.",
                    "tag": "3"
                },
                {
                    "index": "323-4",
                    "sentence": "To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware.",
                    "sentence_kor": "네트워크를 훈련시키기 위해 두 달 동안 카메라 위치와 하드웨어의 차이와 함께 6~14개의 로봇 조작기를 사용하여 80만 건 이상의 그립 시도를 수집했다.",
                    "tag": "3+4"
                },
                {
                    "index": "323-5",
                    "sentence": "Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.",
                    "sentence_kor": "우리의 실험 평가는 우리의 방법이 효과적인 실시간 제어를 달성하고, 새로운 물체를 성공적으로 파악할 수 있으며, 지속적인 서보잉을 통해 실수를 수정한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "923",
            "abstractID": "SPA_abs-324",
            "text": [
                {
                    "index": "324-0",
                    "sentence": "Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions.",
                    "sentence_kor": "모델 없는 강화 학습은 다양한 어려운 문제에 성공적으로 적용되었으며, 최근에는 대규모 신경망 정책과 가치 기능을 처리하도록 확장되었다.",
                    "tag": "1"
                },
                {
                    "index": "324-1",
                    "sentence": "However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems.",
                    "sentence_kor": "그러나 특히 고차원 함수 근사치를 사용할 때 모델이 없는 알고리즘의 샘플 복잡성은 물리적 시스템에 대한 적용 가능성을 제한하는 경향이 있다.",
                    "tag": "1"
                },
                {
                    "index": "324-2",
                    "sentence": "In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks.",
                    "sentence_kor": "본 논문에서는 지속적인 제어 작업을 위한 심층 강화 학습의 샘플 복잡성을 줄이기 위해 알고리즘과 표현을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "324-3",
                    "sentence": "We propose two complementary techniques for improving the efficiency of such algorithms.",
                    "sentence_kor": "우리는 이러한 알고리즘의 효율성을 개선하기 위한 두 가지 보완 기법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "324-4",
                    "sentence": "First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods.",
                    "sentence_kor": "첫째, 우리는 보다 일반적으로 사용되는 정책 구배와 행위자 비판 방법에 대한 대안으로 정규화된 부가 함수(NAF)라고 하는 Q-러닝 알고리즘의 연속 변형을 도출한다.",
                    "tag": "3"
                },
                {
                    "index": "324-5",
                    "sentence": "NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks.",
                    "sentence_kor": "NAF 표현을 사용하면 경험 재생을 통한 Q-러닝을 연속 작업에 적용할 수 있으며 시뮬레이션된 로봇 제어 작업의 성능을 크게 개선할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "324-6",
                    "sentence": "To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning.",
                    "sentence_kor": "접근 방식의 효율성을 더욱 향상시키기 위해, 우리는 모델 없는 강화 학습을 가속화하기 위해 학습된 모델의 사용을 탐구한다.",
                    "tag": "3+4"
                },
                {
                    "index": "324-7",
                    "sentence": "We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.",
                    "sentence_kor": "반복적으로 재설계된 로컬 선형 모델이 특히 이에 효과적이라는 것을 보여주며, 이러한 모델이 적용되는 도메인에서 상당히 빠른 학습을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "924",
            "abstractID": "SPA_abs-325",
            "text": [
                {
                    "index": "325-0",
                    "sentence": "Reinforcement learning can acquire complex behaviors from high-level specifications.",
                    "sentence_kor": "강화 학습은 높은 수준의 사양에서 복잡한 동작을 획득할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "325-1",
                    "sentence": "However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice.",
                    "sentence_kor": "그러나 효과적으로 최적화할 수 있고 정확한 작업을 인코딩하는 비용 함수를 정의하는 것은 실제로 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "325-2",
                    "sentence": "We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems.",
                    "sentence_kor": "우리는 고차원 로봇 시스템의 토크 제어를 응용하여 시연에서 동작을 학습하는 데 역최적 제어(IOC)를 어떻게 사용할 수 있는지 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "325-3",
                    "sentence": "Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems.",
                    "sentence_kor": "우리의 방법은 역최적 제어에서 두 가지 주요 과제를 다룬다. 첫째, 비용에 구조를 부과하기 위한 정보 기능과 효과적인 정규화의 필요성, 둘째, 고차원 연속 시스템의 알려지지 않은 역학 하에서 비용 함수를 학습하는 어려움이다.",
                    "tag": "2"
                },
                {
                    "index": "325-4",
                    "sentence": "To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering.",
                    "sentence_kor": "이전의 과제를 해결하기 위해, 우리는 꼼꼼한 특징 엔지니어링 없이 신경 네트워크와 같은 임의의 비선형 비용 함수를 학습할 수 있는 알고리즘을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "325-5",
                    "sentence": "To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC.",
                    "sentence_kor": "후자의 과제를 해결하기 위해 MaxEnt IOC에 대한 효율적인 샘플 기반 근사치를 공식화한다.",
                    "tag": "3+4"
                },
                {
                    "index": "325-6",
                    "sentence": "We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.",
                    "sentence_kor": "일련의 시뮬레이션 작업과 실제 로봇 조작 문제에 대해 우리의 방법을 평가하여 작업 복잡성과 샘플 효율성 측면에서 이전 방법보다 상당히 개선되었음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "925",
            "abstractID": "SPA_abs-326",
            "text": [
                {
                    "index": "326-0",
                    "sentence": "Efficient exploration in complex environments remains a major challenge for reinforcement learning.",
                    "sentence_kor": "복잡한 환경에서 효율적인 탐구는 강화 학습의 주요 과제로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "326-1",
                    "sentence": "We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions.",
                    "sentence_kor": "무작위 값 함수를 사용하여 계산 및 통계적으로 효율적인 방법으로 탐색하는 간단한 알고리즘인 부트스트래핑 DQN을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "326-2",
                    "sentence": "Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning.",
                    "sentence_kor": "엡실론 탐사와 같은 디더링 전략과 달리, 부트랩된 DQN은 일시적으로 확장된(또는 깊은) 탐사를 수행하며, 이는 기하급수적으로 더 빠른 학습으로 이어질 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "326-3",
                    "sentence": "We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment.",
                    "sentence_kor": "우리는 복잡한 확률적 MDP와 대규모 오락실 학습 환경에서 이러한 이점을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "326-4",
                    "sentence": "Bootstrapped DQN substantially improves learning times and performance across most Atari games.",
                    "sentence_kor": "Bootstracked DQN은 대부분의 Atari 게임에서 학습 시간과 성능을 크게 향상시킨다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "926",
            "abstractID": "SPA_abs-327",
            "text": [
                {
                    "index": "327-0",
                    "sentence": "We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within.",
                    "sentence_kor": "우리는 '계획 모듈'이 내장된 완전히 차별화 가능한 신경 네트워크인 가치 반복 네트워크(VIN)를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "327-1",
                    "sentence": "VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.",
                    "sentence_kor": "VIN은 계획하는 방법을 배울 수 있으며, 강화 학습 정책과 같은 계획 기반 추론을 포함하는 결과를 예측하는 데 적합하다.",
                    "tag": "2"
                },
                {
                    "index": "327-2",
                    "sentence": "Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.",
                    "sentence_kor": "우리의 접근 방식의 핵심은 컨볼루션 신경망으로 표현될 수 있고 표준 역전파를 사용하여 종단 간 훈련을 받은 가치 반복 알고리즘의 새로운 차별화 가능한 근사치이다.",
                    "tag": "2"
                },
                {
                    "index": "327-3",
                    "sentence": "We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task.",
                    "sentence_kor": "우리는 이산적이고 연속적인 경로 계획 도메인 및 자연어 기반 검색 작업을 기반으로 VIN 기반 정책을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "327-4",
                    "sentence": "We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.",
                    "sentence_kor": "우리는 명시적 계획 계산을 학습함으로써 VIN 정책이 보이지 않는 새로운 도메인에 더 잘 일반화된다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "927",
            "abstractID": "SPA_abs-328",
            "text": [
                {
                    "index": "328-0",
                    "sentence": "We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks.",
                    "sentence_kor": "에이전트 팀이 통신 기반 조정 작업을 해결하는 방법을 배울 수 있는 심층 분산 순환 Q-네트워크(DDRQN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "328-1",
                    "sentence": "In these tasks, the agents are not given any pre-designed communication protocol.",
                    "sentence_kor": "이러한 작업에서는 에이전트에게 사전 설계된 통신 프로토콜이 제공되지 않습니다.",
                    "tag": "3"
                },
                {
                    "index": "328-2",
                    "sentence": "Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol.",
                    "sentence_kor": "그러므로, 성공적인 의사소통을 위해서, 그들은 먼저 자동적으로 그들 자신의 통신 프로토콜을 개발하고 동의해야 한다.",
                    "tag": "3+4"
                },
                {
                    "index": "328-3",
                    "sentence": "We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so.",
                    "sentence_kor": "잘 알려진 수수께끼를 기반으로 두 가지 다중 에이전트 학습 문제에 대한 경험적 결과를 제시하여 DDRQN이 이러한 작업을 성공적으로 해결하고 이를 위한 우아한 통신 프로토콜을 발견할 수 있음을 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "328-4",
                    "sentence": "To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols.",
                    "sentence_kor": "우리가 아는 한, 심층 강화 학습이 통신 프로토콜을 학습하는데 성공한 것은 이번이 처음이다.",
                    "tag": "4"
                },
                {
                    "index": "328-5",
                    "sentence": "In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.",
                    "sentence_kor": "또한 DDRQN 아키텍처의 각 주요 구성 요소가 성공에 중요한지 확인하는 절제 실험을 제시한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "928",
            "abstractID": "SPA_abs-329",
            "text": [
                {
                    "index": "329-0",
                    "sentence": "The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves.",
                    "sentence_kor": "바둑은 거대한 검색 공간과 보드 위치와 움직임을 평가하는 어려움 때문에 인공지능에게 가장 어려운 고전 게임으로 여겨져 왔다.",
                    "tag": "1"
                },
                {
                    "index": "329-1",
                    "sentence": "Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves.",
                    "sentence_kor": "여기서는 '가치 네트워크'를 사용하여 이사회 위치를 평가하고 '정책 네트워크'를 사용하여 이동을 선택하는 컴퓨터 바둑에 대한 새로운 접근 방식을 소개합니다.",
                    "tag": "1+2"
                },
                {
                    "index": "329-2",
                    "sentence": "These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.",
                    "sentence_kor": "이러한 심층 신경망은 인간 전문가 게임의 지도 학습과 자기 놀이 게임의 강화 학습의 새로운 조합에 의해 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "329-3",
                    "sentence": "Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play.",
                    "sentence_kor": "사전 검색 없이 신경망은 수천 개의 임의 자가 플레이 게임을 시뮬레이션하는 최첨단 몬테 카를로 트리 검색 프로그램 수준에서 바둑을 둔다.",
                    "tag": "3"
                },
                {
                    "index": "329-4",
                    "sentence": "We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks.",
                    "sentence_kor": "또한 몬테카를로 시뮬레이션을 가치 및 정책 네트워크와 결합한 새로운 검색 알고리즘을 소개한다.",
                    "tag": "3"
                },
                {
                    "index": "329-5",
                    "sentence": "Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.",
                    "sentence_kor": "이 검색 알고리즘을 사용하여, 우리 프로그램 알파고는 다른 바둑 프로그램들과 99.8%의 승률을 달성했고, 인간 유러피언 바둑 챔피언을 5대 0으로 물리쳤다.",
                    "tag": "4+5"
                },
                {
                    "index": "329-6",
                    "sentence": "This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.",
                    "sentence_kor": "컴퓨터 프로그램이 바둑의 풀사이즈 게임에서 인간 프로 선수를 이긴 것은 이번이 처음인데, 이것은 이전에 적어도 10년 후에 있을 것으로 생각되었던 업적이다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "929",
            "abstractID": "SPA_abs-330",
            "text": [
                {
                    "index": "330-0",
                    "sentence": "State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.",
                    "sentence_kor": "최첨단 명명된 엔티티 인식 시스템은 사용 가능한 소규모 감독 교육 코퍼라에서 효과적으로 학습하기 위해 수작업 기능과 도메인별 지식에 크게 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "330-1",
                    "sentence": "In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers.",
                    "sentence_kor": "본 논문에서 우리는 두 가지 새로운 신경 아키텍처, 즉 양방향 LSTM과 조건부 랜덤 필드를 기반으로 하는 것과 시프트 감소 파서에서 영감을 얻은 전환 기반 접근 방식을 사용하여 세그먼트를 구성하고 레이블을 지정하는 다른 하나를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "330-2",
                    "sentence": "Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora.",
                    "sentence_kor": "우리 모델은 단어에 대한 두 가지 정보 출처, 즉 감독된 말뭉치에서 학습한 문자 기반 단어 표현과 주석되지 않은 말뭉치에서 학습한 감독되지 않은 단어 표현에 의존한다.",
                    "tag": "3+4"
                },
                {
                    "index": "330-3",
                    "sentence": "Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.",
                    "sentence_kor": "우리 모델은 가제이터와 같은 특정 언어 지식이나 자원에 의존하지 않고 4개 언어로 NER에서 최첨단 성능을 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "930",
            "abstractID": "SPA_abs-331",
            "text": [
                {
                    "index": "331-0",
                    "sentence": "In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding.",
                    "sentence_kor": "이 연구에서 우리는 언어 이해의 중심 과제인 대규모 언어 모델링을 위한 반복 신경망의 최근 발전을 탐구한다.",
                    "tag": "2"
                },
                {
                    "index": "331-1",
                    "sentence": "We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language.",
                    "sentence_kor": "우리는 현재 모델을 확장하여 이 작업에 존재하는 두 가지 주요 과제인 말뭉치와 어휘 크기, 그리고 복잡하고 장기적인 언어 구조를 다룬다.",
                    "tag": "2+3"
                },
                {
                    "index": "331-2",
                    "sentence": "We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.",
                    "sentence_kor": "우리는 10억 단어 벤치마크에서 문자 컨볼루션 신경망 또는 장단기 메모리와 같은 기술에 대한 철저한 연구를 수행한다.",
                    "tag": "2"
                },
                {
                    "index": "331-3",
                    "sentence": "Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7.",
                    "sentence_kor": "우리의 최고의 단일 모델은 최첨단 난해성을 51.3에서 30.0으로 크게 개선하며(20배 매개 변수 수를 감소시킴) 모델 앙상블은 난해성을 41.0에서 23.7로 개선하여 신기록을 수립한다.",
                    "tag": "3"
                },
                {
                    "index": "331-4",
                    "sentence": "We also release these models for the NLP and ML community to study and improve upon.",
                    "sentence_kor": "우리는 또한 NLP와 ML 커뮤니티를 연구하고 개선하기 위해 이러한 모델을 출시한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "931",
            "abstractID": "SPA_abs-332",
            "text": [
                {
                    "index": "332-0",
                    "sentence": "Teaching machines to read natural language documents remains an elusive challenge.",
                    "sentence_kor": "자연어 문서를 읽는 학습 기계는 여전히 어려운 과제이다.",
                    "tag": "1"
                },
                {
                    "index": "332-1",
                    "sentence": "Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation.",
                    "sentence_kor": "기계 판독 시스템은 그들이 본 문서의 내용에 제시된 질문에 답하는 능력에 대해 시험할 수 있지만, 지금까지 이러한 유형의 평가에 대한 대규모 훈련 및 시험 데이터 세트가 누락되어 왔다.",
                    "tag": "1"
                },
                {
                    "index": "332-2",
                    "sentence": "In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data.",
                    "sentence_kor": "본 연구에서는 이러한 병목 현상을 해결하고 대규모 감독 독서 이해 데이터를 제공하는 새로운 방법론을 정의한다.",
                    "tag": "2"
                },
                {
                    "index": "332-3",
                    "sentence": "This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.",
                    "sentence_kor": "이를 통해 언어 구조에 대한 최소한의 사전 지식으로 실제 문서를 읽고 복잡한 질문에 답하는 방법을 배우는 주의 기반 심층 신경망을 개발할 수 있다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "932",
            "abstractID": "SPA_abs-333",
            "text": [
                {
                    "index": "333-0",
                    "sentence": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.",
                    "sentence_kor": "최근 번역 중에 소스 문장의 일부에 선택적으로 초점을 맞춤으로써 신경 기계 번역(NMT)을 개선하는 데 주의 메커니즘이 사용되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "333-1",
                    "sentence": "However, there has been little work exploring useful architectures for attention-based NMT.",
                    "sentence_kor": "그러나 주의 기반 NMT에 유용한 아키텍처를 탐구하는 작업은 거의 없었다.",
                    "tag": "1"
                },
                {
                    "index": "333-2",
                    "sentence": "This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.",
                    "sentence_kor": "이 논문은 두 가지 단순하고 효과적인 주의 메커니즘 클래스를 조사한다. 즉, 모든 소스 워드에 항상 적용되는 전역 접근법과 한 번에 소스 워드의 하위 집합만 살펴보는 로컬 접근법이다.",
                    "tag": "1+2"
                },
                {
                    "index": "333-3",
                    "sentence": "We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions.",
                    "sentence_kor": "우리는 영어와 독일어 사이의 WMT 번역 작업에 대한 두 가지 접근법의 효과를 양쪽 방향에서 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "333-4",
                    "sentence": "With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout.",
                    "sentence_kor": "국소적인 주의를 기울이면 이미 드롭아웃과 같은 알려진 기법을 통합한 비주의 시스템에 비해 5.0 BLEU 포인트를 크게 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "333-5",
                    "sentence": "Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.",
                    "sentence_kor": "서로 다른 주의 아키텍처를 사용하는 앙상블 모델은 25.9 BLEU 점수로 WMT'15 영어-독일어 번역 작업에서 새로운 최첨단 결과를 확립했으며, NMT 및 n그램 재배열기가 지원하는 기존 최상의 시스템에 비해 1.0 BLEU 점이 향상되었다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "933",
            "abstractID": "SPA_abs-334",
            "text": [
                {
                    "index": "334-0",
                    "sentence": "Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding.",
                    "sentence_kor": "의미 분할과 같은 픽셀 수준의 라벨링 작업은 이미지 이해에서 중심적인 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "334-1",
                    "sentence": "Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks.",
                    "sentence_kor": "최근의 접근법은 픽셀 수준 라벨링 작업을 다루기 위해 이미지 인식을 위한 딥 러닝 기술의 기능을 이용하려고 시도하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "334-2",
                    "sentence": "One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects.",
                    "sentence_kor": "이 방법론의 한 가지 핵심 쟁점은 시각적 대상을 묘사할 수 있는 딥 러닝 기술의 제한된 용량이다.",
                    "tag": "1"
                },
                {
                    "index": "334-3",
                    "sentence": "To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling.",
                    "sentence_kor": "이 문제를 해결하기 위해 CNN(Convolutional Neural Networks)과 CRF(Conditional Random Fields) 기반 확률론적 그래픽 모델링의 장점을 결합한 새로운 형태의 컨볼루션 신경망을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "334-4",
                    "sentence": "To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.",
                    "sentence_kor": "이를 위해 가우스 쌍 전위를 가진 조건부 랜덤 필드에 대한 평균장 근사 추론을 반복 신경망으로 공식화한다.",
                    "tag": "3"
                },
                {
                    "index": "334-5",
                    "sentence": "This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs.",
                    "sentence_kor": "그런 다음 CRF-RNN이라고 하는 이 네트워크는 CNN의 일부로 연결되어 CNN과 CRF 모두의 바람직한 속성을 가진 심층 네트워크를 얻는다.",
                    "tag": "3"
                },
                {
                    "index": "334-6",
                    "sentence": "Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation.",
                    "sentence_kor": "중요한 것은, 우리의 시스템이 CRF 모델링을 CNN과 완전히 통합하여, 일반적인 역전파 알고리즘으로 전체 심층 네트워크를 엔드 투 엔드로 교육할 수 있게 함으로써 객체 묘사를 위한 오프라인 후처리 방법을 피할 수 있다는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "334-7",
                    "sentence": "We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.",
                    "sentence_kor": "우리는 제안된 방법을 의미론적 이미지 분할 문제에 적용하여 까다로운 Pascal VOC 2012 분할 벤치마크에서 최고의 결과를 얻는다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "934",
            "abstractID": "SPA_abs-335",
            "text": [
                {
                    "index": "335-0",
                    "sentence": "We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes.",
                    "sentence_kor": "신경망의 기능을 외부 메모리 자원에 결합함으로써 확장하는데, 외부 메모리 자원은 주의 프로세스에 의해 상호 작용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "335-1",
                    "sentence": "The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.",
                    "sentence_kor": "결합된 시스템은 튜링 머신 또는 Von Neumann 아키텍처와 유사하지만 단대단적으로 차별화되어 경사 하강으로 효율적으로 훈련될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "335-2",
                    "sentence": "Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.",
                    "sentence_kor": "예비 결과는 신경 튜링 기계가 입력 및 출력 예제에서 복사, 정렬 및 연관 호출과 같은 간단한 알고리즘을 추론할 수 있음을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "935",
            "abstractID": "SPA_abs-336",
            "text": [
                {
                    "index": "336-0",
                    "sentence": "Neural machine translation is a recently proposed approach to machine translation.",
                    "sentence_kor": "신경 기계 번역은 최근에 제안된 기계 번역 접근법이다.",
                    "tag": "1+2"
                },
                {
                    "index": "336-1",
                    "sentence": "Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.",
                    "sentence_kor": "기존의 통계 기계 번역과 달리, 신경 기계 번역은 번역 성능을 극대화하기 위해 공동으로 조정될 수 있는 단일 신경망을 구축하는 것을 목표로 한다.",
                    "tag": "1"
                },
                {
                    "index": "336-2",
                    "sentence": "The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.",
                    "sentence_kor": "신경 기계 변환을 위해 최근에 제안된 모델은 종종 인코더-디코더 계열에 속하며 디코더가 변환을 생성하는 고정 길이 벡터로 소스 문장을 인코딩하는 인코더로 구성된다.",
                    "tag": "1"
                },
                {
                    "index": "336-3",
                    "sentence": "In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.",
                    "sentence_kor": "본 논문에서, 우리는 고정 길이 벡터의 사용이 이 기본 인코더-디코더 아키텍처의 성능을 향상시키는 데 병목 현상이라고 추측하고, 모델이 이러한 부분을 구성할 필요 없이 대상 단어를 예측하는 데 관련된 소스 문장의 일부를 자동으로 검색할 수 있도록 함으로써 이를 확장할 것을 제안한다. 노골적으로 딱딱한 부분",
                    "tag": "2"
                },
                {
                    "index": "336-4",
                    "sentence": "With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation.",
                    "sentence_kor": "이 새로운 접근 방식을 통해, 우리는 영어-프랑스어 번역 작업에서 기존의 최첨단 문구 기반 시스템에 필적하는 번역 성능을 달성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "336-5",
                    "sentence": "Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",
                    "sentence_kor": "또한 정성 분석을 통해 모델에 의해 발견된 (소프트-)alignment가 우리의 직관과 잘 일치한다는 것을 알 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "936",
            "abstractID": "SPA_abs-337",
            "text": [
                {
                    "index": "337-0",
                    "sentence": "Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks.",
                    "sentence_kor": "심층 신경 네트워크(DNN)는 어려운 학습 작업에서 우수한 성능을 달성한 강력한 모델이다.",
                    "tag": "1"
                },
                {
                    "index": "337-1",
                    "sentence": "Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.",
                    "sentence_kor": "DNN은 라벨이 붙은 대형 훈련 세트를 사용할 수 있을 때마다 잘 작동하지만 시퀀스를 시퀀스에 매핑하는 데 사용할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "337-2",
                    "sentence": "In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.",
                    "sentence_kor": "본 논문에서, 우리는 시퀀스 구조에 대해 최소한의 가정을 하는 시퀀스 학습에 대한 일반적인 엔드 투 엔드 접근방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "337-3",
                    "sentence": "Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.",
                    "sentence_kor": "이 방법은 다층 장기 단기 메모리(LSTM)를 사용하여 입력 시퀀스를 고정된 차원의 벡터에 매핑한 다음 또 다른 심층 LSTM을 사용하여 벡터로부터 대상 시퀀스를 디코딩한다.",
                    "tag": "3"
                },
                {
                    "index": "337-4",
                    "sentence": "Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words.",
                    "sentence_kor": "우리의 주요 결과는 WMT'14 데이터 세트의 영어-프랑스어 번역 작업에서 LSTM에서 제작한 번역은 전체 테스트 세트에서 34.8의 BLEU 점수를 획득했으며 LSTM의 BLEU 점수는 어휘 외 단어로 벌점을 받았다.",
                    "tag": "4"
                },
                {
                    "index": "337-5",
                    "sentence": "Additionally, the LSTM did not have difficulty on long sentences.",
                    "sentence_kor": "또한 LSTM은 긴 문장에도 어려움을 겪지 않았다.",
                    "tag": "4"
                },
                {
                    "index": "337-6",
                    "sentence": "For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset.",
                    "sentence_kor": "비교를 위해 구문 기반 시만텍 시스템은 동일한 데이터 세트에서 33.3의 BLEU 점수를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "337-7",
                    "sentence": "When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task.",
                    "sentence_kor": "LSTM을 사용하여 앞에서 언급한 시만텍 시스템에서 도출한 1000개 가설을 재순위화했을 때, BLEU 점수는 36.5점으로 증가하여 이 작업에서 이전의 최고 결과에 근접한다.",
                    "tag": "3+4"
                },
                {
                    "index": "337-8",
                    "sentence": "The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice.",
                    "sentence_kor": "LSTM은 또한 어순에 민감하고 능동 및 수동 음성에 상대적으로 불변하는 합리적인 문구와 문장 표현을 학습했다.",
                    "tag": "4"
                },
                {
                    "index": "337-9",
                    "sentence": "Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",
                    "sentence_kor": "마지막으로, 우리는 (대상 문장은 아니지만) 모든 소스 문장의 단어 순서를 반대로 하면 소스 문장과 대상 문장 사이에 많은 단기 종속성이 도입되어 최적화 문제가 더 쉬워졌기 때문에 LSTM의 성능이 눈에 띄게 향상되었다는 것을 발견했다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "937",
            "abstractID": "SPA_abs-338",
            "text": [
                {
                    "index": "338-0",
                    "sentence": "In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN).",
                    "sentence_kor": "본 논문에서 우리는 두 개의 반복 신경망(RNN)으로 구성된 RNN 인코더-디코더라는 새로운 신경망 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "338-1",
                    "sentence": "One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols.",
                    "sentence_kor": "한 RNN은 일련의 기호를 고정 길이 벡터 표현으로 인코딩하고, 다른 하나는 표현을 다른 일련의 기호로 인코딩한다.",
                    "tag": "3"
                },
                {
                    "index": "338-2",
                    "sentence": "The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence.",
                    "sentence_kor": "제안된 모델의 인코더와 디코더는 소스 시퀀스가 주어진 대상 시퀀스의 조건부 확률을 최대화하기 위해 공동으로 훈련된다.",
                    "tag": "3"
                },
                {
                    "index": "338-3",
                    "sentence": "The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model.",
                    "sentence_kor": "통계 기계 번역 시스템의 성능은 기존 로그 선형 모델의 추가 기능으로 RNN 인코더-디코더에 의해 계산된 구문 쌍의 조건부 확률을 사용함으로써 개선된 것으로 경험적으로 밝혀졌다.",
                    "tag": "3+4"
                },
                {
                    "index": "338-4",
                    "sentence": "Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",
                    "sentence_kor": "질적으로, 우리는 제안된 모델이 언어 구문의 의미론적이고 구문론적으로 의미 있는 표현을 학습한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "938",
            "abstractID": "SPA_abs-339",
            "text": [
                {
                    "index": "339-0",
                    "sentence": "The ability to accurately represent sentences is central to language understanding.",
                    "sentence_kor": "문장을 정확하게 표현하는 능력은 언어 이해의 핵심이다.",
                    "tag": "1"
                },
                {
                    "index": "339-1",
                    "sentence": "We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences.",
                    "sentence_kor": "문장의 의미론적 모델링을 위해 채택하는 동적 컨볼루션 신경망(DCNN)이라는 컨볼루션 아키텍처를 설명한다.",
                    "tag": "2"
                },
                {
                    "index": "339-2",
                    "sentence": "The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences.",
                    "sentence_kor": "네트워크는 선형 시퀀스에 대한 전역 풀링 작업인 동적 k-최대 풀링을 사용합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "339-3",
                    "sentence": "The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations.",
                    "sentence_kor": "네트워크는 다양한 길이의 입력 문장을 처리하고 문장에 걸쳐 장단기 관계를 명시적으로 캡처할 수 있는 특징 그래프를 유도한다.",
                    "tag": "2+3"
                },
                {
                    "index": "339-4",
                    "sentence": "The network does not rely on a parse tree and is easily applicable to any language.",
                    "sentence_kor": "네트워크는 구문 분석 트리에 의존하지 않으며 모든 언어에 쉽게 적용할 수 있습니다.",
                    "tag": "2+3"
                },
                {
                    "index": "339-5",
                    "sentence": "We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision.",
                    "sentence_kor": "소규모 이진 및 다중 클래스 정서 예측, 6방향 질문 분류 및 원거리 감독에 의한 트위터 정서 예측의 네 가지 실험에서 DCNN을 테스트한다.",
                    "tag": "3"
                },
                {
                    "index": "339-6",
                    "sentence": "The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.",
                    "sentence_kor": "네트워크는 처음 세 작업에서 우수한 성능을 발휘하고 마지막 작업에서 가장 강력한 기준선과 관련하여 25% 이상의 오류를 감소시킨다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "939",
            "abstractID": "SPA_abs-340",
            "text": [
                {
                    "index": "340-0",
                    "sentence": "Many machine learning algorithms require the input to be represented as a fixed-length feature vector.",
                    "sentence_kor": "많은 기계 학습 알고리즘은 입력을 고정 길이 특징 벡터로 표현해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "340-1",
                    "sentence": "When it comes to texts, one of the most common fixed-length features is bag-of-words.",
                    "sentence_kor": "텍스트에 관해서라면, 가장 일반적인 고정 길이의 특징 중 하나는 단어 가방이다.",
                    "tag": "1"
                },
                {
                    "index": "340-2",
                    "sentence": "Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words.",
                    "sentence_kor": "인기에도 불구하고, 단어 가방 특징에는 두 가지 주요 약점이 있다. 그것은 단어의 순서를 잃고 단어의 의미도 무시한다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "340-3",
                    "sentence": "For example, \"powerful,\" \"strong\" and \"Paris\" are equally distant.",
                    "sentence_kor": "예를 들어, \"힘센\", \"강한\", \"파리\"는 똑같이 멀리 떨어져 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "340-4",
                    "sentence": "In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.",
                    "sentence_kor": "본 논문에서 우리는 문장, 단락 및 문서와 같은 텍스트의 가변 길이 부분에서 고정 길이 특징 표현을 학습하는 감독되지 않은 알고리즘인 단락 벡터를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "340-5",
                    "sentence": "Our algorithm represents each document by a dense vector which is trained to predict words in the document.",
                    "sentence_kor": "우리의 알고리즘은 문서의 단어를 예측하도록 훈련된 밀집 벡터로 각 문서를 나타낸다.",
                    "tag": "2+3"
                },
                {
                    "index": "340-6",
                    "sentence": "Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models.",
                    "sentence_kor": "그것의 구성은 우리의 알고리즘에게 단어 가방 모델의 약점을 극복할 수 있는 잠재력을 준다.",
                    "tag": "3+4"
                },
                {
                    "index": "340-7",
                    "sentence": "Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.",
                    "sentence_kor": "경험적 결과에 따르면 단락 벡터는 텍스트 표현에 대한 다른 기법뿐만 아니라 단어 가방 모델보다 성능이 우수하다.",
                    "tag": "4"
                },
                {
                    "index": "340-8",
                    "sentence": "Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",
                    "sentence_kor": "마지막으로, 우리는 몇 가지 텍스트 분류 및 감정 분석 작업에서 새로운 최첨단 결과를 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "940",
            "abstractID": "SPA_abs-341",
            "text": [
                {
                    "index": "341-0",
                    "sentence": "The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.",
                    "sentence_kor": "최근에 도입된 연속 스킵그램 모델은 많은 정밀한 구문 및 의미적 단어 관계를 포착하는 고품질 분산 벡터 표현을 학습하는 효율적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "341-1",
                    "sentence": "In this paper we present several extensions that improve both the quality of the vectors and the training speed.",
                    "sentence_kor": "본 논문에서 우리는 벡터의 품질과 교육 속도를 모두 향상시키는 몇 가지 확장을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "341-2",
                    "sentence": "By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations.",
                    "sentence_kor": "빈번한 단어를 서브샘플링하여 상당한 속도 향상을 얻고 더 많은 정규 단어 표현을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "341-3",
                    "sentence": "We also describe a simple alternative to the hierarchical softmax called negative sampling.",
                    "sentence_kor": "또한 마이너스 샘플링이라고 하는 계층적 소프트맥스에 대한 간단한 대안을 설명한다.",
                    "tag": "3"
                },
                {
                    "index": "341-4",
                    "sentence": "An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.",
                    "sentence_kor": "단어 표현의 본질적인 한계는 어순에 대한 무관심과 관용적인 문구를 표현할 수 없다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "341-5",
                    "sentence": "For example, the meanings of \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".",
                    "sentence_kor": "예를 들어, \"캐나다\"와 \"에어\"의 의미는 \"에어 캐나다\"를 얻기 위해 쉽게 결합될 수 없다.",
                    "tag": "4"
                },
                {
                    "index": "341-6",
                    "sentence": "Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",
                    "sentence_kor": "이 예에 의해 동기 부여되어, 우리는 텍스트에서 구문을 찾는 간단한 방법을 제시하고 수백만 개의 구문에 대한 좋은 벡터 표현을 배우는 것이 가능하다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "941",
            "abstractID": "SPA_abs-342",
            "text": [
                {
                    "index": "342-0",
                    "sentence": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets.",
                    "sentence_kor": "우리는 매우 큰 데이터 세트에서 단어의 연속 벡터 표현을 계산하기 위한 두 가지 새로운 모델 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "342-1",
                    "sentence": "The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.",
                    "sentence_kor": "이러한 표현의 품질은 단어 유사성 작업으로 측정되며, 그 결과는 서로 다른 유형의 신경망을 기반으로 이전에 가장 잘 수행되었던 기법과 비교된다.",
                    "tag": "2+3"
                },
                {
                    "index": "342-2",
                    "sentence": "We observe large improvements in accuracy at much lower computational cost, i.e.it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.",
                    "sentence_kor": "우리는 훨씬 낮은 계산 비용으로 정확도가 크게 향상되는 것을 관찰한다. i.e.it은 16억 개의 단어 데이터 세트에서 고품질 단어 벡터를 학습하는 데 하루도 채 걸리지 않는다.",
                    "tag": "3"
                },
                {
                    "index": "342-3",
                    "sentence": "Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",
                    "sentence_kor": "또한, 이러한 벡터가 구문 및 의미 단어 유사성을 측정하기 위한 테스트 세트에서 최첨단 성능을 제공한다는 것을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "942",
            "abstractID": "SPA_abs-343",
            "text": [
                {
                    "index": "343-0",
                    "sentence": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time.",
                    "sentence_kor": "이 논문은 한 번에 하나의 데이터 지점을 예측함으로써 장기간의 구조를 가진 복잡한 시퀀스를 생성하기 위해 장기 단기 기억 반복 신경망을 어떻게 사용할 수 있는지 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "343-1",
                    "sentence": "The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued).",
                    "sentence_kor": "이 접근법은 텍스트(데이터가 분리된 곳)와 온라인 필기(데이터가 실제 값인 곳)에 대해 입증된다.",
                    "tag": "2+3"
                },
                {
                    "index": "343-2",
                    "sentence": "It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence.",
                    "sentence_kor": "그런 다음 네트워크가 텍스트 시퀀스에서 예측을 조건화할 수 있도록 하여 필기 합성으로 확장된다.",
                    "tag": "3"
                },
                {
                    "index": "343-3",
                    "sentence": "The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.",
                    "sentence_kor": "그 결과 시스템은 매우 사실적인 필기체를 다양한 스타일로 만들 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "943",
            "abstractID": "SPA_abs-344",
            "text": [
                {
                    "index": "344-0",
                    "sentence": "Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs).",
                    "sentence_kor": "현재의 많은 최신 어휘 연속 음성 인식 시스템(LVCSR)은 신경 네트워크와 은닉 마르코프 모델(HM)의 하이브리드이다.",
                    "tag": "1"
                },
                {
                    "index": "344-1",
                    "sentence": "Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding.",
                    "sentence_kor": "이러한 시스템의 대부분은 음향 모델링, 언어 모델링 및 시퀀스 디코딩을 다루는 별도의 구성 요소를 포함하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "344-2",
                    "sentence": "We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level.",
                    "sentence_kor": "HMM이 문자 수준에서 직접 시퀀스 예측을 수행하는 RNN(Recurrent Neural Network)으로 대체되는 보다 직접적인 접근 방식을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "344-3",
                    "sentence": "Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN.",
                    "sentence_kor": "입력 형상과 원하는 문자 시퀀스의 정렬은 RNN에 내장된 주의 메커니즘에 의해 자동으로 학습된다.",
                    "tag": "3"
                },
                {
                    "index": "344-4",
                    "sentence": "For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames.",
                    "sentence_kor": "예측된 각 문자에 대해 주의 메커니즘은 입력 시퀀스를 스캔하고 관련 프레임을 선택합니다.",
                    "tag": "3"
                },
                {
                    "index": "344-5",
                    "sentence": "We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length.",
                    "sentence_kor": "이 작업을 가속화하기 위한 두 가지 방법, 즉 가장 유망한 프레임의 하위 집합으로 스캔을 제한하고 시간이 지남에 따라 인접 프레임에 포함된 정보를 풀링하여 소스 시퀀스 길이를 줄이는 방법을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "344-6",
                    "sentence": "Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.",
                    "sentence_kor": "n그램 언어 모델을 디코딩 프로세스에 통합하면 다른 HMM 프리 RNN 기반 접근법과 유사한 인식 정확도를 얻을 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "944",
            "abstractID": "SPA_abs-345",
            "text": [
                {
                    "index": "345-0",
                    "sentence": "We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages.",
                    "sentence_kor": "우리는 엔드 투 엔드 딥 러닝 접근 방식을 사용하여 크게 다른 두 언어인 영어 또는 중국어 회화를 인식할 수 있음을 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "345-1",
                    "sentence": "Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.",
                    "sentence_kor": "수공예 구성 요소의 전체 파이프라인을 신경 네트워크로 대체하기 때문에 엔드 투 엔드 학습을 통해 시끄러운 환경, 억양 및 다양한 언어를 포함한 다양한 음성을 처리할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "345-2",
                    "sentence": "Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system.",
                    "sentence_kor": "우리의 접근 방식의 핵심은 HPC 기술의 적용으로 우리의 이전 시스템보다 7배 더 빠른 속도를 내는 것이다.",
                    "tag": "2+3"
                },
                {
                    "index": "345-3",
                    "sentence": "Because of this efficiency, experiments that previously took weeks now run in days.",
                    "sentence_kor": "이러한 효율성 덕분에 이전에는 몇 주가 걸리던 실험이 이제 며칠 만에 실행됩니다.",
                    "tag": "4"
                },
                {
                    "index": "345-4",
                    "sentence": "This enables us to iterate more quickly to identify superior architectures and algorithms.",
                    "sentence_kor": "이를 통해 우수한 아키텍처와 알고리즘을 보다 신속하게 반복할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "345-5",
                    "sentence": "As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets.",
                    "sentence_kor": "그 결과, 몇 가지 경우, 표준 데이터 세트를 벤치마킹할 때 우리 시스템은 인간 노동자의 전사보다 경쟁력이 있다.",
                    "tag": "4+5"
                },
                {
                    "index": "345-6",
                    "sentence": "Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.",
                    "sentence_kor": "마지막으로, 데이터 센터의 GPU와 함께 배치 디스패치라는 기술을 사용하여 시스템을 온라인 환경에서 저렴하게 배포하여 규모에 맞는 사용자에게 서비스를 제공할 수 있다는 것을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "945",
            "abstractID": "SPA_abs-346",
            "text": [
                {
                    "index": "346-0",
                    "sentence": "We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition.",
                    "sentence_kor": "우리는 전화 인식을 위해 심층 신념 네트워크를 사용하는 최근의 진보를 활용하는 어휘 음성 인식(LVSR)을 위한 새로운 맥락 의존적(CD) 모델을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "346-1",
                    "sentence": "We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output.",
                    "sentence_kor": "우리는 DNN이 출력으로 세논(티드 트리폰 상태)에 대한 분포를 생성하도록 훈련시키는 사전 훈련된 심층 신경망 은닉 마르코프 모델(DNN-HM) 하이브리드 아키텍처를 설명한다.",
                    "tag": "2"
                },
                {
                    "index": "346-2",
                    "sentence": "The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error.",
                    "sentence_kor": "심층 신념 네트워크 사전 훈련 알고리즘은 최적화를 돕고 일반화 오류를 줄일 수 있는 심층 신경망을 생성적으로 초기화하는 강력하고 종종 유용한 방법이다.",
                    "tag": "2+3"
                },
                {
                    "index": "346-3",
                    "sentence": "We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance.",
                    "sentence_kor": "우리는 모델의 주요 구성 요소를 설명하고 CD-DNN-HMM을 LVSR에 적용하는 절차를 설명하며 다양한 모델링 선택이 성능에 미치는 영향을 분석한다.",
                    "tag": "3"
                },
                {
                    "index": "346-4",
                    "sentence": "Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.",
                    "sentence_kor": "까다로운 비즈니스 검색 데이터 세트에 대한 실험은 CD-DNN-HM이 최소 훈련 MM을 사용하는 CD-GMM에 비해 5.8%, 9.2%(또는 상대 오류 감소 16.0%, 23.2%)의 절대 문장 정확도가 향상되어 기존의 상황에 의존하는 가우스 혼합 모델(GMM)-HM을 크게 능가할 수 있음을 보여준다.또는 비율(MPE) 및 최대우도(ML) 기준.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "946",
            "abstractID": "SPA_abs-347",
            "text": [
                {
                    "index": "347-0",
                    "sentence": "Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control.",
                    "sentence_kor": "정책 검색 방법을 사용하면 로봇이 광범위한 작업에 대한 제어 정책을 학습할 수 있지만, 정책 검색의 실제 적용에는 인식, 상태 추정 및 낮은 수준의 제어를 위해 수공예화된 구성 요소가 필요한 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "347-1",
                    "sentence": "In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately?",
                    "sentence_kor": "본 논문에서 우리는 다음과 같은 질문에 답하는 것을 목표로 한다. 인식 및 제어 시스템을 전체적으로 함께 훈련하는 것이 각 구성 요소를 개별적으로 교육하는 것보다 더 나은 성능을 제공하는가?",
                    "tag": "1+2"
                },
                {
                    "index": "347-2",
                    "sentence": "To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors.",
                    "sentence_kor": "이를 위해 원시 이미지 관찰을 로봇 모터의 토크에 직접 매핑하는 정책을 학습하는 데 사용할 수 있는 방법을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "347-3",
                    "sentence": "The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method.",
                    "sentence_kor": "정책은 92,000개의 매개 변수를 가진 심층 컨볼루션 신경망(CNN)으로 표현되며, 간단한 궤적 중심 강화 학습 방법에 의해 제공되는 감독 하에 정책 검색을 지도 학습으로 변환하는 부분적으로 관찰된 안내된 정책 검색 방법을 사용하여 훈련된다.",
                    "tag": "3+4"
                },
                {
                    "index": "347-4",
                    "sentence": "We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.",
                    "sentence_kor": "병뚜껑을 조이는 것과 같이 시각과 제어 사이의 긴밀한 조정이 필요한 다양한 실제 조작 작업에 대해 우리의 방법을 평가하고 다양한 이전 정책 검색 방법과 시뮬레이션된 비교를 제시한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "947",
            "abstractID": "SPA_abs-348",
            "text": [
                {
                    "index": "348-0",
                    "sentence": "We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain.",
                    "sentence_kor": "우리는 딥 Q-러닝의 성공에 바탕을 둔 아이디어를 연속 행동 영역에 적용한다.",
                    "tag": "2"
                },
                {
                    "index": "348-1",
                    "sentence": "We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.",
                    "sentence_kor": "우리는 연속적인 작업 공간에서 작동할 수 있는 결정론적 정책 기울기에 기초한 행위자 비판적이고 모델이 없는 알고리즘을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "348-2",
                    "sentence": "Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving.",
                    "sentence_kor": "동일한 학습 알고리즘, 네트워크 아키텍처 및 하이퍼 파라미터를 사용하여 우리의 알고리즘은 카트폴 스윙업, 능숙한 조작, 다리 운동 및 자동차 주행과 같은 고전적인 문제를 포함하여 20개 이상의 시뮬레이션된 물리 작업을 강력하게 해결한다.",
                    "tag": "2+3"
                },
                {
                    "index": "348-3",
                    "sentence": "Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives.",
                    "sentence_kor": "우리의 알고리즘은 도메인과 그 파생물의 역학에 대한 완전한 액세스 권한을 가진 계획 알고리즘에 의해 발견된 것과 성능이 경쟁력 있는 정책을 찾을 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "348-4",
                    "sentence": "We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.",
                    "sentence_kor": "또한 알고리즘이 원시 픽셀 입력에서 직접 엔드 투 엔드 정책을 학습할 수 있는 많은 작업에 대해 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "948",
            "abstractID": "SPA_abs-349",
            "text": [
                {
                    "index": "349-0",
                    "sentence": "We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects.",
                    "sentence_kor": "우리는 물체가 포함된 장면의 RGB-D 뷰에서 로봇 그립을 감지하는 문제를 고려한다.",
                    "tag": "1+2"
                },
                {
                    "index": "349-1",
                    "sentence": "In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features.",
                    "sentence_kor": "이 작업에서는 이 문제를 해결하기 위해 딥 러닝 접근 방식을 적용하여 시간이 많이 걸리는 기능의 수동 설계를 방지한다.",
                    "tag": "2"
                },
                {
                    "index": "349-2",
                    "sentence": "This presents two main challenges.",
                    "sentence_kor": "이것은 두 가지 주요 과제를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "349-3",
                    "sentence": "First, we need to evaluate a huge number of candidate grasps.",
                    "sentence_kor": "첫째, 우리는 후보자들의 엄청난 이해도를 평가할 필요가 있다.",
                    "tag": "3"
                },
                {
                    "index": "349-4",
                    "sentence": "In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second.",
                    "sentence_kor": "탐지를 빠르고 견고하게 하기 위해, 우리는 첫 번째의 상위 탐지가 두 번째에 의해 재평가되는 두 개의 심층 네트워크를 가진 2단계 계단식 구조를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "349-5",
                    "sentence": "The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps.",
                    "sentence_kor": "첫 번째 네트워크는 기능이 적고 실행 속도가 빠르며 후보 파악을 효과적으로 제거할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "349-6",
                    "sentence": "The second, with more features, is slower but has to run only on the top few detections.",
                    "sentence_kor": "두 번째는 기능이 더 많은 경우 속도가 느리지만 상위 몇 개의 탐지에서만 실행해야 합니다.",
                    "tag": "3"
                },
                {
                    "index": "349-7",
                    "sentence": "Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization.",
                    "sentence_kor": "둘째, 멀티모달 그룹 정규화를 기반으로 가중치에 구조화된 정규화를 적용하는 방법을 제시하는 멀티모달 입력을 잘 처리해야 한다.",
                    "tag": "3"
                },
                {
                    "index": "349-8",
                    "sentence": "We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.",
                    "sentence_kor": "우리는 우리의 방법이 로봇 그립 감지에서 이전의 최첨단 방법을 능가하며 두 개의 서로 다른 로봇 플랫폼에서 그립을 성공적으로 실행하는 데 사용될 수 있음을 입증한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "949",
            "abstractID": "SPA_abs-350",
            "text": [
                {
                    "index": "350-0",
                    "sentence": "We propose a deep learning method for single image super-resolution (SR).",
                    "sentence_kor": "단일 이미지 초고해상도(SR)를 위한 딥 러닝 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "350-1",
                    "sentence": "Our method directly learns an end-to-end mapping between the low/high-resolution images.",
                    "sentence_kor": "이 방법은 저해상도/고해상도 이미지 간의 엔드 투 엔드 매핑을 직접 학습한다.",
                    "tag": "2"
                },
                {
                    "index": "350-2",
                    "sentence": "The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one.",
                    "sentence_kor": "매핑은 저해상도 이미지를 입력으로 가져가고 고해상도 이미지를 출력하는 심층 컨볼루션 신경망(CNN)으로 표현된다.",
                    "tag": "2+3"
                },
                {
                    "index": "350-3",
                    "sentence": "We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network.",
                    "sentence_kor": "또한 기존의 희소 코딩 기반 SR 방법을 심층 컨볼루션 네트워크로 볼 수 있음을 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "350-4",
                    "sentence": "But unlike traditional methods that handle each component separately, our method jointly optimizes all layers.",
                    "sentence_kor": "그러나 각 구성요소를 개별적으로 처리하는 기존 방법과 달리, 우리의 방법은 모든 계층을 공동으로 최적화합니다.",
                    "tag": "2+3"
                },
                {
                    "index": "350-5",
                    "sentence": "Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.",
                    "sentence_kor": "우리의 심층 CNN은 가벼운 구조를 가지고 있지만 최첨단 복원 품질을 입증하고 실용적인 온라인 사용을 위해 빠른 속도를 달성한다.",
                    "tag": "3"
                },
                {
                    "index": "350-6",
                    "sentence": "We explore different network structures and parameter settings to achieve trade-offs between performance and speed.",
                    "sentence_kor": "성능과 속도 간의 균형을 달성하기 위해 다양한 네트워크 구조와 매개 변수 설정을 탐색한다.",
                    "tag": "3"
                },
                {
                    "index": "350-7",
                    "sentence": "Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.",
                    "sentence_kor": "또한, 우리는 네트워크를 확장하여 세 가지 컬러 채널에 동시에 대처하고 전반적인 재구성 품질을 개선합니다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "950",
            "abstractID": "SPA_abs-351",
            "text": [
                {
                    "index": "351-0",
                    "sentence": "In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image.",
                    "sentence_kor": "미술, 특히 그림에서, 인간은 이미지의 내용과 스타일 사이의 복잡한 상호작용을 구성함으로써 독특한 시각적 경험을 창조하는 기술을 익혔다.",
                    "tag": "1"
                },
                {
                    "index": "351-1",
                    "sentence": "Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities.",
                    "sentence_kor": "지금까지 이 과정의 알고리즘 기반은 알려져 있지 않으며 유사한 능력을 가진 인공 시스템은 존재하지 않는다.",
                    "tag": "1"
                },
                {
                    "index": "351-2",
                    "sentence": "However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.",
                    "sentence_kor": "그러나, 물체와 얼굴 인식에 가까운 인간에 가까운 성능과 같은 시각 인식의 다른 핵심 영역에서는 최근 심층 신경 네트워크라고 불리는 생물학적으로 영감을 받은 비전 모델의 종류에 의해 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "351-3",
                    "sentence": "Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality.",
                    "sentence_kor": "여기서는 높은 지각 품질의 예술적 이미지를 생성하는 심층 신경망을 기반으로 하는 인공 시스템을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "351-4",
                    "sentence": "The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images.",
                    "sentence_kor": "이 시스템은 신경 표현을 사용하여 임의 이미지의 내용과 스타일을 분리 및 재결합하여 예술적 이미지 생성을 위한 신경 알고리즘을 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "351-5",
                    "sentence": "Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.",
                    "sentence_kor": "더욱이, 성능 최적화 인공 신경 네트워크와 생물학적 시각 사이의 현저한 유사성에 비추어, 우리의 연구는 인간이 예술적 이미지를 만들고 인식하는 방법에 대한 알고리즘적인 이해로 나아가는 길을 제공한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "951",
            "abstractID": "SPA_abs-352",
            "text": [
                {
                    "index": "352-0",
                    "sentence": "We present a model that generates natural language descriptions of images and their regions.",
                    "sentence_kor": "우리는 이미지와 그 지역에 대한 자연어 설명을 생성하는 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "352-1",
                    "sentence": "Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data.",
                    "sentence_kor": "우리의 접근 방식은 이미지 데이터 세트와 그 문장 설명을 활용하여 언어와 시각 데이터 사이의 모달 간 대응에 대해 배운다.",
                    "tag": "2"
                },
                {
                    "index": "352-2",
                    "sentence": "Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding.",
                    "sentence_kor": "우리의 정렬 모델은 이미지 영역에 대한 컨볼루션 신경 네트워크, 문장에 대한 양방향 반복 신경 네트워크 및 다중 모드 임베딩을 통해 두 가지 양식을 정렬하는 구조화된 목표의 새로운 조합을 기반으로 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "352-3",
                    "sentence": "We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions.",
                    "sentence_kor": "그런 다음 유추 정렬을 사용하여 이미지 영역에 대한 새로운 설명을 생성하는 방법을 배우는 다중 모드 순환 신경망 아키텍처를 설명한다.",
                    "tag": "3+4"
                },
                {
                    "index": "352-4",
                    "sentence": "We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets.",
                    "sentence_kor": "우리는 우리의 정렬 모델이 Flickr8K, Flickr30K 및 MSCOCO 데이터 세트에 대한 검색 실험을 통해 최첨단 결과를 산출한다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "352-5",
                    "sentence": "We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.",
                    "sentence_kor": "그런 다음 생성된 설명이 전체 이미지와 영역 수준 주석의 새로운 데이터 세트 모두에서 검색 기준선을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "952",
            "abstractID": "SPA_abs-353",
            "text": [
                {
                    "index": "353-0",
                    "sentence": "Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images.",
                    "sentence_kor": "기계 번역 및 객체 감지 분야의 최근 연구에서 영감을 받아 이미지의 내용을 설명하는 방법을 자동으로 학습하는 주의 기반 모델을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "353-1",
                    "sentence": "We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound.",
                    "sentence_kor": "우리는 표준 역전파 기법을 사용하여 결정론적 방법으로 그리고 변동 하한을 최대화하여 확률적으로 이 모델을 훈련시킬 수 있는 방법을 설명한다.",
                    "tag": "2+3"
                },
                {
                    "index": "353-2",
                    "sentence": "We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence.",
                    "sentence_kor": "우리는 또한 시각화를 통해 모델이 출력 시퀀스에서 해당 단어를 생성하면서 돌출된 물체에 대한 시선을 고정하는 방법을 자동으로 배울 수 있는 방법을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "353-3",
                    "sentence": "We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.",
                    "sentence_kor": "다음 세 가지 벤치마크 데이터 세트에서 최첨단 성능을 통해 주의의 사용을 검증합니다. Flickr8k, Flickr30k 및 MS COCO.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "953",
            "abstractID": "SPA_abs-354",
            "text": [
                {
                    "index": "354-0",
                    "sentence": "Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.",
                    "sentence_kor": "이미지의 내용을 자동으로 설명하는 것은 컴퓨터 비전과 자연어 처리를 연결하는 인공지능의 근본적인 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "354-1",
                    "sentence": "In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image.",
                    "sentence_kor": "본 논문에서는 컴퓨터 비전과 기계 번역의 최근 발전을 결합하고 이미지를 설명하는 자연스러운 문장을 생성하는 데 사용할 수 있는 심층 반복 아키텍처를 기반으로 하는 생성 모델을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "354-2",
                    "sentence": "The model is trained to maximize the likelihood of the target description sentence given the training image.",
                    "sentence_kor": "모델은 교육 이미지가 주어진 대상 설명 문장의 가능성을 최대화하기 위해 훈련된다.",
                    "tag": "2+3"
                },
                {
                    "index": "354-3",
                    "sentence": "Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions.",
                    "sentence_kor": "여러 데이터 세트에 대한 실험은 모델의 정확성과 이미지 설명만으로 학습하는 언어의 유창성을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "354-4",
                    "sentence": "Our model is often quite accurate, which we verify both qualitatively and quantitatively.",
                    "sentence_kor": "우리의 모델은 종종 꽤 정확하며, 우리는 질적으로나 양적으로나 검증한다.",
                    "tag": "4"
                },
                {
                    "index": "354-5",
                    "sentence": "For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69.",
                    "sentence_kor": "예를 들어, 파스칼 데이터 세트의 현재 최첨단 BLEU-1 점수는 25점이지만, 우리의 접근 방식은 약 69점의 인간 성능과 비교할 때 59점을 산출한다.",
                    "tag": "3+4"
                },
                {
                    "index": "354-6",
                    "sentence": "We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28.",
                    "sentence_kor": "또한 Flickr30k에서는 56에서 66으로, SBU에서는 19에서 28로 BLEU-1 점수가 향상되었음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "354-7",
                    "sentence": "Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.",
                    "sentence_kor": "마지막으로, 새로 출시된 COCO 데이터 세트에서 최신 기술인 27.7의 BLEU-4를 달성하였다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "954",
            "abstractID": "SPA_abs-355",
            "text": [
                {
                    "index": "355-0",
                    "sentence": "We consider the automated recognition of human actions in surveillance videos.",
                    "sentence_kor": "우리는 감시 비디오에서 인간의 행동을 자동으로 인식하는 것을 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "355-1",
                    "sentence": "Most current methods build classifiers based on complex handcrafted features computed from the raw inputs.",
                    "sentence_kor": "대부분의 현재 방법은 원시 입력에서 계산된 복잡한 수공예 기능을 기반으로 분류기를 구축한다.",
                    "tag": "1"
                },
                {
                    "index": "355-2",
                    "sentence": "Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs.",
                    "sentence_kor": "컨볼루션 신경망(CNN)은 원시 입력에 직접 작용할 수 있는 심층 모델의 한 유형이다.",
                    "tag": "1"
                },
                {
                    "index": "355-3",
                    "sentence": "However, such models are currently limited to handling 2D inputs.",
                    "sentence_kor": "그러나 이러한 모델은 현재 2D 입력 처리로 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "355-4",
                    "sentence": "In this paper, we develop a novel 3D CNN model for action recognition.",
                    "sentence_kor": "본 논문에서 우리는 동작 인식을 위한 새로운 3D CNN 모델을 개발한다.",
                    "tag": "2"
                },
                {
                    "index": "355-5",
                    "sentence": "This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames.",
                    "sentence_kor": "이 모델은 3D 컨볼루션을 수행하여 공간 및 시간 차원 모두에서 형상을 추출하여 인접한 여러 프레임에 인코딩된 모션 정보를 캡처한다.",
                    "tag": "4"
                },
                {
                    "index": "355-6",
                    "sentence": "The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels.",
                    "sentence_kor": "개발된 모델은 입력 프레임에서 여러 개의 정보 채널을 생성하고, 마지막 특징 표현은 모든 채널의 정보를 결합합니다.",
                    "tag": "4"
                },
                {
                    "index": "355-7",
                    "sentence": "To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models.",
                    "sentence_kor": "성능을 더욱 높이기 위해 높은 수준의 기능으로 출력을 정규화하고 다양한 모델의 예측을 결합할 것을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "355-8",
                    "sentence": "We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.",
                    "sentence_kor": "우리는 공항 감시 비디오의 실제 환경에서 인간의 행동을 인식하기 위해 개발된 모델을 적용하고 기본 방법에 비해 우수한 성능을 달성한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "955",
            "abstractID": "SPA_abs-356",
            "text": [
                {
                    "index": "356-0",
                    "sentence": "Convolutional networks are powerful visual models that yield hierarchies of features.",
                    "sentence_kor": "컨볼루션 네트워크는 특징의 계층 구조를 생성하는 강력한 시각적 모델이다.",
                    "tag": "1"
                },
                {
                    "index": "356-1",
                    "sentence": "We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation.",
                    "sentence_kor": "우리는 컨볼루션 네트워크 자체, 훈련된 엔드 투 엔드, 픽셀 투 픽셀이 의미론적 분할의 최첨단 수준을 능가한다는 것을 보여준다.",
                    "tag": "1+2"
                },
                {
                    "index": "356-2",
                    "sentence": "Our key insight is to build \"fully convolutional\" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.",
                    "sentence_kor": "우리의 핵심 통찰력은 임의의 크기의 입력을 받아 효율적인 추론과 학습으로 그에 상응하는 크기의 출력을 생성하는 \"완전히 컨볼루션\" 네트워크를 구축하는 것이다.",
                    "tag": "2"
                },
                {
                    "index": "356-3",
                    "sentence": "We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models.",
                    "sentence_kor": "우리는 완전 컨볼루션 네트워크의 공간을 정의 및 상세하게 설명하고, 공간적으로 조밀한 예측 작업에 대한 적용을 설명하고, 이전 모델에 대한 연결을 도출한다.",
                    "tag": "2+3"
                },
                {
                    "index": "356-4",
                    "sentence": "We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task.",
                    "sentence_kor": "우리는 현대의 분류 네트워크(AlexNet, VGG Net 및 GoogLeNet)를 완전한 컨볼루션 네트워크로 조정하고 세분화 작업에 미세 조정하여 학습된 표현을 전송한다.",
                    "tag": "3"
                },
                {
                    "index": "356-5",
                    "sentence": "We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.",
                    "sentence_kor": "그런 다음 깊고 거친 계층의 의미 정보와 얕고 미세한 계층의 외관 정보를 결합하여 정확하고 상세한 세분화를 생성하는 새로운 아키텍처를 정의한다.",
                    "tag": "3+4"
                },
                {
                    "index": "356-6",
                    "sentence": "Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.",
                    "sentence_kor": "우리의 완전 컨볼루션 네트워크는 PASCAL VOC(2012년 평균 아이유 62.2%로 상대적인 20% 개선), NYUdv2 및 SIFT Flow의 최첨단 세분화를 달성하지만 추론은 일반적인 이미지의 경우 1/3초가 걸린다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "956",
            "abstractID": "SPA_abs-357",
            "text": [
                {
                    "index": "357-0",
                    "sentence": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.",
                    "sentence_kor": "최첨단 객체 감지 네트워크는 객체 위치를 가정하기 위해 지역 제안 알고리즘에 따라 달라진다.",
                    "tag": "1"
                },
                {
                    "index": "357-1",
                    "sentence": "Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck.",
                    "sentence_kor": "SPPnet과 Fast R-CNN과 같은 발전은 이러한 탐지 네트워크의 실행 시간을 단축시켜 지역 제안 계산을 병목 현상으로 노출시켰다.",
                    "tag": "1"
                },
                {
                    "index": "357-2",
                    "sentence": "In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.",
                    "sentence_kor": "본 연구에서는 탐지 네트워크와 전체 이미지 컨볼루션 기능을 공유하여 거의 비용이 들지 않는 지역 제안을 가능하게 하는 지역 제안 네트워크(RPN)를 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "357-3",
                    "sentence": "An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.",
                    "sentence_kor": "RPN은 각 위치에서 객체 경계와 객체성 점수를 동시에 예측하는 완전 컨볼루션 네트워크이다.",
                    "tag": "3"
                },
                {
                    "index": "357-4",
                    "sentence": "The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.",
                    "sentence_kor": "RPN은 Fast R-CNN이 탐지를 위해 사용하는 고품질 영역 제안을 생성하기 위해 종단 간 훈련을 받는다.",
                    "tag": "3"
                },
                {
                    "index": "357-5",
                    "sentence": "We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look.",
                    "sentence_kor": "우리는 또한 RPN과 Fast R-CNN을 '주의' 메커니즘과 함께 최근 널리 사용되는 신경망의 용어를 사용하여 단일 네트워크에 병합한다.",
                    "tag": "3+4"
                },
                {
                    "index": "357-6",
                    "sentence": "For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image.",
                    "sentence_kor": "매우 깊은 VGG-16 모델의 경우, 탐지 시스템은 GPU에서 5fps(모든 단계 포함)의 프레임률을 갖는 동시에 이미지당 300개의 제안만으로 PASCAL VOC 2007, 2012 및 MS COCO 데이터 세트에서 최첨단 객체 감지 정확도를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "357-7",
                    "sentence": "In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks.",
                    "sentence_kor": "ILSVRC와 COCO 2015 경기에서 고속 R-CNN과 RPN은 여러 트랙에서 1위를 차지한 참가자들의 기초이다.",
                    "tag": "4"
                },
                {
                    "index": "357-8",
                    "sentence": "Code has been made publicly available.",
                    "sentence_kor": "코드가 공개되었습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "957",
            "abstractID": "SPA_abs-358",
            "text": [
                {
                    "index": "358-0",
                    "sentence": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.",
                    "sentence_kor": "표준 PASCAL VOC 데이터 세트에서 측정한 객체 감지 성능은 지난 몇 년 동안 안정되었습니다.",
                    "tag": "1"
                },
                {
                    "index": "358-1",
                    "sentence": "The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context.",
                    "sentence_kor": "가장 성능이 좋은 방법은 일반적으로 여러 로우 레벨 이미지 기능을 하이 레벨 컨텍스트와 결합하는 복잡한 앙상블 시스템입니다.",
                    "tag": "1"
                },
                {
                    "index": "358-2",
                    "sentence": "In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%.",
                    "sentence_kor": "본 논문에서 우리는 평균 정밀도(mAP)를 VOC 2012의 이전 최고 결과와 비교하여 30% 이상 향상시켜 53.3%의 mAP를 달성하는 간단하고 확장 가능한 탐지 알고리즘을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "358-3",
                    "sentence": "Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.",
                    "sentence_kor": "우리의 접근 방식은 두 가지 핵심 통찰력을 결합한다. (1) 개체를 국산화 및 분할하기 위해 상향식 영역 제안에 고용량 컨볼루션 신경망(CNN)을 적용할 수 있으며, (2) 레이블링된 훈련 데이터가 부족할 경우 보조 작업에 대한 사전 교육을 감독하고 도메인별 미세 조정을 통해 상당한 성능 부(boo)를 산출한다.세인트",
                    "tag": "2+3"
                },
                {
                    "index": "358-4",
                    "sentence": "Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.",
                    "sentence_kor": "지역 제안을 CNN과 결합하기 때문에, 우리는 우리의 방법을 R-CNN: CNN 기능이 있는 지역이라고 부른다.",
                    "tag": "3"
                },
                {
                    "index": "358-5",
                    "sentence": "We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture.",
                    "sentence_kor": "또한 R-CNN을 유사한 CNN 아키텍처를 기반으로 최근 제안된 슬라이딩 윈도우 검출기인 OverFeat와 비교한다.",
                    "tag": "3"
                },
                {
                    "index": "358-6",
                    "sentence": "We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset.",
                    "sentence_kor": "우리는 R-CNN이 200 클래스 ILSVRC2013 탐지 데이터 세트에서 OverFeat를 큰 차이로 능가한다는 것을 발견했다.",
                    "tag": "3+4"
                },
                {
                    "index": "358-7",
                    "sentence": "Source code for the complete system is available at this http URL.",
                    "sentence_kor": "전체 시스템의 소스 코드는 이 HTTP URL에서 사용할 수 있습니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "958",
            "abstractID": "SPA_abs-359",
            "text": [
                {
                    "index": "359-0",
                    "sentence": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image.",
                    "sentence_kor": "기존의 심층 컨볼루션 신경망(CNN)에는 고정 크기(예: 224x224) 입력 이미지가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "359-1",
                    "sentence": "This requirement is \"artificial\" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale.",
                    "sentence_kor": "이 요건은 \"인공적\"이며 임의의 크기/규모의 이미지 또는 하위 이미지에 대한 인식 정확도를 저하시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "359-2",
                    "sentence": "In this work, we equip the networks with another pooling strategy, \"spatial pyramid pooling\", to eliminate the above requirement.",
                    "sentence_kor": "본 연구에서는 위의 요구사항을 제거하기 위해 네트워크에 또 다른 풀링 전략인 \"공간 피라미드 풀링\"을 갖추었다.",
                    "tag": "2"
                },
                {
                    "index": "359-3",
                    "sentence": "The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale.",
                    "sentence_kor": "SPP-net이라고 하는 새로운 네트워크 구조는 이미지 크기/규모에 관계없이 고정 길이 표현을 생성할 수 있습니다.",
                    "tag": "2+3"
                },
                {
                    "index": "359-4",
                    "sentence": "Pyramid pooling is also robust to object deformations.",
                    "sentence_kor": "피라미드 풀링은 객체 변형에도 강력합니다.",
                    "tag": "3"
                },
                {
                    "index": "359-5",
                    "sentence": "With these advantages, SPP-net should in general improve all CNN-based image classification methods.",
                    "sentence_kor": "이러한 장점으로 SPP-net은 일반적으로 모든 CNN 기반 이미지 분류 방법을 개선해야 한다.",
                    "tag": "3"
                },
                {
                    "index": "359-6",
                    "sentence": "On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs.",
                    "sentence_kor": "ImageNet 2012 데이터 세트에서 SPP-net은 설계가 다르지만 다양한 CNN 아키텍처의 정확도를 향상시킨다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "359-7",
                    "sentence": "On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.",
                    "sentence_kor": "Pascal VOC 2007 및 Caltech101 데이터 세트에서 SPP-net은 단일 전체 이미지 표현과 미세 조정 없이 최첨단 분류 결과를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "359-8",
                    "sentence": "The power of SPP-net is also significant in object detection.",
                    "sentence_kor": "SPP-net의 힘은 물체 감지에서도 중요합니다.",
                    "tag": "4"
                },
                {
                    "index": "359-9",
                    "sentence": "Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors.",
                    "sentence_kor": "SPP-net을 사용하여 전체 이미지에서 기능 맵을 한 번만 계산한 다음 임의 영역(하위 이미지)에서 기능을 풀링하여 검출기 훈련을 위한 고정 길이 표현을 생성한다.",
                    "tag": "4"
                },
                {
                    "index": "359-10",
                    "sentence": "This method avoids repeatedly computing the convolutional features.",
                    "sentence_kor": "이 방법은 컨볼루션 특징을 반복적으로 계산하는 것을 방지한다.",
                    "tag": "1"
                },
                {
                    "index": "359-11",
                    "sentence": "In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.",
                    "sentence_kor": "테스트 이미지 처리에서 우리의 방법은 R-CNN 방법보다 24-102배 빠르지만 Pascal VOC 2007에서 더 낫거나 비슷한 정확도를 달성한다.",
                    "tag": "1"
                },
                {
                    "index": "359-12",
                    "sentence": "In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams.",
                    "sentence_kor": "ImageNet Large Scale Visual Recognition Challenge(ILSVRC) 2014에서 우리의 방법은 모든 38개 팀 중 물체 감지에서 2위, 이미지 분류에서 3위를 차지했다.",
                    "tag": "1"
                },
                {
                    "index": "359-13",
                    "sentence": "This manuscript also introduces the improvement made for this competition.",
                    "sentence_kor": "이 원고는 또한 이 대회를 위해 만들어진 개선 사항들을 소개하고 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "959",
            "abstractID": "SPA_abs-360",
            "text": [
                {
                    "index": "360-0",
                    "sentence": "Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to.",
                    "sentence_kor": "장면 라벨링은 이미지의 각 픽셀에 해당 픽셀이 속한 개체의 범주를 라벨링하는 것으로 구성됩니다.",
                    "tag": "1"
                },
                {
                    "index": "360-1",
                    "sentence": "We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel.",
                    "sentence_kor": "원시 픽셀에서 훈련된 다중 스케일 컨볼루션 네트워크를 사용하여 각 픽셀을 중심으로 여러 크기의 영역을 인코딩하는 조밀한 형상 벡터를 추출하는 방법을 제안한다.",
                    "tag": "1+2"
                },
                {
                    "index": "360-2",
                    "sentence": "The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information.",
                    "sentence_kor": "이 방법을 사용하면 엔지니어링된 기능의 필요성을 완화하고 질감, 모양 및 상황별 정보를 캡처하는 강력한 표현을 생성할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "360-3",
                    "sentence": "We report results using multiple postprocessing methods to produce the final labeling.",
                    "sentence_kor": "우리는 최종 라벨을 생성하기 위해 여러 후처리 방법을 사용하여 결과를 보고한다.",
                    "tag": "3"
                },
                {
                    "index": "360-4",
                    "sentence": "Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations.",
                    "sentence_kor": "그 중에서 장면을 가장 잘 설명하는 최적의 구성 요소 집합을 분할 구성 요소 풀에서 자동으로 검색하는 기술을 제안한다. 이러한 구성 요소는 예를 들어 분할 트리 또는 모든 감독 제품군에서 가져올 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "360-5",
                    "sentence": "The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.",
                    "sentence_kor": "이 시스템은 SIFT Flow 데이터 세트(33개 클래스)와 바르셀로나 데이터 세트(170개 클래스)에 대한 기록 정확도와 스탠포드 배경 데이터 세트(8개 클래스)에 대한 기록 정확도를 산출하는 동시에, 기능 추출을 포함한 320×240 이미지 레이블을 1초 이내에 생성하는 경쟁 접근 방식보다 훨씬 빠르다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "960",
            "abstractID": "SPA_abs-361",
            "text": [
                {
                    "index": "361-0",
                    "sentence": "Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks.",
                    "sentence_kor": "컨볼루션 네트워크는 다양한 작업을 위한 대부분의 최첨단 컴퓨터 비전 솔루션의 핵심이다.",
                    "tag": "1"
                },
                {
                    "index": "361-1",
                    "sentence": "Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks.",
                    "sentence_kor": "2014년 이후 매우 심층적인 컨볼루션 네트워크가 주류가 되어 다양한 벤치마크에서 상당한 이득을 얻기 시작했다.",
                    "tag": "1"
                },
                {
                    "index": "361-2",
                    "sentence": "Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios.",
                    "sentence_kor": "모델 크기와 계산 비용 증가는 대부분의 작업(훈련에 충분한 라벨링된 데이터가 제공되는 한)에서 즉각적인 품질 향상으로 이어지는 경향이 있지만, 계산 효율성과 낮은 매개 변수 수는 여전히 모바일 비전 및 빅데이터 시나리오와 같은 다양한 사용 사례에 대한 요소를 가능하게 하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "361-3",
                    "sentence": "Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization.",
                    "sentence_kor": "여기서는 적절하게 인수분해된 컨볼루션과 공격적인 정규화를 통해 추가된 연산을 최대한 효율적으로 활용하는 방식으로 네트워크를 확장하는 방법을 탐구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "361-4",
                    "sentence": "We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.",
                    "sentence_kor": "ILSVRC 2012 분류 과제 검증 세트에 대한 우리의 방법을 벤치마킹하면 추론당 계산 비용이 50억 곱하기 adds이고 매개 변수를 2500만 이하인 네트워크를 사용한 단일 프레임 평가에 대해 21.2% top-1 및 5.6% top-5 오류가 최신 기술에 비해 상당한 이득을 얻는다.",
                    "tag": "2+3"
                },
                {
                    "index": "361-5",
                    "sentence": "With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.",
                    "sentence_kor": "4개 모델의 앙상블과 멀티 크롭 평가를 통해 유효성 검사 세트에 대한 3.5%, 유효성 검사 세트에 대한 17.3% TOP-1 오류를 보고한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "961",
            "abstractID": "SPA_abs-362",
            "text": [
                {
                    "index": "362-0",
                    "sentence": "Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years.",
                    "sentence_kor": "매우 심층적인 컨볼루션 네트워크는 최근 몇 년 동안 이미지 인식 성능의 가장 큰 진보의 중심이었다.",
                    "tag": "1"
                },
                {
                    "index": "362-1",
                    "sentence": "One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost.",
                    "sentence_kor": "한 예로 상대적으로 낮은 계산 비용으로 매우 우수한 성능을 달성하는 것으로 나타난 인셉션 아키텍처가 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "362-2",
                    "sentence": "Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network.",
                    "sentence_kor": "최근, 보다 전통적인 아키텍처와 연계된 잔류 연결의 도입으로 2015년 ILSVRC 과제에서 최첨단 성능을 얻었다. 그 성능은 최신 세대 Inception-v3 네트워크와 유사했다.",
                    "tag": "1"
                },
                {
                    "index": "362-3",
                    "sentence": "This raises the question of whether there are any benefit in combining the Inception architecture with residual connections.",
                    "sentence_kor": "이것은 인셉션 아키텍처와 잔여 연결을 결합하는 데 어떤 이득이 있는지에 대한 의문을 제기한다.",
                    "tag": "1+2"
                },
                {
                    "index": "362-4",
                    "sentence": "Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly.",
                    "sentence_kor": "여기서 우리는 잔여 연결을 사용한 훈련이 인셉션 네트워크의 훈련을 크게 가속화한다는 명확한 경험적 증거를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "362-5",
                    "sentence": "There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin.",
                    "sentence_kor": "또한 잔류 인셉션 네트워크가 잔존 연결 없이 비슷하게 비싼 인셉션 네트워크를 근소한 차이로 능가한다는 증거가 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "362-6",
                    "sentence": "We also present several new streamlined architectures for both residual and non-residual Inception networks.",
                    "sentence_kor": "우리는 또한 잔여 및 비거주 인셉션 네트워크 모두에 대한 몇 가지 새로운 유선형 아키텍처를 제시한다.",
                    "tag": "3"
                },
                {
                    "index": "362-7",
                    "sentence": "These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly.",
                    "sentence_kor": "이러한 변화는 ILSVRC 2012 분류 과제의 단일 프레임 인식 성능을 크게 개선한다.",
                    "tag": "3"
                },
                {
                    "index": "362-8",
                    "sentence": "We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks.",
                    "sentence_kor": "또한 적절한 활성화 확장이 매우 광범위한 잔류 인셉션 네트워크의 훈련을 어떻게 안정화시키는지 입증한다.",
                    "tag": "3"
                },
                {
                    "index": "362-9",
                    "sentence": "With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge.",
                    "sentence_kor": "세 개의 잔류 및 하나의 Inception-v4 앙상블을 사용하여 ImageNet 분류(CLS) 과제 테스트 세트에서 3.08%의 상위 5개 오류를 달성했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "962",
            "abstractID": "SPA_abs-363",
            "text": [
                {
                    "index": "363-0",
                    "sentence": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors.",
                    "sentence_kor": "심층 잔류 네트워크는 강력한 정확성과 우수한 융합 동작을 보여주는 매우 심층적인 아키텍처 제품군으로 부상했다.",
                    "tag": "1"
                },
                {
                    "index": "363-1",
                    "sentence": "In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation.",
                    "sentence_kor": "본 논문에서 우리는 잔여 구성 요소 뒤의 전파 공식을 분석하는데, 이는 스킵 연결 및 추가 후 활성화로 신원 매핑을 사용할 때 전방 및 후방 신호가 한 블록에서 다른 블록으로 직접 전파될 수 있음을 시사한다.",
                    "tag": "1+2"
                },
                {
                    "index": "363-2",
                    "sentence": "A series of ablation experiments support the importance of these identity mappings.",
                    "sentence_kor": "일련의 절제 실험은 이러한 신원 매핑의 중요성을 뒷받침한다.",
                    "tag": "1"
                },
                {
                    "index": "363-3",
                    "sentence": "This motivates us to propose a new residual unit, which makes training easier and improves generalization.",
                    "sentence_kor": "이는 교육을 더 쉽게 하고 일반화를 개선하는 새로운 잔여 단위를 제안하도록 동기를 부여한다.",
                    "tag": "1+2"
                },
                {
                    "index": "363-4",
                    "sentence": "We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.",
                    "sentence_kor": "CIFAR-10(오차 4.62%)과 CIFAR-100의 경우 1001 레이어 ResNet을 사용하고 ImageNet의 경우 200 레이어 ResNet을 사용하여 개선된 결과를 보고한다.",
                    "tag": "3+4"
                },
                {
                    "index": "363-5",
                    "sentence": "Code is available at: this https URL",
                    "sentence_kor": "코드 사용 가능 위치: 이 https URL",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "963",
            "abstractID": "SPA_abs-364",
            "text": [
                {
                    "index": "364-0",
                    "sentence": "Deeper neural networks are more difficult to train.",
                    "sentence_kor": "더 깊은 신경망은 훈련시키기가 더 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "364-1",
                    "sentence": "We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.",
                    "sentence_kor": "이전에 사용한 것보다 훨씬 더 심층적인 네트워크 훈련을 용이하게 하기 위한 잔여 학습 프레임워크를 제시한다.",
                    "tag": "1+2"
                },
                {
                    "index": "364-2",
                    "sentence": "We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.",
                    "sentence_kor": "참조되지 않은 기능을 학습하는 대신 계층 입력을 참조하여 잔여 기능을 학습하는 것으로 계층을 명시적으로 재구성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "364-3",
                    "sentence": "We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.",
                    "sentence_kor": "우리는 이러한 잔여 네트워크가 최적화하기 쉽고 상당히 증가한 깊이로부터 정확도를 얻을 수 있다는 것을 보여주는 포괄적인 경험적 증거를 제공한다.",
                    "tag": "2+3"
                },
                {
                    "index": "364-4",
                    "sentence": "On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.",
                    "sentence_kor": "ImageNet 데이터 세트에서 VGG 네트워크보다 최대 8배 더 깊지만 복잡성은 낮은 최대 152개의 레이어로 잔여 넷을 평가합니다.",
                    "tag": "3"
                },
                {
                    "index": "364-5",
                    "sentence": "An ensemble of these residual nets achieves 3.57% error on the ImageNet test set.",
                    "sentence_kor": "이러한 잔류 네트의 앙상블은 ImageNet 테스트 세트에서 3.57%의 오차를 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "364-6",
                    "sentence": "This result won the 1st place on the ILSVRC 2015 classification task.",
                    "sentence_kor": "이 결과는 ILSVRC 2015 분류 과제에서 1위를 차지했다.",
                    "tag": "4"
                },
                {
                    "index": "364-7",
                    "sentence": "We also present analysis on CIFAR-10 with 100 and 1000 layers.",
                    "sentence_kor": "우리는 또한 100과 1000 레이어의 CIFAR-10에 대한 분석을 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "364-8",
                    "sentence": "The depth of representations is of central importance for many visual recognition tasks.",
                    "sentence_kor": "표현 깊이는 많은 시각적 인식 작업에서 가장 중요하다.",
                    "tag": "4"
                },
                {
                    "index": "364-9",
                    "sentence": "Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.",
                    "sentence_kor": "우리의 매우 깊은 표현 덕분에 COCO 개체 감지 데이터 세트에서 28%의 상대적 향상을 얻었다.",
                    "tag": "4"
                },
                {
                    "index": "364-10",
                    "sentence": "Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
                    "sentence_kor": "심층 잔류 네트는 ILSVRC & COCO 2015 대회에 출품한 토대이며, ImageNet 탐지, ImageNet 로컬라이제이션, COCO 탐지 및 COCO 세분화 작업에서도 1위를 차지했다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "964",
            "abstractID": "SPA_abs-365",
            "text": [
                {
                    "index": "365-0",
                    "sentence": "Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.",
                    "sentence_kor": "컨볼루션 신경망은 예외적으로 강력한 모델 클래스를 정의하지만, 여전히 입력 데이터에 대해 계산 및 매개 변수 효율적인 방식으로 공간적으로 불변할 수 있는 능력 부족으로 인해 제한된다.",
                    "tag": "1"
                },
                {
                    "index": "365-1",
                    "sentence": "In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.",
                    "sentence_kor": "이 작업에서는 네트워크 내에서 데이터를 공간적으로 조작할 수 있는 새로운 학습 가능한 모듈인 공간 변환기를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "365-2",
                    "sentence": "This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.",
                    "sentence_kor": "이 차별화 가능한 모듈은 기존 컨볼루션 아키텍처에 삽입될 수 있으며, 신경망은 최적화 프로세스에 대한 추가 훈련 감독 또는 수정 없이 피처 맵 자체에서 조건부로 피처 맵을 공간적으로 능동적으로 변환할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "365-3",
                    "sentence": "We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.",
                    "sentence_kor": "우리는 공간 변압기를 사용하면 변환, 스케일, 회전 및 보다 일반적인 뒤틀림에 대한 불변성을 학습하는 모델이 생성되어 여러 벤치마크와 여러 종류의 변환에 대한 최첨단 성능을 얻을 수 있음을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "965",
            "abstractID": "SPA_abs-366",
            "text": [
                {
                    "index": "366-0",
                    "sentence": "We propose a deep convolutional neural network architecture codenamed \"Inception\", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014).",
                    "sentence_kor": "우리는 이미지넷 대규모 시각적 인식 챌린지 2014(ILSVRC 2014)에서 분류 및 탐지를 위한 새로운 최첨단 기술을 설정하는 역할을 담당한 \"인셉션\"이라는 이름의 심층 컨볼루션 신경망 아키텍처를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "366-1",
                    "sentence": "The main hallmark of this architecture is the improved utilization of the computing resources inside the network.",
                    "sentence_kor": "이 아키텍처의 주요 특징은 네트워크 내부의 컴퓨팅 자원의 활용도 향상입니다.",
                    "tag": "4"
                },
                {
                    "index": "366-2",
                    "sentence": "This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant.",
                    "sentence_kor": "이는 계산 예산을 일정하게 유지하면서 네트워크의 깊이와 폭을 늘릴 수 있도록 세심하게 조작된 설계에 의해 달성되었다.",
                    "tag": "4"
                },
                {
                    "index": "366-3",
                    "sentence": "To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.",
                    "sentence_kor": "품질을 최적화하기 위해, 아키텍처 결정은 Hebbian 원리와 다중 스케일 처리의 직관성에 기초했습니다.",
                    "tag": "3+4"
                },
                {
                    "index": "366-4",
                    "sentence": "One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
                    "sentence_kor": "ILSVRC 2014에 대한 제출에 사용된 특정 화신 중 하나는 22계층 심층 네트워크인 GoogLeNet이라고 하며, 그 품질은 분류 및 검출의 맥락에서 평가된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "966",
            "abstractID": "SPA_abs-367",
            "text": [
                {
                    "index": "367-0",
                    "sentence": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting.",
                    "sentence_kor": "본 연구에서는 대규모 이미지 인식 설정에서 컨볼루션 네트워크 깊이가 정확도에 미치는 영향을 조사한다.",
                    "tag": "2"
                },
                {
                    "index": "367-1",
                    "sentence": "Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers.",
                    "sentence_kor": "우리의 주된 기여는 매우 작은(3x3) 컨볼루션 필터를 가진 아키텍처를 사용하여 깊이가 증가하는 네트워크에 대한 철저한 평가이며, 이는 깊이를 16-19 중량 레이어로 밀어냄으로써 선행 기술 구성을 크게 개선할 수 있음을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "367-2",
                    "sentence": "These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.",
                    "sentence_kor": "이러한 연구 결과는 ImageNet Challenge 2014 제출의 기초가 되었으며, 팀은 현지화 및 분류 트랙에서 각각 1위와 2위를 확보했습니다.",
                    "tag": "4"
                },
                {
                    "index": "367-3",
                    "sentence": "We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results.",
                    "sentence_kor": "또한 우리의 표현이 다른 데이터 세트에 잘 일반화되어 최첨단 결과를 달성한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "367-4",
                    "sentence": "We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.",
                    "sentence_kor": "우리는 컴퓨터 비전에 심층 시각적 표현의 사용에 대한 추가 연구를 용이하게 하기 위해 최고 성능의 두 가지 ConvNet 모델을 공개적으로 사용할 수 있도록 했다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "967",
            "abstractID": "SPA_abs-368",
            "text": [
                {
                    "index": "368-0",
                    "sentence": "The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods.",
                    "sentence_kor": "최신 세대의 CNN(Convolutional Neural Networks)은 이미지 인식 및 객체 감지에 대한 까다로운 벤치마크에서 인상적인 결과를 달성하여 이러한 방법에 대한 커뮤니티의 관심을 크게 높이고 있다.",
                    "tag": "1"
                },
                {
                    "index": "368-1",
                    "sentence": "Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector.",
                    "sentence_kor": "그럼에도 불구하고 CNN 방법이 서로 어떻게 비교되는지, 그리고 Bag-of-Visual-Words 및 Envanced Fisher Vector와 같은 이전의 최첨단 얕은 표현과 비교되는지는 여전히 불확실하다.",
                    "tag": "1"
                },
                {
                    "index": "368-2",
                    "sentence": "This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details.",
                    "sentence_kor": "본 논문은 이러한 새로운 기법에 대한 엄격한 평가를 실시하여 서로 다른 심층 아키텍처를 탐색하고 공통적인 관점에서 비교하며 중요한 구현 세부 사항을 식별하고 공개한다.",
                    "tag": "1"
                },
                {
                    "index": "368-3",
                    "sentence": "We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance.",
                    "sentence_kor": "성능에 부정적인 영향을 미치지 않고 CNN 출력 계층의 차원성을 크게 줄일 수 있다는 사실을 포함하여 CNN 기반 표현의 몇 가지 유용한 속성을 식별한다.",
                    "tag": "1"
                },
                {
                    "index": "368-4",
                    "sentence": "We also identify aspects of deep and shallow methods that can be successfully shared.",
                    "sentence_kor": "우리는 또한 성공적으로 공유할 수 있는 깊고 얕은 방법의 측면을 파악한다.",
                    "tag": "1"
                },
                {
                    "index": "368-5",
                    "sentence": "In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost.",
                    "sentence_kor": "특히 CNN 기반 방법에 일반적으로 적용되는 데이터 증강 기법이 얕은 방법에도 적용될 수 있으며 유사한 성능 향상을 가져올 수 있음을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "368-6",
                    "sentence": "Source code and models to reproduce the experiments in the paper is made publicly available.",
                    "sentence_kor": "본 논문에서 실험을 재현하기 위한 소스 코드와 모델은 공개적으로 제공된다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "968",
            "abstractID": "SPA_abs-369",
            "text": [
                {
                    "index": "369-0",
                    "sentence": "We present an integrated framework for using Convolutional Networks for classification, localization and detection.",
                    "sentence_kor": "우리는 분류, 지역화 및 탐지에 컨볼루션 네트워크를 사용하기 위한 통합 프레임워크를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "369-1",
                    "sentence": "We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet.",
                    "sentence_kor": "우리는 ConvNet 내에서 멀티스케일 및 슬라이딩 윈도우 접근법이 어떻게 효율적으로 구현될 수 있는지 보여준다.",
                    "tag": "2"
                },
                {
                    "index": "369-2",
                    "sentence": "We also introduce a novel deep learning approach to localization by learning to predict object boundaries.",
                    "sentence_kor": "우리는 또한 객체 경계를 예측하는 방법을 배움으로써 현지화에 대한 새로운 딥 러닝 접근법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "369-3",
                    "sentence": "Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence.",
                    "sentence_kor": "그러면 탐지 신뢰도를 높이기 위해 경계 상자가 억제되지 않고 누적됩니다.",
                    "tag": "4"
                },
                {
                    "index": "369-4",
                    "sentence": "We show that different tasks can be learned simultaneously using a single shared network.",
                    "sentence_kor": "우리는 단일 공유 네트워크를 사용하여 서로 다른 작업을 동시에 학습할 수 있음을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "369-5",
                    "sentence": "This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks.",
                    "sentence_kor": "이 통합 프레임워크는 ImageNet Large Scale Visual Recognition Challenge 2013(ILSVRC2013)의 현지화 작업의 승자이며 탐지 및 분류 작업에 대해 매우 경쟁력 있는 결과를 얻었다.",
                    "tag": "4+5"
                },
                {
                    "index": "369-6",
                    "sentence": "In post-competition work, we establish a new state of the art for the detection task.",
                    "sentence_kor": "경기 후 작업에서, 우리는 탐지 작업에 대한 새로운 최첨단 기술을 확립한다.",
                    "tag": "5"
                },
                {
                    "index": "369-7",
                    "sentence": "Finally, we release a feature extractor from our best model called OverFeat.",
                    "sentence_kor": "마지막으로, 우리는 OverFeat이라는 최고 모델에서 기능 추출기를 출시한다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "969",
            "abstractID": "SPA_abs-370",
            "text": [
                {
                    "index": "370-0",
                    "sentence": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout.",
                    "sentence_kor": "우리는 드롭아웃이라고 하는 최근 도입된 근사 모델 평균화 기법을 활용하기 위해 모델을 설계하는 문제를 고려한다.",
                    "tag": "2+3"
                },
                {
                    "index": "370-1",
                    "sentence": "We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique.",
                    "sentence_kor": "우리는 드롭아웃에 의한 최적화를 촉진하고 드롭아웃의 빠른 근사 모델 평균화 기법의 정확도를 향상시키도록 설계된 maxout(출력이 입력 세트의 최대값이고 드롭아웃의 자연스러운 동반자이기 때문에 명명됨)이라는 간단한 새 모델을 정의한다.",
                    "tag": "2+3"
                },
                {
                    "index": "370-2",
                    "sentence": "We empirically verify that the model successfully accomplishes both of these tasks.",
                    "sentence_kor": "우리는 모델이 이 두 가지 작업을 성공적으로 수행하는지 경험적으로 검증한다.",
                    "tag": "3"
                },
                {
                    "index": "370-3",
                    "sentence": "We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.",
                    "sentence_kor": "우리는 MNIST, CIFAR-10, CIFAR-100 및 SVHN의 네 가지 벤치마크 데이터 세트에서 최신 분류 성능을 입증하기 위해 maxout 및 dropout을 사용한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "970",
            "abstractID": "SPA_abs-371",
            "text": [
                {
                    "index": "371-0",
                    "sentence": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field.",
                    "sentence_kor": "수용 분야 내의 로컬 패치에 대한 모델 차별성을 강화하기 위해 \"네트워크 내 네트워크\"(NIN)라는 새로운 심층 네트워크 구조를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "371-1",
                    "sentence": "The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input.",
                    "sentence_kor": "기존의 컨볼루션 레이어는 선형 필터를 사용한 다음 비선형 활성화 함수를 사용하여 입력을 스캔한다.",
                    "tag": "3"
                },
                {
                    "index": "371-2",
                    "sentence": "Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field.",
                    "sentence_kor": "대신, 우리는 수용 영역 내의 데이터를 추상화하기 위해 더 복잡한 구조를 가진 미세 신경망을 구축한다.",
                    "tag": "2+3"
                },
                {
                    "index": "371-3",
                    "sentence": "We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator.",
                    "sentence_kor": "우리는 다층 퍼셉트론으로 마이크로 신경망을 인스턴스화하는데, 이것은 강력한 함수 근사치이다.",
                    "tag": "4"
                },
                {
                    "index": "371-4",
                    "sentence": "The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer.",
                    "sentence_kor": "특징 맵은 CNN과 유사한 방식으로 마이크로 네트워크를 입력 위로 미끄러뜨려 얻은 후 다음 레이어에 공급된다.",
                    "tag": "3"
                },
                {
                    "index": "371-5",
                    "sentence": "Deep NIN can be implemented by stacking mutiple of the above described structure.",
                    "sentence_kor": "Deep NIN은 위에서 설명한 구조의 다중을 쌓음으로써 구현될 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "371-6",
                    "sentence": "With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers.",
                    "sentence_kor": "마이크로 네트워크를 통한 향상된 로컬 모델링을 통해 분류 계층의 형상 맵에 대한 글로벌 평균 풀링을 활용할 수 있으며, 이는 해석하기 쉽고 기존의 완전 연결 계층보다 과적합 가능성이 적다.",
                    "tag": "4"
                },
                {
                    "index": "371-7",
                    "sentence": "We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.",
                    "sentence_kor": "CIFAR-10 및 CIFAR-100에서 NIN을 사용한 최첨단 분류 성능과 SVHN 및 MNIST 데이터 세트에서 합리적인 성능을 시연했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "971",
            "abstractID": "SPA_abs-372",
            "text": [
                {
                    "index": "372-0",
                    "sentence": "We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework.",
                    "sentence_kor": "우리는 생성적 적대 네트워크(GAN) 프레임워크에 적용하는 다양한 새로운 아키텍처 기능과 훈련 절차를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "372-1",
                    "sentence": "We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic.",
                    "sentence_kor": "우리는 GAN의 두 가지 응용 프로그램인 준지도 학습과 인간이 시각적으로 현실적이라고 생각하는 이미지 생성에 초점을 맞춘다.",
                    "tag": "2"
                },
                {
                    "index": "372-2",
                    "sentence": "Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels.",
                    "sentence_kor": "생성 모델에 대한 대부분의 작업과 달리, 우리의 주된 목표는 테스트 데이터에 높은 가능성을 할당하는 모델을 교육하는 것이 아니며, 모델이 레이블을 사용하지 않고도 잘 학습할 수 있도록 요구하지 않는다.",
                    "tag": "2"
                },
                {
                    "index": "372-3",
                    "sentence": "Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN.",
                    "sentence_kor": "새로운 기술을 사용하여 MNIST, CIFAR-10 및 SVHN에 대한 준감독 분류에서 최첨단 결과를 얻는다.",
                    "tag": "4"
                },
                {
                    "index": "372-4",
                    "sentence": "The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%.",
                    "sentence_kor": "생성된 이미지는 시각적 튜링 테스트에 의해 확인된 고품질이다. 우리 모델은 인간이 실제 데이터와 구별할 수 없는 MNIST 샘플과 21.3%의 인간 오류율을 산출하는 CIFAR-10 샘플을 생성한다.",
                    "tag": "4+5"
                },
                {
                    "index": "372-5",
                    "sentence": "We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.",
                    "sentence_kor": "우리는 또한 전례 없는 해상도의 ImageNet 샘플을 제시하고 우리의 방법을 통해 모델이 ImageNet 클래스의 인식 가능한 기능을 학습할 수 있음을 보여준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "972",
            "abstractID": "SPA_abs-373",
            "text": [
                {
                    "index": "373-0",
                    "sentence": "In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications.",
                    "sentence_kor": "최근 몇 년 동안, 컨볼루션 네트워크(CNN)를 사용한 감독 학습은 컴퓨터 비전 애플리케이션에 크게 채택되었다.",
                    "tag": "1"
                },
                {
                    "index": "373-1",
                    "sentence": "Comparatively, unsupervised learning with CNNs has received less attention.",
                    "sentence_kor": "상대적으로 CNN을 이용한 무감독 학습은 주목을 덜 받았다.",
                    "tag": "1"
                },
                {
                    "index": "373-2",
                    "sentence": "In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning.",
                    "sentence_kor": "이 연구에서 우리는 지도 학습에 대한 CNN의 성공과 비지도 학습 사이의 격차를 해소하는 데 도움이 되기를 바란다.",
                    "tag": "1"
                },
                {
                    "index": "373-3",
                    "sentence": "We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.",
                    "sentence_kor": "우리는 특정 구조적 제약이 있는 심층 컨볼루션 생성적 적대 네트워크(DCGANs)라는 CNN 클래스를 소개하고 이들이 비지도 학습의 강력한 후보임을 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "373-4",
                    "sentence": "Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator.",
                    "sentence_kor": "다양한 이미지 데이터 세트에 대한 교육을 통해 심층 컨볼루션 대립 쌍이 생성기와 판별기 모두에서 객체 부품에서 장면으로 표현 계층을 학습한다는 설득력 있는 증거를 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "373-5",
                    "sentence": "Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",
                    "sentence_kor": "또한 학습된 기능을 새로운 작업에 사용하여 일반적인 이미지 표현으로 적용 가능성을 입증한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "973",
            "abstractID": "SPA_abs-374",
            "text": [
                {
                    "index": "374-0",
                    "sentence": "This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation.",
                    "sentence_kor": "이 논문은 이미지 생성을 위한 심층 반복 주의 기록기(DRAW) 신경 네트워크 아키텍처를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "374-1",
                    "sentence": "DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images.",
                    "sentence_kor": "DRAW 네트워크는 복잡한 이미지를 반복적으로 구성할 수 있는 순차적 변형 자동 인코딩 프레임워크와 인간의 눈 모양을 모방하는 새로운 공간 주의 메커니즘을 결합한다.",
                    "tag": "4"
                },
                {
                    "index": "374-2",
                    "sentence": "The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.",
                    "sentence_kor": "이 시스템은 MNIST의 생성 모델에 대한 최신 기술을 크게 향상시키며, 스트리트 뷰 하우스 번호 데이터 세트에서 훈련하면 육안으로 실제 데이터와 구별할 수 없는 이미지를 생성한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "974",
            "abstractID": "SPA_abs-375",
            "text": [
                {
                    "index": "375-0",
                    "sentence": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. ",
                    "sentence_kor": "우리는 적대적인 과정을 통해 생성 모델을 추정하기 위한 새로운 프레임워크를 제안한다. 우리는 두 가지 모델, 즉 데이터 분포를 포착하는 생성 모델 G와 샘플이 G가 아닌 훈련 데이터에서 나왔을 확률을 추정하는 차별 모델 D를 동시에 훈련한다.",
                    "tag": "2+3"
                },
                {
                    "index": "375-1",
                    "sentence": "The training procedure for G is to maximize the probability of D making a mistake.",
                    "sentence_kor": "G에 대한 훈련 절차는 D가 실수할 확률을 최대화하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "375-2",
                    "sentence": "This framework corresponds to a minimax two-player game.",
                    "sentence_kor": "이 프레임워크는 미니맥스 2인용 게임에 해당한다.",
                    "tag": "4"
                },
                {
                    "index": "375-3",
                    "sentence": "In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere.",
                    "sentence_kor": "임의의 함수 G와 D의 공간에는 G가 훈련 데이터 분포를 복구하고 D는 모든 곳에 1/2가 되는 고유한 솔루션이 존재한다.",
                    "tag": "4"
                },
                {
                    "index": "375-4",
                    "sentence": "In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation.",
                    "sentence_kor": "G와 D가 다층 퍼셉트론으로 정의되는 경우, 역전파를 통해 시스템 전체를 훈련시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "375-5",
                    "sentence": "There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.",
                    "sentence_kor": "샘플 훈련 또는 생성 중에 마르코프 체인이나 언롤된 근사 추론 네트워크가 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "375-6",
                    "sentence": "Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.",
                    "sentence_kor": "실험은 생성된 샘플의 정성적 및 정량적 평가를 통해 프레임워크의 잠재력을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "975",
            "abstractID": "SPA_abs-376",
            "text": [
                {
                    "index": "376-0",
                    "sentence": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data.",
                    "sentence_kor": "라벨이 부착되지 않은 데이터에서만 높은 수준의 클래스별 형상 감지기를 구축하는 문제를 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "376-1",
                    "sentence": "For example, is it possible to learn a face detector using only unlabeled images?",
                    "sentence_kor": "예를 들어 라벨이 부착되지 않은 영상만 사용하여 얼굴 감지기를 학습할 수 있습니까?",
                    "tag": "1"
                },
                {
                    "index": "376-2",
                    "sentence": "To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet).",
                    "sentence_kor": "이에 답하기 위해 대규모 이미지 데이터 세트에서 풀링 및 로컬 대비 정규화를 통해 로컬에 연결된 9계층 희소 자동 인코더를 교육한다(모델에는 10억 개의 연결이 있으며, 데이터 세트에는 인터넷에서 다운로드한 1,000만 개의 200x200 픽셀 이미지가 있다).",
                    "tag": "2+3"
                },
                {
                    "index": "376-3",
                    "sentence": "We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days.",
                    "sentence_kor": "1,000대의 머신(16,000 코어)이 있는 클러스터에서 모델 병렬 처리 및 비동기 SGD를 사용하여 3일 동안 이 네트워크를 교육한다.",
                    "tag": "2+3"
                },
                {
                    "index": "376-4",
                    "sentence": "Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not.",
                    "sentence_kor": "널리 알려진 직관과는 달리, 우리의 실험 결과는 이미지를 얼굴이 포함되거나 포함되지 않은 것으로 라벨을 붙일 필요 없이 얼굴 감지기를 훈련시키는 것이 가능하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "376-5",
                    "sentence": "Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.",
                    "sentence_kor": "제어 실험에 따르면 이 형상 검출기는 변환뿐만 아니라 스케일링 및 평면 외 회전에도 강하다.",
                    "tag": "4"
                },
                {
                    "index": "376-6",
                    "sentence": "We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies.",
                    "sentence_kor": "우리는 또한 동일한 네트워크가 고양이 얼굴과 인체와 같은 다른 높은 수준의 개념에 민감하다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "376-7",
                    "sentence": "Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.",
                    "sentence_kor": "이러한 학습된 기능을 시작으로, 우리는 ImageNet에서 2만 개의 객체 범주를 인식하는 데 15.8%의 정확도를 얻도록 네트워크를 훈련시켰는데, 이는 이전의 최첨단 기술에 비해 70%의 상대적 개선이다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "976",
            "abstractID": "SPA_abs-377",
            "text": [
                {
                    "index": "377-0",
                    "sentence": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success.",
                    "sentence_kor": "이론적 및 경험적 증거는 신경망의 깊이가 성공에 결정적이라는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "377-1",
                    "sentence": "However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem.",
                    "sentence_kor": "그러나 깊이가 증가함에 따라 훈련이 더욱 어려워지고 매우 심층적인 네트워크의 훈련은 여전히 미해결 문제로 남아 있다.",
                    "tag": "1"
                },
                {
                    "index": "377-2",
                    "sentence": "Here we introduce a new architecture designed to overcome this.",
                    "sentence_kor": "여기서는 이를 극복하기 위해 설계된 새로운 아키텍처를 소개합니다.",
                    "tag": "1"
                },
                {
                    "index": "377-3",
                    "sentence": "Our so-called highway networks allow unimpeded information flow across many layers on information highways.",
                    "sentence_kor": "우리의 소위 고속도로 네트워크는 정보 고속도로의 많은 층에 걸쳐 방해받지 않는 정보 흐름을 가능하게 한다.",
                    "tag": "1"
                },
                {
                    "index": "377-4",
                    "sentence": "They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow.",
                    "sentence_kor": "그들은 장기 단기 기억 반복 네트워크에서 영감을 얻었고 정보 흐름을 조절하기 위해 적응형 게이트 장치를 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "377-5",
                    "sentence": "Even with hundreds of layers, highway networks can be trained directly through simple gradient descent.",
                    "sentence_kor": "수백 개의 층이 있더라도 고속도로 네트워크는 간단한 경사 하강을 통해 직접 훈련될 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "377-6",
                    "sentence": "This enables the study of extremely deep and efficient architectures.",
                    "sentence_kor": "이를 통해 매우 깊고 효율적인 아키텍처를 연구할 수 있습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "977",
            "abstractID": "SPA_abs-378",
            "text": [
                {
                    "index": "378-0",
                    "sentence": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.",
                    "sentence_kor": "심층 신경망의 훈련은 이전 계층의 매개변수가 변경됨에 따라 훈련 중에 각 계층의 입력 분포가 변경된다는 사실로 인해 복잡하다.",
                    "tag": "1"
                },
                {
                    "index": "378-1",
                    "sentence": "This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.",
                    "sentence_kor": "이는 낮은 학습률과 신중한 매개변수 초기화를 요구하여 훈련을 느리게 하며, 포화 비선형성으로 모델을 훈련하는 것이 어렵기로 악명 높다.",
                    "tag": "1"
                },
                {
                    "index": "378-2",
                    "sentence": "We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.",
                    "sentence_kor": "우리는 이 현상을 내부 공변량 이동이라고 하며 계층 입력을 정규화하여 문제를 해결한다.",
                    "tag": "2"
                },
                {
                    "index": "378-3",
                    "sentence": "Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch.",
                    "sentence_kor": "우리의 방법은 정규화를 모델 아키텍처의 일부로 만들고 각 훈련 미니 배치에 대한 정규화를 수행하는 것에서 강점을 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "378-4",
                    "sentence": "Batch Normalization allows us to use much higher learning rates and be less careful about initialization.",
                    "sentence_kor": "배치 정규화를 사용하면 훨씬 더 높은 학습 속도를 사용하고 초기화에 덜 주의를 기울일 수 있습니다.",
                    "tag": "3+4"
                },
                {
                    "index": "378-5",
                    "sentence": "It also acts as a regularizer, in some cases eliminating the need for Dropout.",
                    "sentence_kor": "또한 정규화의 역할을 수행하며, 경우에 따라 중퇴할 필요가 없어집니다.",
                    "tag": "4"
                },
                {
                    "index": "378-6",
                    "sentence": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.",
                    "sentence_kor": "최첨단 이미지 분류 모델에 적용되는 배치 정규화는 14배 적은 교육 단계로 동일한 정확도를 달성하고 원래 모델을 상당한 차이로 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "378-7",
                    "sentence": "Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.",
                    "sentence_kor": "배치 정규화된 네트워크의 앙상블을 사용하여 ImageNet 분류에 대해 가장 잘 발표된 결과를 개선한다. 즉, 4.9%의 유효성 검사 오류(및 4.8%)에 도달하여 인간 측정기의 정확도를 초과한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "978",
            "abstractID": "SPA_abs-379",
            "text": [
                {
                    "index": "379-0",
                    "sentence": "Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.",
                    "sentence_kor": "정류된 활성화 장치(수정기)는 최첨단 신경망에 필수적이다.",
                    "tag": "1"
                },
                {
                    "index": "379-1",
                    "sentence": "In this work, we study rectifier neural networks for image classification from two aspects.",
                    "sentence_kor": "본 연구에서는 두 가지 측면에서 이미지 분류를 위한 정류기 신경망을 연구한다.",
                    "tag": "2"
                },
                {
                    "index": "379-2",
                    "sentence": "First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit.",
                    "sentence_kor": "먼저, 매개 변수 수정 선형 단위(PRE)를 제안한다.LU)는 기존의 정류 장치를 일반화합니다.",
                    "tag": "2"
                },
                {
                    "index": "379-3",
                    "sentence": "PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk.",
                    "sentence_kor": "PReLU는 추가 계산 비용이 거의 없고 과적합 위험이 거의 없어 모델 피팅을 개선한다.",
                    "tag": "4"
                },
                {
                    "index": "379-4",
                    "sentence": "Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities.",
                    "sentence_kor": "둘째, 정류기 비선형성을 특히 고려하는 강력한 초기화 방법을 도출한다.",
                    "tag": "4"
                },
                {
                    "index": "379-5",
                    "sentence": "This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures.",
                    "sentence_kor": "이 방법을 사용하면 매우 심층적인 수정 모델을 처음부터 직접 교육하고 더 깊거나 더 넓은 네트워크 아키텍처를 조사할 수 있습니다.",
                    "tag": "5"
                },
                {
                    "index": "379-6",
                    "sentence": "Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset.",
                    "sentence_kor": "PReLU 네트워크(PReLU-net)를 기반으로 ImageNet 2012 분류 데이터 세트에서 4.94%의 테스트 오류를 달성했다.",
                    "tag": "4"
                },
                {
                    "index": "379-7",
                    "sentence": "This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%).",
                    "sentence_kor": "이는 ILSVRC 2014 우승자(GoogLeNet, 6.66%)보다 26% 개선된 것이다.",
                    "tag": "4"
                },
                {
                    "index": "379-8",
                    "sentence": "To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.",
                    "sentence_kor": "우리가 아는 바로는, 우리의 결과는 이 시각적 인식 과제에서 인간 수준의 성능(5.1%, Rusakovsky 등)을 능가하는 첫 번째 결과이다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "979",
            "abstractID": "SPA_abs-380",
            "text": [
                {
                    "index": "380-0",
                    "sentence": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data.",
                    "sentence_kor": "대규모 피드포워드 신경망이 작은 훈련 세트에서 훈련되는 경우 일반적으로 보류된 테스트 데이터에서 성능이 떨어진다.",
                    "tag": "1"
                },
                {
                    "index": "380-1",
                    "sentence": "This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case.",
                    "sentence_kor": "이러한 \"과적합\"은 각 훈련 케이스에서 특징 검출기의 절반을 무작위로 생략함으로써 크게 감소한다.",
                    "tag": "1"
                },
                {
                    "index": "380-2",
                    "sentence": "This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors.",
                    "sentence_kor": "이렇게 하면 형상 검출기가 다른 여러 특정 형상 검출기의 컨텍스트에서만 유용한 복잡한 공동 적응을 방지할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "380-3",
                    "sentence": "Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate.",
                    "sentence_kor": "대신, 각각의 뉴런 일반적으로 내부 상황은 그것이 그것을 운용할 수 있는 결합에 관하여. 다양한 주어진 정답을 생산하는 데 도움이 된다 형상을 감지하는 방법을 배운다.",
                    "tag": "1"
                },
                {
                    "index": "380-4",
                    "sentence": "Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.",
                    "sentence_kor": "무작위 \"중퇴\"는 많은 벤치마크 작업을 크게 개선하고 음성 및 객체 인식에 대한 새로운 기록을 수립합니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "980",
            "abstractID": "SPA_abs-381",
            "text": [
                {
                    "index": "381-0",
                    "sentence": "Training state-of-the-art, deep neural networks is computationally expensive.",
                    "sentence_kor": "최첨단 심층 신경망을 훈련시키는 데는 계산 비용이 많이 든다.",
                    "tag": "1"
                },
                {
                    "index": "381-1",
                    "sentence": "One way to reduce the training time is to normalize the activities of the neurons.",
                    "sentence_kor": "훈련 시간을 줄이는 한 가지 방법은 뉴런의 활동을 정상화하는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "381-2",
                    "sentence": "A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.",
                    "sentence_kor": "최근에 도입된 배치 정규화라고 불리는 기술은 훈련 사례의 미니 배치에 걸쳐 뉴런에 대한 합산 입력의 분포를 사용하여 평균과 분산을 계산하며, 이 평균은 각 훈련 사례의 해당 뉴런에 대한 합산 입력을 정규화하는 데 사용된다.",
                    "tag": "1"
                },
                {
                    "index": "381-3",
                    "sentence": "This significantly reduces the training time in feed-forward neural networks.",
                    "sentence_kor": "이는 피드포워드 신경망의 훈련 시간을 크게 단축시킨다.",
                    "tag": "1"
                },
                {
                    "index": "381-4",
                    "sentence": "However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks.",
                    "sentence_kor": "그러나 배치 정규화의 효과는 미니 배치 크기에 따라 달라지며 이를 반복 신경망에 적용하는 방법은 명확하지 않다.",
                    "tag": "1"
                },
                {
                    "index": "381-5",
                    "sentence": "In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.",
                    "sentence_kor": "본 논문에서, 우리는 단일 훈련 사례의 레이어에서 뉴런에 대한 모든 합산 입력으로부터 정규화에 사용되는 평균과 분산을 계산하여 배치 정규화를 계층 정규화로 전환한다.",
                    "tag": "2+3"
                },
                {
                    "index": "381-6",
                    "sentence": "Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity.",
                    "sentence_kor": "배치 정규화와 마찬가지로, 우리는 또한 각 뉴런에 정규화 후에 비선형성 전에 적용되는 자체 적응 편향과 이득을 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "381-7",
                    "sentence": "Unlike batch normalization, layer normalization performs exactly the same computation at training and test times.",
                    "sentence_kor": "배치 정규화와 달리 계층 정규화는 훈련 및 테스트 시간에 정확히 동일한 계산을 수행합니다.",
                    "tag": "3"
                },
                {
                    "index": "381-8",
                    "sentence": "It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step.",
                    "sentence_kor": "또한 각 시간 단계에서 정규화 통계를 별도로 계산하여 반복 신경망에 적용하는 것도 간단하다.",
                    "tag": "3"
                },
                {
                    "index": "381-9",
                    "sentence": "Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.",
                    "sentence_kor": "계층 정규화는 반복 네트워크에서 은닉 상태 역학을 안정화시키는 데 매우 효과적이다.",
                    "tag": "5"
                },
                {
                    "index": "381-10",
                    "sentence": "Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.",
                    "sentence_kor": "경험적으로, 우리는 계층 정규화가 이전에 발표된 기법에 비해 훈련 시간을 상당히 줄일 수 있다는 것을 보여준다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "981",
            "abstractID": "SPA_abs-382",
            "text": [
                {
                    "index": "382-0",
                    "sentence": "The move from hand-designed features to learned features in machine learning has been wildly successful.",
                    "sentence_kor": "수작업으로 설계된 기능에서 기계 학습에서 학습된 기능으로 전환은 크게 성공적이었다.",
                    "tag": "1"
                },
                {
                    "index": "382-1",
                    "sentence": "In spite of this, optimization algorithms are still designed by hand.",
                    "sentence_kor": "그럼에도 불구하고 최적화 알고리즘은 여전히 손으로 설계됩니다.",
                    "tag": "1"
                },
                {
                    "index": "382-2",
                    "sentence": "In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way.",
                    "sentence_kor": "이 논문에서 우리는 최적화 알고리즘의 설계가 학습 문제로 캐스팅될 수 있는 방법을 보여줌으로써 알고리즘이 자동으로 관심 있는 문제의 구조를 활용하는 방법을 배울 수 있도록 한다.",
                    "tag": "1"
                },
                {
                    "index": "382-3",
                    "sentence": "Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure.",
                    "sentence_kor": "LSTM에 의해 구현된 학습된 알고리즘은 훈련 받은 작업에서 일반적이고 손으로 설계된 경쟁사보다 성능이 우수하며 유사한 구조의 새로운 작업에 잘 일반화된다.",
                    "tag": "1"
                },
                {
                    "index": "382-4",
                    "sentence": "We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.",
                    "sentence_kor": "간단한 볼록 문제, 신경 네트워크 훈련 및 신경 예술로 이미지 스타일링을 포함한 여러 작업에서 이를 입증한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "982",
            "abstractID": "SPA_abs-383",
            "text": [
                {
                    "index": "383-0",
                    "sentence": "Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result.",
                    "sentence_kor": "결과의 사실성을 유지하면서 사용자가 제어하는 방식으로 이미지 외관을 수정해야 하기 때문에 현실적인 이미지 조작은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "383-1",
                    "sentence": "Unless the user has considerable artistic skill, it is easy to \"fall off\" the manifold of natural images while editing.",
                    "sentence_kor": "사용자가 상당한 예술적 기술을 가지고 있지 않다면, 편집하는 동안 다양한 자연 이미지에서 떨어져 나가기 쉽다.",
                    "tag": "1"
                },
                {
                    "index": "383-2",
                    "sentence": "In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network.",
                    "sentence_kor": "본 논문에서는 생성적 적대 신경망을 사용하여 데이터에서 직접 자연 이미지 매니폴드를 학습할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "383-3",
                    "sentence": "We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times.",
                    "sentence_kor": "그런 다음 이미지 편집 작업의 클래스를 정의하고 해당 출력이 항상 학습된 다양체에 놓이도록 제한한다.",
                    "tag": "2"
                },
                {
                    "index": "383-4",
                    "sentence": "The model automatically adjusts the output keeping all edits as realistic as possible.",
                    "sentence_kor": "모델은 모든 편집 내용을 가능한 한 사실적으로 유지하면서 출력을 자동으로 조정합니다.",
                    "tag": "3"
                },
                {
                    "index": "383-5",
                    "sentence": "All our manipulations are expressed in terms of constrained optimization and are applied in near-real time.",
                    "sentence_kor": "우리의 모든 조작은 제한된 최적화의 관점에서 표현되며 거의 실시간으로 적용된다.",
                    "tag": "4"
                },
                {
                    "index": "383-6",
                    "sentence": "We evaluate our algorithm on the task of realistic photo manipulation of shape and color.",
                    "sentence_kor": "우리는 모양과 색상의 현실적인 사진 조작 작업에 대한 우리의 알고리즘을 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "383-7",
                    "sentence": "The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.",
                    "sentence_kor": "제시된 방법은 한 이미지를 다른 이미지처럼 변경하고 사용자의 낙서를 기반으로 처음부터 새로운 이미지를 생성하는 데 사용할 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "983",
            "abstractID": "SPA_abs-384",
            "text": [
                {
                    "index": "384-0",
                    "sentence": "Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example.",
                    "sentence_kor": "Gatys 등은 최근 딥 네트워크가 하나의 텍스처 예에서 아름다운 텍스처와 정형화된 이미지를 생성할 수 있음을 입증했다.",
                    "tag": "1"
                },
                {
                    "index": "384-1",
                    "sentence": "However, their methods requires a slow and memory-consuming optimization process.",
                    "sentence_kor": "그러나 이들의 방법은 느리고 메모리를 많이 소모하는 최적화 프로세스가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "384-2",
                    "sentence": "We propose here an alternative approach that moves the computational burden to a learning stage.",
                    "sentence_kor": "여기서 우리는 계산 부담을 학습 단계로 옮기는 대체 접근법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "384-3",
                    "sentence": "Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image.",
                    "sentence_kor": "텍스처의 단일 예를 들어, 우리의 접근 방식은 임의의 크기의 동일한 텍스처의 여러 샘플을 생성하고 주어진 이미지에서 다른 이미지로 예술적 스타일을 전송하기 위해 콤팩트 피드 포워드 컨볼루션 네트워크를 훈련시킨다.",
                    "tag": "2+3"
                },
                {
                    "index": "384-4",
                    "sentence": "The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster.",
                    "sentence_kor": "그 결과 네트워크는 놀라울 정도로 가볍고 Gatys~et~al에 버금가는 품질의 질감을 생성할 수 있지만 수백 배 더 빠릅니다.",
                    "tag": "4"
                },
                {
                    "index": "384-5",
                    "sentence": "More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.",
                    "sentence_kor": "보다 일반적으로, 우리의 접근 방식은 복잡하고 표현적인 손실 기능으로 훈련된 생성 피드 포워드 모델의 전력과 유연성을 강조한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "984",
            "abstractID": "SPA_abs-385",
            "text": [
                {
                    "index": "385-0",
                    "sentence": "Recent research on deep neural networks has focused primarily on improving accuracy.",
                    "sentence_kor": "심층 신경망에 대한 최근 연구는 주로 정확도 향상에 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "385-1",
                    "sentence": "For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level.",
                    "sentence_kor": "주어진 정확도 수준의 경우 일반적으로 해당 정확도 수준을 달성하는 여러 DNN 아키텍처를 식별할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "385-2",
                    "sentence": "With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training.",
                    "sentence_kor": "동일한 정확도로, 소형 DNN 아키텍처는 적어도 세 가지 이점을 제공한다. (1) 소형 DNN은 분산 훈련 중에 서버 간 통신을 덜 필요로 한다.",
                    "tag": "1"
                },
                {
                    "index": "385-3",
                    "sentence": "(2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car.",
                    "sentence_kor": "(2) 소형 DNN은 클라우드에서 자율 자동차로 새 모델을 수출하는 데 필요한 대역폭이 적습니다.",
                    "tag": "1"
                },
                {
                    "index": "385-4",
                    "sentence": "(3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory.",
                    "sentence_kor": "(3) 소형 DNN은 메모리가 제한된 FPGA 및 기타 하드웨어에 배치할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "385-5",
                    "sentence": "To provide all of these advantages, we propose a small DNN architecture called SqueezeNet.",
                    "sentence_kor": "이러한 모든 이점을 제공하기 위해 스퀴즈넷이라는 작은 DNN 아키텍처를 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "385-6",
                    "sentence": "SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.",
                    "sentence_kor": "스퀴즈넷은 파라미터가 50배 적은 ImageNet에서 AlexNet 수준의 정확도를 달성합니다.",
                    "tag": "1"
                },
                {
                    "index": "385-7",
                    "sentence": "Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).",
                    "sentence_kor": "또한 모델 압축 기술을 사용하여 스퀴즈넷을 0.5 미만으로 압축할 수 있습니다.MB(AlexNet보다 510배 작습니다.",
                    "tag": "1"
                },
                {
                    "index": "385-8",
                    "sentence": "The SqueezeNet architecture is available for download here: this https URL",
                    "sentence_kor": "스퀴즈넷 아키텍처는 다음 https URL에서 다운로드할 수 있습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "985",
            "abstractID": "SPA_abs-386",
            "text": [
                {
                    "index": "386-0",
                    "sentence": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets.",
                    "sentence_kor": "최첨단 심층 신경 네트워크(DNN)는 수억 개의 연결을 가지고 있으며 계산 및 메모리 집약적이어서 하드웨어 리소스와 전력 예산이 제한된 임베디드 시스템에 배포하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "386-1",
                    "sentence": "While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.",
                    "sentence_kor": "사용자 지정 하드웨어가 계산에 도움이 되지만, DRAM에서 가중치를 가져오는 것은 ALU 작업보다 두 배나 더 비싸고 필요한 전력을 지배한다.",
                    "tag": "1"
                },
                {
                    "index": "386-2",
                    "sentence": "Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM.",
                    "sentence_kor": "이전에 제안한 '딥 압축'은 대형 DNN(알렉스넷 및 VGGNet)을 온칩 SRAM에 완전히 장착할 수 있게 해준다.",
                    "tag": "1"
                },
                {
                    "index": "386-3",
                    "sentence": "This compression is achieved by pruning the redundant connections and having multiple connections share the same weight.",
                    "sentence_kor": "이 압축은 중복 연결을 정리하고 여러 연결부가 동일한 중량을 공유하도록 함으로써 달성된다.",
                    "tag": "1"
                },
                {
                    "index": "386-4",
                    "sentence": "We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing.",
                    "sentence_kor": "우리는 이 압축 네트워크 모델에 대해 추론을 수행하고 가중치 공유를 통해 발생하는 희박한 매트릭스 벡터 곱셈을 가속화하는 에너지 효율적인 추론 엔진(EIE)을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "386-5",
                    "sentence": "Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.",
                    "sentence_kor": "DRAM에서 SRAM으로 이동하면 EIE 120배 에너지 절약, 희소성 활용 시 10배 절약, 중량 공유 시 8배 절약, ReLU에서 제로 활성화를 건너뛰면 3배 절약됩니다.",
                    "tag": "1"
                },
                {
                    "index": "386-6",
                    "sentence": "Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression.",
                    "sentence_kor": "9개의 DNN 벤치마크에서 평가된 EIE는 압축이 없는 동일한 DNN의 CPU 및 GPU 구현에 비해 189배, 13배 빠르다.",
                    "tag": "1"
                },
                {
                    "index": "386-7",
                    "sentence": "EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW.",
                    "sentence_kor": "EIE는 압축된 네트워크에서 직접 작동하는 102GOPS/s의 처리 능력을 가지고 있으며, 압축되지 않은 네트워크의 3TOPS/s에 해당하며, 600mW의 전력 소산만으로 1.88x10^4 프레임/s로 AlexNet의 FC 계층을 처리합니다.",
                    "tag": "1"
                },
                {
                    "index": "386-8",
                    "sentence": "It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively.",
                    "sentence_kor": "CPU와 GPU보다 각각 2만4000배, 3400배 높은 에너지 효율이다.",
                    "tag": "1"
                },
                {
                    "index": "386-9",
                    "sentence": "Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.",
                    "sentence_kor": "DaDianNao에 비해 EIE는 처리량, 에너지 효율성 및 면적 효율성이 2.9배, 19배, 3배 더 높습니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "986",
            "abstractID": "SPA_abs-387",
            "text": [
                {
                    "index": "387-0",
                    "sentence": "We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time.",
                    "sentence_kor": "런타임에 이진 가중치와 활성화가 있는 신경 네트워크인 이진화된 신경망을 훈련시키는 방법을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "387-1",
                    "sentence": "At training-time the binary weights and activations are used for computing the parameters gradients.",
                    "sentence_kor": "훈련 시 이항 가중치와 활성화는 매개변수 구배 계산에 사용된다.",
                    "tag": "3"
                },
                {
                    "index": "387-2",
                    "sentence": "During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency.",
                    "sentence_kor": "전진 패스 동안 BNN은 메모리 크기와 액세스를 대폭 줄이고 대부분의 산술 연산을 비트 단위 연산으로 대체하므로 전력 효율성이 크게 향상될 것으로 예상된다.",
                    "tag": "3"
                },
                {
                    "index": "387-3",
                    "sentence": "To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks.",
                    "sentence_kor": "BNN의 효과를 검증하기 위해 Torch7과 Theano 프레임워크에 대해 두 세트의 실험을 수행한다.",
                    "tag": "3"
                },
                {
                    "index": "387-4",
                    "sentence": "On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets.",
                    "sentence_kor": "두 가지 모두에서 BNN은 MNIST, CIFAR-10 및 SVHN 데이터 세트에 대해 거의 최첨단 결과를 달성했다.",
                    "tag": "4"
                },
                {
                    "index": "387-5",
                    "sentence": "Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy.",
                    "sentence_kor": "마지막으로 중요한 것은, 분류 정확도의 손실 없이 최적화되지 않은 GPU 커널보다 MNIST BNN을 7배 더 빠르게 실행할 수 있는 이진 행렬 곱셈 GPU 커널을 작성했다.",
                    "tag": "4"
                },
                {
                    "index": "387-6",
                    "sentence": "The code for training and running our BNNs is available on-line.",
                    "sentence_kor": "BNN 교육 및 실행을 위한 코드를 온라인으로 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "987",
            "abstractID": "SPA_abs-388",
            "text": [
                {
                    "index": "388-0",
                    "sentence": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering.",
                    "sentence_kor": "메모리 및 주의 메커니즘이 있는 신경 네트워크 아키텍처는 질문 답변에 필요한 특정 추론 기능을 나타낸다.",
                    "tag": "1"
                },
                {
                    "index": "388-1",
                    "sentence": "One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks.",
                    "sentence_kor": "그러한 아키텍처 중 하나인 동적 메모리 네트워크(DMN)는 다양한 언어 작업에서 높은 정확도를 얻었다.",
                    "tag": "1"
                },
                {
                    "index": "388-2",
                    "sentence": "However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images.",
                    "sentence_kor": "그러나, 아키텍처가 훈련 중에 뒷받침 사실을 표시하지 않을 때 질문 답변에 대해 강력한 결과를 달성하는지 또는 이미지와 같은 다른 양식에 적용할 수 있는지 여부는 보여주지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "388-3",
                    "sentence": "Based on an analysis of the DMN, we propose several improvements to its memory and input modules.",
                    "sentence_kor": "DMN의 분석을 기반으로 메모리 및 입력 모듈에 대한 몇 가지 개선을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "388-4",
                    "sentence": "Together with these changes we introduce a novel input module for images in order to be able to answer visual questions.",
                    "sentence_kor": "이러한 변화와 함께 시각적 질문에 답할 수 있도록 이미지에 대한 새로운 입력 모듈을 소개한다.",
                    "tag": "1+2"
                },
                {
                    "index": "388-5",
                    "sentence": "Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.",
                    "sentence_kor": "우리의 새로운 DMN+ 모델은 사실 감독을 지원하지 않고 시각적 질문 응답 데이터 세트와 \\babi-10k 텍스트 질문 응답 데이터 세트 모두에서 최첨단 기술을 향상시킨다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "988",
            "abstractID": "SPA_abs-389",
            "text": [
                {
                    "index": "389-0",
                    "sentence": "This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.",
                    "sentence_kor": "본 논문은 이미지에서 자연어 질문에 답하는 방법을 배우는 누적 주의 네트워크(SAN)를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "389-1",
                    "sentence": "SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer.",
                    "sentence_kor": "SAN은 질문에 대한 의미론적 표현을 쿼리로 사용하여 이미지에서 답변과 관련된 영역을 검색합니다.",
                    "tag": "1"
                },
                {
                    "index": "389-2",
                    "sentence": "We argue that image question answering (QA) often requires multiple steps of reasoning.",
                    "sentence_kor": "우리는 이미지 질문 답변(QA)에는 종종 여러 단계의 추론이 필요하다고 주장한다.",
                    "tag": "2"
                },
                {
                    "index": "389-3",
                    "sentence": "Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively.",
                    "sentence_kor": "따라서, 우리는 답을 점진적으로 추론하기 위해 이미지를 여러 번 쿼리하는 다중 계층 SAN을 개발한다.",
                    "tag": "3"
                },
                {
                    "index": "389-4",
                    "sentence": "Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches.",
                    "sentence_kor": "4개의 이미지 QA 데이터 세트에 대해 수행된 실험은 제안된 SAN이 이전의 최첨단 접근 방식을 크게 능가한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "389-5",
                    "sentence": "The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.",
                    "sentence_kor": "주의 계층의 시각화는 SAN이 계층별 질문의 답변으로 이어지는 관련 시각적 단서를 찾는 과정을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "989",
            "abstractID": "SPA_abs-390",
            "text": [
                {
                    "index": "390-0",
                    "sentence": "Batch Normalization is quite effective at accelerating and improving the training of deep models.",
                    "sentence_kor": "배치 정규화는 심층 모델의 교육을 가속화하고 개선하는 데 매우 효과적입니다.",
                    "tag": "1"
                },
                {
                    "index": "390-1",
                    "sentence": "However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples.",
                    "sentence_kor": "그러나 훈련용 미니바치가 작거나 독립 표본으로 구성되지 않으면 그 효과는 감소한다.",
                    "tag": "1"
                },
                {
                    "index": "390-2",
                    "sentence": "We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference.",
                    "sentence_kor": "우리는 이것이 미니배치의 모든 예에 대한 모델 레이어 입력의 의존성과 훈련과 추론 사이에 생성되는 다른 활성화 때문이라고 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "390-3",
                    "sentence": "We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch.",
                    "sentence_kor": "우리는 훈련 및 추론 모델이 전체 미니배치가 아닌 개별 예제에 따라 동일한 출력을 생성하도록 보장하기 위해 간단하고 효과적인 확장인 배치 재규격화를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "390-4",
                    "sentence": "Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches.",
                    "sentence_kor": "배치 재규격화로 훈련된 모델은 소형 또는 비 i.i.d. 미니밴치로 훈련할 때 배트노름보다 성능이 훨씬 우수하다.",
                    "tag": "3+4"
                },
                {
                    "index": "390-5",
                    "sentence": "At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.",
                    "sentence_kor": "이와 동시에 배치 재규격화는 초기화에 대한 무감각과 교육 효율성과 같은 배트노름의 이점을 그대로 유지합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "990",
            "abstractID": "SPA_abs-391",
            "text": [
                {
                    "index": "391-0",
                    "sentence": "Scale variation is one of the key challenges in object detection.",
                    "sentence_kor": "규모 변화는 객체 감지 시 주요 과제 중 하나입니다.",
                    "tag": "1"
                },
                {
                    "index": "391-1",
                    "sentence": "In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection.",
                    "sentence_kor": "이 작업에서는 먼저 물체 감지에서 스케일 변화에 대한 수용 영역의 효과를 조사하기 위한 통제된 실험을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "391-2",
                    "sentence": "Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power.",
                    "sentence_kor": "탐색 실험의 결과를 바탕으로, 우리는 균일한 표현력을 가진 척도별 특징 맵을 생성하는 것을 목표로 하는 새로운 트라이던트 네트워크(TridentNet)를 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "391-3",
                    "sentence": "We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields.",
                    "sentence_kor": "우리는 각 지점이 동일한 변환 매개 변수를 공유하지만 수용 필드는 다른 병렬 다중 지점 아키텍처를 구성한다.",
                    "tag": "3"
                },
                {
                    "index": "391-4",
                    "sentence": "Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training.",
                    "sentence_kor": "그런 다음 교육을 위해 적절한 규모의 객체 인스턴스를 샘플링하여 각 분기를 전문화하는 스케일 인식 교육 계획을 채택한다.",
                    "tag": "2+3"
                },
                {
                    "index": "391-5",
                    "sentence": "As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector.",
                    "sentence_kor": "보너스로 트라이던트넷의 빠른 근사 버전은 바닐라 검출기에 비해 추가 파라미터 및 계산 비용 없이 상당한 개선을 달성할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "391-6",
                    "sentence": "On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP.",
                    "sentence_kor": "COCO 데이터 세트에서 ResNet-101 백본을 사용하는 TridentNet은 48.4 mAP의 최첨단 단일 모델 결과를 달성한다.",
                    "tag": "4"
                },
                {
                    "index": "391-7",
                    "sentence": "Codes are available at this https URL.",
                    "sentence_kor": "코드는 이 https URL에서 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "991",
            "abstractID": "SPA_abs-392",
            "text": [
                {
                    "index": "392-0",
                    "sentence": "Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.",
                    "sentence_kor": "생성적 적대 네트워크(GAN)를 사용한 비지도 학습은 크게 성공한 것으로 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "392-1",
                    "sentence": "Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.",
                    "sentence_kor": "정규 GAN은 판별자를 시그모이드 교차 엔트로피 손실 함수를 가진 분류자로 가정한다.",
                    "tag": "1"
                },
                {
                    "index": "392-2",
                    "sentence": "However, we found that this loss function may lead to the vanishing gradients problem during the learning process.",
                    "sentence_kor": "그러나 이 손실 함수는 학습 과정 중에 그레이디언트 소실 문제로 이어질 수 있다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "392-3",
                    "sentence": "To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.",
                    "sentence_kor": "이러한 문제를 극복하기 위해 본 논문에서 판별기에 최소 제곱 손실 함수를 채택하는 최소 제곱 생성 적대 네트워크(LSGAN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "392-4",
                    "sentence": "We show that minimizing the objective function of LSGAN yields minimizing the Pearson χ2 divergence.",
                    "sentence_kor": "LSGAN의 목적 함수를 최소화하면 피어슨 µ2의 분기가 최소화된다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "392-5",
                    "sentence": "There are two benefits of LSGANs over regular GANs.",
                    "sentence_kor": "LSGAN은 일반 GAN에 비해 두 가지 이점이 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "392-6",
                    "sentence": "First, LSGANs are able to generate higher quality images than regular GANs.",
                    "sentence_kor": "첫째, LSGAN은 일반 GAN보다 더 높은 품질의 이미지를 생성할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "392-7",
                    "sentence": "Second, LSGANs perform more stable during the learning process.",
                    "sentence_kor": "둘째, LSGAN은 학습 과정에서 더 안정적인 성능을 발휘합니다.",
                    "tag": "4"
                },
                {
                    "index": "392-8",
                    "sentence": "We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.",
                    "sentence_kor": "우리는 5개의 장면 데이터 세트에서 LSGAN을 평가하며, 실험 결과는 LSGAN에 의해 생성된 이미지가 일반 GAN에 의해 생성된 이미지보다 더 나은 품질을 가지고 있음을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "392-9",
                    "sentence": "We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.",
                    "sentence_kor": "또한 LSGAN의 안정성을 설명하기 위해 LSGAN과 일반 GAN 간의 두 가지 비교 실험을 수행한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "992",
            "abstractID": "SPA_abs-393",
            "text": [
                {
                    "index": "393-0",
                    "sentence": "The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation.",
                    "sentence_kor": "구문 기반이든 신경 기반이든 기존의 기계 번역 시스템은 거의 전적으로 명시적 세분화를 가진 단어 수준 모델링에 의존해 왔다.",
                    "tag": "1"
                },
                {
                    "index": "393-1",
                    "sentence": "In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation?",
                    "sentence_kor": "본 논문에서 우리는 기본적인 질문을 한다. 신경 기계 번역이 명확한 세분화 없이 문자 시퀀스를 생성할 수 있는가?",
                    "tag": "1"
                },
                {
                    "index": "393-2",
                    "sentence": "To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.",
                    "sentence_kor": "이 질문에 답하기 위해 WMT'15의 병렬 말뭉치를 사용하여 하위 워드 레벨 인코더와 4개 언어 쌍(En-C, En-De, En-Ru 및 En-Fi)의 문자 레벨 디코더를 사용하여 주의 기반 인코더-디코더를 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "393-3",
                    "sentence": "Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs.",
                    "sentence_kor": "우리의 실험은 문자 수준 디코더가 있는 모델이 네 가지 언어 쌍 모두에서 하위 워드 수준 디코더가 있는 모델보다 성능이 우수하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "393-4",
                    "sentence": "Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.",
                    "sentence_kor": "또한 문자 수준 디코더를 가진 신경 모델의 앙상블은 En-C, En-De 및 En-Fi의 최첨단 비신경 기계 번역 시스템을 능가하고 En-Ru에서 유사한 성능을 발휘한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "993",
            "abstractID": "SPA_abs-394",
            "text": [
                {
                    "index": "394-0",
                    "sentence": "Object category localization is a challenging problem in computer vision.",
                    "sentence_kor": "객체 범주 위치 파악은 컴퓨터 비전에서 어려운 문제이다.",
                    "tag": "1"
                },
                {
                    "index": "394-1",
                    "sentence": "Standard supervised training requires bounding box annotations of object instances.",
                    "sentence_kor": "표준 지도 교육에는 객체 인스턴스의 경계 상자 주석이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "394-2",
                    "sentence": "This time-consuming annotation process is sidestepped in weakly supervised learning.",
                    "sentence_kor": "시간이 많이 걸리는 이 주석 프로세스는 약하게 지도되는 학습에서 제외된다.",
                    "tag": "1"
                },
                {
                    "index": "394-3",
                    "sentence": "In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations.",
                    "sentence_kor": "이 경우 감독 정보는 해당 위치가 없는 이미지에서 개체 인스턴스의 부재/존재를 나타내는 이진 레이블로 제한됩니다.",
                    "tag": "1"
                },
                {
                    "index": "394-4",
                    "sentence": "We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images.",
                    "sentence_kor": "우리는 검출기를 반복적으로 훈련시키고 양성 훈련 이미지에서 물체 위치를 유추하는 다중 인스턴스 학습 접근 방식을 따른다.",
                    "tag": "1"
                },
                {
                    "index": "394-5",
                    "sentence": "Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations.",
                    "sentence_kor": "우리의 주된 기여는 다중 다중 인스턴스 학습 절차로, 교육이 잘못된 객체 위치에 너무 일찍 고정되는 것을 방지한다.",
                    "tag": "2"
                },
                {
                    "index": "394-6",
                    "sentence": "This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features.",
                    "sentence_kor": "이 절차는 피셔 벡터 및 컨볼루션 신경망 특징과 같은 고차원 표현을 사용할 때 특히 중요하다.",
                    "tag": "3"
                },
                {
                    "index": "394-7",
                    "sentence": "We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior.",
                    "sentence_kor": "우리는 또한 객체성을 사전에 통합하여 현지화 정확도를 향상시키는 윈도우 개선 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "394-8",
                    "sentence": "We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach.",
                    "sentence_kor": "접근 방식의 효과를 검증하는 PASCAL VOC 2007 데이터 세트를 사용하여 자세한 실험 평가를 제시한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "994",
            "abstractID": "SPA_abs-395",
            "text": [
                {
                    "index": "395-0",
                    "sentence": "This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output.",
                    "sentence_kor": "이 논문은 반복 신경망이 입력 수신과 출력 방출 사이에서 몇 가지 계산 단계를 수행할 수 있도록 하는 알고리즘인 적응 계산 시간(ACT)을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "395-1",
                    "sentence": "ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients.",
                    "sentence_kor": "ACT는 네트워크 아키텍처에 최소한의 변경을 요구하며, 결정론적이고 차별화 가능하며, 매개변수 기울기에 노이즈를 추가하지 않는다.",
                    "tag": "4"
                },
                {
                    "index": "395-2",
                    "sentence": "Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers.",
                    "sentence_kor": "이항 벡터의 패리티 결정, 이항 논리 연산 적용, 정수 추가, 실수 정렬 등 네 가지 합성 문제에 대한 실험 결과가 제공된다.",
                    "tag": "3"
                },
                {
                    "index": "395-3",
                    "sentence": "Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem.",
                    "sentence_kor": "전반적으로 ACT를 사용하면 성능이 크게 향상되어 계산 단계 수를 문제 요구사항에 성공적으로 적응시킨다.",
                    "tag": "4"
                },
                {
                    "index": "395-4",
                    "sentence": "We also present character-level language modelling results on the Hutter prize Wikipedia dataset.",
                    "sentence_kor": "또한 Hutter Prize Wikipedia 데이터 세트에 대한 문자 수준 언어 모델링 결과를 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "395-5",
                    "sentence": "In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences.",
                    "sentence_kor": "이 경우 ACT는 성능을 크게 향상시키지는 않지만, 단어와 문장 끝 사이의 공간과 같이 예측하기 어려운 전환에 더 많은 계산이 할당되어 데이터 구조에 대한 흥미로운 통찰력을 제공한다.",
                    "tag": "3+4"
                },
                {
                    "index": "395-6",
                    "sentence": "This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.",
                    "sentence_kor": "이는 ACT 또는 다른 적응형 계산 방법이 시퀀스 데이터에서 세그먼트 경계를 유추하기 위한 일반적인 방법을 제공할 수 있음을 시사한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "995",
            "abstractID": "SPA_abs-396",
            "text": [
                {
                    "index": "396-0",
                    "sentence": "Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition.",
                    "sentence_kor": "표준 PASCAL VOC Challenge 데이터 세트에서 측정한 객체 감지 성능은 대회 말년에 정체되었다.",
                    "tag": "1"
                },
                {
                    "index": "396-1",
                    "sentence": "The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context.",
                    "sentence_kor": "가장 성능이 좋은 방법은 일반적으로 여러 로우 레벨 이미지 기능을 하이 레벨 컨텍스트와 결합하는 복잡한 앙상블 시스템입니다.",
                    "tag": "1"
                },
                {
                    "index": "396-2",
                    "sentence": "In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent.",
                    "sentence_kor": "본 논문에서 우리는 VOC 2012의 이전 최고 결과에 비해 평균 평균 정밀도(mAP)를 50% 이상 향상시켜 62.4%의 mAP를 달성하는 간단하고 확장 가능한 탐지 알고리즘을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "396-3",
                    "sentence": "Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly.",
                    "sentence_kor": "우리의 접근 방식은 두 가지 아이디어를 결합한다. (1) 개체를 국산화 및 분할하기 위해 상향식 영역 제안에 고용량 컨볼루션 네트워크(CNN)를 적용할 수 있으며, (2) 라벨링된 훈련 데이터가 부족한 경우 보조 작업에 대한 사전 교육을 감독한 후 도메인별 미세 조정을 통해 성능을 크게 향상시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "396-4",
                    "sentence": "Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.",
                    "sentence_kor": "지역 제안을 CNN과 결합하기 때문에 결과 모델을 R-CNN 또는 지역 기반 컨볼루션 네트워크라고 부른다.",
                    "tag": "1"
                },
                {
                    "index": "396-5",
                    "sentence": "Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
                    "sentence_kor": "전체 시스템의 소스 코드는 http://www.cs.berkeley.edu/에서 확인할 수 있다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "996",
            "abstractID": "SPA_abs-397",
            "text": [
                {
                    "index": "397-0",
                    "sentence": "Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc.",
                    "sentence_kor": "인스턴스 수준의 인간 분석은 실제 시나리오에서 일반적이며 인간 부분 분할, 조밀한 포즈 추정, 인간-객체 상호작용 등과 같은 여러 가지 징후를 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "397-1",
                    "sentence": "Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance.",
                    "sentence_kor": "모델은 이미지 패널에서 다양한 인간 인스턴스를 구별하고 각 인스턴스의 세부 정보를 나타내는 풍부한 기능을 학습해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "397-2",
                    "sentence": "In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN.",
                    "sentence_kor": "본 논문에서 우리는 인스턴스 수준 인간 분석을 해결하기 위한 엔드 투 엔드 파이프라인인 구문 분석 R-CNN을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "397-3",
                    "sentence": "It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances.",
                    "sentence_kor": "지역 기반 접근법의 특성과 사람의 외모를 종합적으로 고려하여 일련의 인간 인스턴스를 동시에 처리하므로 인스턴스의 세부 사항을 나타낼 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "397-4",
                    "sentence": "Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis.",
                    "sentence_kor": "R-CNN 파싱은 매우 유연하고 효율적이며, 이는 인간 인스턴스 분석의 많은 문제에 적용된다.",
                    "tag": "4"
                },
                {
                    "index": "397-5",
                    "sentence": "Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets.",
                    "sentence_kor": "우리의 접근 방식은 CIHP(Crowd Instance-level Human 구문 분석), MHP v2.0(Multi-Human 구문 분석) 및 DensePose-COCO 데이터 세트에서 모든 최첨단 방법을 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "397-6",
                    "sentence": "Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task.",
                    "sentence_kor": "제안된 구문 분석 R-CNN을 기반으로 COCO 2018 도전 밀도 추정 작업에서 1위에 도달했다.",
                    "tag": "5"
                },
                {
                    "index": "397-7",
                    "sentence": "Code and models are public available.",
                    "sentence_kor": "코드 및 모델은 공개되어 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "997",
            "abstractID": "SPA_abs-398",
            "text": [
                {
                    "index": "398-0",
                    "sentence": "This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors.",
                    "sentence_kor": "본 논문은 최적의 범주별 3D 키포인트 세트를 검출기와 함께 학습하기 위한 엔드 투 엔드 기하학적 추론 프레임워크인 KeypointNet을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "398-1",
                    "sentence": "Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task.",
                    "sentence_kor": "단일 이미지가 주어지면 KeypointNet은 다운스트림 작업에 최적화된 3D 키포인트를 추출합니다.",
                    "tag": "3"
                },
                {
                    "index": "398-2",
                    "sentence": "We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object.",
                    "sentence_kor": "우리는 물체의 두 관점 사이의 상대적인 포즈를 복구하기 위한 최적의 핵심 포인트를 찾는 차별화 가능한 목표를 제안함으로써 3D 포즈 추정에 이 프레임워크를 입증한다.",
                    "tag": "2+3"
                },
                {
                    "index": "398-3",
                    "sentence": "Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category.",
                    "sentence_kor": "우리 모델은 객체 범주의 보기 각도와 인스턴스에서 기하학적으로 의미적으로 일관된 핵심점을 발견한다.",
                    "tag": "4"
                },
                {
                    "index": "398-4",
                    "sentence": "Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation.",
                    "sentence_kor": "중요한 것은 실제 키포인트 주석을 전혀 사용하지 않는 엔드 투 엔드 프레임워크가 자세 추정 작업에서 동일한 신경망 아키텍처를 사용하는 완전한 감독 기준선을 능가한다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "398-5",
                    "sentence": "The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at this http URL.",
                    "sentence_kor": "ShapeNet의 차량, 의자 및 평면 범주에서 발견된 3D 키포인트는 이 http URL에서 시각화됩니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "998",
            "abstractID": "SPA_abs-399",
            "text": [
                {
                    "index": "399-0",
                    "sentence": "Multi-person pose estimation in the wild is challenging.",
                    "sentence_kor": "야생에서 다인 포즈 추정은 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "399-1",
                    "sentence": "Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable.",
                    "sentence_kor": "최첨단 인간 감지기는 우수한 성능을 보여 주었지만 위치 파악 및 인식에 있어 작은 오류가 불가피하다.",
                    "tag": "1"
                },
                {
                    "index": "399-2",
                    "sentence": "These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results.",
                    "sentence_kor": "이러한 오류는 특히 사람 감지 결과에 따라 달라지는 방법의 경우 1인 자세 추정기(SPEP)에 고장을 일으킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "399-3",
                    "sentence": "In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes.",
                    "sentence_kor": "본 논문에서, 우리는 부정확한 인간 경계 박스가 있을 때 포즈 추정을 용이하게 하기 위한 새로운 지역 다인 포즈 추정(RMPE) 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "399-4",
                    "sentence": "Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG).",
                    "sentence_kor": "프레임워크는 세 가지 요소로 구성됩니다. 대칭 공간 변압기 네트워크(SSTN), 파라메트릭 포즈 비최대 억제(NMS) 및 포즈 유도 제안 생성기(PGG).",
                    "tag": "3"
                },
                {
                    "index": "399-5",
                    "sentence": "Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.",
                    "sentence_kor": "우리의 방법은 부정확한 경계 상자와 중복 탐지를 처리할 수 있어 MPII(다인칭) 데이터 세트의 최첨단 방법에 비해 mAP가 17% 증가할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "399-6",
                    "sentence": "Our model and source codes are publicly available.",
                    "sentence_kor": "우리의 모델과 소스 코드는 공개적으로 이용 가능하다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "999",
            "abstractID": "SPA_abs-400",
            "text": [
                {
                    "index": "400-0",
                    "sentence": "Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis.",
                    "sentence_kor": "최근 GAN(Generative Adversarial Networks)의 발전은 얼굴 표정 합성 작업에 인상적인 결과를 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "400-1",
                    "sentence": "The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression.",
                    "sentence_kor": "가장 성공적인 아키텍처는 StarGAN으로, GANs 생성 프로세스를 특정 도메인의 이미지, 즉 동일한 표현을 공유하는 사람들의 이미지 세트를 조건으로 한다.",
                    "tag": "1"
                },
                {
                    "index": "400-2",
                    "sentence": "While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset.",
                    "sentence_kor": "이 접근 방식은 효과적이지만 데이터 집합의 내용에 따라 결정되는 개별 표현식만 생성할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "400-3",
                    "sentence": "To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression.",
                    "sentence_kor": "이러한 한계를 해결하기 위해 본 논문에서 우리는 연속적인 다양체로 인간의 표정을 정의하는 해부학적 얼굴 움직임을 설명하는 액션 유닛(AU) 주석을 기반으로 한 새로운 GAN 조건화 방식을 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "400-4",
                    "sentence": "Our approach allows controlling the magnitude of activation of each AU and combine several of them.",
                    "sentence_kor": "우리의 접근 방식을 통해 각 AU의 활성화 크기를 제어하고 이들 중 몇 가지를 결합할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "400-5",
                    "sentence": "Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions.",
                    "sentence_kor": "또한, 우리는 활성화된 AU로 주석이 달린 이미지만 필요로 하는 모델을 훈련시키고 네트워크가 배경 및 조명 조건 변화에 견고하게 만드는 주의 메커니즘을 활용하기 위한 완전히 감독되지 않은 전략을 제안한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "1000",
            "abstractID": "SPA_abs-401",
            "text": [
                {
                    "index": "401-0",
                    "sentence": "Domain adaptation is critical for success in new, unseen environments.",
                    "sentence_kor": "도메인 적응은 보이지 않는 새로운 환경에서 성공을 거두기 위해 매우 중요합니다.",
                    "tag": "1"
                },
                {
                    "index": "401-1",
                    "sentence": "Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.",
                    "sentence_kor": "형상 공간에 적용되는 적대적 적응 모델은 도메인 불변 표현을 발견하지만 시각화하기 어렵고 픽셀 수준 및 낮은 수준의 도메인 이동을 포착하지 못하는 경우도 있다.",
                    "tag": "1"
                },
                {
                    "index": "401-2",
                    "sentence": "Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs.",
                    "sentence_kor": "최근 연구에서는 주기 일관성 제약과 결합된 생성적 적대 네트워크가 정렬된 이미지 쌍을 사용하지 않더라도 도메인 간에 이미지를 매핑하는 데 놀라울 정도로 효과적이라는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "401-3",
                    "sentence": "We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.",
                    "sentence_kor": "우리는 차별적으로 훈련된 새로운 주기 일치 적대적 도메인 적응 모델을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "401-4",
                    "sentence": "CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.",
                    "sentence_kor": "CyCADA는 픽셀 레벨과 기능 레벨 모두에서 표현을 조정하고 작업 손실을 활용하는 동안 주기 일관성을 적용하며 정렬 쌍이 필요하지 않다.",
                    "tag": "4"
                },
                {
                    "index": "401-5",
                    "sentence": "Our model can be applied in a variety of visual recognition and prediction settings.",
                    "sentence_kor": "우리 모델은 다양한 시각적 인식 및 예측 설정에 적용될 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "401-6",
                    "sentence": "We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.",
                    "sentence_kor": "합성 도메인에서 실제 도메인으로의 전환을 입증하는 도로 장면의 숫자 분류 및 의미 분할을 포함하여 여러 적응 작업에 걸쳐 새로운 최첨단 결과를 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1001",
            "abstractID": "SPA_abs-402",
            "text": [
                {
                    "index": "402-0",
                    "sentence": "On the one hand, deep neural networks are effective in learning large datasets.",
                    "sentence_kor": "한편, 심층 신경망은 대규모 데이터 세트를 학습하는 데 효과적이다.",
                    "tag": "1"
                },
                {
                    "index": "402-1",
                    "sentence": "On the other, they are inefficient with their data usage.",
                    "sentence_kor": "반면 데이터 사용 효율은 낮습니다.",
                    "tag": "1"
                },
                {
                    "index": "402-2",
                    "sentence": "They often require copious amount of labeled-data to train their scads of parameters.",
                    "sentence_kor": "그들은 종종 그들의 광범위한 매개변수를 훈련시키기 위해 많은 양의 라벨링 데이터가 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "402-3",
                    "sentence": "Training larger and deeper networks is hard without appropriate regularization, particularly while using a small dataset.",
                    "sentence_kor": "특히 소규모 데이터 세트를 사용하는 경우 적절한 정규화 없이는 더 크고 심층적인 네트워크를 교육하기가 어렵습니다.",
                    "tag": "1"
                },
                {
                    "index": "402-4",
                    "sentence": "Laterally, collecting well-annotated data is expensive, time-consuming and often infeasible.",
                    "sentence_kor": "한편, 잘 알려진 데이터를 수집하는 것은 비용이 많이 들고 시간이 많이 걸리며 실현 불가능한 경우가 많습니다.",
                    "tag": "1"
                },
                {
                    "index": "402-5",
                    "sentence": "A popular way to regularize these networks is to simply train the network with more data from an alternate representative dataset.",
                    "sentence_kor": "이러한 네트워크를 정규화하는 일반적인 방법은 대체 대표 데이터 세트의 더 많은 데이터로 네트워크를 훈련시키는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "402-6",
                    "sentence": "This can lead to adverse effects if the statistics of the representative dataset are dissimilar to our target.",
                    "sentence_kor": "이는 대표적인 데이터 세트의 통계가 우리의 목표와 다를 경우 역효과를 초래할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "402-7",
                    "sentence": "This predicament is due to the problem of domain shift.",
                    "sentence_kor": "이 곤경은 도메인 이동의 문제 때문이다.",
                    "tag": "1"
                },
                {
                    "index": "402-8",
                    "sentence": "Data from a shifted domain might not produce bespoke features when a feature extractor from the representative domain is used.",
                    "sentence_kor": "이동된 도메인의 데이터는 대표 도메인의 기능 추출기를 사용할 때 맞춤형 기능을 생성하지 못할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "402-9",
                    "sentence": "Several techniques of domain adaptation have been proposed in the past to solve this problem.",
                    "sentence_kor": "과거에 이 문제를 해결하기 위해 몇 가지 도메인 적응 기술이 제안되었다.",
                    "tag": "1"
                },
                {
                    "index": "402-10",
                    "sentence": "In this paper, we propose a new technique (d-SNE) of domain adaptation that cleverly uses stochastic neighborhood embedding techniques and a novel modified-Hausdorff distance.",
                    "sentence_kor": "본 논문에서, 우리는 확률적 이웃 임베딩 기술과 새로운 수정된 하우스도르프 거리를 교묘하게 사용하는 도메인 적응의 새로운 기법(d-SNE)을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "402-11",
                    "sentence": "The proposed technique is learnable end-to-end and is therefore, ideally suited to train neural networks.",
                    "sentence_kor": "제안된 기술은 학습 가능한 엔드 투 엔드이므로 신경망을 훈련하는 데 이상적이다.",
                    "tag": "1"
                },
                {
                    "index": "402-12",
                    "sentence": "Extensive experiments demonstrate that d-SNE outperforms the current states-of-the-art and is robust to the variances in different datasets, even in the one-shot and semi-supervised learning settings.",
                    "sentence_kor": "광범위한 실험에 따르면 d-SNE은 현재 최첨단 기술을 능가하며 원샷 및 준감독 학습 설정에서도 서로 다른 데이터 세트의 분산에 강하다.",
                    "tag": "1"
                },
                {
                    "index": "402-13",
                    "sentence": "d-SNE also demonstrates the ability to generalize to multiple domains concurrently.",
                    "sentence_kor": "d-SNE는 또한 동시에 여러 도메인에 일반화할 수 있는 능력을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1002",
            "abstractID": "SPA_abs-403",
            "text": [
                {
                    "index": "403-0",
                    "sentence": "Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community.",
                    "sentence_kor": "초고해상도, 디블러링 등 동영상 복원 작업이 컴퓨터 비전 커뮤니티에서 주목받고 있다.",
                    "tag": "1"
                },
                {
                    "index": "403-1",
                    "sentence": "A challenging benchmark named REDS is released in the NTIRE19 Challenge.",
                    "sentence_kor": "REDS라는 도전적인 벤치마크가 NTIRE19 Challenge에 발표되었습니다.",
                    "tag": "1"
                },
                {
                    "index": "403-2",
                    "sentence": "This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur.",
                    "sentence_kor": "이 새로운 벤치마크는 (1) 큰 모션이 주어진 여러 프레임을 정렬하는 방법과 (2) 다양한 모션과 블러를 사용하여 서로 다른 프레임을 효과적으로 융합하는 방법 등 두 가지 측면에서 기존 방법에 도전한다.",
                    "tag": "2+3"
                },
                {
                    "index": "403-3",
                    "sentence": "In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges.",
                    "sentence_kor": "본 연구에서는 이러한 과제를 해결하기 위해 EDVR이라는 향상된 변형 가능 네트워크를 갖춘 새로운 비디오 복원 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "403-4",
                    "sentence": "First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner.",
                    "sentence_kor": "첫째, 큰 모션을 처리하기 위해 변형 가능한 컨볼루션을 사용하여 형상 수준에서 프레임 정렬이 거친 방식으로 수행되는 피라미드, 계단식 및 변형 가능한(PCD) 정렬 모듈을 고안한다.",
                    "tag": "3"
                },
                {
                    "index": "403-5",
                    "sentence": "Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration.",
                    "sentence_kor": "둘째, 후속 복원을 위해 중요한 기능을 강조하기 위해 일시적으로나 공간적으로 주의를 기울이는 시간 및 공간 주의(TSA) 융합 모듈을 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "403-6",
                    "sentence": "Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges.",
                    "sentence_kor": "이러한 모듈 덕분에 EDVR은 우승을 차지했으며 NTIRE19 비디오 복원 및 개선 과제에서 4개 트랙 모두에서 2위를 크게 앞질렀다.",
                    "tag": "4"
                },
                {
                    "index": "403-7",
                    "sentence": "EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring.",
                    "sentence_kor": "또한 EDVR은 비디오 초고해상도 및 디블러링에 대해 최신 발표된 방법보다 우수한 성능을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "403-8",
                    "sentence": "The code is available at this https URL.",
                    "sentence_kor": "코드는 이 https URL에서 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1003",
            "abstractID": "SPA_abs-404",
            "text": [
                {
                    "index": "404-0",
                    "sentence": "Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution.",
                    "sentence_kor": "최근 심층 신경망을 기반으로 한 여러 모델이 단일 이미지 초고해상도 재구성 정확도와 계산 성능 측면에서 큰 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "404-1",
                    "sentence": "In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction.",
                    "sentence_kor": "이러한 방법에서 낮은 해상도(LR) 입력 영상은 재구성 전에 일반적으로 이관 보간이라는 단일 필터를 사용하여 고해상도(HR) 공간으로 상향 조정된다.",
                    "tag": "1"
                },
                {
                    "index": "404-2",
                    "sentence": "This means that the super-resolution (SR) operation is performed in HR space.",
                    "sentence_kor": "이는 초해상도(SR) 작업이 HR 공간에서 수행됨을 의미합니다.",
                    "tag": "1"
                },
                {
                    "index": "404-3",
                    "sentence": "We demonstrate that this is sub-optimal and adds computational complexity.",
                    "sentence_kor": "우리는 이것이 차선책이며 계산 복잡성을 더한다는 것을 입증한다.",
                    "tag": "1"
                },
                {
                    "index": "404-4",
                    "sentence": "In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU.",
                    "sentence_kor": "본 논문에서 우리는 단일 K2 GPU에서 1080p 비디오의 실시간 SR이 가능한 최초의 컨볼루션 신경망(CNN)을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "404-5",
                    "sentence": "To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space.",
                    "sentence_kor": "이를 위해 LR 공간에서 특징 맵을 추출하는 새로운 CNN 아키텍처를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "404-6",
                    "sentence": "In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output.",
                    "sentence_kor": "또한 최종 LR 기능 맵을 HR 출력으로 상향 조정하기 위해 일련의 업스케일링 필터를 학습하는 효율적인 하위 픽셀 컨볼루션 레이어를 도입한다.",
                    "tag": "2+3"
                },
                {
                    "index": "404-7",
                    "sentence": "By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation.",
                    "sentence_kor": "이를 통해 SR 파이프라인의 수공예 바이큐빅 필터를 각 기능 맵에 대해 특별히 훈련된 보다 복잡한 업스케일링 필터로 효과적으로 교체하는 동시에 전체 SR 작업의 계산 복잡성을 줄인다.",
                    "tag": "4"
                },
                {
                    "index": "404-8",
                    "sentence": "We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.",
                    "sentence_kor": "우리는 공개적으로 사용 가능한 데이터 세트의 이미지와 비디오를 사용하여 제안된 접근 방식을 평가하고 그것이 훨씬 더 나은 성능(이미지에서는 +0.15dB, 비디오에서는 +0.39dB)이며 이전 CNN 기반 방법보다 훨씬 빠른 속도임을 보여준다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "1004",
            "abstractID": "SPA_abs-405",
            "text": [
                {
                    "index": "405-0",
                    "sentence": "Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors?",
                    "sentence_kor": "더 빠르고 깊은 컨볼루션 신경망을 사용한 단일 이미지 초고해상도 정확성과 속도의 혁신에도 불구하고, 한 가지 핵심 문제는 대부분 해결되지 않은 채 남아 있다. 즉, 대규모 업스케일링 인자에서 초고해상도 시 미세한 텍스처 세부 사항을 어떻게 복구할 것인가?",
                    "tag": "1"
                },
                {
                    "index": "405-1",
                    "sentence": "The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function.",
                    "sentence_kor": "최적화 기반 초해상도 방법의 동작은 주로 목적 함수의 선택에 의해 이루어진다.",
                    "tag": "1"
                },
                {
                    "index": "405-2",
                    "sentence": "Recent work has largely focused on minimizing the mean squared reconstruction error.",
                    "sentence_kor": "최근 연구는 평균 제곱 재구성 오류를 최소화하는 데 크게 초점을 맞추고 있다.",
                    "tag": "1"
                },
                {
                    "index": "405-3",
                    "sentence": "The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.",
                    "sentence_kor": "결과 추정치는 피크 신호 대 잡음비가 높지만, 종종 고주파 세부 정보가 부족하고 높은 해상도에서 예상되는 충실도와 일치하지 않는다는 점에서 지각적으로 불만족스럽다.",
                    "tag": "1"
                },
                {
                    "index": "405-4",
                    "sentence": "In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR).",
                    "sentence_kor": "본 논문에서 우리는 이미지 초고해상도(SR)를 위한 생성적 적대 네트워크(GAN)인 SRGAN을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "405-5",
                    "sentence": "To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors.",
                    "sentence_kor": "우리가 아는 바로는, 그것은 4배 상향 조정 요소에 대해 광 현실적 자연 이미지를 추론할 수 있는 첫 번째 프레임워크이다.",
                    "tag": "4"
                },
                {
                    "index": "405-6",
                    "sentence": "To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss.",
                    "sentence_kor": "이를 달성하기 위해 적대적 손실과 콘텐츠 손실로 구성된 지각 손실 함수를 제안한다.",
                    "tag": "3"
                },
                {
                    "index": "405-7",
                    "sentence": "The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images.",
                    "sentence_kor": "적대적 손실은 초고해상도 이미지와 원본 광현실 이미지를 구별하도록 훈련된 판별기 네트워크를 사용하여 우리의 솔루션을 자연 이미지 매니폴드로 밀어낸다.",
                    "tag": "3+4"
                },
                {
                    "index": "405-8",
                    "sentence": "In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space.",
                    "sentence_kor": "또한 픽셀 공간의 유사성 대신 지각 유사성에 의해 동기 부여되는 콘텐츠 손실을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "405-9",
                    "sentence": "Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks.",
                    "sentence_kor": "당사의 심층 잔류 네트워크는 공개 벤치마크에서 심하게 다운샘플링된 이미지로부터 사실적인 질감을 복구할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "405-10",
                    "sentence": "An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN.",
                    "sentence_kor": "광범위한 평균 오피니언 점수(MOS) 테스트는 SRGAN을 사용한 지각 품질에서 크게 향상된 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "405-11",
                    "sentence": "The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.",
                    "sentence_kor": "SRGAN으로 얻은 MOS 점수는 최첨단 방법으로 얻은 점수보다 원래의 고해상도 이미지에 더 가깝다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1005",
            "abstractID": "SPA_abs-406",
            "text": [
                {
                    "index": "406-0",
                    "sentence": "Non-local self-similarity is well-known to be an effective prior for the image denoising problem.",
                    "sentence_kor": "비로컬 자기 유사성은 이미지 노이즈 제거 문제에 효과적인 것으로 잘 알려져 있다.",
                    "tag": "1"
                },
                {
                    "index": "406-1",
                    "sentence": "However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information.",
                    "sentence_kor": "그러나 로컬 정보만 이용하더라도 로컬 모델이 아닌 방법을 능가하는 컨볼루션 신경망에 이를 통합하기 위한 작업은 거의 수행되지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "406-2",
                    "sentence": "In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields.",
                    "sentence_kor": "본 논문에서, 우리는 그래프 컨볼루션 연산을 기반으로 레이어를 사용하는 새로운 엔드 투 엔드 훈련식 신경망 아키텍처를 제안하며, 이를 통해 비 로컬 수용 필드를 가진 뉴런을 생성한다.",
                    "tag": "2+3"
                },
                {
                    "index": "406-3",
                    "sentence": "The graph convolution operation generalizes the classic convolution to arbitrary graphs.",
                    "sentence_kor": "그래프 합성곱 연산은 고전적인 합성곱을 임의 그래프에 일반화한다.",
                    "tag": "1"
                },
                {
                    "index": "406-4",
                    "sentence": "In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns.",
                    "sentence_kor": "이 연구에서 그래프는 네트워크의 숨겨진 특징 간의 유사성에서 동적으로 계산되어 네트워크의 강력한 표현 학습 능력을 활용하여 자기 유사 패턴을 밝혀낸다.",
                    "tag": "2+3"
                },
                {
                    "index": "406-5",
                    "sentence": "We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution.",
                    "sentence_kor": "우리는 이 특정 그래프 컨볼루션의 소멸 그레이디언트 및 초과 매개 변수화 문제를 해결하는 경량 에지 조건 컨볼루션(Lightweight Edge-Conditioned Convolution)을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "406-6",
                    "sentence": "Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.",
                    "sentence_kor": "광범위한 실험은 합성 가우스 노이즈와 실제 노이즈 모두에 대한 질적 및 정량적 결과를 개선한 최첨단 성능을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1006",
            "abstractID": "SPA_abs-407",
            "text": [
                {
                    "index": "407-0",
                    "sentence": "Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process.",
                    "sentence_kor": "실제 세계에서 자율 주행 차량에 대한 알고리즘 개발 및 테스트 과정은 비용이 많이 들고 시간이 많이 걸린다.",
                    "tag": "1"
                },
                {
                    "index": "407-1",
                    "sentence": "Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments.",
                    "sentence_kor": "또한 기계 지능과 딥 러닝의 최근 발전을 활용하기 위해서는 다양한 조건과 환경에서 주석이 달린 많은 훈련 데이터를 수집해야 한다.",
                    "tag": "1"
                },
                {
                    "index": "407-2",
                    "sentence": "We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals.",
                    "sentence_kor": "우리는 언리얼 엔진을 기반으로 하는 새로운 시뮬레이터를 제시하는데, 이 두 가지 목표에 대해 물리적이고 시각적으로 사실적인 시뮬레이션을 제공한다.",
                    "tag": "2+3"
                },
                {
                    "index": "407-3",
                    "sentence": "Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink).",
                    "sentence_kor": "당사의 시뮬레이터에는 널리 사용되는 프로토콜(예: MavLink)을 지원하는 실시간 HITL(Hardware-in-the-Loop) 시뮬레이션을 위해 고주파에서 작동할 수 있는 물리 엔진이 포함되어 있다.",
                    "tag": "3"
                },
                {
                    "index": "407-4",
                    "sentence": "The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols.",
                    "sentence_kor": "시뮬레이터는 처음부터 새로운 유형의 차량, 하드웨어 플랫폼 및 소프트웨어 프로토콜을 수용할 수 있도록 확장 가능하도록 설계되었습니다.",
                    "tag": "4"
                },
                {
                    "index": "407-5",
                    "sentence": "In addition, the modular design enables various components to be easily usable independently in other projects.",
                    "sentence_kor": "또한 모듈형 설계는 다양한 요소들을 다른 프로젝트에서 독립적으로 쉽게 사용할 수 있게 한다.",
                    "tag": "4"
                },
                {
                    "index": "407-6",
                    "sentence": "We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.",
                    "sentence_kor": "먼저 쿼드로터를 자율 주행 차량으로 구현한 다음 소프트웨어 구성 요소를 실제 비행과 실험적으로 비교함으로써 시뮬레이터를 시연한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "1007",
            "abstractID": "SPA_abs-408",
            "text": [
                {
                    "index": "408-0",
                    "sentence": "Learning compressed representations of multivariate time series (MTS) facilitates data analysis in the presence of noise and redundant information, and for a large number of variates and time steps.",
                    "sentence_kor": "다변량 시계열(MTS)의 압축된 표현을 학습하면 잡음과 중복 정보가 존재하며 많은 변형과 시간 단계에 대한 데이터 분석이 용이하다.",
                    "tag": "1"
                },
                {
                    "index": "408-1",
                    "sentence": "However, classical dimensionality reduction approaches are designed for vectorial data and cannot deal explicitly with missing values.",
                    "sentence_kor": "그러나 기존의 치수 감소 접근법은 벡터 데이터를 위해 설계되었으며 결측값을 명시적으로 처리할 수 없다.",
                    "tag": "2"
                },
                {
                    "index": "408-2",
                    "sentence": "In this work, we propose a novel autoencoder architecture based on recurrent neural networks to generate compressed representations of MTS.",
                    "sentence_kor": "본 연구에서는 MTS의 압축된 표현을 생성하기 위해 반복 신경망을 기반으로 하는 새로운 자동 인코더 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "408-3",
                    "sentence": "The proposed model can process inputs characterized by variable lengths and it is specifically designed to handle missing data.",
                    "sentence_kor": "제안된 모델은 가변 길이로 특징지어지는 입력을 처리할 수 있으며 결측 데이터를 처리하도록 특별히 설계되었다.",
                    "tag": "3"
                },
                {
                    "index": "408-4",
                    "sentence": "Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned to a kernel function that operates in input space and that handles missing values.",
                    "sentence_kor": "자동 인코더는 입력 공간에서 작동하고 결측값을 처리하는 커널 함수에 쌍별 유사성이 정렬되는 고정 길이 벡터 표현을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "408-5",
                    "sentence": "This allows to learn good representations, even in the presence of a significant amount of missing data.",
                    "sentence_kor": "이렇게 하면 상당한 양의 결측 데이터가 있는 경우에도 양호한 표현을 학습할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "408-6",
                    "sentence": "To show the effectiveness of the proposed approach, we evaluate the quality of the learned representations in several classification tasks, including those involving medical data, and we compare to other methods for dimensionality reduction.",
                    "sentence_kor": "제안된 접근방식의 효과를 보여주기 위해 의료 데이터를 포함한 여러 분류 작업에서 학습된 표현의 품질을 평가하고 차원 축소를 위한 다른 방법과 비교한다.",
                    "tag": "2+3"
                },
                {
                    "index": "408-7",
                    "sentence": "Successively, we design two frameworks based on the proposed architecture: one for imputing missing data and another for one-class classification.",
                    "sentence_kor": "연속적으로, 우리는 제안된 아키텍처를 기반으로 두 가지 프레임워크를 설계한다. 하나는 누락된 데이터를 귀속시키는 것이고 다른 하나는 하나의 클래스 분류를 위한 것이다.",
                    "tag": "3"
                },
                {
                    "index": "408-8",
                    "sentence": "Finally, we analyze under what circumstances an autoencoder with recurrent layers can learn better compressed representations of MTS than feed-forward architectures.",
                    "sentence_kor": "마지막으로, 어떤 상황에서 반복 계층을 가진 자동 인코더가 피드 포워드 아키텍처보다 MTS의 더 나은 압축 표현을 배울 수 있는지 분석한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1008",
            "abstractID": "SPA_abs-409",
            "text": [
                {
                    "index": "409-0",
                    "sentence": "Localization of salient facial landmark points, such as eye corners or the tip of the nose, is still considered a challenging computer vision problem despite recent efforts.",
                    "sentence_kor": "눈 모서리 또는 코 끝과 같은 두드러진 얼굴 랜드마크 포인트의 국소화는 최근의 노력에도 불구하고 여전히 어려운 컴퓨터 시력 문제로 간주된다.",
                    "tag": "1"
                },
                {
                    "index": "409-1",
                    "sentence": "This is especially evident in unconstrained environments, i.e., in the presence of background clutter and large head pose variations.",
                    "sentence_kor": "이는 특히 배경 잡음과 큰 머리 포즈 변화가 있는 등 제약이 없는 환경에서 두드러집니다.",
                    "tag": "1"
                },
                {
                    "index": "409-2",
                    "sentence": "Most methods that achieve state-of-the-art accuracy are slow, and, thus, have limited applications.",
                    "sentence_kor": "최첨단 정확도를 달성하는 대부분의 방법은 속도가 느리기 때문에 적용 범위가 제한되어 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "409-3",
                    "sentence": "We describe a method that can accurately estimate the positions of relevant facial landmarks in real-time even on hardware with limited processing power, such as mobile devices.",
                    "sentence_kor": "모바일 기기와 같이 처리 능력이 제한된 하드웨어에서도 실시간으로 관련 안면 랜드마크의 위치를 정확하게 추정할 수 있는 방법을 설명한다.",
                    "tag": "1"
                },
                {
                    "index": "409-4",
                    "sentence": "This is achieved with a sequence of estimators based on ensembles of regression trees.",
                    "sentence_kor": "이는 회귀 트리의 앙상블을 기반으로 한 일련의 추정기로 달성된다.",
                    "tag": "1"
                },
                {
                    "index": "409-5",
                    "sentence": "The trees use simple pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast.",
                    "sentence_kor": "트리는 내부 노드에서 간단한 픽셀 강도 비교를 사용하여 이미지 영역을 매우 빠르게 처리할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "409-6",
                    "sentence": "We test the developed system on several publicly available datasets and analyse its processing speed on various devices.",
                    "sentence_kor": "우리는 개발된 시스템을 공개 가능한 여러 데이터 세트에서 테스트하고 다양한 장치에서 처리 속도를 분석한다.",
                    "tag": "1"
                },
                {
                    "index": "409-7",
                    "sentence": "Experimental results show that our method has practical value.",
                    "sentence_kor": "실험 결과는 우리의 방법이 실질적인 가치가 있다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1009",
            "abstractID": "SPA_abs-410",
            "text": [
                {
                    "index": "410-0",
                    "sentence": "We describe efforts to adapt the Tesseract open source OCR engine for multiple scripts and languages.",
                    "sentence_kor": "우리는 복수의 스크립트 및 언어에 대해 Tesseract 오픈 소스 OCR 엔진을 적용하기 위한 노력을 설명한다.",
                    "tag": "1"
                },
                {
                    "index": "410-1",
                    "sentence": "Effort has been concentrated on enabling generic multi-lingual operation such that negligible customization is required for a new language beyond providing a corpus of text.",
                    "sentence_kor": "텍스트 말뭉치를 제공하는 것 이상의 새로운 언어에 대해 무시할 수 있는 커스터마이징이 요구되도록 일반적인 다국어 작동을 가능하게 하는 데 노력이 집중되었다.",
                    "tag": "1"
                },
                {
                    "index": "410-2",
                    "sentence": "Although change was required to various modules, including physical layout analysis, and linguistic post-processing, no change was required to the character classifier beyond changing a few limits.",
                    "sentence_kor": "물리적 레이아웃 분석 및 언어 후처리를 포함한 다양한 모듈에 변경이 필요했지만, 문자 분류기에는 몇 가지 한계를 변경하는 것 이상의 변경이 필요하지 않았다.",
                    "tag": "1"
                },
                {
                    "index": "410-3",
                    "sentence": "The Tesseract classifier has adapted easily to Simplified Chinese.",
                    "sentence_kor": "큐브 분류기는 중국어 간체에 쉽게 적응했다.",
                    "tag": "1"
                },
                {
                    "index": "410-4",
                    "sentence": "Test results on English, a mixture of European languages, and Russian, taken from a random sample of books, show a reasonably consistent word error rate between 3.72% and 5.78%, and Simplified Chinese has a character error rate of only 3.77%.",
                    "sentence_kor": "무작위 책 샘플에서 추출한 유럽어와 러시아어의 혼합된 영어의 시험 결과는 3.72%에서 5.78% 사이의 단어 오류율이 상당히 일관되게 나타나며 중국어 간체는 문자 오류율이 3.77%에 불과하다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1010",
            "abstractID": "SPA_abs-411",
            "text": [
                {
                    "index": "411-0",
                    "sentence": "Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time.",
                    "sentence_kor": "컨볼루션 운영과 반복 운영 모두 한 번에 하나의 로컬 이웃을 처리하는 블록입니다.",
                    "tag": "1"
                },
                {
                    "index": "411-1",
                    "sentence": "In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies.",
                    "sentence_kor": "본 논문에서 우리는 장기 종속성을 포착하기 위한 일반적인 구성 블록 제품군으로 로컬이 아닌 작업을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "411-2",
                    "sentence": "Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions.",
                    "sentence_kor": "컴퓨터 비전의 고전적인 비로컬 평균 방법에서 영감을 받아 비로컬 연산은 위치의 응답을 모든 위치에서 형상의 가중 합으로 계산한다.",
                    "tag": "3"
                },
                {
                    "index": "411-3",
                    "sentence": "This building block can be plugged into many computer vision architectures.",
                    "sentence_kor": "이 빌딩 블록은 많은 컴퓨터 비전 아키텍처에 연결할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "411-4",
                    "sentence": "On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets.",
                    "sentence_kor": "비디오 분류 작업에서, 종과 휘파람이 없어도, 우리의 비로컬 모델은 키네틱스와 카라데스 데이터 세트 모두에서 현재 경쟁 우승자를 경쟁하거나 능가할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "411-5",
                    "sentence": "In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks.",
                    "sentence_kor": "정적 이미지 인식에서 로컬이 아닌 모델은 객체 감지/분할을 개선하고 COCO 작업군에 대한 포즈 추정치를 향상시킨다.",
                    "tag": "4"
                },
                {
                    "index": "411-6",
                    "sentence": "Code is available at this https URL .",
                    "sentence_kor": "코드는 이 https URL에서 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1011",
            "abstractID": "SPA_abs-412",
            "text": [
                {
                    "index": "412-0",
                    "sentence": "We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand.",
                    "sentence_kor": "우리는 멀티 카메라 시스템을 사용하여 손의 관절과 같이 폐색되기 쉬운 키포인트에 대한 미세한 감지기를 훈련시키는 접근 방식을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "412-1",
                    "sentence": "We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand.",
                    "sentence_kor": "우리는 이 절차를 다중 뷰 부트스트랩이라고 부른다. 첫째, 초기 키포인트 검출기는 손의 여러 보기에서 노이즈가 있는 레이블을 생성하는 데 사용된다.",
                    "tag": "4"
                },
                {
                    "index": "412-2",
                    "sentence": "The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers.",
                    "sentence_kor": "그런 다음 잡음이 많은 탐지는 다중 뷰 형상을 사용하여 3D로 삼각측량하거나 특이치로 표시됩니다.",
                    "tag": "4"
                },
                {
                    "index": "412-3",
                    "sentence": "Finally, the reprojected triangulations are used as new labeled training data to improve the detector.",
                    "sentence_kor": "마지막으로, 재투사된 삼각측정은 검출기를 개선하기 위한 새로운 라벨링 훈련 데이터로 사용된다.",
                    "tag": "4"
                },
                {
                    "index": "412-4",
                    "sentence": "We repeat this process, generating more labeled data in each iteration.",
                    "sentence_kor": "이 과정을 반복하여 각 반복에서 레이블링된 데이터를 더 많이 생성합니다.",
                    "tag": "4"
                },
                {
                    "index": "412-5",
                    "sentence": "We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector.",
                    "sentence_kor": "주어진 검출기에 대한 목표 참 및 거짓 양성률을 달성하기 위해 최소 조회 수와 관련된 결과를 분석적으로 도출한다.",
                    "tag": "4"
                },
                {
                    "index": "412-6",
                    "sentence": "The method is used to train a hand keypoint detector for single images.",
                    "sentence_kor": "이 방법은 단일 이미지에 대한 핸드 키포인트 검출기를 교육하는 데 사용됩니다.",
                    "tag": "4"
                },
                {
                    "index": "412-7",
                    "sentence": "The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors.",
                    "sentence_kor": "결과로 나온 키포인트 검출기는 RGB 영상에서 실시간으로 실행되며 깊이 센서를 사용하는 방법과 비슷한 정확도를 가지고 있다.",
                    "tag": "4"
                },
                {
                    "index": "412-8",
                    "sentence": "The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.",
                    "sentence_kor": "여러 뷰에 걸쳐 삼각형을 이루는 단일 뷰 디텍터는 복잡한 객체 상호 작용으로 3D 마커리스 손 동작 캡처를 가능하게 합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1012",
            "abstractID": "SPA_abs-413",
            "text": [
                {
                    "index": "413-0",
                    "sentence": "Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos.",
                    "sentence_kor": "실시간 다인용 2D 포즈 추정은 기계가 이미지와 비디오의 사람을 이해할 수 있게 하는 핵심 요소이다.",
                    "tag": "1"
                },
                {
                    "index": "413-1",
                    "sentence": "In this work, we present a realtime approach to detect the 2D pose of multiple people in an image.",
                    "sentence_kor": "이 작업에서는 이미지에서 여러 사람의 2D 포즈를 감지하는 실시간 접근 방식을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "413-2",
                    "sentence": "The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image.",
                    "sentence_kor": "제안된 방법은 신체 부위를 이미지의 개인과 연결하는 방법을 배우기 위해 우리가 PAF(Part Affinity Fields)라고 부르는 비모수 표현을 사용한다.",
                    "tag": "2+3"
                },
                {
                    "index": "413-3",
                    "sentence": "This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image.",
                    "sentence_kor": "이 상향식 시스템은 이미지의 인원수에 관계없이 높은 정확도와 실시간 성능을 달성합니다.",
                    "tag": "4"
                },
                {
                    "index": "413-4",
                    "sentence": "In previous work, PAFs and body part location estimation were refined simultaneously across training stages.",
                    "sentence_kor": "이전 연구에서는 PAF와 신체 부위 위치 추정을 교육 단계에서 동시에 개선하였다.",
                    "tag": "1"
                },
                {
                    "index": "413-5",
                    "sentence": "We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy.",
                    "sentence_kor": "우리는 PAF와 신체 부위 위치 미세화가 모두 아닌 PAF 전용 미세화가 런타임 성능과 정확성을 크게 향상시킨다는 것을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "413-6",
                    "sentence": "We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released.",
                    "sentence_kor": "또한 공개적으로 공개한 내부 주석 발 데이터 세트를 기반으로 최초의 결합 신체 및 발 키포인트 감지기를 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "413-7",
                    "sentence": "We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually.",
                    "sentence_kor": "결합된 검출기가 순차적으로 실행하는 것에 비해 추론 시간을 단축할 뿐만 아니라 각 구성 요소의 정확도를 개별적으로 유지한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "413-8",
                    "sentence": "This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.",
                    "sentence_kor": "이 작업은 신체, 발, 손 및 얼굴 키포인트를 포함한 다인용 2D 포즈 감지를 위한 최초의 오픈 소스 실시간 시스템인 OpenPose의 출시로 절정에 달했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1013",
            "abstractID": "SPA_abs-414",
            "text": [
                {
                    "index": "414-0",
                    "sentence": "Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.",
                    "sentence_kor": "포즈 머신은 풍부한 암묵적 공간 모델을 학습하기 위한 순차적 예측 프레임워크를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "414-1",
                    "sentence": "In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.",
                    "sentence_kor": "본 연구에서 우리는 포즈 추정 작업을 위해 이미지 특징 및 이미지 의존적 공간 모델을 학습하기 위해 컨볼루션 네트워크를 포즈 머신 프레임워크에 통합하는 방법에 대한 체계적인 설계를 보여준다.",
                    "tag": "2+3"
                },
                {
                    "index": "414-2",
                    "sentence": "The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.",
                    "sentence_kor": "본 논문의 기여는 관절형 포즈 추정과 같은 구조화된 예측 작업에서 변수 간의 장거리 의존성을 암시적으로 모델링하는 것이다.",
                    "tag": "3"
                },
                {
                    "index": "414-3",
                    "sentence": "We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.",
                    "sentence_kor": "우리는 명시적인 그래픽 모델 스타일 추론 없이 이전 단계의 신념 맵에서 직접 작동하는 컨볼루션 네트워크로 구성된 순차적 아키텍처를 설계하여 부품 위치에 대해 점점 더 정교한 추정치를 산출함으로써 이를 달성한다.",
                    "tag": "3"
                },
                {
                    "index": "414-4",
                    "sentence": "Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.",
                    "sentence_kor": "우리의 접근 방식은 중간 감독을 시행하는 자연스러운 학습 목표 기능을 제공하여 역 전파 그레이디언트를 보충하고 학습 절차를 조절함으로써 훈련 중 그레이디언트 소멸의 특성 어려움을 해결한다.",
                    "tag": "3+4"
                },
                {
                    "index": "414-5",
                    "sentence": "We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",
                    "sentence_kor": "우리는 MPII, LSP 및 FLIC 데이터 세트를 포함한 표준 벤치마크에서 최첨단 성능을 입증하고 경쟁 방법을 능가한다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "1014",
            "abstractID": "SPA_abs-415",
            "text": [
                {
                    "index": "415-0",
                    "sentence": "We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.",
                    "sentence_kor": "우리는 전체 모델이 제한된 메모리 양에 맞도록 텍스트 분류를 위한 소형 아키텍처를 생산하는 문제를 고려한다.",
                    "tag": "2"
                },
                {
                    "index": "415-1",
                    "sentence": "After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings.",
                    "sentence_kor": "해싱 문헌에서 영감을 얻은 다른 솔루션을 고려한 후, 우리는 단어 임베딩을 저장하기 위해 제품 정량화를 기반으로 하는 방법을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "415-2",
                    "sentence": "While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts.",
                    "sentence_kor": "원래의 기술은 정확도 상실로 이어지지만, 우리는 양자화 인공물을 우회하기 위해 이 방법을 채택한다.",
                    "tag": "4"
                },
                {
                    "index": "415-3",
                    "sentence": "Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy.",
                    "sentence_kor": "여러 벤치마크에 대해 수행된 우리의 실험은 우리의 접근 방식이 일반적으로 fastText보다 2배 적은 메모리를 요구하지만 정확도에 대해서는 약간 열등하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "415-4",
                    "sentence": "As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",
                    "sentence_kor": "그 결과, 메모리 사용과 정확도 사이의 절충이라는 측면에서 최첨단 기술을 능가한다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "1015",
            "abstractID": "SPA_abs-416",
            "text": [
                {
                    "index": "416-0",
                    "sentence": "In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks.",
                    "sentence_kor": "본 논문에서 우리는 이미지 생성 작업에 대한 주의 주도 장기 의존성 모델링을 허용하는 SAGAN(Self-Attentive Adversarial Network)을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "416-1",
                    "sentence": "Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps.",
                    "sentence_kor": "기존의 컨볼루션 GAN은 저해상도 기능 맵에서 공간적으로 로컬 포인트만 함수로 고해상도 세부 정보를 생성한다.",
                    "tag": "6"
                },
                {
                    "index": "416-2",
                    "sentence": "In SAGAN, details can be generated using cues from all feature locations.",
                    "sentence_kor": "SAGAN에서는 모든 형상 위치의 신호를 사용하여 세부 정보를 생성할 수 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "416-3",
                    "sentence": "Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.",
                    "sentence_kor": "또한 판별기는 이미지의 먼 부분에 있는 매우 상세한 형상이 서로 일치하는지 확인할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "416-4",
                    "sentence": "Furthermore, recent work has shown that generator conditioning affects GAN performance.",
                    "sentence_kor": "또한, 최근 연구는 발전기 조건화가 GAN 성능에 영향을 미친다는 것을 보여주었다.",
                    "tag": "4"
                },
                {
                    "index": "416-5",
                    "sentence": "Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics.",
                    "sentence_kor": "이 통찰력을 활용하여 GAN 생성기에 스펙트럼 정규화를 적용하고 이것이 훈련 역학을 개선한다는 것을 발견했다.",
                    "tag": "4"
                },
                {
                    "index": "416-6",
                    "sentence": "The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset.",
                    "sentence_kor": "제안된 SAGAN은 최첨단 결과를 달성하여 가장 잘 발표된 Inception 점수를 36.8에서 52.52로 높이고 까다로운 ImageNet 데이터 세트에서 프레셰 Inception 거리를 27.62에서 18.65로 줄인다.",
                    "tag": "4"
                },
                {
                    "index": "416-7",
                    "sentence": "Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.",
                    "sentence_kor": "주의 계층을 시각화하면 생성자가 고정된 모양의 로컬 영역보다는 개체 모양에 해당하는 이웃을 활용한다는 것을 알 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1016",
            "abstractID": "SPA_abs-417",
            "text": [
                {
                    "index": "417-0",
                    "sentence": "Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures.",
                    "sentence_kor": "유사성 검색은 이미지나 비디오와 같은 복잡한 데이터를 처리하는 전문 데이터베이스 시스템에서 응용 프로그램을 찾습니다. 이미지나 비디오는 일반적으로 고차원 기능으로 표현되며 특정 인덱싱 구조가 필요합니다.",
                    "tag": "1"
                },
                {
                    "index": "417-1",
                    "sentence": "This paper tackles the problem of better utilizing GPUs for this task.",
                    "sentence_kor": "이 논문은 이 작업에 GPU를 더 잘 활용하는 문제를 다룬다.",
                    "tag": "2"
                },
                {
                    "index": "417-2",
                    "sentence": "While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.",
                    "sentence_kor": "GPU는 데이터 병렬 작업에서 탁월하지만, 이전 접근 방식은 k-min 선택과 같이 병렬화가 덜 노출되거나 메모리 계층을 잘 사용하지 못하는 알고리즘에 의해 병목 현상이 발생한다.",
                    "tag": "3"
                },
                {
                    "index": "417-3",
                    "sentence": "We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art.",
                    "sentence_kor": "우리는 이전 GPU 상태보다 8.5배 빠른 가장 가까운 이웃 구현을 가능하게 하는 이론적 최고 성능의 최대 55%에서 작동하는 k-선택 설계를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "417-4",
                    "sentence": "We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization.",
                    "sentence_kor": "제품 양자화에 기초한 브루트 포스, 근사 및 압축 도메인 검색에 최적화된 설계를 제안하여 다양한 유사성 검색 시나리오에 적용한다.",
                    "tag": "3"
                },
                {
                    "index": "417-5",
                    "sentence": "In all these setups, we outperform the state of the art by large margins.",
                    "sentence_kor": "이러한 모든 설정에서 우리는 최첨단 기술을 큰 폭으로 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "417-6",
                    "sentence": "Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs.",
                    "sentence_kor": "우리의 구현을 통해 Yfcc100M 데이터 세트의 9500만 개의 이미지에 대해 35분 만에 높은 정확도의 k-NN 그래프를 구축하고 맥스웰 타이탄 X GPU 4개에서 12시간 이내에 10억 개의 벡터를 연결하는 그래프를 구성할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "417-7",
                    "sentence": "We have open-sourced our approach for the sake of comparison and reproducibility.",
                    "sentence_kor": "우리는 비교와 재현성을 위해 우리의 접근 방식을 개방했다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "1017",
            "abstractID": "SPA_abs-418",
            "text": [
                {
                    "index": "418-0",
                    "sentence": "This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs.",
                    "sentence_kor": "본 논문은 대상 마르코프 의사결정 프로세스(MDP)를 소규모 MDP의 계층 구조로 분해하고 대상 MDP의 값 함수를 소규모 MDP의 값 함수의 부가적 조합으로 분해하는 것에 기초한 계층적 강화 학습에 대한 MAXQ 접근방식을 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "418-1",
                    "sentence": "The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions.",
                    "sentence_kor": "이 논문은 MAXQ 계층을 정의하고, 표현력에 대한 공식적인 결과를 입증하며, 상태 추상화의 안전한 사용을 위한 5가지 조건을 설정한다.",
                    "tag": "2+3"
                },
                {
                    "index": "418-2",
                    "sentence": "The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction.",
                    "sentence_kor": "이 논문은 온라인 모델 없는 학습 알고리즘 MAXQ-Q를 제시하고, 5가지 종류의 상태 추상화가 존재하는 경우에도 wih 확률 1을 재귀적으로 최적의 정책으로 알려진 일종의 국소 최적 정책으로 수렴한다는 것을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "418-3",
                    "sentence": "The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning.",
                    "sentence_kor": "이 논문은 세 가지 영역에서 일련의 실험을 통해 MAXQ 표현과 MAXQ-Q를 평가하고 MAXQ-Q(상태 추상화 포함)가 플랫 Q 학습보다 훨씬 빠르게 재귀적으로 최적의 정책으로 수렴된다는 것을 실험적으로 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "418-4",
                    "sentence": "The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration.",
                    "sentence_kor": "MAXQ가 가치 함수의 표현을 학습한다는 사실은 중요한 이점이 있다. 그것은 정책 반복의 정책 개선 단계와 유사한 절차를 통해 개선된 비계층적 정책을 계산하고 실행할 수 있게 해준다.",
                    "tag": "4+5"
                },
                {
                    "index": "418-5",
                    "sentence": "The paper demonstrates the effectiveness of this non-hierarchical execution experimentally.",
                    "sentence_kor": "이 논문은 이 비계층적 실행의 효과를 실험적으로 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "418-6",
                    "sentence": "Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.",
                    "sentence_kor": "마지막으로, 이 논문은 관련 작업과의 비교와 계층적 강화 학습의 설계 절충에 대한 논의로 마무리된다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1018",
            "abstractID": "SPA_abs-419",
            "text": [
                {
                    "index": "419-0",
                    "sentence": "We present the first massively distributed architecture for deep reinforcement learning.",
                    "sentence_kor": "심층 강화 학습을 위한 최초의 대규모 분산 아키텍처를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "419-1",
                    "sentence": "This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience.",
                    "sentence_kor": "이 아키텍처는 네 가지 주요 구성 요소, 즉 새로운 행동을 생성하는 병렬 행위자, 저장된 경험으로 훈련되는 병렬 학습자, 가치 함수 또는 행동 정책을 나타내는 분산 신경 네트워크 및 분산 경험 저장소를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "419-2",
                    "sentence": "We used our architecture to implement the Deep Q-Network algorithm (DQN).",
                    "sentence_kor": "우리는 우리의 아키텍처를 사용하여 심층 Q-네트워크 알고리즘(DQN)을 구현했다.",
                    "tag": "1"
                },
                {
                    "index": "419-3",
                    "sentence": "Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters.",
                    "sentence_kor": "우리의 분산 알고리즘은 동일한 하이퍼 파라미터를 사용하여 아케이드 학습 환경의 아타리 2600 게임에서 49개의 게임에 적용되었습니다.",
                    "tag": "1"
                },
                {
                    "index": "419-4",
                    "sentence": "Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.",
                    "sentence_kor": "우리의 성능은 49개의 게임 중 41개에서 비분산 DQN을 능가했고 또한 대부분의 게임에서 이러한 결과를 달성하는 데 필요한 벽 시간을 몇 배나 줄였다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1019",
            "abstractID": "SPA_abs-420",
            "text": [
                {
                    "index": "420-0",
                    "sentence": "In this paper, we consider the task of learning control policies for text-based games.",
                    "sentence_kor": "본 논문에서 우리는 텍스트 기반 게임에 대한 학습 제어 정책을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "420-1",
                    "sentence": "In these games, all interactions in the virtual world are through text and the underlying state is not observed.",
                    "sentence_kor": "이러한 게임에서 가상 세계의 모든 상호 작용은 텍스트를 통해 이루어지며 기본 상태는 관찰되지 않습니다.",
                    "tag": "1"
                },
                {
                    "index": "420-2",
                    "sentence": "The resulting language barrier makes such environments challenging for automatic game players.",
                    "sentence_kor": "결과적으로 언어 장벽은 자동 게임 플레이어들에게 그러한 환경을 어렵게 만든다.",
                    "tag": "1"
                },
                {
                    "index": "420-3",
                    "sentence": "We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback.",
                    "sentence_kor": "우리는 게임 보상을 피드백으로 사용하여 상태 표현과 행동 정책을 공동으로 학습하기 위해 심층 강화 학습 프레임워크를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "420-4",
                    "sentence": "This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.",
                    "sentence_kor": "이 프레임워크는 텍스트 설명을 게임 상태의 의미를 포착하는 벡터 표현으로 매핑할 수 있게 해준다.",
                    "tag": "1"
                },
                {
                    "index": "420-5",
                    "sentence": "We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations.",
                    "sentence_kor": "우리는 상태 표현을 위해 단어 가방과 빅램 가방을 사용하여 기준선과 비교하여 두 게임 세계에 대한 우리의 접근 방식을 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "420-6",
                    "sentence": "Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.",
                    "sentence_kor": "우리의 알고리즘은 표현 학습의 중요성을 입증하는 두 세계 모두에서 기준선을 능가한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1020",
            "abstractID": "SPA_abs-421",
            "text": [
                {
                    "index": "421-0",
                    "sentence": "Deep Reinforcement Learning has yielded proficient controllers for complex tasks.",
                    "sentence_kor": "심층 강화 학습은 복잡한 작업에 능숙한 제어자를 제공했다.",
                    "tag": "1"
                },
                {
                    "index": "421-1",
                    "sentence": "However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point.",
                    "sentence_kor": "그러나 이러한 컨트롤러는 메모리가 제한되어 있으며 각 결정 지점에서 완전한 게임 화면을 인식할 수 있어야 합니다.",
                    "tag": "1+2"
                },
                {
                    "index": "421-2",
                    "sentence": "To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM.",
                    "sentence_kor": "이러한 단점을 해결하기 위해 본 논문은 첫 번째 후 컨볼루션 완전 연결 레이어를 반복 LSTM으로 교체하여 심층 Q 네트워크(DQN)에 재발성을 추가하는 효과를 조사한다.",
                    "tag": "2+3"
                },
                {
                    "index": "421-3",
                    "sentence": "The resulting Deep Recurrent Q-Network(DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens.",
                    "sentence_kor": "그 결과 DRQN(Deep Recurrent Q-Network)은 각 시간 단계에서 단일 프레임만 볼 수 있지만 시간을 통한 정보를 성공적으로 통합하고 표준 아타리 게임에서 DQN의 성능과 깜박이는 게임 화면을 특징으로 하는 부분적으로 관찰된 등가물을 복제한다.",
                    "tag": "4+5"
                },
                {
                    "index": "421-4",
                    "sentence": "Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability.",
                    "sentence_kor": "또한 부분 관측치로 훈련하고 점진적으로 더 완전한 관측치로 평가할 때 DRQN의 성능은 관측 가능성의 함수로 확장된다.",
                    "tag": "5"
                },
                {
                    "index": "421-5",
                    "sentence": "Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's.",
                    "sentence_kor": "반대로, 전체 관찰로 훈련하고 부분 관찰로 평가할 때 DRQN의 성능은 DQN보다 낮게 저하된다.",
                    "tag": "5"
                },
                {
                    "index": "421-6",
                    "sentence": "Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.",
                    "sentence_kor": "따라서 동일한 길이의 이력을 감안할 때, 재발성은 DQN의 입력 계층에 프레임 역사를 쌓는 것에 대한 실행 가능한 대안이며, 재발성은 게임을 배울 때 체계적인 이점을 주지 않지만, 관찰의 질이 변경되면 평가 시간에 재발 네트가 더 잘 적응할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1021",
            "abstractID": "SPA_abs-422",
            "text": [
                {
                    "index": "422-0",
                    "sentence": "This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer.",
                    "sentence_kor": "이 보고서는 프로그래머가 제공한 최소한의 수공예 지식으로 모든 도메인별 지식을 발견하기 위해 자가 플레이를 사용하는 체스 엔진인 기린을 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "422-1",
                    "sentence": "Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition.",
                    "sentence_kor": "기계 학습을 사용하여 수작업 평가 기능에 대한 매개 변수 조정만 수행하는 이전의 시도와 달리, 기린의 학습 시스템은 자동 기능 추출 및 패턴 인식도 수행한다.",
                    "tag": "4"
                },
                {
                    "index": "422-2",
                    "sentence": "The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters.",
                    "sentence_kor": "훈련된 평가 기능은 최첨단 체스 엔진의 평가 기능과 비슷한 성능을 발휘합니다. 이 모든 기능에는 컴퓨터 체스 전문가와 인간 체스 마스터가 수년간 튜닝한 수공예 패턴 인식기 수천 줄이 포함되어 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "422-3",
                    "sentence": "Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.",
                    "sentence_kor": "기린은 엔드 투 엔드 기계 학습을 체스 게임에 사용하는 가장 성공적인 시도이다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "1022",
            "abstractID": "SPA_abs-423",
            "text": [
                {
                    "index": "423-0",
                    "sentence": "This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only.",
                    "sentence_kor": "본 논문은 시각적 인식만으로 로봇 조작기를 제어하는 기계 학습 기반 시스템을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "423-1",
                    "sentence": "The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time.",
                    "sentence_kor": "구성에 대한 사전 지식 없이 원시 픽셀 이미지에서만 로봇 컨트롤러를 자율적으로 학습할 수 있는 기능이 처음으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "423-2",
                    "sentence": "We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation.",
                    "sentence_kor": "우리는 최근 심층 강화 학습의 성공을 기반으로 외부 시각 관찰을 사용하여 3관절 로봇 조작기로 목표 도달 학습 시스템을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "423-3",
                    "sentence": "A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation.",
                    "sentence_kor": "시뮬레이션 훈련 후 목표 도달 수행을 위해 심층 Q 네트워크(DQN)가 시연되었다.",
                    "tag": "2+3"
                },
                {
                    "index": "423-4",
                    "sentence": "Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.",
                    "sentence_kor": "네트워크를 실제 하드웨어로 전송하고 순진한 방식으로 관찰하는 것은 실패했지만, 카메라 이미지를 합성 이미지로 대체할 때 네트워크가 작동한다는 것이 실험 결과 밝혀졌다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1023",
            "abstractID": "SPA_abs-424",
            "text": [
                {
                    "index": "424-0",
                    "sentence": "Experience replay lets online reinforcement learning agents remember and reuse experiences from the past.",
                    "sentence_kor": "온라인 강화 학습 에이전트는 경험 재생을 통해 과거의 경험을 기억하고 재사용할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "424-1",
                    "sentence": "In prior work, experience transitions were uniformly sampled from a replay memory.",
                    "sentence_kor": "이전 작업에서 경험 전환은 재생 메모리에서 균일하게 샘플링되었습니다.",
                    "tag": "1"
                },
                {
                    "index": "424-2",
                    "sentence": "However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance.",
                    "sentence_kor": "그러나 이 접근 방식은 중요도에 관계없이 원래 경험했던 것과 동일한 빈도로 전환을 재생합니다.",
                    "tag": "1"
                },
                {
                    "index": "424-3",
                    "sentence": "In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently.",
                    "sentence_kor": "본 논문에서 우리는 중요한 전환을 더 자주 재생하여 보다 효율적으로 학습할 수 있도록 경험의 우선순위를 정하는 프레임워크를 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "424-4",
                    "sentence": "We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games.",
                    "sentence_kor": "우리는 많은 Atari 게임에서 인간 수준의 성능을 달성한 강화 학습 알고리즘인 Deep Q-Networks(DQN)에서 우선 순위가 지정된 경험 재생을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "424-5",
                    "sentence": "DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.",
                    "sentence_kor": "경험 재생 우선 순위를 매긴 DQN은 49개 게임 중 41개 게임에서 균일한 재생을 통해 새로운 최첨단 DQN을 능가합니다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "1024",
            "abstractID": "SPA_abs-425",
            "text": [
                {
                    "index": "425-0",
                    "sentence": "In recent years there have been many successes of using deep representations in reinforcement learning.",
                    "sentence_kor": "최근 몇 년 동안 강화 학습에 심층 표현을 사용하는 데 많은 성공이 있었다.",
                    "tag": "1"
                },
                {
                    "index": "425-1",
                    "sentence": "Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders.",
                    "sentence_kor": "그럼에도 불구하고 이러한 애플리케이션 중 다수는 컨볼루션 네트워크, LSTM 또는 자동 인코더와 같은 기존 아키텍처를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "425-2",
                    "sentence": "In this paper, we present a new neural network architecture for model-free reinforcement learning.",
                    "sentence_kor": "본 논문에서는 모델 없는 강화 학습을 위한 새로운 신경망 아키텍처를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "425-3",
                    "sentence": "Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function.",
                    "sentence_kor": "우리의 결투 네트워크는 두 개의 개별 추정기, 즉 하나는 상태 가치 함수에 대한 추정기, 하나는 상태 의존적 조치 이점 함수에 대한 추정기를 나타낸다.",
                    "tag": "2"
                },
                {
                    "index": "425-4",
                    "sentence": "The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm.",
                    "sentence_kor": "이 인수 분해의 주요 이점은 기본 강화 학습 알고리즘에 어떠한 변화도 가하지 않고 행동에 걸쳐 학습을 일반화하는 것이다.",
                    "tag": "5"
                },
                {
                    "index": "425-5",
                    "sentence": "Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions.",
                    "sentence_kor": "우리의 결과는 이 아키텍처가 많은 유사한 가치의 조치가 있을 때 더 나은 정책 평가로 이어진다는 것을 보여준다.",
                    "tag": "5"
                },
                {
                    "index": "425-6",
                    "sentence": "Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.",
                    "sentence_kor": "또한 결투 아키텍처를 통해 RL 에이전트는 Atari 2600 도메인에서 최첨단 기술을 능가할 수 있습니다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "1025",
            "abstractID": "SPA_abs-426",
            "text": [
                {
                    "index": "426-0",
                    "sentence": "Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity.",
                    "sentence_kor": "심층 신경망을 강화 학습 과제를 위한 함수 근사치로 사용하는 것은 최근 실제 복잡성에 접근하는 문제를 해결하는 데 매우 강력한 것으로 나타났다.",
                    "tag": "1"
                },
                {
                    "index": "426-1",
                    "sentence": "Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN).",
                    "sentence_kor": "이러한 결과를 벤치마크로 사용하여 심층 Q 네트워크(DQN)의 학습 프로세스 품질에서 할인 요인이 할 수 있는 역할에 대해 논의한다.",
                    "tag": "2+3"
                },
                {
                    "index": "426-2",
                    "sentence": "When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps.",
                    "sentence_kor": "할인 요소가 최종 값까지 점진적으로 증가하면 학습 단계의 수를 크게 줄일 수 있음을 경험적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "426-3",
                    "sentence": "When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments.",
                    "sentence_kor": "다양한 학습 속도와 함께 사용할 경우 여러 실험에서 원래의 DQN을 능가한다는 것을 경험적으로 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "426-4",
                    "sentence": "We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting.",
                    "sentence_kor": "우리는 이 현상을 대략적인 동적 프로그래밍 설정에서 사용될 때 신경망의 불안정성과 연관시킨다.",
                    "tag": "4"
                },
                {
                    "index": "426-5",
                    "sentence": "We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.",
                    "sentence_kor": "우리는 또한 학습 과정 중에 국지적 최적 상태에 빠질 수 있는 가능성을 설명하고, 따라서 우리의 논의를 탐사/탐색 딜레마와 연결시킨다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1026",
            "abstractID": "SPA_abs-427",
            "text": [
                {
                    "index": "427-0",
                    "sentence": "A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels.",
                    "sentence_kor": "강화 학습에 대한 딥 러닝 접근법은 일반 학습자가 인간과 초인적인 수준에서 다양한 오락실 게임을 플레이하기 위해 시각 입력을 훈련시킬 수 있게 했다.",
                    "tag": "1"
                },
                {
                    "index": "427-1",
                    "sentence": "Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN).",
                    "sentence_kor": "구글 딥마인드 팀의 설립자들은 이 접근 방식을 딥 Q 네트워크라고 불렀다.",
                    "tag": "1"
                },
                {
                    "index": "427-2",
                    "sentence": "We present an extension of DQN by \"soft\" and \"hard\" attention mechanisms.",
                    "sentence_kor": "우리는 \"소프트\" 및 \"하드\" 주의 메커니즘에 의한 DQN의 확장을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "427-3",
                    "sentence": "Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN.",
                    "sentence_kor": "여러 Atari 2600 게임에서 제안된 DARQN(Deep Attention Recurrent Q-Network) 알고리즘의 테스트는 DQN보다 우수한 성능 수준을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "427-4",
                    "sentence": "Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.",
                    "sentence_kor": "또한, 내장된 주의 메커니즘은 에이전트가 결정을 내릴 때 초점을 맞추는 게임 화면의 영역을 강조함으로써 훈련 과정을 온라인상에서 직접 모니터링할 수 있게 한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1027",
            "abstractID": "SPA_abs-428",
            "text": [
                {
                    "index": "428-0",
                    "sentence": "Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance.",
                    "sentence_kor": "복잡한 시각 작업에 대한 정책은 심층 Q 네트워크(DQN)라는 접근 방식을 사용하여 심층 강화 학습을 통해 성공적으로 학습되었지만, 우수한 성과를 달성하려면 상대적으로 큰 (과제별) 네트워크와 광범위한 훈련이 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "428-1",
                    "sentence": "In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient.",
                    "sentence_kor": "본 연구에서는 정책 증류라는 새로운 방법을 제시하는데, 이를 통해 강화 학습제의 정책을 추출하고 전문가 수준에서 수행하는 동시에 훨씬 더 효율적이고 새로운 네트워크를 훈련시킬 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "428-2",
                    "sentence": "Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy.",
                    "sentence_kor": "또한 동일한 방법을 사용하여 여러 작업별 정책을 단일 정책으로 통합할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "428-3",
                    "sentence": "We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.",
                    "sentence_kor": "우리는 Atari 도메인을 사용하여 이러한 주장을 입증하고 다중 작업 증류제가 단일 작업 교사뿐만 아니라 공동 훈련된 DQN 에이전트를 능가한다는 것을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1028",
            "abstractID": "SPA_abs-429",
            "text": [
                {
                    "index": "429-0",
                    "sentence": "In recent years there is a growing interest in using deep representations for reinforcement learning.",
                    "sentence_kor": "최근 몇 년 동안 강화 학습을 위해 심층 표현을 사용하는 것에 대한 관심이 증가하고 있다.",
                    "tag": "1"
                },
                {
                    "index": "429-1",
                    "sentence": "In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter.",
                    "sentence_kor": "본 논문에서, 우리는 맹목적이지 않은 문제에서 심층 Q 네트워크(DQN)를 분석하는 방법론과 도구를 제시한다.",
                    "tag": "2"
                },
                {
                    "index": "429-2",
                    "sentence": "Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically.",
                    "sentence_kor": "또한, 우리는 새로운 모델인 SAMDP(Semi Aggregated Markov Decision Process)와 이를 자동으로 학습하는 알고리즘을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "429-3",
                    "sentence": "The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work.",
                    "sentence_kor": "SAMDP 모델을 사용하면 특징에서 직접 시공간 추상화를 식별할 수 있으며 향후 연구에서 하위 목표 검출기로 사용될 수 있다.",
                    "tag": "6"
                },
                {
                    "index": "429-4",
                    "sentence": "Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success.",
                    "sentence_kor": "도구를 사용하여 DQN이 학습한 기능이 상태 공간을 계층적 방식으로 집계하여 성공을 설명한다는 것을 밝혀냈다.",
                    "tag": "4"
                },
                {
                    "index": "429-5",
                    "sentence": "Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.",
                    "sentence_kor": "또한, 우리는 세 가지 다른 Atari2600 게임에 대해 DQN이 학습한 정책을 이해하고 설명할 수 있으며 강화 학습에서 심층 신경망을 해석, 디버깅 및 최적화하는 방법을 제안할 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1029",
            "abstractID": "SPA_abs-430",
            "text": [
                {
                    "index": "430-0",
                    "sentence": "Many real-world applications can be described as large-scale games of imperfect information.",
                    "sentence_kor": "많은 실제 애플리케이션은 불완전한 정보의 대규모 게임으로 설명될 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "430-1",
                    "sentence": "To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain.",
                    "sentence_kor": "이러한 도전적인 도메인을 다루기 위해 이전 작업은 도메인의 수작업으로 추상화한 내쉬 평형을 계산하는 데 초점을 맞추었다.",
                    "tag": "1"
                },
                {
                    "index": "430-2",
                    "sentence": "In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge.",
                    "sentence_kor": "본 논문에서 우리는 사전 도메인 지식 없이 대략적인 내쉬 평형을 학습하기 위한 최초의 확장 가능한 엔드 투 엔드 접근 방식을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "430-3",
                    "sentence": "Our method combines fictitious self-play with deep reinforcement learning.",
                    "sentence_kor": "우리의 방법은 가상의 자기 놀이와 심층 강화 학습을 결합한다.",
                    "tag": "3"
                },
                {
                    "index": "430-4",
                    "sentence": "When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged.",
                    "sentence_kor": "Leduc 포커에 적용되었을 때, NFSP(Neural Fictional Self-Play)는 내쉬 평형에 접근한 반면, 일반적인 강화 학습 방법은 분기되었다.",
                    "tag": "4"
                },
                {
                    "index": "430-5",
                    "sentence": "In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.",
                    "sentence_kor": "실제 규모의 포커 게임인 Limit Texas Holdem에서 NFSP는 상당한 도메인 전문지식을 바탕으로 최첨단 초인간 알고리즘의 성능에 접근하는 전략을 배웠다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1030",
            "abstractID": "SPA_abs-431",
            "text": [
                {
                    "index": "431-0",
                    "sentence": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement.",
                    "sentence_kor": "우리는 보장된 단조로운 개선을 통해 정책을 최적화하기 위한 반복적인 절차를 설명한다.",
                    "tag": "1"
                },
                {
                    "index": "431-1",
                    "sentence": "By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO).",
                    "sentence_kor": "이론적으로 정당화된 절차에 대해 몇 가지 근사치를 만들어 신뢰 영역 정책 최적화(TRPO)라는 실용적인 알고리즘을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "431-2",
                    "sentence": "This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks.",
                    "sentence_kor": "이 알고리즘은 자연 정책 기울기 방법과 유사하며 신경망과 같은 대규모 비선형 정책을 최적화하는 데 효과적이다.",
                    "tag": "4"
                },
                {
                    "index": "431-3",
                    "sentence": "Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input.",
                    "sentence_kor": "우리의 실험은 시뮬레이션된 로봇 수영, 깡충깡충 뛰기 및 걸음걸이 학습, 화면 이미지를 입력으로 사용하는 아타리 게임 등 다양한 작업에서 강력한 성능을 입증한다.",
                    "tag": "4"
                },
                {
                    "index": "431-4",
                    "sentence": "Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.",
                    "sentence_kor": "이론에서 벗어나는 근사치에도 불구하고 TRPO는 하이퍼 파라미터 튜닝이 거의 없이 단조로운 개선을 제공하는 경향이 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1031",
            "abstractID": "SPA_abs-432",
            "text": [
                {
                    "index": "432-0",
                    "sentence": "Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters.",
                    "sentence_kor": "모델 예측 제어(MPC)는 로봇 시스템, 특히 쿼드콥터와 같은 자율 항공 차량을 제어하는 효과적인 방법이다.",
                    "tag": "1"
                },
                {
                    "index": "432-1",
                    "sentence": "However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments.",
                    "sentence_kor": "그러나 MPC의 적용은 계산적으로 까다로울 수 있으며, 일반적으로 시스템 상태를 추정해야 하며, 이는 복잡하고 구조화되지 않은 환경에서 어려울 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "432-2",
                    "sentence": "Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found.",
                    "sentence_kor": "강화 학습은 원칙적으로 명시적 상태 추정의 필요성을 포기하고 센서 판독값을 동작에 직접 매핑하는 정책을 획득할 수 있지만 효과적인 정책이 발견되기 전에 훈련 중에 치명적인 고장을 일으킬 수 있는 불안정한 시스템에 적용하기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "432-3",
                    "sentence": "We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment.",
                    "sentence_kor": "우리는 MPC가 훈련 시 데이터를 생성하는 데 사용되는 안내 정책 검색 프레임워크에서 계측된 훈련 환경에서 제공하는 전체 상태 관찰 하에 MPC와 강화 학습을 결합할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "432-4",
                    "sentence": "This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors.",
                    "sentence_kor": "이 데이터는 차량의 내장 센서에서 원시 관찰에만 액세스할 수 있는 심층 신경망 정책을 훈련하는 데 사용됩니다.",
                    "tag": "4"
                },
                {
                    "index": "432-5",
                    "sentence": "After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC.",
                    "sentence_kor": "훈련 후, 신경 네트워크 정책은 MPC의 계산 비용의 일부만으로 전체 상태에 대한 지식 없이 로봇을 성공적으로 제어할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "432-6",
                    "sentence": "We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.",
                    "sentence_kor": "시뮬레이션된 온보드 센서를 사용하고 테스트 시 명시적 상태 추정 없이 시뮬레이션된 쿼드로터의 장애물 회피 정책을 학습하여 방법을 평가한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1032",
            "abstractID": "SPA_abs-433",
            "text": [
                {
                    "index": "433-0",
                    "sentence": "Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks.",
                    "sentence_kor": "정책 기울기 방법은 누적 보상을 직접 최적화하고 신경망과 같은 비선형 함수 근사치에 직접적으로 사용될 수 있기 때문에 강화 학습에서 매력적인 접근법이다.",
                    "tag": "1"
                },
                {
                    "index": "433-1",
                    "sentence": "The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data.",
                    "sentence_kor": "두 가지 주요 과제는 일반적으로 필요한 샘플의 수와 들어오는 데이터의 비정상성에도 불구하고 안정적이고 꾸준한 개선을 얻는 어려움이다.",
                    "tag": "2"
                },
                {
                    "index": "433-2",
                    "sentence": "We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).",
                    "sentence_kor": "우리는 TD(람다)와 유사한 장점 함수의 지수 가중 추정기를 사용하여 일부 편향을 희생하여 정책 경사도 추정치의 분산을 실질적으로 줄이기 위해 값 함수를 사용하여 첫 번째 과제를 해결한다.",
                    "tag": "3"
                },
                {
                    "index": "433-3",
                    "sentence": "We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.",
                    "sentence_kor": "우리는 신경망에 의해 표현되는 정책과 가치 함수 모두에 대해 신뢰 영역 최적화 절차를 사용하여 두 번째 과제를 해결한다.",
                    "tag": "3"
                },
                {
                    "index": "433-4",
                    "sentence": "Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground.",
                    "sentence_kor": "우리의 접근 방식은 매우 도전적인 3D 이동 작업, 두 발로 걷는 로봇과 네 발로 걷는 로봇을 위한 러닝 보트를 배우고, 두 발로 걷는 사람이 땅바닥에 누워 시작할 때부터 일어설 수 있는 정책을 학습하는 것에 대한 강력한 경험적 결과를 산출한다.",
                    "tag": "4"
                },
                {
                    "index": "433-5",
                    "sentence": "In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques.",
                    "sentence_kor": "수작업으로 만들어진 정책 표현을 사용하는 이전 작업과 대조적으로, 우리의 신경망 정책은 원시 운동학에서 공동 토크로 직접 매핑된다.",
                    "tag": "4"
                },
                {
                    "index": "433-6",
                    "sentence": "Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",
                    "sentence_kor": "우리의 알고리즘은 완전히 모델이 없으며, 3D 바이피드의 학습 작업에 필요한 시뮬레이션 경험의 양은 실시간 1-2주에 해당한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1033",
            "abstractID": "SPA_abs-434",
            "text": [
                {
                    "index": "434-0",
                    "sentence": "This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation.",
                    "sentence_kor": "본 논문은 호환되는 함수 근사치를 가진 연속 정책을 위한 심층 강화 학습 알고리즘인 GProp을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "434-1",
                    "sentence": "The algorithm is based on two innovations.",
                    "sentence_kor": "이 알고리즘은 두 가지 혁신에 기초한다.",
                    "tag": "3"
                },
                {
                    "index": "434-2",
                    "sentence": "Firstly, we present a temporal-difference based method for learning the gradient of the value-function.",
                    "sentence_kor": "첫째, 가치 함수의 기울기를 학습하기 위한 시간적 차이 기반 방법을 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "434-3",
                    "sentence": "Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively.",
                    "sentence_kor": "둘째, 우리는 가치 함수, 그 기울기를 추정하고 각각 행위자의 정책을 결정하는 세 개의 신경망을 구성하는 편차자-행위자-비판(DAC) 모델을 제시한다.",
                    "tag": "4"
                },
                {
                    "index": "434-4",
                    "sentence": "We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark.",
                    "sentence_kor": "우리는 두 가지 어려운 작업에 대해 GProp을 평가한다. 즉, 기울기를 정확하게 추정하는 강화 학습 알고리즘의 능력을 조사하도록 설계된 비모수 회귀 데이터 세트로 구성된 상황별 밴디트 문제와 도전적인 강화 학습 벤치마크인 문어 팔이다.",
                    "tag": "4"
                },
                {
                    "index": "434-5",
                    "sentence": "GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.",
                    "sentence_kor": "GProp은 밴디트 작업에 대한 철저한 감독 방식으로 경쟁력을 갖추고 있으며 문어 팔에서 지금까지 최고의 성능을 발휘합니다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "1034",
            "abstractID": "SPA_abs-435",
            "text": [
                {
                    "index": "435-0",
                    "sentence": "Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces.",
                    "sentence_kor": "최근 연구는 심층 신경망이 연속 상태 및 작업 공간을 특징으로 하는 강화 학습 영역에서 가치 기능과 정책을 모두 근사할 수 있다는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "435-1",
                    "sentence": "However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces.",
                    "sentence_kor": "그러나 우리가 아는 한 구조화된(매개변수화된) 연속 동작 공간에서 심층 신경망을 사용하는 데 성공한 이전 연구는 없다.",
                    "tag": "1"
                },
                {
                    "index": "435-2",
                    "sentence": "To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables.",
                    "sentence_kor": "이 공백을 메우기 위해 본 논문은 시뮬레이션된 로보컵 축구의 영역 내에서 학습하는 데 초점을 맞추고 있으며, 이 영역에는 각 동작 유형이 연속 변수로 매개 변수화되어 있다.",
                    "tag": "2+3"
                },
                {
                    "index": "435-3",
                    "sentence": "The best learned agent can score goals more reliably than the 2012 RoboCup champion agent.",
                    "sentence_kor": "가장 잘 배운 에이전트는 2012 로보컵 챔피언 에이전트보다 더 안정적으로 골을 넣을 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "435-4",
                    "sentence": "As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.",
                    "sentence_kor": "이와 같이, 본 논문은 매개 변수화된 행동 공간 MDP 클래스로의 심층 강화 학습의 성공적인 확장을 나타낸다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1035",
            "abstractID": "SPA_abs-436",
            "text": [
                {
                    "index": "436-0",
                    "sentence": "Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning.",
                    "sentence_kor": "복잡한 영역에서 효율적이고 확장 가능한 탐색을 달성하는 것은 강화 학습에서 주요 과제를 제기한다.",
                    "tag": "1"
                },
                {
                    "index": "436-1",
                    "sentence": "While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space.",
                    "sentence_kor": "탐색 문제에 대한 베이지안 및 PAC-MDP 접근방식은 강력한 공식 보장을 제공하지만 상태-조치 공간 열거에 대한 의존성 때문에 더 높은 차원에서는 비실용적인 경우가 많다.",
                    "tag": "1"
                },
                {
                    "index": "436-2",
                    "sentence": "Hence, exploration in complex domains is often performed with simple epsilon-greedy methods.",
                    "sentence_kor": "따라서 복잡한 도메인에서의 탐사는 종종 단순한 엡실론 탐방법으로 수행된다.",
                    "tag": "1"
                },
                {
                    "index": "436-3",
                    "sentence": "In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards.",
                    "sentence_kor": "본 논문에서는 원시 픽셀 입력과 지연 보상을 처리해야 하는 까다로운 Atari 게임 도메인을 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "436-4",
                    "sentence": "We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics.",
                    "sentence_kor": "우리는 톰슨 샘플링과 볼츠만 탐사를 포함한 몇 가지 더 정교한 탐사 전략을 평가하고 시스템 역학의 동시에 학습된 모델에서 탐사 보너스를 할당한 새로운 탐사 방법을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "436-5",
                    "sentence": "By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces.",
                    "sentence_kor": "학습된 모델을 신경망을 사용하여 매개 변수화함으로써 복잡한 고차원 상태 공간이 있는 작업에 적용할 수 있는 확장 가능하고 효율적인 탐색 보너스 접근 방식을 개발할 수 있다.",
                    "tag": "2"
                },
                {
                    "index": "436-6",
                    "sentence": "In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods.",
                    "sentence_kor": "Atari 도메인에서, 우리의 방법은 이전 방법에서 주요 과제를 제기하는 다양한 게임에서 가장 일관된 개선을 제공한다.",
                    "tag": "4"
                },
                {
                    "index": "436-7",
                    "sentence": "In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.",
                    "sentence_kor": "원시 게임 점수 외에도, 우리는 이 벤치마크에 대한 탐구의 영향을 평가하기 위해 ATARI 학습 도메인에 대한 AUC-100 메트릭도 개발한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "1036",
            "abstractID": "SPA_abs-437",
            "text": [
                {
                    "index": "437-0",
                    "sentence": "Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames.",
                    "sentence_kor": "비전 기반 강화 학습(RL) 문제, 특히 최근 벤치마크 아라케이드 학습 환경(ALE)의 아타리 게임에 의해 동기 부여되어 미래(이미지) 프레임이 이전 프레임뿐만 아니라 제어 변수나 동작에 의존하는 시공간 예측 문제를 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "437-1",
                    "sentence": "While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability.",
                    "sentence_kor": "자연 장면으로 구성되지는 않았지만, 아타리 게임의 프레임은 크기가 고차원적이며, 하나 이상의 객체가 직접 동작에 의해 제어되고 간접적으로 영향을 받는 많은 다른 객체와 함께 수십 개의 객체가 포함될 수 있으며, 객체의 진입과 이탈을 포함할 수 있으며, 깊은 부분적 관찰성을 포함할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "437-2",
                    "sentence": "We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks.",
                    "sentence_kor": "우리는 컨볼루션 신경망과 반복 신경망을 기반으로 인코딩, 동작 조건 변환 및 디코딩 계층으로 구성된 두 개의 심층 신경망 아키텍처를 제안하고 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "437-3",
                    "sentence": "Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games.",
                    "sentence_kor": "실험 결과는 제안된 아키텍처가 일부 게임에서 약 100단계 액션 조건 미래를 제어하는 데도 유용한 시각적 현실적 프레임을 생성할 수 있다는 것을 보여준다.",
                    "tag": "1"
                },
                {
                    "index": "437-4",
                    "sentence": "To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.",
                    "sentence_kor": "우리가 아는 한, 이 논문은 제어 입력에 의해 조건화된 고차원 비디오에 대한 장기 예측을 수행하고 평가하는 첫 번째 논문이다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1037",
            "abstractID": "SPA_abs-438",
            "text": [
                {
                    "index": "438-0",
                    "sentence": "Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems.",
                    "sentence_kor": "매우 고차원적인 관찰을 사용하는 연속 상태-행동 공간의 데이터 효율적인 강화 학습(RL)은 완전 자율 시스템 개발의 핵심 과제로 남아있다.",
                    "tag": "1"
                },
                {
                    "index": "438-1",
                    "sentence": "We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy (\"torques\") from pixel information only.",
                    "sentence_kor": "우리는 RL 에이전트가 픽셀 정보에서만 폐쇄 루프 제어 정책(\"토크\")을 학습하는 이 난제의 특히 중요한 사례인 픽셀 대 토크 문제를 고려한다.",
                    "tag": "1"
                },
                {
                    "index": "438-2",
                    "sentence": "We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information.",
                    "sentence_kor": "이러한 폐쇄 루프 정책을 픽셀 정보에서 직접 학습하는 데이터 효율적인 모델 기반 강화 학습 알고리즘을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "438-3",
                    "sentence": "The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space.",
                    "sentence_kor": "핵심 요소는 이 저차원 특징 공간에서 예측 모델과 공동으로 이미지의 저차원 특징 임베딩을 학습하기 위한 심층 동적 모델이다.",
                    "tag": "3"
                },
                {
                    "index": "438-4",
                    "sentence": "Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control.",
                    "sentence_kor": "공동 학습은 우리가 폐쇄 루프 제어에 사용하는 적응형 비선형 모델 예측 제어 전략의 핵심에 있는 장기 예측에 중요하다.",
                    "tag": "4"
                },
                {
                    "index": "438-5",
                    "sentence": "Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.",
                    "sentence_kor": "연속 상태 및 동작을 위한 최첨단 RL 방법과 비교하여, 우리의 접근 방식은 빠르게 학습하고, 고차원 상태 공간으로 확장하며, 가볍고, 픽셀에서 토크로 완전히 자율적인 엔드 투 엔드 학습을 향한 중요한 단계이다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1038",
            "abstractID": "SPA_abs-439",
            "text": [
                {
                    "index": "439-0",
                    "sentence": "We present a unified framework for learning continuous control policies using backpropagation.",
                    "sentence_kor": "우리는 역전파를 이용한 지속적인 통제 정책을 학습하기 위한 통일된 프레임워크를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "439-1",
                    "sentence": "It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise.",
                    "sentence_kor": "그것은 벨만 방정식의 확률성을 외인성 잡음의 결정론적 함수로 처리하여 확률적 제어를 지원한다.",
                    "tag": "1"
                },
                {
                    "index": "439-2",
                    "sentence": "The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions.",
                    "sentence_kor": "이 제품은 가치 함수가 있는 모델 없는 방법에서 가치 함수가 없는 모델 기반 방법에 이르는 일반적인 정책 기울기 알고리즘의 스펙트럼이다.",
                    "tag": "1"
                },
                {
                    "index": "439-3",
                    "sentence": "We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors.",
                    "sentence_kor": "학습된 모델을 사용하지만 모델이 예측한 궤적에서의 관측 대신 환경에서 관찰만 요구하므로 복합 모델 오류의 영향을 최소화한다.",
                    "tag": "1"
                },
                {
                    "index": "439-4",
                    "sentence": "We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation.",
                    "sentence_kor": "이러한 알고리즘을 장난감 확률적 제어 문제에 먼저 적용한 다음 시뮬레이션의 몇 가지 물리 기반 제어 문제에 적용한다.",
                    "tag": "1"
                },
                {
                    "index": "439-5",
                    "sentence": "One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.",
                    "sentence_kor": "이러한 변형 중 하나인 SVG(1)는 연속 영역에서 학습 모델, 가치 함수 및 정책의 효과를 동시에 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1039",
            "abstractID": "SPA_abs-440",
            "text": [
                {
                    "index": "440-0",
                    "sentence": "The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents.",
                    "sentence_kor": "예상치 못한 다양한 환경에서 목표별 작업을 계획하고 실행할 수 있는 능력은 지능형 에이전트의 핵심 요건입니다.",
                    "tag": "1"
                },
                {
                    "index": "440-1",
                    "sentence": "In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations (\"visual imagination\").",
                    "sentence_kor": "본 논문에서는 에이전트가 외부 세계 역학의 내부 모델을 어떻게 구비할 수 있는지, 그리고 이 모델을 사용하여 여러 내부 시뮬레이션(\"시각적 상상\")을 실행하여 새로운 행동을 계획하는 방법에 대해 알아본다.",
                    "tag": "2+3"
                },
                {
                    "index": "440-2",
                    "sentence": "Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws.",
                    "sentence_kor": "우리 모델은 원시 시각적 입력을 직접 처리하고 객체(수정) 중심의 시각적 일별을 기반으로 한 새로운 객체 중심 예측 공식을 사용하여 학습된 물리적 법칙의 변환 불변성을 시행한다.",
                    "tag": "3"
                },
                {
                    "index": "440-3",
                    "sentence": "The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before.",
                    "sentence_kor": "에이전트는 다양한 환경 집합과 무작위적인 상호작용을 통해 훈련 데이터를 수집하며, 그 결과 모델을 사용하여 에이전트가 이전에 보지 못한 새로운 환경에서 목표 지향 행동을 계획할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "440-4",
                    "sentence": "We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.",
                    "sentence_kor": "우리는 우리의 에이전트가 공을 목표 위치로 밀어 넣거나 다른 공과 충돌해야 하는 시뮬레이션 당구 게임을 하기 위한 행동을 정확하게 계획할 수 있다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1040",
            "abstractID": "SPA_abs-441",
            "text": [
                {
                    "index": "441-0",
                    "sentence": "We present an active detection model for localizing objects in scenes.",
                    "sentence_kor": "장면에서 개체를 로컬라이징하기 위한 활성 탐지 모델을 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "441-1",
                    "sentence": "The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object.",
                    "sentence_kor": "이 모델은 클래스에 따라 다르며 에이전트가 대상 개체의 정확한 위치를 식별하기 위해 후보 영역에 주의를 집중할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "441-2",
                    "sentence": "This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning.",
                    "sentence_kor": "이 에이전트는 하향식 추론에 따라 대상 개체의 가장 구체적인 위치를 결정하는 것을 목표로 간단한 변환 작업을 사용하여 경계 상자를 변형하는 방법을 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "441-3",
                    "sentence": "The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset.",
                    "sentence_kor": "제안된 지역화 에이전트는 심층 강화 학습을 사용하여 훈련되고 Pascal VOC 2007 데이터 세트에서 평가된다.",
                    "tag": "1"
                },
                {
                    "index": "441-4",
                    "sentence": "We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.",
                    "sentence_kor": "제안된 모델에 의해 안내된 에이전트는 이미지에서 11개에서 25개 영역만 분석한 후 객체의 단일 인스턴스를 현지화할 수 있으며 객체 로컬라이제이션에 객체 제안을 사용하지 않는 시스템 중에서 최상의 탐지 결과를 얻을 수 있음을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1041",
            "abstractID": "SPA_abs-442",
            "text": [
                {
                    "index": "442-0",
                    "sentence": "We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively.",
                    "sentence_kor": "출력 시퀀스를 반복적으로 해독하는 심층 Q 네트워크(DQN)를 사용하여 시퀀스 대 시퀀스 학습을 위한 새로운 스키마를 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "442-1",
                    "sentence": "The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts.",
                    "sentence_kor": "여기서 목표는 디코더가 먼저 시퀀스의 더 쉬운 부분을 다루도록 한 다음 방향을 돌려 어려운 부분에 대처할 수 있도록 하는 것입니다.",
                    "tag": "2+3"
                },
                {
                    "index": "442-2",
                    "sentence": "Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN.",
                    "sentence_kor": "특히 각 반복에서 인코더-디코더 장단기 메모리(LSTM) 네트워크를 사용하여 입력 시퀀스에서 DQN의 내부 상태를 나타내고 잠재적 작업 목록을 공식화한다.",
                    "tag": "3"
                },
                {
                    "index": "442-3",
                    "sentence": "Take rephrasing a natural sentence as an example.",
                    "sentence_kor": "자연스러운 문장을 바꿔 쓰는 것을 예로 들어보자.",
                    "tag": "3"
                },
                {
                    "index": "442-4",
                    "sentence": "This list can contain ranked potential words.",
                    "sentence_kor": "이 목록에는 순위가 매겨진 잠재적 단어가 포함될 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "442-5",
                    "sentence": "Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence.",
                    "sentence_kor": "다음으로, DQN은 현재 디코딩된 시퀀스를 수정하기 위해 목록에서 선택할 동작(예: 워드)을 결정하는 방법을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "442-6",
                    "sentence": "The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration.",
                    "sentence_kor": "새로 수정된 출력 시퀀스는 이후 다음 디코딩 반복을 위해 DQN에 대한 입력으로 사용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "442-7",
                    "sentence": "In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded.",
                    "sentence_kor": "또한 각 반복에서 강화 학습의 주의를 기울여 이전에 디코딩하기 어려운 시퀀스 부분을 탐색한다.",
                    "tag": "3"
                },
                {
                    "index": "442-8",
                    "sentence": "For evaluation, the proposed strategy was trained to decode ten thousands natural sentences.",
                    "sentence_kor": "평가를 위해 제안된 전략은 1만 개의 자연스러운 문장을 해독하도록 훈련되었다.",
                    "tag": "3"
                },
                {
                    "index": "442-9",
                    "sentence": "Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.",
                    "sentence_kor": "우리의 실험에 따르면 왼쪽에서 오른쪽으로 그리디 빔 검색 LSTM 디코더와 비교할 때 제안된 방법은 훈련 세트에서 문장을 디코딩할 때는 경쟁적으로 잘 수행되었지만 BLEU 점수 측면에서 보이지 않는 문장을 디코딩할 때는 기준선을 크게 앞질렀다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1042",
            "abstractID": "SPA_abs-443",
            "text": [
                {
                    "index": "443-0",
                    "sentence": "We present a novel definition of the reinforcement learning state, actions and reward function that allows a deep Q-network (DQN) to learn to control an optimization hyperparameter.",
                    "sentence_kor": "우리는 심층 Q 네트워크(DQN)가 최적화 하이퍼 파라미터를 제어하는 방법을 배울 수 있도록 하는 강화 학습 상태, 행동 및 보상 기능에 대한 새로운 정의를 제시한다.",
                    "tag": "2+3"
                },
                {
                    "index": "443-1",
                    "sentence": "Using Q-learning with experience replay, we train two DQNs to accept a state representation of an objective function as input and output the expected discounted return of rewards, or q-values, connected to the actions of either adjusting the learning rate or leaving it unchanged.",
                    "sentence_kor": "경험 재생과 함께 Q-러닝을 사용하여, 우리는 두 개의 DQN을 교육하여 목표 함수의 상태 표현을 입력으로 수용하고 학습 속도를 조정하거나 변경하지 않은 조치와 연결된 예상 할인 보상 수익률 또는 q-값을 출력한다.",
                    "tag": "3"
                },
                {
                    "index": "443-2",
                    "sentence": "The two DQNs learn a policy similar to a line search, but differ in the number of allowed actions.",
                    "sentence_kor": "두 DQN은 행 검색과 유사한 정책을 학습하지만 허용된 작업 수는 다릅니다.",
                    "tag": "4"
                },
                {
                    "index": "443-3",
                    "sentence": "The trained DQNs in combination with a gradient-based update routine form the basis of the Q-gradient descent algorithms.",
                    "sentence_kor": "그레이디언트 기반 업데이트 루틴과 함께 훈련된 DQN은 Q-그레이디언트 하강 알고리즘의 기초를 형성한다.",
                    "tag": "4"
                },
                {
                    "index": "443-4",
                    "sentence": "To demonstrate the viability of this framework, we show that the DQN's q-values associated with optimal action converge and that the Q-gradient descent algorithms outperform gradient descent with an Armijo or nonmonotone line search.",
                    "sentence_kor": "이 프레임워크의 실행 가능성을 입증하기 위해 최적의 동작과 관련된 DQN의 q-값이 수렴되고 Q-그레이디언트 하강 알고리즘이 아르미조 또는 비모노톤 라인 검색으로 경사 하강보다 성능이 우수하다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "443-5",
                    "sentence": "Unlike traditional optimization methods, Q-gradient descent can incorporate any objective statistic and by varying the actions we gain insight into the type of learning rate adjustment strategies that are successful for neural network optimization.",
                    "sentence_kor": "기존 최적화 방법과 달리 Q-그레이디언트 강하법은 모든 객관적 통계를 통합할 수 있으며, 동작을 변경하여 신경망 최적화에 성공적인 학습 속도 조정 전략의 유형에 대한 통찰력을 얻을 수 있다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1043",
            "abstractID": "SPA_abs-444",
            "text": [
                {
                    "index": "444-0",
                    "sentence": "The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents.",
                    "sentence_kor": "상호정보는 기계학습의 모든 분야에 적용되는 핵심 통계량이며, 이것이 다중 데이터 양식에 대한 밀도 모델 훈련이든, 잡음이 많은 전송 채널의 효율성을 최대화하든, 또는 인공 에이전트의 탐사를 위한 행동 정책을 학습할 때 그러하다.",
                    "tag": "1"
                },
                {
                    "index": "444-1",
                    "sentence": "Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications.",
                    "sentence_kor": "상호 정보의 최적화를 포함하는 대부분의 학습 알고리즘은 현대 기계 학습 애플리케이션에 적합하지 않은 지수 복잡성을 가진 열거형 알고리즘인 Blahut-Ariimoto 알고리즘에 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "444-2",
                    "sentence": "This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning.",
                    "sentence_kor": "본 논문은 변동 추론과 딥 러닝의 기법을 병합하여 상호 정보의 확장 가능한 최적화를 위한 새로운 접근 방식을 제공한다.",
                    "tag": "2"
                },
                {
                    "index": "444-3",
                    "sentence": "We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment.",
                    "sentence_kor": "우리는 상호 정보가 권한 부여라고 알려진 잘 알려진 내부 추진력의 정의를 형성하는 내재적 동기 학습 문제에 초점을 맞춤으로써 접근 방식을 개발한다.",
                    "tag": "2+3"
                },
                {
                    "index": "444-4",
                    "sentence": "Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.",
                    "sentence_kor": "시각적 입력 스트림을 처리하기 위한 컨볼루션 네트워크와 결합된 상호 정보에 대한 다양한 하한을 사용하여, 우리는 확장 가능한 정보 최대화와 픽셀에서 동작으로 직접 권한 부여 기반 추론을 허용하는 확률적 최적화 알고리즘을 개발한다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "1044",
            "abstractID": "SPA_abs-445",
            "text": [
                {
                    "index": "445-0",
                    "sentence": "The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning.",
                    "sentence_kor": "최근 도입된 심층 Q-Networks(DQN) 알고리즘은 심층 신경 네트워크와 강화 학습의 첫 번째 성공적인 조합 중 하나로 주목을 받고 있다.",
                    "tag": "1"
                },
                {
                    "index": "445-1",
                    "sentence": "Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI.",
                    "sentence_kor": "이 약속은 AI의 일반적인 역량을 평가하는 데 사용되는 수십 개의 아타리 2600 게임으로 구성된 도전적인 프레임워크인 ALE(Acade Learning Environment)에서 입증되었다.",
                    "tag": "1"
                },
                {
                    "index": "445-2",
                    "sentence": "It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general.",
                    "sentence_kor": "이전 접근 방식보다 훨씬 더 나은 결과를 얻었으며, 우수한 표현을 학습하는 능력이 매우 강력하고 일반적이라는 것을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "445-3",
                    "sentence": "This paper attempts to understand the principles that underlie DQN's impressive performance and to better contextualize its success.",
                    "sentence_kor": "본 논문은 DQN의 인상적인 성과를 뒷받침하는 원칙을 이해하고 DQN의 성공을 더 잘 설명하려고 한다.",
                    "tag": "2"
                },
                {
                    "index": "445-4",
                    "sentence": "We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts.",
                    "sentence_kor": "우리는 이러한 개념을 사용하는 간단한 선형 표현을 제안하여 DQN 네트워크에 의해 인코딩된 주요 표현 편중의 중요성을 체계적으로 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "445-5",
                    "sentence": "Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE.",
                    "sentence_kor": "이러한 특성을 통합하여 ALE에서 DQN에 대한 경쟁력 있는 성능을 달성하는 계산적으로 실용적인 기능 세트를 얻는다.",
                    "tag": "3+4"
                },
                {
                    "index": "445-6",
                    "sentence": "Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game.",
                    "sentence_kor": "DQN의 장단점에 대한 통찰력을 제공하는 것 외에도, 우리는 ALE에 대한 일반적인 표현을 제공하여 각 게임에 대한 표현 학습 부담을 크게 줄인다.",
                    "tag": "3+4"
                },
                {
                    "index": "445-7",
                    "sentence": "Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.",
                    "sentence_kor": "또한 ALE의 향후 작업과의 비교를 위해 간단하고 재현 가능한 벤치마크를 제공한다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "1045",
            "abstractID": "SPA_abs-446",
            "text": [
                {
                    "index": "446-0",
                    "sentence": "With the demand for machine learning increasing, so does the demand for tools which make it easier to use.",
                    "sentence_kor": "머신러닝에 대한 수요가 증가함에 따라 사용하기 쉬운 도구에 대한 수요도 증가하고 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "446-1",
                    "sentence": "Automated machine learning (AutoML) tools have been developed to address this need, such as the Tree-Based Pipeline Optimization Tool (TPOT) which uses genetic programming to build optimal pipelines.",
                    "sentence_kor": "유전자 프로그래밍을 사용하여 최적의 파이프라인을 구축하는 TPOT(Tree-Based Pipeline Optimization Tool)와 같은 자동화된 기계 학습(AutoML) 도구가 이러한 요구를 해결하기 위해 개발되었다.",
                    "tag": "1"
                },
                {
                    "index": "446-2",
                    "sentence": "We introduce Layered TPOT, a modification to TPOT which aims to create pipelines equally good as the original, but in significantly less time.",
                    "sentence_kor": "우리는 TPOT에 대한 수정인 Layered TPOT를 소개하는데, 이는 원본과 똑같이 좋은 파이프라인을 만들지만 훨씬 짧은 시간 안에 만드는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "446-3",
                    "sentence": "This approach evaluates candidate pipelines on increasingly large subsets of the data according to their fitness, using a modified evolutionary algorithm to allow for separate competition between pipelines trained on different sample sizes.",
                    "sentence_kor": "이 접근법은 서로 다른 표본 크기에 대해 훈련된 파이프라인 간의 개별 경쟁을 허용하기 위해 수정된 진화 알고리즘을 사용하여 데이터의 점점 더 큰 부분 집합에 대한 후보 파이프라인을 평가한다.",
                    "tag": "2+3"
                },
                {
                    "index": "446-4",
                    "sentence": "Empirical evaluation shows that, on sufficiently large datasets, Layered TPOT indeed finds better models faster.",
                    "sentence_kor": "경험적 평가에 따르면 충분히 큰 데이터 세트에서 레이어드 TPOT는 실제로 더 나은 모델을 더 빨리 찾는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1046",
            "abstractID": "SPA_abs-447",
            "text": [
                {
                    "index": "447-0",
                    "sentence": "Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost.",
                    "sentence_kor": "신경 아키텍처 검색(NAS)은 심층 신경망을 자동으로 조정하도록 제안되었지만, NASNet, PNAS와 같은 기존 검색 알고리즘은 대개 값비싼 계산 비용을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "447-1",
                    "sentence": "Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search.",
                    "sentence_kor": "신경망의 기능을 유지하면서 신경 구조를 변경하는 네트워크 형태론은 검색 중에 더 효율적인 훈련을 가능하게 하여 NAS에 도움이 될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "447-2",
                    "sentence": "In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search.",
                    "sentence_kor": "본 논문에서, 우리는 효율적인 신경 아키텍처 검색을 위해 베이지안 최적화가 네트워크 형태주의를 안내할 수 있는 새로운 프레임워크를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "447-3",
                    "sentence": "The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space.",
                    "sentence_kor": "프레임워크는 검색 공간을 효율적으로 탐색하기 위해 신경망 커널과 트리 구조화 획득 함수 최적화 알고리즘을 개발한다.",
                    "tag": "4"
                },
                {
                    "index": "447-4",
                    "sentence": "Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods.",
                    "sentence_kor": "실제 벤치마크 데이터 세트에 대한 집중적인 실험이 최신 방법에 비해 개발된 프레임워크의 우수한 성능을 입증하기 위해 수행되었다.",
                    "tag": "3+4"
                },
                {
                    "index": "447-5",
                    "sentence": "Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras.",
                    "sentence_kor": "또한, 우리는 우리의 방법인 Auto-Keras를 기반으로 오픈 소스 AutoML 시스템을 구축한다.",
                    "tag": "2+3"
                },
                {
                    "index": "447-6",
                    "sentence": "The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.",
                    "sentence_kor": "시스템은 CPU와 GPU에서 병렬로 실행되며 서로 다른 GPU 메모리 제한에 대한 적응형 검색 전략을 사용합니다.",
                    "tag": "3"
                }
            ]
        },
        {
            "absNo": "1047",
            "abstractID": "SPA_abs-448",
            "text": [
                {
                    "index": "448-0",
                    "sentence": "Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability.",
                    "sentence_kor": "GAN(Generative Adversarial Networks)은 강력한 생성 모델이지만 훈련 불안정성을 겪는다.",
                    "tag": "1"
                },
                {
                    "index": "448-1",
                    "sentence": "The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge.",
                    "sentence_kor": "최근에 제안된 와서스테인 GAN(WGAN)은 GAN의 안정적인 훈련을 위해 진전을 이루지만, 때로는 여전히 저품질 샘플만 생성하거나 수렴하지 못할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "448-2",
                    "sentence": "We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior.",
                    "sentence_kor": "우리는 이러한 문제가 종종 WGAN에서 가중치 클리핑을 사용하여 비평가에 대한 립시츠 제한을 시행하여 원하지 않는 행동을 초래할 수 있다는 것을 발견했다.",
                    "tag": "1"
                },
                {
                    "index": "448-3",
                    "sentence": "We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.",
                    "sentence_kor": "우리는 가중치 클리핑에 대한 대안을 제안한다. 즉, 입력과 관련하여 비평가의 기울기 표준을 처벌한다.",
                    "tag": "1"
                },
                {
                    "index": "448-4",
                    "sentence": "Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.",
                    "sentence_kor": "우리가 제안한 방법은 표준 WGAN보다 성능이 우수하며 101계층 ResNets 및 이산 데이터에 대한 언어 모델을 포함하여 거의 하이퍼 파라미터 튜닝 없이 광범위한 GAN 아키텍처를 안정적으로 교육할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "448-5",
                    "sentence": "We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.",
                    "sentence_kor": "우리는 또한 CIFAR-10과 LSUN 침실에서도 고품질의 제품을 생산합니다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1048",
            "abstractID": "SPA_abs-449",
            "text": [
                {
                    "index": "449-0",
                    "sentence": "We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels.",
                    "sentence_kor": "우리는 판별기 네트워크가 클래스 레이블을 출력하도록 하여 생성적 적대 네트워크(GAN)를 준감독 컨텍스트로 확장한다.",
                    "tag": "1"
                },
                {
                    "index": "449-1",
                    "sentence": "We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes.",
                    "sentence_kor": "우리는 N 클래스 중 하나에 속하는 입력을 가진 데이터 세트에서 생성 모델 G와 판별기 D를 훈련시킨다.",
                    "tag": "1"
                },
                {
                    "index": "449-2",
                    "sentence": "At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G.",
                    "sentence_kor": "훈련 시 D는 입력이 어떤 N+1 클래스에 속하는지 예측하기 위해 만들어지며, 여기서 G의 출력에 해당하는 추가 클래스가 추가된다.",
                    "tag": "1"
                },
                {
                    "index": "449-3",
                    "sentence": "We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.",
                    "sentence_kor": "우리는 이 방법을 사용하여 보다 데이터 효율적인 분류기를 만들 수 있으며 일반 GAN보다 더 높은 품질의 샘플을 생성할 수 있음을 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1049",
            "abstractID": "SPA_abs-450",
            "text": [
                {
                    "index": "450-0",
                    "sentence": "Machine learning models are powerful but fallible.",
                    "sentence_kor": "기계 학습 모델은 강력하지만 실수할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "450-1",
                    "sentence": "Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities.",
                    "sentence_kor": "모델 오분류 또는 기타 오류를 일으키도록 의도적으로 조작된 입력인 적대적 예를 생성하면 모델 가정 및 취약성에 대한 중요한 통찰력을 얻을 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "450-2",
                    "sentence": "Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks.",
                    "sentence_kor": "이미지 분류자를 대상으로 하는 적대적 예제 생성에 대한 최근 상당한 연구에도 불구하고 텍스트 분류자를 위한 적대적 예제 생성을 탐구하는 작업은 상대적으로 거의 없다. 또한, 기존의 많은 적대적 예제 생성 알고리즘은 대상 모델 매개변수에 대한 전체 액세스를 요구하므로 실제 문제가 되는 많은 경우 이를 비현실적으로 만든다.ld 공격",
                    "tag": "1"
                },
                {
                    "index": "450-3",
                    "sentence": "In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers.",
                    "sentence_kor": "이 연구에서 우리는 DANCin SEQ2를 소개한다.주로 블랙박스 텍스트 분류기를 대상으로 하는 적대적 텍스트 예제 생성을 위한 GAN에서 영감을 받은 알고리즘인 SEQ.",
                    "tag": "2"
                },
                {
                    "index": "450-4",
                    "sentence": "We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.",
                    "sentence_kor": "우리는 적대적 텍스트 예제 생성을 강화 학습 문제로 다시 설명하고 우리의 알고리즘이 실제 공격 시나리오에서 의미론적으로 의미 있는 적대적 텍스트 예제를 생성하기 위한 예비적이지만 유망한 단계를 제공한다는 것을 입증한다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "1050",
            "abstractID": "SPA_abs-451",
            "text": [
                {
                    "index": "451-0",
                    "sentence": "Convolutional Neural Networks have achieved significant success across multiple computer vision tasks.",
                    "sentence_kor": "컨볼루션 신경망은 여러 컴퓨터 비전 작업에서 상당한 성공을 거두었다.",
                    "tag": "1"
                },
                {
                    "index": "451-1",
                    "sentence": "However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems.",
                    "sentence_kor": "그러나 중요한 보안 민감 시스템에서의 배치를 제한하는 세심하게 조작되고 인간이 감지할 수 없는 적대적 노이즈 패턴에는 취약하다.",
                    "tag": "1"
                },
                {
                    "index": "451-2",
                    "sentence": "This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations.",
                    "sentence_kor": "본 논문은 그러한 적대적 동요의 영향을 효과적으로 완화하기 위한 강력한 방어 메커니즘을 제공하는 계산적으로 효율적인 이미지 강화 접근방식을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "451-3",
                    "sentence": "We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes.",
                    "sentence_kor": "우리는 심층 이미지 복원 네트워크가 자연 이미지 매니폴드로 이동시켜 올바른 클래스에 대한 분류를 복원할 수 있는 매핑 기능을 학습한다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "451-4",
                    "sentence": "A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images.",
                    "sentence_kor": "우리 접근 방식의 두드러진 특징은 공격에 대한 견고성을 제공하는 것 외에도 이미지 품질을 향상시키고 깨끗한 이미지에 대한 모델 성능을 유지한다는 것이다.",
                    "tag": "4"
                },
                {
                    "index": "451-5",
                    "sentence": "Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images.",
                    "sentence_kor": "또한 제안된 방법은 분류기를 수정하지 않거나 적대적 이미지를 탐지하기 위한 별도의 메커니즘이 필요하다.",
                    "tag": "4"
                },
                {
                    "index": "451-6",
                    "sentence": "The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings.",
                    "sentence_kor": "이 계획의 효과는 그레이 박스 설정에서 강력한 방어를 입증하는 광범위한 실험을 통해 입증되었다.",
                    "tag": "4"
                },
                {
                    "index": "451-7",
                    "sentence": "The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms.",
                    "sentence_kor": "제안된 계획은 간단하며 다음과 같은 장점이 있다. (1) 모델 훈련이나 매개변수 최적화가 필요하지 않으며 (2) 다른 기존 방어 메커니즘을 보완하며 (3) 공격 모델 및 공격 유형에 구애받지 않으며 (4) 모든 인기 있는 공격 알고리즘에서 우수한 성능을 제공한다.",
                    "tag": "4+5"
                },
                {
                    "index": "451-8",
                    "sentence": "Our codes are publicly available at this https URL.",
                    "sentence_kor": "당사의 코드는 이 https URL에서 공개적으로 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1051",
            "abstractID": "SPA_abs-452",
            "text": [
                {
                    "index": "452-0",
                    "sentence": "Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it.",
                    "sentence_kor": "심층 신경망은 이미지 분류에서 인상적인 실험 결과를 달성했지만, 적대적 섭동, 즉 네트워크를 잘못 분류하게 하는 입력 이미지에 대한 최소한의 변경과 관련하여 놀랍게도 불안정할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "452-1",
                    "sentence": "With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety.",
                    "sentence_kor": "인식 모듈 및 자율 주행용 엔드 투 엔드 컨트롤러를 포함한 잠재적 애플리케이션으로 인해 안전성에 대한 우려가 제기되고 있다.",
                    "tag": "1"
                },
                {
                    "index": "452-2",
                    "sentence": "We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT).",
                    "sentence_kor": "만족도 모듈로 이론(SMT)을 기반으로 피드 포워드 다층 신경망을 위한 새로운 자동 검증 프레임워크를 개발한다.",
                    "tag": "1"
                },
                {
                    "index": "452-3",
                    "sentence": "We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image.",
                    "sentence_kor": "사람에 의해 동일한 등급이 할당될 수 있는 긁힘이나 카메라 각도 또는 조명 조건의 변경과 같은 이미지 분류 결정의 안전에 초점을 맞추고 오리의 작은 이웃 내에서 분류의 불변성의 측면에서 개별 결정에 대한 안전을 정의한다.지날 이미지",
                    "tag": "1"
                },
                {
                    "index": "452-4",
                    "sentence": "We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer.",
                    "sentence_kor": "우리는 이산화를 채택하여 영역을 철저히 검색하고 분석 계층을 계층별로 전파한다.",
                    "tag": "1"
                },
                {
                    "index": "452-5",
                    "sentence": "Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations.",
                    "sentence_kor": "우리의 방법은 네트워크 코드와 직접 작동하며, 기존 방법과 달리 적대적 예가 존재하는 경우 해당 영역 및 조작 제품군에 대해 발견되도록 보장할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "452-6",
                    "sentence": "If found, adversarial examples can be shown to human testers and/or used to fine-tune the network.",
                    "sentence_kor": "발견될 경우 적대적인 예를 인간 시험자에게 보여주거나 네트워크를 미세 조정하는 데 사용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "452-7",
                    "sentence": "We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks.",
                    "sentence_kor": "Z3를 사용하여 기술을 구현하고 정규화 및 딥 러닝 네트워크를 포함한 최첨단 네트워크에서 평가한다.",
                    "tag": "1"
                },
                {
                    "index": "452-8",
                    "sentence": "We also compare against existing techniques to search for adversarial examples and estimate network robustness.",
                    "sentence_kor": "또한 적대적인 예를 검색하고 네트워크 견고성을 추정하기 위해 기존 기법과 비교한다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1052",
            "abstractID": "SPA_abs-453",
            "text": [
                {
                    "index": "453-0",
                    "sentence": "In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research.",
                    "sentence_kor": "본 논문에서 우리는 근본적인 강화 학습 연구를 위한 광범위하고 가볍고 유연한 플랫폼인 ELF를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "453-1",
                    "sentence": "Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense).",
                    "sentence_kor": "ELF를 사용하여 3가지 게임 환경(Mini-RTS, Capture the Flag 및 타워 디펜스)을 갖춘 고도로 사용자 지정 가능한 실시간 전략(RTS) 엔진을 구현한다.",
                    "tag": "3"
                },
                {
                    "index": "453-2",
                    "sentence": "Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a Macbook Pro notebook.",
                    "sentence_kor": "미니 RTS는 스타크래프트의 축소판으로서 핵심 게임 역학을 포착하고 맥북 프로 노트북에서 코어당 40,000 프레임/초속 FPS(프레임/초속)로 구동된다.",
                    "tag": "4"
                },
                {
                    "index": "453-3",
                    "sentence": "When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU.",
                    "sentence_kor": "최신 강화 학습 방법과 결합하면, 이 시스템은 CPU 6개와 GPU 1개로 하루 만에 내장된 AI에 대항하여 풀 게임 봇을 훈련시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "453-4",
                    "sentence": "In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like Arcade Learning Environment.",
                    "sentence_kor": "또한 당사의 플랫폼은 환경-에이전트 통신 토폴로지, RL 방법의 선택, 게임 파라미터의 변화, 아케이드 학습 환경과 같은 기존 C/C++ 기반 게임 환경을 호스팅할 수 있습니다.",
                    "tag": "4"
                },
                {
                    "index": "453-5",
                    "sentence": "Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS.",
                    "sentence_kor": "ELF를 사용하여 훈련 매개 변수를 철저히 탐색하고 긴 수평적 훈련 및 점진적 커리큘럼과 결합된 Leaky ReLU 및 배치 정규화 네트워크가 Mini-RTS의 전체 게임에서 규칙 기반 내장 AI를 70% 이상 능가한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "453-6",
                    "sentence": "Strong performance is also achieved on the other two games.",
                    "sentence_kor": "나머지 두 경기에서도 강세를 보이고 있다.",
                    "tag": "4"
                },
                {
                    "index": "453-7",
                    "sentence": "In game replays, we show our agents learn interesting strategies.",
                    "sentence_kor": "게임 재생에서, 우리는 우리의 에이전트들이 흥미로운 전략을 배우는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "453-8",
                    "sentence": "ELF, along with its RL platform, is open-sourced at this https URL.",
                    "sentence_kor": "ELF는 RL 플랫폼과 함께 이 https URL에서 오픈소스된다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1053",
            "abstractID": "SPA_abs-454",
            "text": [
                {
                    "index": "454-0",
                    "sentence": "Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles.",
                    "sentence_kor": "협력 멀티 에이전트 시스템은 네트워크 패킷 라우팅 및 자율 차량 조정과 같은 많은 실제 문제를 모델링하는 데 자연스럽게 사용될 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "454-1",
                    "sentence": "There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems.",
                    "sentence_kor": "이러한 시스템에 대한 분산형 정책을 효율적으로 학습할 수 있는 새로운 강화 학습 방법이 절실히 필요하다.",
                    "tag": "1"
                },
                {
                    "index": "454-2",
                    "sentence": "To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients.",
                    "sentence_kor": "이를 위해 우리는 반사실적 다중 에이전트(COMA) 정책 그레이디언트라는 새로운 다중 에이전트 행위자 비판 방법을 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "454-3",
                    "sentence": "COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies.",
                    "sentence_kor": "COMA는 중앙집중식 비평가를 사용하여 Q-기능을 추정하며 분산형 행위자를 사용하여 에이전트의 정책을 최적화한다.",
                    "tag": "3"
                },
                {
                    "index": "454-4",
                    "sentence": "In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed.",
                    "sentence_kor": "또한 다중 에이전트 신용 할당의 과제를 해결하기 위해, 다른 에이전트의 조치를 고정시키면서 단일 에이전트의 조치를 무시하는 반사실적 기준선을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "454-5",
                    "sentence": "COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.",
                    "sentence_kor": "COMA는 또한 하나의 전진 패스에서 반사실적 기준선을 효율적으로 계산할 수 있는 비평 표현을 사용한다.",
                    "tag": "3"
                },
                {
                    "index": "454-6",
                    "sentence": "We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability.",
                    "sentence_kor": "우리는 상당한 부분 관측성을 가진 분산형 변형을 사용하여 스타크래프트 유닛 미세 관리의 테스트 베드에서 COMA를 평가한다.",
                    "tag": "3"
                },
                {
                    "index": "454-7",
                    "sentence": "COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.",
                    "sentence_kor": "COMA는 이 설정에서 다른 다중 에이전트 행위자 비평 방법에 비해 평균 성능을 크게 향상시키며, 최고의 성능을 발휘하는 에이전트는 전체 상태에 액세스할 수 있는 최첨단 중앙 집중식 컨트롤러에 비해 경쟁력이 있다.",
                    "tag": "5"
                }
            ]
        },
        {
            "absNo": "1054",
            "abstractID": "SPA_abs-455",
            "text": [
                {
                    "index": "455-0",
                    "sentence": "Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science.",
                    "sentence_kor": "분자에 대한 지도 학습은 화학, 약물 발견, 그리고 재료 과학에서 유용할 수 있는 믿을 수 없는 잠재력을 가지고 있다.",
                    "tag": "1"
                },
                {
                    "index": "455-1",
                    "sentence": "Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature.",
                    "sentence_kor": "다행히도, 분자 대칭에 불변하는 몇몇 유망하고 밀접하게 관련된 신경 네트워크 모델은 이미 문헌에 설명되어 있다.",
                    "tag": "1"
                },
                {
                    "index": "455-2",
                    "sentence": "These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph.",
                    "sentence_kor": "이러한 모델은 전체 입력 그래프의 함수를 계산하기 위한 메시지 전달 알고리즘 및 집계 절차를 학습한다.",
                    "tag": "1"
                },
                {
                    "index": "455-3",
                    "sentence": "At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach.",
                    "sentence_kor": "이 시점에서 다음 단계는 이 일반적인 접근방식의 특히 효과적인 변형을 찾아 화학적 예측 벤치마크에 적용함으로써 해결하거나 접근방식의 한계에 도달할 때까지 수행한다.",
                    "tag": "1"
                },
                {
                    "index": "455-4",
                    "sentence": "In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.",
                    "sentence_kor": "본 논문에서 우리는 기존 모델을 메시지 전달 신경망(MPNN)이라고 하는 단일 공통 프레임워크로 재구성하고 이 프레임워크 내에서 새로운 변형을 추가로 탐구한다.",
                    "tag": "2+3"
                },
                {
                    "index": "455-5",
                    "sentence": "Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.",
                    "sentence_kor": "MPNN을 사용하여 우리는 중요한 분자 특성 예측 벤치마크에서 최신 결과를 입증한다. 이러한 결과는 향후 연구가 더 큰 분자 또는 더 정확한 지상 실측 라벨이 있는 데이터 세트에 초점을 맞춰야 할 정도로 충분히 강력하다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "1055",
            "abstractID": "SPA_abs-456",
            "text": [
                {
                    "index": "456-0",
                    "sentence": "Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery.",
                    "sentence_kor": "다중 작업 학습(MTL)은 자연어 처리 및 음성 인식에서 컴퓨터 비전 및 약물 발견에 이르기까지 기계 학습의 많은 응용 분야에서 성공을 이끌었다.",
                    "tag": "1"
                },
                {
                    "index": "456-1",
                    "sentence": "This article aims to give a general overview of MTL, particularly in deep neural networks.",
                    "sentence_kor": "이 논문은 특히 심층 신경망의 MTL에 대한 일반적인 개요를 제공하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "456-2",
                    "sentence": "It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances.",
                    "sentence_kor": "딥 러닝에서 MTL에 대한 가장 일반적인 두 가지 방법을 소개하고 문헌의 개요를 제공하며 최근의 진보를 논의한다.",
                    "tag": "3"
                },
                {
                    "index": "456-3",
                    "sentence": "In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",
                    "sentence_kor": "특히 MTL 작동 방식을 조명하고 적절한 보조 작업 선택을 위한 지침을 제공하여 ML 실무자가 MTL을 적용할 수 있도록 돕고자 한다.",
                    "tag": "2"
                }
            ]
        },
        {
            "absNo": "1056",
            "abstractID": "SPA_abs-457",
            "text": [
                {
                    "index": "457-0",
                    "sentence": "Docking is an important tool in computational drug discovery that aims to predict the binding pose of a ligand to a target protein through a combination of pose scoring and optimization.",
                    "sentence_kor": "도킹은 포즈 스코어링과 최적화의 조합을 통해 표적 단백질에 대한 리간드의 결합 포즈를 예측하는 것을 목표로 하는 계산 약물 발견에서 중요한 도구이다.",
                    "tag": "2"
                },
                {
                    "index": "457-1",
                    "sentence": "A scoring function that is differentiable with respect to atom positions can be used for both scoring and gradient-based optimization of poses for docking.",
                    "sentence_kor": "원자 위치와 관련하여 구별 가능한 점수 매기기 기능은 도킹을 위한 포즈의 점수 매기기 및 경사도 기반 최적화에 모두 사용할 수 있다.",
                    "tag": "3"
                },
                {
                    "index": "457-2",
                    "sentence": "In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target.",
                    "sentence_kor": "주어진 생물학적 목표를 향해 활동적인 분자로 라이브러리를 풍부하게 하기 위해, 우리는 그 대상에 대해 활동적인 것으로 알려진 작은 분자 세트로 모델을 미세 조정할 것을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "457-3",
                    "sentence": "Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules.",
                    "sentence_kor": "황색포도상구균에 대해, 이 모델은 약화학자들이 설계한 6051 억제 시험 분자의 14%를 재생산한 반면, 플라스모디움 팔시파룸(말라리아)에 대해서는 1240 시험 분자의 28%를 재생산했다.",
                    "tag": "4"
                },
                {
                    "index": "457-4",
                    "sentence": "When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.",
                    "sentence_kor": "점수 매기기 기능과 결합하면, 우리 모델은 완전한 신약 설계 주기를 수행하여 약물 발견을 위한 새로운 분자 세트를 생성할 수 있다.",
                    "tag": "3+4"
                }
            ]
        },
        {
            "absNo": "1057",
            "abstractID": "SPA_abs-458",
            "text": [
                {
                    "index": "458-0",
                    "sentence": "Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training.",
                    "sentence_kor": "최근 조밀한 연결은 훈련 중 경사 흐름과 암묵적인 심층 감독을 용이하게 하기 때문에 컴퓨터 시각에서 상당한 관심을 끌고 있다.",
                    "tag": "1"
                },
                {
                    "index": "458-1",
                    "sentence": "Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks.",
                    "sentence_kor": "특히 피드 포워드 방식으로 각 레이어를 다른 레이어에 연결하는 DenseNet은 자연 이미지 분류 작업에서 인상적인 성능을 보여주었다.",
                    "tag": "1"
                },
                {
                    "index": "458-2",
                    "sentence": "We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems.",
                    "sentence_kor": "우리는 고밀도 연결의 정의를 다중 모드 분할 문제로 확장하는 3D 완전 컨볼루션 신경망인 HyperDenseNet을 제안한다.",
                    "tag": "1"
                },
                {
                    "index": "458-3",
                    "sentence": "Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths.",
                    "sentence_kor": "각 영상 촬영장비에는 경로가 있으며, 조밀한 연결은 동일한 경로 내의 계층 쌍 사이뿐만 아니라 다른 경로의 계층 간에도 발생합니다.",
                    "tag": "1"
                },
                {
                    "index": "458-4",
                    "sentence": "This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network.",
                    "sentence_kor": "이는 몇 가지 양식을 모델링하는 기존의 다중 모드 CNN 접근방식과 대조되는데, 융합을 위해 일반적으로 입력 또는 네트워크 출력에서 단일 조인트 계층(또는 추상화 수준)에 전적으로 의존한다.",
                    "tag": "1"
                },
                {
                    "index": "458-5",
                    "sentence": "Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation.",
                    "sentence_kor": "따라서 제안된 네트워크는 모든 추상화 수준 내 및 그 사이의 양식 간에 더 복잡한 조합을 학습할 수 있는 완전한 자유를 가지며, 이는 학습 표현을 상당히 증가시킨다.",
                    "tag": "1"
                },
                {
                    "index": "458-6",
                    "sentence": "We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images.",
                    "sentence_kor": "우리는 경쟁률이 높은 두 가지 서로 다른 다중 모달 뇌 조직 분할 과제인 iSEG 2017과 MRBrainS 2013에 대한 광범위한 평가를 보고하며, 전자는 6개월 유아 데이터에 초점을 맞추고 후자는 성인 이미지에 초점을 맞춘다.",
                    "tag": "1"
                },
                {
                    "index": "458-7",
                    "sentence": "HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks.",
                    "sentence_kor": "HyperDenseNet은 많은 최첨단 세분화 네트워크에 비해 크게 향상되어 두 벤치마크 모두에서 상위권을 차지했습니다.",
                    "tag": "1"
                },
                {
                    "index": "458-8",
                    "sentence": "We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning.",
                    "sentence_kor": "우리는 또한 기능 재사용에 대한 종합적인 실험 분석을 제공하여 다중 모달 표현 학습에서 초밀도 연결의 중요성을 확인한다.",
                    "tag": "2+3"
                },
                {
                    "index": "458-9",
                    "sentence": "Our code is publicly available at this https URL.",
                    "sentence_kor": "이 https URL에서 코드를 공개적으로 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1058",
            "abstractID": "SPA_abs-459",
            "text": [
                {
                    "index": "459-0",
                    "sentence": "Spatial studies of transcriptome provide biologists with gene expression maps of heterogeneous and complex tissues.",
                    "sentence_kor": "전사체의 공간 연구는 생물학자들에게 이질적이고 복잡한 조직의 유전자 발현 지도를 제공한다.",
                    "tag": "1"
                },
                {
                    "index": "459-1",
                    "sentence": "However, most experimental protocols for spatial transcriptomics suffer from the need to select beforehand a small fraction of genes to be quantified over the entire transcriptome.",
                    "sentence_kor": "그러나 공간 전사체학에 대한 대부분의 실험 프로토콜은 전체 전사체 전체에 걸쳐 정량화할 유전자의 작은 부분을 미리 선택해야 하는 필요성 때문에 어려움을 겪고 있다.",
                    "tag": "1"
                },
                {
                    "index": "459-2",
                    "sentence": "Standard single-cell RNA sequencing (scRNA-seq) is more prevalent, easier to implement and can in principle capture any gene but cannot recover the spatial location of the cells.",
                    "sentence_kor": "표준 단세포 RNA 염기서열 분석(scRNA-seq)은 더 보편적이고 구현하기 쉬우며 원칙적으로 어떤 유전자도 포착할 수 있지만 세포의 공간적 위치를 회복할 수 없다.",
                    "tag": "1"
                },
                {
                    "index": "459-3",
                    "sentence": "In this manuscript, we focus on the problem of imputation of missing genes in spatial transcriptomic data based on (unpaired) standard scRNA-seq data from the same biological tissue.",
                    "sentence_kor": "이 원고에서는 동일한 생물학적 조직의 표준 scRNA-seq 데이터를 기반으로 공간 전사체 데이터에서 누락된 유전자의 귀속 문제에 초점을 맞춘다.",
                    "tag": "2+3"
                },
                {
                    "index": "459-4",
                    "sentence": "Building upon domain adaptation work, we propose gimVI, a deep generative model for the integration of spatial transcriptomic data and scRNA-seq data that can be used to impute missing genes.",
                    "sentence_kor": "도메인 적응 작업을 기반으로, 누락된 유전자를 귀속시키는 데 사용할 수 있는 공간 전사체 데이터와 scRNA-seq 데이터의 통합을 위한 심층 생성 모델인 gimVI를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "459-5",
                    "sentence": "After describing our generative model and an inference procedure for it, we compare gimVI to alternative methods from computational biology or domain adaptation on real datasets and outperform Seurat Anchors, Liger and CORAL to impute held-out genes.",
                    "sentence_kor": "우리의 생성 모델과 그것에 대한 추론 절차를 설명한 후, 우리는 김을 비교한다.VI는 실제 데이터 세트에 대한 계산 생물학 또는 도메인 적응에서 대안적인 방법으로 Seurat Anchor, Liger 및 CORAL을 능가하여 보류 유전자를 귀속시킨다.",
                    "tag": "2+3"
                }
            ]
        },
        {
            "absNo": "1059",
            "abstractID": "SPA_abs-460",
            "text": [
                {
                    "index": "460-0",
                    "sentence": "Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions.",
                    "sentence_kor": "심층 신경망의 해석성은 모델이 기능 선택을 수행하고 분류 결정을 도출하는 방법에 대한 더 나은 이해를 목표로 하는 기계 학습 연구의 최근 부상하고 있는 분야이다.",
                    "tag": "1"
                },
                {
                    "index": "460-1",
                    "sentence": "In this paper, two neural network architectures are trained on spectrogram and raw waveform data for audio classification tasks on a newly created audio dataset and layer-wise relevance propagation (LRP), a previously proposed interpretability method, is applied to investigate the models' feature selection and decision making.",
                    "sentence_kor": "본 논문에서, 두 개의 신경망 아키텍처는 새로 생성된 오디오 데이터 세트의 오디오 분류 작업을 위한 스펙트로그램 및 원시 파형 데이터에 대해 훈련되며, 이전에 제안된 해석성 방법인 계층별 관련성 전파(LRP)가 모델의 특징 선택 및 의사결정을 조사하기 위해 적용된다.",
                    "tag": "2+3"
                },
                {
                    "index": "460-2",
                    "sentence": "It is demonstrated that the networks are highly reliant on feature marked as relevant by LRP through systematic manipulation of the input data.",
                    "sentence_kor": "네트워크는 입력 데이터의 체계적인 조작을 통해 LRP에 의해 관련성이 있는 것으로 표시된 기능에 매우 의존한다는 것이 입증되었다.",
                    "tag": "4"
                },
                {
                    "index": "460-3",
                    "sentence": "Our results show that by making deep audio classifiers interpretable, one can analyze and compare the properties and strategies of different models beyond classification accuracy, which potentially opens up new ways for model improvements.",
                    "sentence_kor": "우리의 결과는 심층 오디오 분류기를 해석할 수 있게 함으로써 분류 정확도를 넘어 다양한 모델의 속성과 전략을 분석하고 비교할 수 있다는 것을 보여주며, 이는 잠재적으로 모델 개선을 위한 새로운 방법을 열어준다.",
                    "tag": "4+5"
                }
            ]
        },
        {
            "absNo": "1060",
            "abstractID": "SPA_abs-461",
            "text": [
                {
                    "index": "461-0",
                    "sentence": "Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales.",
                    "sentence_kor": "오디오 신호는 높은 시간 해상도로 샘플링되며, 오디오를 합성하는 방법을 배우려면 다양한 시간 간격에 걸쳐 구조를 캡처해야 한다.",
                    "tag": "2+3"
                },
                {
                    "index": "461-1",
                    "sentence": "Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation.",
                    "sentence_kor": "생성적 적대 네트워크(GAN)는 지역 및 전역적으로 일관된 이미지를 생성하는 데 큰 성공을 거두었지만 오디오 생성에는 거의 적용되지 않았다.",
                    "tag": "4"
                },
                {
                    "index": "461-2",
                    "sentence": "In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio.",
                    "sentence_kor": "이 논문에서 우리는 원시파형 오디오의 비지도 합성에 GAN을 적용하는 첫 번째 시도인 WaveGAN을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "461-3",
                    "sentence": "WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation.",
                    "sentence_kor": "WaveGAN은 효과 발생에 적합한 전역 일관성을 가진 1초 단위의 오디오 파형 합성이 가능하다.",
                    "tag": "4"
                },
                {
                    "index": "461-4",
                    "sentence": "Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano.",
                    "sentence_kor": "우리의 실험은 WaveGAN이 레이블 없이 작은 어휘 음성 데이터 세트에 대해 훈련할 때 이해하기 쉬운 단어 생성을 배우고 드럼, 새 발성 및 피아노와 같은 다른 영역의 오디오를 합성할 수 있다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "461-5",
                    "sentence": "We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.",
                    "sentence_kor": "WaveGAN을 이미지 생성용으로 설계된 GAN을 이미지 유사 오디오 기능 표현에 적용하는 방법과 비교하여 두 가지 접근 방식이 모두 유망하다는 것을 발견했다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1061",
            "abstractID": "SPA_abs-462",
            "text": [
                {
                    "index": "462-0",
                    "sentence": "What is a good visual representation for autonomous agents?",
                    "sentence_kor": "자율 에이전트의 좋은 시각적 표현은 무엇입니까?",
                    "tag": "1"
                },
                {
                    "index": "462-1",
                    "sentence": "We address this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a complex environment to a target object, e.g. go to the refrigerator.",
                    "sentence_kor": "우리는 이 질문을 시맨틱 비주얼 내비게이션의 맥락에서 다루는데, 이것은 로봇이 복잡한 환경을 통해 대상 물체로 이동하는 문제(예: 냉장고로 가는 것)이다.",
                    "tag": "1+2"
                },
                {
                    "index": "462-2",
                    "sentence": "Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues.",
                    "sentence_kor": "환경의 메트릭 의미 맵을 획득하고 탐색 계획을 사용하는 대신, 우리의 접근 방식은 공간 레이아웃과 의미적 상황 단서를 포착하는 표현 위에 탐색 정책을 학습한다.",
                    "tag": "3"
                },
                {
                    "index": "462-3",
                    "sentence": "We propose to using high level semantic and contextual features including segmentation and detection masks obtained by off-the-shelf state-of-the-art vision as observations and use deep network to learn the navigation policy.",
                    "sentence_kor": "우리는 기성 최첨단 비전으로 얻은 세분화 및 감지 마스크를 포함한 높은 수준의 의미 및 상황별 기능을 관찰로 사용하고 심층 네트워크를 사용하여 탐색 정책을 학습할 것을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "462-4",
                    "sentence": "This choice allows using additional data, from orthogonal sources, to better train different parts of the model the representation extraction is trained on large standard vision datasets while the navigation component leverages large synthetic environments for training.",
                    "sentence_kor": "이 선택을 통해 직교 소스의 추가 데이터를 사용하여 모델의 다양한 부분을 더 잘 훈련시킬 수 있으며, 탐색 구성요소는 대규모 합성 환경을 교육에 활용한다.",
                    "tag": "4"
                },
                {
                    "index": "462-5",
                    "sentence": "This combination of real and synthetic is possible because equitable feature representations are available in both (e.g., segmentation and detection masks), which alleviates the need for domain adaptation.",
                    "sentence_kor": "이러한 실제 및 합성 조합은 도메인 적응의 필요성을 완화시키는 분할 및 탐지 마스크와 같은 두 가지 모두에서 공평한 기능 표현을 사용할 수 있기 때문에 가능하다.",
                    "tag": "3"
                },
                {
                    "index": "462-6",
                    "sentence": "Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1].",
                    "sentence_kor": "표현 및 항법 정책은 액티브 비전 데이터 세트[1]에 설명된 대로 실제 비합성 환경에 쉽게 적용할 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "462-7",
                    "sentence": "Our approach gets successfully to the target in 54% of the cases in unexplored environments, compared to 46% for non-learning based approach, and 28% for the learning-based baseline.",
                    "sentence_kor": "비학습 기반 접근법의 경우 46%, 학습 기반 기준선의 경우 28%에 비해, 우리의 접근 방식은 탐색되지 않은 환경에서 54%의 사례에서 목표에 성공적으로 도달한다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1062",
            "abstractID": "SPA_abs-463",
            "text": [
                {
                    "index": "463-0",
                    "sentence": "Audio tagging aims to predict one or several labels in an audio clip.",
                    "sentence_kor": "오디오 태그는 오디오 클립에서 하나 또는 여러 개의 레이블을 예측하는 것을 목표로 합니다.",
                    "tag": "2"
                },
                {
                    "index": "463-1",
                    "sentence": "Many previous works use weakly labelled data (WLD) for audio tagging, where only presence or absence of sound events is known, but the order of sound events is unknown.",
                    "sentence_kor": "많은 이전 연구에서는 사운드 이벤트의 유무만 알려질 뿐 사운드 이벤트의 순서는 알 수 없는 오디오 태깅에 약하게 레이블링된 데이터(WLD)를 사용한다.",
                    "tag": "1"
                },
                {
                    "index": "463-2",
                    "sentence": "To use the order information of sound events, we propose sequential labelled data (SLD), where both the presence or absence and the order information of sound events are known.",
                    "sentence_kor": "소리 이벤트의 순서 정보를 사용하기 위해 소리 이벤트의 유무 및 순서 정보가 모두 알려진 순차 레이블링 데이터(SLD)를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "463-3",
                    "sentence": "To utilize SLD in audio tagging, we propose a Convolutional Recurrent Neural Network followed by a Connectionist Temporal Classification (CRNN-CTC) objective function to map from an audio clip spectrogram to SLD.",
                    "sentence_kor": "오디오 태깅에서 SLD를 활용하기 위해, 우리는 합성곱 반복 신경망에 이어 오디오 클립 스펙트로그램에서 SLD로 매핑하는 연결주의 시간 분류(CRNN-CTC) 목표 함수를 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "463-4",
                    "sentence": "Experiments show that CRNN-CTC obtains an Area Under Curve (AUC) score of 0.986 in audio tagging, outperforming the baseline CRNN of 0.908 and 0.815 with Max Pooling and Average Pooling, respectively.",
                    "sentence_kor": "실험에 따르면 CRNN-CTC는 오디오 태깅에서 0.986의 AUC(Area Under Curve) 점수를 얻어 각각 최대 풀링 및 평균 풀링으로 기준 CRNN 0.908과 0.815를 능가한다.",
                    "tag": "4"
                },
                {
                    "index": "463-5",
                    "sentence": "In addition, we show CRNN-CTC has the ability to predict the order of sound events in an audio clip.",
                    "sentence_kor": "또한 CRNN-CTC가 오디오 클립의 사운드 이벤트 순서를 예측할 수 있음을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1063",
            "abstractID": "SPA_abs-464",
            "text": [
                {
                    "index": "464-0",
                    "sentence": "Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene.",
                    "sentence_kor": "환경 오디오 태깅은 관심 있는 음향 장면에서 특정 음향 이벤트의 유무만 예측하는 것을 목표로 한다.",
                    "tag": "2"
                },
                {
                    "index": "464-1",
                    "sentence": "In this paper we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning.",
                    "sentence_kor": "본 논문에서 우리는 음향 모델링과 특징 학습의 두 부분으로 각각 오디오 태깅에 기여한다.",
                    "tag": "3"
                },
                {
                    "index": "464-2",
                    "sentence": "We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multi-label classification task.",
                    "sentence_kor": "다중 레이블 분류 작업을 처리하기 위해 비지도 기능 학습을 통합한 축소 심층 신경망(DNN) 프레임워크를 사용할 것을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "464-3",
                    "sentence": "For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multi-label classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available.",
                    "sentence_kor": "음향 모델링의 경우, 프레임 레벨 라벨이 아닌 청크(또는 발화) 레벨만 사용할 수 있다는 점을 고려하여 청크의 많은 상황별 프레임 세트를 DNN에 공급하여 예상 태그에 대한 다중 라벨 분류를 수행한다.",
                    "tag": "4"
                },
                {
                    "index": "464-4",
                    "sentence": "Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs.",
                    "sentence_kor": "DNN의 일반화 기능을 개선하기 위해 중퇴 및 배경 소음 인식 훈련도 채택된다.",
                    "tag": "3"
                },
                {
                    "index": "464-5",
                    "sentence": "For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to generate new data-driven features from the Mel-Filter Banks (MFBs) features.",
                    "sentence_kor": "비지도 기능 학습을 위해 대칭 또는 비대칭 딥 노이즈 제거 자동 인코더(sDA 또는 aDA)를 사용하여 MFB(Mel-Filter Banks) 기능에서 새로운 데이터 기반 기능을 생성할 것을 제안한다.",
                    "tag": "4"
                },
                {
                    "index": "464-6",
                    "sentence": "The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline.",
                    "sentence_kor": "배경 소음에 대해 평활되고 상황별 정보와 함께 더욱 압축되는 새로운 기능은 DNN 기준선의 성능을 더욱 향상시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "464-7",
                    "sentence": "Compared with the standard Gaussian Mixture Model (GMM) baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set.",
                    "sentence_kor": "DCASE 2016 오디오 태깅 도전의 표준 가우스 혼합 모델(GMM) 기준선과 비교하여, 우리가 제안한 방법은 개발 세트에서 0.21에서 0.13으로 상당한 EER(Equal Error Rate) 감소를 얻는다.",
                    "tag": "4"
                },
                {
                    "index": "464-8",
                    "sentence": "The proposed aDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set.",
                    "sentence_kor": "제안된 aDA 시스템은 개발 세트의 강력한 DNN 기준선에 비해 6.7%의 EER 감소를 얻을 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "464-9",
                    "sentence": "Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.",
                    "sentence_kor": "마지막으로, 그 결과는 또한 우리의 접근 방식이 DCASE 2016 오디오 태깅 작업의 평가 세트에서 0.15 EER로 최첨단 성능을 얻는 반면 이 과제의 1등 EER는 0.17임을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1064",
            "abstractID": "SPA_abs-465",
            "text": [
                {
                    "index": "465-0",
                    "sentence": "Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel.",
                    "sentence_kor": "단일 오디오 채널에서 물체가 심하게 겹치는 경우가 많기 때문에 비디오에서 소리가 나는 방법을 배우는 것은 어렵습니다.",
                    "tag": "1"
                },
                {
                    "index": "465-1",
                    "sentence": "Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of \"true\" mixed sounds.",
                    "sentence_kor": "시각적으로 안내되는 오디오 소스 분리를 위한 현재의 방법은 인위적으로 혼합된 비디오 클립으로 훈련함으로써 문제를 옆으로 비켜나지만, 이는 훈련 데이터 수집에 다루기 어려운 제한을 가하고 심지어 \"진정한\" 혼합 소리의 속성을 학습하는 것을 방해할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "465-2",
                    "sentence": "We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos.",
                    "sentence_kor": "레이블이 지정되지 않은 다중 소스 비디오에서 객체 레벨 사운드를 학습할 수 있는 공동 분리 훈련 패러다임을 소개한다.",
                    "tag": "2"
                },
                {
                    "index": "465-3",
                    "sentence": "Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair.",
                    "sentence_kor": "우리의 새로운 훈련 목표는 비슷하게 보이는 개체에 대한 심층 신경망의 분리된 오디오를 일관되게 식별할 수 있는 동시에 각 소스 훈련 쌍에 대한 정확한 비디오 레벨 오디오 트랙을 재현할 것을 요구한다.",
                    "tag": "2"
                },
                {
                    "index": "465-4",
                    "sentence": "Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training.",
                    "sentence_kor": "우리의 접근 방식은 훈련 중에 물체를 개별적으로 관찰하지 않은 경우에도 현실적인 테스트 비디오에서 소리를 분리한다.",
                    "tag": "3"
                },
                {
                    "index": "465-5",
                    "sentence": "We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.",
                    "sentence_kor": "MUSIC, AudioSet 및 AV-Bench 데이터 세트에 대한 시각적 안내 오디오 소스 분리 및 오디오 노이즈 제거에 대한 최첨단 결과를 얻는다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1065",
            "abstractID": "SPA_abs-466",
            "text": [
                {
                    "index": "466-0",
                    "sentence": "Stereo matching algorithms usually consist of four steps, including matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement.",
                    "sentence_kor": "스테레오 매칭 알고리즘은 일반적으로 매칭 비용 계산, 매칭 비용 집계, 시차 계산 및 시차 개선을 포함한 4단계로 구성됩니다.",
                    "tag": "1"
                },
                {
                    "index": "466-1",
                    "sentence": "Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution.",
                    "sentence_kor": "기존 CNN 기반 방법은 CNN을 채택하여 4단계 중 일부를 해결하거나 서로 다른 네트워크를 사용하여 다른 단계를 처리하므로 전체적인 최적 솔루션을 얻기 어렵다.",
                    "tag": "1"
                },
                {
                    "index": "466-2",
                    "sentence": "In this paper, we propose a network architecture to incorporate all steps of stereo matching.",
                    "sentence_kor": "본 논문에서, 우리는 스테레오 매칭의 모든 단계를 통합하는 네트워크 아키텍처를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "466-3",
                    "sentence": "The network consists of three parts.",
                    "sentence_kor": "네트워크는 세 부분으로 구성되어 있습니다.",
                    "tag": "3"
                },
                {
                    "index": "466-4",
                    "sentence": "The first part calculates the multi-scale shared features.",
                    "sentence_kor": "첫 번째 부분은 다중 스케일 공유 형상을 계산합니다.",
                    "tag": "3"
                },
                {
                    "index": "466-5",
                    "sentence": "The second part performs matching cost calculation, matching cost aggregation and disparity calculation to estimate the initial disparity using shared features.",
                    "sentence_kor": "두 번째 파트에서는 공유 기능을 사용하여 초기 불균형을 추정하기 위해 일치 비용 계산, 일치 비용 집계 및 불균형 계산을 수행합니다.",
                    "tag": "3"
                },
                {
                    "index": "466-6",
                    "sentence": "The initial disparity and the shared features are used to calculate the feature constancy that measures correctness of the correspondence between two input images.",
                    "sentence_kor": "초기 시차 및 공유 형상은 두 입력 영상 사이의 대응 정확성을 측정하는 형상 항상성을 계산하는 데 사용됩니다.",
                    "tag": "3"
                },
                {
                    "index": "466-7",
                    "sentence": "The initial disparity and the feature constancy are then fed to a sub-network to refine the initial disparity.",
                    "sentence_kor": "그런 다음 초기 불균형과 기능 항상성을 하위 네트워크에 공급하여 초기 불균형을 개선합니다.",
                    "tag": "3"
                },
                {
                    "index": "466-8",
                    "sentence": "The proposed method has been evaluated on the Scene Flow and KITTI datasets.",
                    "sentence_kor": "제안된 방법은 장면 흐름 및 KITTI 데이터 세트에서 평가되었다.",
                    "tag": "3"
                },
                {
                    "index": "466-9",
                    "sentence": "It achieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015 benchmarks while maintaining a very fast running time.",
                    "sentence_kor": "매우 빠른 실행 시간을 유지하면서 KITTI 2012 및 KITTI 2015 벤치마크에서 최첨단 성능을 달성합니다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1066",
            "abstractID": "SPA_abs-467",
            "text": [
                {
                    "index": "467-0",
                    "sentence": "Depth prediction is one of the fundamental problems in computer vision.",
                    "sentence_kor": "깊이 예측은 컴퓨터 비전의 근본적인 문제 중 하나이다.",
                    "tag": "1"
                },
                {
                    "index": "467-1",
                    "sentence": "In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks.",
                    "sentence_kor": "본 논문에서, 우리는 다양한 깊이 추정 작업에 대한 선호도 매트릭스를 학습하기 위한 간단하지만 효과적인 컨볼루션 공간 전파 네트워크(CSPN)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "467-2",
                    "sentence": "Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN).",
                    "sentence_kor": "특히, 반복 컨볼루션 작동 방식으로 전파를 수행하고 인접 픽셀 간의 친화도를 심층 컨볼루션 신경망(CNN)을 통해 학습하는 효율적인 선형 전파 모델이다.",
                    "tag": "4"
                },
                {
                    "index": "467-3",
                    "sentence": "We can append this module to any output from a state-of-the-art (SOTA) depth estimation networks to improve their performances.",
                    "sentence_kor": "이 모듈을 최첨단 SOTA(깊이 추정) 네트워크의 출력물에 추가하여 성능을 향상시킬 수 있다.",
                    "tag": "4"
                },
                {
                    "index": "467-4",
                    "sentence": "In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume.",
                    "sentence_kor": "실제로, 우리는 CSPN을 두 가지 측면에서 더 확장한다. 1) 깊이 완성 작업에 유용한 희소 깊이 맵을 추가 입력으로 받아들인다. 2) CNN에서 일반적으로 사용되는 3D 컨볼루션 작업과 유사하게, 우리는 3D 비용 볼륨을 사용하는 스테레오 매칭 작업에서 효과적인 하나의 추가 차원으로 특징을 처리할 수 있는 3D CSPN을 제안한다.",
                    "tag": "3+4"
                },
                {
                    "index": "467-5",
                    "sentence": "For the tasks of sparse to dense, a.k.a depth completion.",
                    "sentence_kor": "희박하게 밀도가 높은 작업의 경우, 깊이 완료라고 한다.",
                    "tag": "4"
                },
                {
                    "index": "467-6",
                    "sentence": "We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network.",
                    "sentence_kor": "인기 있는 NYU v2 및 KITTI 데이터 세트에 대해 제안된 CPSN 접속 알고리즘을 실험했는데, 여기서 제안된 알고리즘은 고품질(예: 깊이 오류 30% 더 감소)을 생성할 뿐만 아니라 이전 SOTA 공간 전파 네트워크보다 더 빠르게(예: 2배에서 5배 더 빠른) 실행된다는 것을 보여준다.",
                    "tag": "4"
                },
                {
                    "index": "467-7",
                    "sentence": "We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module.",
                    "sentence_kor": "또한 장면 플로우 및 KITTI 스테레오 데이터 세트에서 스테레오 매칭 알고리즘을 평가했으며 제안된 모듈의 효과를 보여주는 KITTI 스테레오 2012 및 2015 벤치마크에서 모두 1위를 차지했다.",
                    "tag": "4"
                },
                {
                    "index": "467-8",
                    "sentence": "The code of CSPN proposed in this work will be released at this https URL.",
                    "sentence_kor": "이 작업에서 제안된 CSPN 코드는 이 HTTPS URL에서 릴리스됩니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1067",
            "abstractID": "SPA_abs-468",
            "text": [
                {
                    "index": "468-0",
                    "sentence": "This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60 fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps.",
                    "sentence_kor": "본 논문은 NVidia Titan X에서 60fps로 실행되는 실시간 스테레오 매칭을 위한 최초의 엔드 투 엔드 심층 아키텍처인 StereoNet을 제시하여 고품질의 에지 보존 및 양자화가 없는 불균형 맵을 제작한다.",
                    "tag": "1"
                },
                {
                    "index": "468-1",
                    "sentence": "A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches.",
                    "sentence_kor": "본 논문의 핵심 통찰력은 네트워크가 기존의 스테레오 매칭 접근 방식보다 훨씬 높은 하위 픽셀 매칭 정밀도를 달성한다는 것이다.",
                    "tag": "1"
                },
                {
                    "index": "468-2",
                    "sentence": "This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision.",
                    "sentence_kor": "이를 통해 높은 시차 정밀도를 달성하는 데 필요한 모든 정보를 인코딩하는 매우 낮은 해상도 비용 볼륨을 사용하여 실시간 성능을 달성할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "468-3",
                    "sentence": "Spatial precision is achieved by employing a learned edge-aware upsampling function.",
                    "sentence_kor": "공간 정밀도는 학습된 에지 인식 업샘플링 기능을 사용하여 달성된다.",
                    "tag": "1"
                },
                {
                    "index": "468-4",
                    "sentence": "Our model uses a Siamese network to extract features from the left and right image.",
                    "sentence_kor": "우리 모델은 샴 네트워크를 사용하여 왼쪽과 오른쪽 이미지에서 형상을 추출한다.",
                    "tag": "1"
                },
                {
                    "index": "468-5",
                    "sentence": "A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks.",
                    "sentence_kor": "불균형의 첫 번째 추정치는 매우 낮은 해상도 비용 볼륨으로 계산한 다음 계층적으로 모델은 콤팩트 픽셀 대 픽셀 정제 네트워크를 사용하는 학습된 업샘플링 기능을 통해 고주파 세부 정보를 다시 도입한다.",
                    "tag": "1"
                },
                {
                    "index": "468-6",
                    "sentence": "Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output.",
                    "sentence_kor": "이 기능은 색상 입력을 가이드로 활용하여 고품질 에지 인식 출력을 생성할 수 있습니다.",
                    "tag": "1"
                },
                {
                    "index": "468-7",
                    "sentence": "We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.",
                    "sentence_kor": "우리는 여러 벤치마크에서 설득력 있는 결과를 얻어, 제안된 방법이 어떻게 허용 가능한 계산 예산으로 최고의 유연성을 제공하는지를 보여준다.",
                    "tag": "1"
                }
            ]
        },
        {
            "absNo": "1068",
            "abstractID": "SPA_abs-469",
            "text": [
                {
                    "index": "469-0",
                    "sentence": "Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures.",
                    "sentence_kor": "의미 파싱은 자연어(NL) 발화를 일반적으로 트리 구조로 표현되는 형식적 의미 표현(MR)으로 변환하는 작업이다.",
                    "tag": "1"
                },
                {
                    "index": "469-1",
                    "sentence": "Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models.",
                    "sentence_kor": "해당 MR로 NL 발언에 주석을 다는 것은 비용과 시간이 많이 걸리기 때문에 레이블이 지정된 데이터의 제한된 가용성은 종종 데이터 중심 감독 모델의 병목 현상이 된다.",
                    "tag": "1"
                },
                {
                    "index": "469-2",
                    "sentence": "We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances.",
                    "sentence_kor": "제한된 양의 병렬 데이터와 쉽게 사용할 수 있는 레이블이 없는 NL 발화로부터 모두 학습하는 반감독 의미 구문 분석을 위한 변형 자동 인코딩 모델인 Struct VAE를 소개한다.",
                    "tag": "2+3"
                },
                {
                    "index": "469-3",
                    "sentence": "StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables.",
                    "sentence_kor": "StructureVAE는 라벨이 부착되지 않은 데이터에서 트리 구조 잠재 변수로 관찰되지 않은 잠재 MR을 모델링한다.",
                    "tag": "2"
                },
                {
                    "index": "469-4",
                    "sentence": "Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",
                    "sentence_kor": "ATIS 도메인과 Python 코드 생성에 대한 의미 파싱에 대한 실험은 레이블이 지정되지 않은 추가 데이터로 StructureVAE가 강력한 감독 모델을 능가한다는 것을 보여준다.",
                    "tag": "4"
                }
            ]
        },
        {
            "absNo": "1069",
            "abstractID": "SPA_abs-470",
            "text": [
                {
                    "index": "470-0",
                    "sentence": "We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate.",
                    "sentence_kor": "우리는 유망한 궤적의 메모리 버퍼를 활용하여 정책 기울기 추정치의 분산을 줄이는 간단하고 새로운 방법인 MAPO(Memory Augmented Policy Optimization)를 제시한다.",
                    "tag": "1"
                },
                {
                    "index": "470-1",
                    "sentence": "MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks.",
                    "sentence_kor": "MAPO는 구조화된 예측 및 조합 최적화 작업과 같은 개별 작업이 있는 결정론적 환경에 적용할 수 있다.",
                    "tag": "1"
                },
                {
                    "index": "470-2",
                    "sentence": "We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer.",
                    "sentence_kor": "우리는 예상 수익 목표를 메모리 버퍼 내부의 높은 수익 궤적에 대한 기대와 버퍼 외부 궤적에 대한 별도의 기대라는 두 항의 가중치 합으로 표현한다.",
                    "tag": "1"
                },
                {
                    "index": "470-3",
                    "sentence": "To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training.",
                    "sentence_kor": "MAPO의 효율적인 알고리즘을 만들기 위해 (1) 훈련을 가속화하고 안정화시키기 위한 메모리 가중치 클리핑, (2) 높은 보상 궤적을 발견하기 위한 체계적인 탐색, (3) 훈련을 스케일업하기 위한 메모리 버퍼 내외부의 분산 샘플링을 제안한다.",
                    "tag": "2+3"
                },
                {
                    "index": "470-4",
                    "sentence": "MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards.",
                    "sentence_kor": "MAPO는 특히 보상이 희박한 작업에서 정책 그라데이션의 샘플 효율성과 견고성을 개선한다.",
                    "tag": "4"
                },
                {
                    "index": "470-5",
                    "sentence": "We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing).",
                    "sentence_kor": "자연어에서 약하게 감독되는 프로그램 합성(의미적 구문 분석)에 대해 MAPO를 평가한다.",
                    "tag": "4"
                },
                {
                    "index": "470-6",
                    "sentence": "On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%.",
                    "sentence_kor": "WikiTableQuestions 벤치마크에서 최첨단 기술을 2.6% 향상시켜 46.3%의 정확도를 달성했습니다.",
                    "tag": "4"
                },
                {
                    "index": "470-7",
                    "sentence": "On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision.",
                    "sentence_kor": "WikiSQL 벤치마크에서 MAPO는 감독만 약해도 74.9%의 정확도를 달성하며, 전체 감독을 통해 여러 강력한 기준선을 능가합니다.",
                    "tag": "4"
                },
                {
                    "index": "470-8",
                    "sentence": "Our source code is available at this https URL",
                    "sentence_kor": "이 https URL에서 소스 코드를 사용할 수 있습니다.",
                    "tag": "6"
                }
            ]
        },
        {
            "absNo": "1070",
            "abstractID": "SPA_abs-471",
            "text": [
                {
                    "index": "471-0",
                    "sentence": "Cross-correlator plays a significant role in many visual perception tasks, such as object detection and tracking.",
                    "sentence_kor": "교차 상관기는 물체 감지 및 추적과 같은 많은 시각적 인식 작업에서 중요한 역할을 한다.",
                    "tag": "1"
                },
                {
                    "index": "471-1",
                    "sentence": "Beyond the linear cross-correlator, this paper proposes a kernel cross-correlator (KCC) that breaks traditional limitations.",
                    "sentence_kor": "이 논문은 선형 교차 상관기 외에도 기존의 한계를 뛰어넘는 커널 교차 상관기(KCC)를 제안한다.",
                    "tag": "2"
                },
                {
                    "index": "471-2",
                    "sentence": "First, by introducing the kernel trick, the KCC extends the linear cross-correlation to non-linear space, which is more robust to signal noises and distortions.",
                    "sentence_kor": "첫째, 커널 트릭을 도입함으로써 KCC는 선형 교차 상관을 비선형 공간으로 확장하여 신호 잡음과 왜곡에 더 강하다.",
                    "tag": "3+4"
                },
                {
                    "index": "471-3",
                    "sentence": "Second, the connection to the existing works shows that KCC provides a unified solution for correlation filters.",
                    "sentence_kor": "둘째, 기존 작업과의 연계는 KCC가 상관 필터에 대한 통일된 솔루션을 제공한다는 것을 보여준다.",
                    "tag": "3+4"
                },
                {
                    "index": "471-4",
                    "sentence": "Third, KCC is applicable to any kernel function and is not limited to circulant structure on training data, thus it is able to predict affine transformations with customized properties.",
                    "sentence_kor": "셋째, KCC는 모든 커널 함수에 적용 가능하며 훈련 데이터의 순환기 구조에만 국한되지 않기 때문에 맞춤형 특성을 가진 아핀 변환을 예측할 수 있다.",
                    "tag": "3+4"
                },
                {
                    "index": "471-5",
                    "sentence": "Last, by leveraging the fast Fourier transform (FFT), KCC eliminates direct calculation of kernel vectors, thus achieves better performance yet still with a reasonable computational cost.",
                    "sentence_kor": "마지막으로, 고속 푸리에 변환(FFT)을 활용하여 KCC는 커널 벡터의 직접 계산을 제거하여 합리적인 계산 비용으로 더 나은 성능을 달성한다.",
                    "tag": "3+4"
                },
                {
                    "index": "471-6",
                    "sentence": "Comprehensive experiments on visual tracking and human activity recognition using wearable devices demonstrate its robustness, flexibility, and efficiency.",
                    "sentence_kor": "웨어러블 기기를 이용한 시각적 추적 및 인간 활동 인식에 대한 종합적인 실험은 견고성, 유연성 및 효율성을 입증한다.",
                    "tag": "3+4"
                },
                {
                    "index": "471-7",
                    "sentence": "The source codes of both experiments are released at this https URL",
                    "sentence_kor": "두 실험의 소스 코드는 이 https URL에서 공개됩니다.",
                    "tag": "6"
                }
            ]
        }
    ]
}