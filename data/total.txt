Humor is an important social phenomenon, serving complex social and psychological functions.	1
However, despite being studied for millennia humor is computationally not well understood, often considered an AI-complete problem.	1
In this work, we introduce a novel setting in humor mining: automatically detecting funny and unusual scientific papers.	2
We are inspired by the Ig Nobel prize, a satirical prize awarded annually to celebrate funny scientific achievements (example past winner: “Are cows more likely to lie down the longer they stand?”).	1
This challenging task has unique characteristics that make it particularly suitable for automatic learning.	3
We construct a dataset containing thousands of funny papers and use it to learn classifiers, combining findings from psychology and linguistics with recent advances in NLP.	3
We use our models to identify potentially funny papers in a large dataset of over 630,000 articles.	3
The results demonstrate the potential of our methods, and more broadly the utility of integrating state-of-the-art NLP methods with insights from more traditional disciplines	4
----------
This paper presents a novel task to generate poll questions for social media posts.	2
It offers an easy way to hear the voice from the public and learn from their feelings to important social topics.	2
While most related work tackles formal languages (e.g., exam papers), we generate poll questions for short and colloquial social media messages exhibiting severe data sparsity.	3
To deal with that, we propose to encode user comments and discover latent topics therein as contexts.	3
They are then incorporated into a sequence-to-sequence (S2S) architecture for question generation and its extension with dual decoders to additionally yield poll choices (answers).	3
For experiments, we collect a large-scale Chinese dataset from Sina Weibo containing over 20K polls.	3
The results show that our model outperforms the popular S2S models without exploiting topics from comments and the dual decoder design can further benefit the prediction of both questions and answers.	4
Human evaluations further exhibit our superiority in yielding high-quality polls helpful to draw user engagements.	4
----------
Detecting online hate is a difficult task that even state-of-the-art models struggle with.	1
Typically, hate speech detection models are evaluated by measuring their performance on held-out test data using metrics such as accuracy and F1 score.	1
However, this approach makes it difficult to identify specific model weak points.	1
It also risks overestimating generalisable model performance due to increasingly well-evidenced systematic gaps and biases in hate speech datasets.	1
To enable more targeted diagnostic insights, we introduce HateCheck, a suite of functional tests for hate speech detection models.	2
We specify 29 model functionalities motivated by a review of previous research and a series of interviews with civil society stakeholders.	3
We craft test cases for each functionality and validate their quality through a structured annotation process.	3
To illustrate HateCheck’s utility, we test near-state-of-the-art transformer models as well as two popular commercial models, revealing critical model weaknesses.	3
----------
Recent studies constructing direct interactions between the claim and each single user response (a comment or a relevant article) to capture evidence have shown remarkable success in interpretable claim verification.	1
Owing to different single responses convey different cognition of individual users (i.e., audiences), the captured evidence belongs to the perspective of individual cognition.	1
However, individuals’ cognition of social things is not always able to truly reflect the objective.	1
There may be one-sided or biased semantics in their opinions on a claim.	1
The captured evidence correspondingly contains some unobjective and biased evidence fragments, deteriorating task performance.	1
In this paper, we propose a Dual-view model based on the views of Collective and Individual Cognition (CICD) for interpretable claim verification.	2
From the view of the collective cognition, we not only capture the word-level semantics based on individual users, but also focus on sentence-level semantics (i.e., the overall responses) among all users and adjust the proportion between them to generate global evidence.	3
From the view of individual cognition, we select the top-k articles with high degree of difference and interact with the claim to explore the local key evidence fragments.	3
To weaken the bias of individual cognition-view evidence, we devise inconsistent loss to suppress the divergence between global and local evidence for strengthening the consistent shared evidence between the both.	3
Experiments on three benchmark datasets confirm that CICD achieves state-of-the-art performance.	4
----------
Rap generation, which aims to produce lyrics and corresponding singing beats, needs to model both rhymes and rhythms.	1
Previous works for rap generation focused on rhyming lyrics, but ignored rhythmic beats, which are important for rap performance.	1
In this paper, we develop DeepRapper, a Transformer-based rap generation system that can model both rhymes and rhythms.	2
Since there is no available rap datasets with rhythmic beats, we develop a data mining pipeline to collect a large-scale rap dataset, which includes a large number of rap songs with aligned lyrics and rhythmic beats.	3
Second, we design a Transformer-based autoregressive language model which carefully models rhymes and rhythms.	3
Specifically, we generate lyrics in the reverse order with rhyme representation and constraint for rhyme enhancement, and insert a beat symbol into lyrics for rhythm/beat modeling.	3
To our knowledge, DeepRapper is the first system to generate rap with both rhymes and rhythms.	4
Both objective and subjective evaluations demonstrate that DeepRapper generates creative and high-quality raps with rhymes and rhythms.	5
----------
Text style transfer aims to alter the style (e.g., sentiment) of a sentence while preserving its content.	1
A common approach is to map a given sentence to content representation that is free of style, and the content representation is fed to a decoder with a target style.	1
Previous methods in filtering style completely remove tokens with style at the token level, which incurs the loss of content information.	1
In this paper, we propose to enhance content preservation by implicitly removing the style information of each token with reverse attention, and thereby retain the content.	2
Furthermore, we fuse content information when building the target style representation, making it dynamic with respect to the content.	2
Our method creates not only style-independent content representation, but also content-dependent style representation in transferring style.	3
Empirical results show that our method outperforms the state-of-the-art baselines by a large margin in terms of content preservation.	4
In addition, it is also competitive in terms of style transfer accuracy and fluency.	4
----------
This paper focuses on Seq2Seq (S2S) constrained text generation where the text generator is constrained to mention specific words which are inputs to the encoder in the generated outputs.	2
Pre-trained S2S models or a Copy Mechanism are trained to copy the surface tokens from encoders to decoders, but they cannot guarantee constraint satisfaction.	1
Constrained decoding algorithms always produce hypotheses satisfying all constraints.	1
However, they are computationally expensive and can lower the generated text quality.	1
In this paper, we propose Mention Flags (MF), which traces whether lexical constraints are satisfied in the generated outputs in an S2S decoder.	2
The MF models can be trained to generate tokens in a hypothesis until all constraints are satisfied, guaranteeing high constraint satisfaction.	3
Our experiments on the Common Sense Generation task (CommonGen) (Lin et al., 2020), End2end Restaurant Dialog task (E2ENLG) (Duˇsek et al., 2020) and Novel Object Captioning task (nocaps) (Agrawal et al., 2019) show that the MF models maintain higher constraint satisfaction and text quality than the baseline models and other constrained decoding algorithms, achieving state-of-the-art performance on all three tasks.	4
These results are achieved with a much lower run-time than constrained decoding algorithms.	4
We also show that the MF models work well in the low-resource setting.	4
----------
Concept-to-text Natural Language Generation is the task of expressing an input meaning representation in natural language.	1
Previous approaches in this task have been able to generalise to rare or unseen instances by relying on a delexicalisation of the input.	1
However, this often requires that the input appears verbatim in the output text.	1
This poses challenges in multilingual settings, where the task expands to generate the output text in multiple languages given the same input.	1
In this paper, we explore the application of multilingual models in concept-to-text and propose Language Agnostic Delexicalisation, a novel delexicalisation method that uses multilingual pretrained embeddings, and employs a character-level post-editing model to inflect words in their correct form during relexicalisation.	2
Our experiments across five datasets and five languages show that multilingual models outperform monolingual models in concept-to-text and that our framework outperforms previous approaches, especially in low resource conditions.	4
----------
Nowadays, open-domain dialogue models can generate acceptable responses according to the historical context based on the large-scale pre-trained language models.	1
However, they generally concatenate the dialogue history directly as the model input to predict the response, which we named as the flat pattern and ignores the dynamic information flow across dialogue utterances.	1
In this work, we propose the DialoFlow model, in which we introduce a dynamic flow mechanism to model the context flow, and design three training objectives to capture the information dynamics across dialogue utterances by addressing the semantic influence brought about by each utterance in large-scale pre-training.	2
Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset demonstrate that our DialoFlow significantly outperforms the DialoGPT on the dialogue generation task.	4
Besides, we propose the Flow score, an effective automatic metric for evaluating interactive human-bot conversation quality based on the pre-trained DialoFlow, which presents high chatbot-level correlation (r=0.9) with human ratings among 11 chatbots.	2
Code and pre-trained models will be public.	6
----------
The goal of dialogue state tracking (DST) is to predict the current dialogue state given all previous dialogue contexts.	1
Existing approaches generally predict the dialogue state at every turn from scratch.	1
However, the overwhelming majority of the slots in each turn should simply inherit the slot values from the previous turn.	1
Therefore, the mechanism of treating slots equally in each turn not only is inefficient but also may lead to additional errors because of the redundant slot value generation.	1
To address this problem, we devise the two-stage DSS-DST which consists of the Dual Slot Selector based on the current turn dialogue, and the Slot Value Generator based on the dialogue history.	2
The Dual Slot Selector determines each slot whether to update slot value or to inherit the slot value from the previous turn from two aspects: (1) if there is a strong relationship between it and the current turn dialogue utterances; (2) if a slot value with high reliability can be obtained for it through the current turn dialogue.	3
The slots selected to be updated are permitted to enter the Slot Value Generator to update values by a hybrid method, while the other slots directly inherit the values from the previous turn.	3
Empirical results show that our method achieves 56.93%, 60.73%, and 58.04% joint accuracy on MultiWOZ 2.0, MultiWOZ 2.1, and MultiWOZ 2.2 datasets respectively and achieves a new state-of-the-art performance with significant improvements.	4
----------
One of the difficulties in training dialogue systems is the lack of training data.	1
We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator.	1
Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents.	2
In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language.	3
With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions.	3
In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer.	3
We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning.	4
We also show that our method leads to improvements in dialogue system performance on complete datasets.	4
----------
Maintaining a consistent persona is essential for dialogue agents.	1
Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models.	1
This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model.	2
Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding.	3
In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner.	3
Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.	4
----------
Multi-intent SLU can handle multiple intents in an utterance, which has attracted increasing attention.	1
However, the state-of-the-art joint models heavily rely on autoregressive approaches, resulting in two issues: slow inference speed and information leakage.	1
In this paper, we explore a non-autoregressive model for joint multiple intent detection and slot filling, achieving more fast and accurate.	2
Specifically, we propose a Global-Locally Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction layer is proposed to model slot dependency for alleviating uncoordinated slots problem while a global intent-slot graph interaction layer is introduced to model the interaction between multiple intents and all slots in the utterance.	2
Experimental results on two public datasets show that our framework achieves state-of-the-art performance while being 11.5 times faster.	4
----------
Both performance and efficiency are crucial factors for sequence labeling tasks in many real-world scenarios.	1
Although the pre-trained models (PTMs) have significantly improved the performance of various sequence labeling tasks, their computational cost is expensive.	1
To alleviate this problem, we extend the recent successful early-exit mechanism to accelerate the inference of PTMs for sequence labeling tasks.	1
However, existing early-exit mechanisms are specifically designed for sequence-level tasks, rather than sequence labeling.	1
In this paper, we first propose a simple extension of sentence-level early-exit for sequence labeling tasks.	2
To further reduce the computational cost, we also propose a token-level early-exit mechanism that allows partial tokens to exit early at different layers.	2
Considering the local dependency inherent in sequence labeling, we employed a window-based criterion to decide for a token whether or not to exit.	3
The token-level early-exit brings the gap between training and inference, so we introduce an extra self-sampling fine-tuning stage to alleviate it.	3
The extensive experiments on three popular sequence labeling tasks show that our approach can save up to 66%∼75% inference cost with minimal performance degradation.	4
Compared with competitive compressed models such as DistilBERT, our approach can achieve better performance under the same speed-up ratios of 2×, 3×, and 4×.	4
----------
Although the existing Named Entity Recognition (NER) models have achieved promising performance, they suffer from certain drawbacks.	1
The sequence labeling-based NER models do not perform well in recognizing long entities as they focus only on word-level information, while the segment-based NER models which focus on processing segment instead of single word are unable to capture the word-level dependencies within the segment.	1
Moreover, as boundary detection and type prediction may cooperate with each other for the NER task, it is also important for the two sub-tasks to mutually reinforce each other by sharing their information.	1
In this paper, we propose a novel Modularized Interaction Network (MIN) model which utilizes both segment-level information and word-level dependencies, and incorporates an interaction mechanism to support information sharing between boundary detection and type prediction to enhance the performance for the NER task.	2
We have conducted extensive experiments based on three NER benchmark datasets.	3
The performance results have shown that the proposed MIN model has outperformed the current state-of-the-art models.	4
----------
Capturing interactions among event arguments is an essential step towards robust event argument extraction (EAE).	1
However, existing efforts in this direction suffer from two limitations: 1) The argument role type information of contextual entities is mainly utilized as training signals, ignoring the potential merits of directly adopting it as semantically rich input features; 2) The argument-level sequential semantics, which implies the overall distribution pattern of argument roles over an event mention, is not well characterized.	1
To tackle the above two bottlenecks, we formalize EAE as a Seq2Seq-like learning problem for the first time, where a sentence with a specific event trigger is mapped to a sequence of event argument roles.	2
A neural architecture with a novel Bi-directional Entity-level Recurrent Decoder (BERD) is proposed to generate argument roles by incorporating contextual entities’ argument role predictions, like a word-by-word text generation process, thereby distinguishing implicit argument distribution patterns within an event more accurately.	3
----------
Many joint entity relation extraction models setup two separated label spaces for the two sub-tasks (i.e., entity detection and relation classification).	1
We argue that this setting may hinder the information interaction between entities and relations.	1
In this work, we propose to eliminate the different treatment on the two sub-tasks’ label spaces.	2
The input of our model is a table containing all word pairs from a sentence.	3
Entities and relations are represented by squares and rectangles in the table.	3
We apply a unified classifier to predict each cell’s label, which unifies the learning of two sub-tasks.	3
For testing, an effective (yet fast) approximate decoder is proposed for finding squares and rectangles from tables.	3
Experiments on three benchmarks (ACE04, ACE05, SciERC) show that, using only half the number of parameters, our model achieves competitive accuracy with the best extractor, and is faster.	4
----------
Continual learning has gained increasing attention in recent years, thanks to its biological interpretation and efficiency in many real-world applications.	1
As a typical task of continual learning, continual relation extraction (CRE) aims to extract relations between entities from texts, where the samples of different relations are delivered into the model continuously.	1
Some previous works have proved that storing typical samples of old relations in memory can help the model keep a stable understanding of old relations and avoid forgetting them.	1
However, most methods heavily depend on the memory size in that they simply replay these memorized samples in subsequent tasks.	1
To fully utilize memorized samples, in this paper, we employ relation prototype to extract useful information of each relation.	2
Specifically, the prototype embedding for a specific relation is computed based on memorized samples of this relation, which is collected by K-means algorithm.	3
The prototypes of all observed relations at current learning stage are used to re-initialize a memory network to refine subsequent sample embeddings, which ensures the model’s stable understanding on all observed relations when learning a new task.	3
Compared with previous CRE models, our model utilizes the memory information sufficiently and efficiently, resulting in enhanced CRE performance.	3
Our experiments show that the proposed model outperforms the state-of-the-art CRE models and has great advantage in avoiding catastrophic forgetting.	4
The code and datasets are released on https://github.com/fd2014cl/RP-CRE.	6
----------
Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind.	1
In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions.	2
Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance.	1
To this end, we propose mRASP2, a training method to obtain a single unified multilingual translation model.	2
mRASP2 is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations.	3
For English-centric directions, mRASP2 achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks.	4
For non-English directions, mRASP2 achieves an improvement of average 10+ BLEU compared with the multilingual baseline	4
----------
One of the reasons Transformer translation models are popular is that self-attention networks for context modelling can be easily parallelized at sequence level.	1
However, the computational complexity of a self-attention network is O(n2), increasing quadratically with sequence length.	1
By contrast, the complexity of LSTM-based approaches is only O(n).	1
In practice, however, LSTMs are much slower to train than self-attention networks as they cannot be parallelized at sequence level: to model context, the current LSTM state relies on the full LSTM computation of the preceding state.	1
This has to be computed n times for a sequence of length n. The linear transformations involved in the LSTM gate and state computations are the major cost factors in this.	1
To enable sequence-level parallelization of LSTMs, we approximate full LSTM context modelling by computing hidden states and gates with the current input and a simple bag-of-words representation of the preceding tokens context.	2
This allows us to compute each input step efficiently in parallel, avoiding the formerly costly sequential linear transformations.	4
We then connect the outputs of each parallel step with computationally cheap element-wise computations.	3
We call this the Highly Parallelized LSTM.	3
To further constrain the number of LSTM parameters, we compute several small HPLSTMs in parallel like multi-head attention in the Transformer.	3
The experiments show that our MHPLSTM decoder achieves significant BLEU improvements, while being even slightly faster than the self-attention network in training, and much faster than the standard LSTM.	4
----------
Word alignment and machine translation are two closely related tasks.	1
Neural translation models, such as RNN-based and Transformer models, employ a target-to-source attention mechanism which can provide rough word alignments, but with a rather low accuracy.	1
High-quality word alignment can help neural machine translation in many different ways, such as missing word detection, annotation transfer and lexicon injection.	1
Existing methods for learning word alignment include statistical word aligners (e.g. GIZA++) and recently neural word alignment models.	1
This paper presents a bidirectional Transformer based alignment (BTBA) model for unsupervised learning of the word alignment task.	2
Our BTBA model predicts the current target word by attending the source context and both left-side and right-side target context to produce accurate target-to-source attention (alignment).	3
We further fine-tune the target-to-source attention in the BTBA model to obtain better alignments using a full context based optimization method and self-supervised training.	3
We test our method on three word alignment tasks and show that our method outperforms both previous neural word alignment approaches and the popular statistical word aligner GIZA++.	4
----------
Multilingual neural machine translation aims at learning a single translation model for multiple languages.	1
These jointly trained models often suffer from performance degradationon rich-resource language pairs.	1
We attribute this degeneration to parameter interference.	1
In this paper, we propose LaSS to jointly train a single unified multilingual MT model.	2
LaSS learns Language Specific Sub-network (LaSS) for each language pair to counter parameter interference.	3
Comprehensive experiments on IWSLT and WMT datasets with various Transformer architectures show that LaSS obtains gains on 36 language pairs by up to 1.2 BLEU.	3+4
Besides, LaSS shows its strong generalization performance at easy adaptation to new language pairs and zero-shot translation.	4
LaSS boosts zero-shot translation with an average of 8.3 BLEU on 30 language pairs.	4
Codes and trained models are available at https://github.com/NLP-Playground/LaSS.	6
----------
While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data.	1
Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field.	1
One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data.	1
Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts.	1
In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation.	2
A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.	4
----------
As a fine-grained task, the annotation cost of aspect term extraction is extremely high.	1
Recent attempts alleviate this issue using domain adaptation that transfers common knowledge across domains.	1
Since most aspect terms are domain-specific, they cannot be transferred directly.	1
Existing methods solve this problem by associating aspect terms with pivot words (we call this passive domain adaptation because the transfer of aspect terms relies on the links to pivots).	1
However, all these methods need either manually labeled pivot words or expensive computing resources to build associations.	1
In this paper, we propose a novel active domain adaptation method.	2
Our goal is to transfer aspect terms by actively supplementing transferable knowledge.	2
To this end, we construct syntactic bridges by recognizing syntactic roles as pivots instead of as links to pivots.	3
We also build semantic bridges by retrieving transferable semantic prototypes.	3
Extensive experiments show that our method significantly outperforms previous approaches.	4
----------
With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms.	1
We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image.	1
However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset.	1
In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection.	2
Specifically, we first encode different modalities to capture hidden representations.	3
Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset.	3
Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs.	3
Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	4
----------
The product reviews summarization task aims to automatically produce a short summary for a set of reviews of a given product.	1
Such summaries are expected to aggregate a range of different opinions in a concise, coherent and informative manner.	1
This challenging task gives rise to two shortcomings in existing work.	1
First, summarizers tend to favor generic content that appears in reviews for many different products, resulting in template-like, less informative summaries.	1
Second, as reviewers often disagree on the pros and cons of a given product, summarizers sometimes yield inconsistent, self-contradicting summaries.	1
We propose the PASS system (Perturb-and-Select Summarizer) that employs a large pre-trained Transformer-based model (T5 in our case), which follows a few-shot fine-tuning scheme.	2
A key component of the PASS system relies on applying systematic perturbations to the model’s input during inference, which allows it to generate multiple different summaries per product.	3
We develop a method for ranking these summaries according to desired criteria, coherence in our case, enabling our system to almost entirely avoid the problem of self-contradiction.	3
We compare our system against strong baselines on publicly available datasets, and show that it produces summaries which are more informative, diverse and coherent.	4
----------
For sentence-level extractive summarization, there is a disproportionate ratio of selected and unselected sentences, leading to flatting the summary features when maximizing the accuracy.	1
The imbalanced classification of summarization is inherent, which can’t be addressed by common algorithms easily.	1
In this paper, we conceptualize the single-document extractive summarization as a rebalance problem and present a deep differential amplifier framework.	2
Specifically, we first calculate and amplify the semantic difference between each sentence and all other sentences, and then apply the residual unit as the second item of the differential amplifier to deepen the architecture.	3
Finally, to compensate for the imbalance, the corresponding objective loss of minority class is boosted by a weighted cross-entropy.	3
In contrast to previous approaches, this model pays more attention to the pivotal information of one sentence, instead of all the informative context modeling by recurrent or Transformer architecture.	4
We demonstrate experimentally on two benchmark datasets that our summarizer performs competitively against state-of-the-art methods.	4
Our source code will be available on Github.	6
----------
In this paper, we address a novel task, Multiple TimeLine Summarization (MTLS), which extends the flexibility and versatility of Time-Line Summarization (TLS).	2
Given any collection of time-stamped news articles, MTLS automatically discovers important yet different stories and generates a corresponding time-line for each story.	3
To achieve this, we propose a novel unsupervised summarization framework based on two-stage affinity propagation.	2
We also introduce a quantitative evaluation measure for MTLS based on previousTLS evaluation methods.	2
Experimental results show that our MTLS framework demonstrates high effectiveness and MTLS task can give bet-ter results than TLS.	4
----------
Recently, opinion summarization, which is the generation of a summary from multiple reviews, has been conducted in a self-supervised manner by considering a sampled review as a pseudo summary.	1
However, non-text data such as image and metadata related to reviews have been considered less often.	1
To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum.	2
Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary.	3
To resolve the inherent heterogeneity of multimodal data, we propose a multimodal training pipeline.	2
We first pretrain the text encoder–decoder based solely on text modality data.	3
Subsequently, we pretrain the non-text modality encoders by considering the pretrained text decoder as a pivot for the homogeneous representation of multimodal data.	3
Finally, to fuse multimodal representations, we train the entire framework in an end-to-end manner.	3
We demonstrate the superiority of MultimodalSum by conducting experiments on Yelp and Amazon datasets.	3+4
----------
In recent years, reference-based and supervised summarization evaluation metrics have been widely explored.	1
However, collecting human-annotated references and ratings are costly and time-consuming.	1
To avoid these limitations, we propose a training-free and reference-free summarization evaluation metric.	2
Our metric consists of a centrality-weighted relevance score and a self-referenced redundancy score.	3
The relevance score is computed between the pseudo reference built from the source document and the given summary, where the pseudo reference content is weighted by the sentence centrality to provide importance guidance.	3
Besides an F1-based relevance score, we also design an F𝛽-based variant that pays more attention to the recall score.	3
As for the redundancy score of the summary, we compute a self-masked similarity score with the summary itself to evaluate the redundant information in the summary.	3
Finally, we combine the relevance and redundancy scores to produce the final evaluation score of the given summary.	3
Extensive experiments show that our methods can significantly outperform existing methods on both multi-document and single-document summarization evaluation.	4
The source code is released at https://github.com/Chen-Wang-CUHK/Training-Free-and-Ref-Free-Summ-Evaluation.	6
----------
Short textual descriptions of entities provide summaries of their key attributes and have been shown to be useful sources of background knowledge for tasks such as entity linking and question answering.	1
However, generating entity descriptions, especially for new and long-tail entities, can be challenging since relevant information is often scattered across multiple sources with varied content and style.	1
We introduce DESCGEN: given mentions spread over multiple documents, the goal is to generate an entity summary description.	2
DESCGEN consists of 37K entity descriptions from Wikipedia and Fandom, each paired with nine evidence documents on average.	3
The documents were collected using a combination of entity linking and hyperlinks into the entity pages, which together provide high-quality distant supervision.	3
Compared to other multi-document summarization tasks, our task is entity-centric, more abstractive, and covers a wide range of domains.	3
We also propose a two-stage extract-then-generate baseline and show that there exists a large gap (19.9% in ROUGE-L) between state-of-art models and human performance, suggesting that the data will support significant future work.	4
----------
With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations.	1
One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures.	1
In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions.	2
In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence).	3
We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations.	4
Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.	4
----------
Backdoor attacks are a kind of insidious security threat against machine learning models.	1
After being injected with a backdoor in training, the victim model will produce adversary-specified outputs on the inputs embedded with predesigned triggers but behave properly on normal inputs during inference.	1
As a sort of emergent attack, backdoor attacks in natural language processing (NLP) are investigated insufficiently.	1
As far as we know, almost all existing textual backdoor attack methods insert additional contents into normal samples as triggers, which causes the trigger-embedded samples to be detected and the backdoor attacks to be blocked without much effort.	1
In this paper, we propose to use the syntactic structure as the trigger in textual backdoor attacks.	2
We conduct extensive experiments to demonstrate that the syntactic trigger-based attack method can achieve comparable attack performance (almost 100% success rate) to the insertion-based methods but possesses much higher invisibility and stronger resistance to defenses.	3
These results also reveal the significant insidiousness and harmfulness of textual backdoor attacks.	4
All the code and data of this paper can be obtained at https://github.com/thunlp/HiddenKiller.	6
----------
Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages.	1
Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup.	1
Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders.	1
We propose a novel method for investigating the inductive biases of language models using artificial languages.	2
These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order.	3
We then use them to train and test language models.	3
This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models.	3
Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others.	4
Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages.	4
----------
Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn.	1
In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights.	2
In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model’s layers simultaneously and highlighting intra-layer properties and inter-layer differences.	3
We show that contextualization is neither driven by polysemy nor by pure context variation.	4
We also provide insights on why BERT fails to model words in the middle of the functionality continuum.	4
----------
Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations.	1
Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions.	1
Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks.	1
In this paper, we seek to improve the faithfulness of attention-based explanations for text classification.	2
We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights.	3
Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attention-based explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance.	4
Finally, we demonstrate that TaSc consistently provides more faithful attention-based explanations compared to three widely-used interpretability techniques.	4
----------
Car-focused navigation services are based on turns and distances of named streets, whereas navigation instructions naturally used by humans are centered around physical objects called landmarks.	1
We present a neural model that takes OpenStreetMap representations as input and learns to generate navigation instructions that contain visible and salient landmarks from human natural language instructions.	2
Routes on the map are encoded in a location- and rotation-invariant graph representation that is decoded into natural language instructions.	3
Our work is based on a novel dataset of 7,672 crowd-sourced instances that have been verified by human navigation in Street View.	3
Our evaluation shows that the navigation instructions generated by our system have similar properties as human-generated instructions, and lead to successful human navigation in Street View.	4
----------
Vision-language pre-training (VLP) on large-scale image-text pairs has achieved huge success for the cross-modal downstream tasks.	1
The most existing pre-training methods mainly adopt a two-step training procedure, which firstly employs a pre-trained object detector to extract region-based visual features, then concatenates the image representation and text embedding as the input of Transformer to train.	1
However, these methods face problems of using task-specific visual representation of the specific object detector for generic cross-modal understanding, and the computation inefficiency of two-stage pipeline.	1
In this paper, we propose the first end-to-end vision-language pre-trained model for both V+L understanding and generation, namely E2E-VLP, where we build a unified Transformer framework to jointly learn visual representation, and semantic alignments between image and text.	2
We incorporate the tasks of object detection and image captioning into pre-training with a unified Transformer encoder-decoder architecture for enhancing visual learning.	3
An extensive set of experiments have been conducted on well-established vision-language downstream tasks to demonstrate the effectiveness of this novel VLP paradigm.	3
----------
Despite the achievements of large-scale multimodal pre-training approaches, cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.	1
To bridge the semantic gap between the two modalities, previous studies mainly focus on word-region alignment at the object level, lacking the matching between the linguistic relation among the words and the visual relation among the regions.	1
The neglect of such relation consistency impairs the contextualized representation of image-text pairs and hinders the model performance and the interpretability.	1
In this paper, we first propose a novel metric, Intra-modal Self-attention Distance (ISD), to quantify the relation consistency by measuring the semantic distance between linguistic and visual relations.	2
In response, we present Inter-modal Alignment on Intra-modal Self-attentions (IAIS), a regularized training method to optimize the ISD and calibrate intra-modal self-attentions from the two modalities mutually via inter-modal alignment.	2
The IAIS regularizer boosts the performance of prevailing models on Flickr30k and MS COCO datasets by a considerable margin, which demonstrates the superiority of our approach.	4
----------
We present Knowledge Enhanced Multimodal BART (KM-BART), which is a Transformer-based sequence-to-sequence model capable of reasoning about commonsense knowledge from multimodal inputs of images and texts.	2
We adapt the generative BART architecture (Lewis et al., 2020) to a multimodal model with visual and textual inputs.	3
We further develop novel pretraining tasks to improve the model performance on the Visual Commonsense Generation (VCG) task.	3
In particular, our pretraining task of Knowledge-based Commonsense Generation (KCG) boosts model performance on the VCG task by leveraging commonsense knowledge from a large language model pretrained on external commonsense knowledge graphs.	3
To the best of our knowledge, we are the first to propose a dedicated task for improving model performance on the VCG task.	5
Experimental results show that our model reaches state-of-the-art performance on the VCG task (Park et al., 2020) by applying these novel pretraining tasks.	4
----------
Transformers have advanced the field of natural language processing (NLP) on a variety of important tasks.	1
At the cornerstone of the Transformer architecture is the multi-head attention (MHA) mechanism which models pairwise interactions between the elements of the sequence.	1
Despite its massive success, the current framework ignores interactions among different heads, leading to the problem that many of the heads are redundant in practice, which greatly wastes the capacity of the model.	1
To improve parameter efficiency, we re-formulate the MHA as a latent variable model from a probabilistic perspective.	1
We present cascaded head-colliding attention (CODA) which explicitly models the interactions between attention heads through a hierarchical variational distribution.	2
We conduct extensive experiments and demonstrate that CODA outperforms the transformer baseline, by 0.6 perplexity on Wikitext-103 in language modeling, and by 0.6 BLEU on WMT14 EN-DE in machine translation, due to its improvements on the parameter efficiency.	4
----------
Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student).	1
The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student’s output distributions.	1
However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly.	1
In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models.	2
In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible.	4
----------
State-of-the-art parameter-efficient fine-tuning methods rely on introducing adapter modules between the layers of a pretrained language model.	1
However, such modules are trained separately for each task and thus do not enable sharing information across tasks.	1
In this paper, we show that we can learn adapter parameters for all layers and tasks by generating them using shared hypernetworks, which condition on task, adapter position, and layer id in a transformer model.	2
This parameter-efficient multi-task learning framework allows us to achieve the best of both worlds by sharing knowledge across tasks via hypernetworks while enabling the model to adapt to each individual task through task-specific adapters.	3
Experiments on the well-known GLUE benchmark show improved performance in multi-task learning while adding only 0.29% parameters per task.	4
We additionally demonstrate substantial performance improvements in few-shot domain generalization across a variety of tasks.	4
Our code is publicly available in https://github.com/rabeehk/hyperformer.	6
----------
Pre-trained multilingual language models, e.g., multilingual-BERT, are widely used in cross-lingual tasks, yielding the state-of-the-art performance.	1
However, such models suffer from a large performance gap between source and target languages, especially in the zero-shot setting, where the models are fine-tuned only on English but tested on other languages for the same task.	1
We tackle this issue by incorporating language-agnostic information, specifically, universal syntax such as dependency relations and POS tags, into language models, based on the observation that universal syntax is transferable across different languages.	2
Our approach, called COunterfactual SYntax (COSY), includes the design of SYntax-aware networks as well as a COunterfactual training method to implicitly force the networks to learn not only the semantics but also the syntax.	3
To evaluate COSY, we conduct cross-lingual experiments on natural language inference and question answering using mBERT and XLM-R as network backbones.	3
Our results show that COSY achieves the state-of-the-art performance for both tasks, without using auxiliary training data.	4
----------
Recent studies on neural networks with pre-trained weights (i.e., BERT) have mainly focused on a low-dimensional subspace, where the embedding vectors computed from input words (or their contexts) are located.	1
In this work, we propose a new approach, called OoMMix, to finding and regularizing the remainder of the space, referred to as out-of-manifold, which cannot be accessed through the words.	2
Specifically, we synthesize the out-of-manifold embeddings based on two embeddings obtained from actually-observed words, to utilize them for fine-tuning the network.	3
A discriminator is trained to detect whether an input embedding is located inside the manifold or not, and simultaneously, a generator is optimized to produce new embeddings that can be easily identified as out-of-manifold by the discriminator.	3
These two modules successfully collaborate in a unified and end-to-end manner for regularizing the out-of-manifold.	3
Our extensive evaluation on various text classification benchmarks demonstrates the effectiveness of our approach, as well as its good compatibility with existing data augmentation techniques which aim to enhance the manifold.	4
----------
Stereotypical language expresses widely-held beliefs about different social categories.	1
Many stereotypes are overtly negative, while others may appear positive on the surface, but still lead to negative consequences.	1
In this work, we present a computational approach to interpreting stereotypes in text through the Stereotype Content Model (SCM), a comprehensive causal theory from social psychology.	2
The SCM proposes that stereotypes can be understood along two primary dimensions: warmth and competence.	2
We present a method for defining warmth and competence axes in semantic embedding space, and show that the four quadrants defined by this subspace accurately represent the warmth and competence concepts, according to annotated lexicons.	2
We then apply our computational SCM model to textual stereotype data and show that it compares favourably with survey-based studies in the psychological literature.	4
Furthermore, we explore various strategies to counter stereotypical beliefs with anti-stereotypes.	2
It is known that countering stereotypes with anti-stereotypical examples is one of the most effective ways to reduce biased thinking, yet the problem of generating anti-stereotypes has not been previously studied.	1
Thus, a better understanding of how to generate realistic and effective anti-stereotypes can contribute to addressing pressing societal concerns of stereotyping, prejudice, and discrimination.	5
----------
Misinformation has recently become a well-documented matter of public concern.	1
Existing studies on this topic have hitherto adopted a coarse concept of misinformation, which incorporates a broad spectrum of story types ranging from political conspiracies to misinterpreted pranks.	1
This paper aims to structurize these misinformation stories by leveraging fact-check articles.	2
Our intuition is that key phrases in a fact-check article that identify the misinformation type(s) (e.g., doctored images, urban legends) also act as rationales that determine the verdict of the fact-check (e.g., false).	1
We experiment on rationalized models with domain knowledge as weak supervision to extract these phrases as rationales, and then cluster semantically similar rationales to summarize prevalent misinformation types.	3
Using archived fact-checks from Snopes.com, we identify ten types of misinformation stories.	3
We discuss how these types have evolved over the last ten years and compare their prevalence between the 2016/2020 US presidential elections and the H1N1/COVID-19 pandemics.	3
----------
Inferring social relations from dialogues is vital for building emotionally intelligent robots to interpret human language better and act accordingly.	1
We model the social network as an And-or Graph, named SocAoG, for the consistency of relations among a group and leveraging attributes as inference cues.	3
Moreover, we formulate a sequential structure prediction task, and propose an 𝛼-𝛽-𝛾 strategy to incrementally parse SocAoG for the dynamic inference upon any incoming utterance: (i) an 𝛼 process predicting attributes and relations conditioned on the semantics of dialogues, (ii) a 𝛽 process updating the social relations based on related attributes, and (iii) a 𝛾 process updating individual’s attributes based on interpersonal social relations.	2
Empirical results on DialogRE and MovieGraph show that our model infers social relations more accurately than the state-of-the-art methods.	4
Moreover, the ablation study shows the three processes complement each other, and the case study demonstrates the dynamic relational inference.	4
----------
We present a data-driven, end-to-end approach to transaction-based dialog systems that performs at near-human levels in terms of verbal response quality and factual grounding accuracy.	2
We show that two essential components of the system produce these results: a sufficiently large and diverse, in-domain labeled dataset, and a neural network-based, pre-trained model that generates both verbal responses and API call predictions.	4
In terms of data, we introduce TicketTalk, a movie ticketing dialog dataset with 23,789 annotated conversations.	2
The conversations range from completely open-ended and unrestricted to more structured, both in terms of their knowledge base, discourse features, and number of turns.	1
In qualitative human evaluations, model-generated responses trained on just 10,000 TicketTalk dialogs were rated to “make sense” 86.5% of the time, almost the same as human responses in the same contexts.	3
Our simple, API-focused annotation schema results in a much easier labeling task making it faster and more cost effective.	3
It is also the key component for being able to predict API calls accurately.	3
We handle factual grounding by incorporating API calls in the training data, allowing our model to learn which actions to take and when.	3
Trained on the same 10,000-dialog set, the model’s API call predictions were rated to be correct 93.9% of the time in our evaluations, surpassing the ratings for the corresponding human labels.	3
We show how API prediction and response generation scores improve as the dataset size incrementally increases from 5000 to 21,000 dialogs.	3
Our analysis also clearly illustrates the benefits of pre-training.	3
To facilitate future work on transaction-based dialog systems, we are publicly releasing the TicketTalk dataset at https://git.io/JL8an.	6
----------
In this paper, we explore the ability to model and infer personality types of opponents, predict their responses, and use this information to adapt a dialog agent’s high-level strategy in negotiation tasks.	2
Inspired by the idea of incorporating a theory of mind (ToM) into machines, we introduce a probabilistic formulation to encapsulate the opponent’s personality type during both learning and inference.	3
We test our approach on the CraigslistBargain dataset (He et al. 2018) and show that our method using ToM inference achieves a 20% higher dialog agreement rate compared to baselines on a mixed population of opponents.	4
We also demonstrate that our model displays diverse negotiation behavior with different types of opponents.	4
----------
In this paper, we propose Inverse Adversarial Training (IAT) algorithm for training neural dialogue systems to avoid generic responses and model dialogue history better.	2
In contrast to standard adversarial training algorithms, IAT encourages the model to be sensitive to the perturbation in the dialogue history and therefore learning from perturbations.	3
By giving higher rewards for responses whose output probability reduces more significantly when dialogue history is perturbed, the model is encouraged to generate more diverse and consistent responses.	3
By penalizing the model when generating the same response given perturbed dialogue history, the model is forced to better capture dialogue history and generate more informative responses.	3
Experimental results on two benchmark datasets show that our approach can better model dialogue history and generate more diverse and consistent responses.	4
In addition, we point out a problem of the widely used maximum mutual information (MMI) based methods for improving the diversity of dialogue response generation models and demonstrate it empirically.	4
----------
Knowledge-grounded dialogue systems are intended to convey information that is based on evidence provided in a given source text.	1
We discuss the challenges of training a generative neural dialogue model for such systems that is controlled to stay faithful to the evidence.	1
Existing datasets contain a mix of conversational responses that are faithful to selected evidence as well as more subjective or chit-chat style responses.	1
We propose different evaluation measures to disentangle these different styles of responses by quantifying the informativeness and objectivity.	2
At training time, additional inputs based on these evaluation measures are given to the dialogue model.	3
At generation time, these additional inputs act as stylistic controls that encourage the model to generate responses that are faithful to the provided evidence.	3
We also investigate the usage of additional controls at decoding time using resampling techniques.	3
In addition to automatic metrics, we perform a human evaluation study where raters judge the output of these controlled generation models to be generally more objective and faithful to the evidence compared to baseline dialogue systems.	3
----------
Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress.	1
Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem.	1
Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider extraction solely based on the content of an individual paper, without considering the paper’s place in the broader literature.	1
In contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers.	3
On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks.	4
When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction.	5
We release software tools to facilitate citation-aware SciIE development.	4
----------
Current event-centric knowledge graphs highly rely on explicit connectives to mine relations between events.	1
Unfortunately, due to the sparsity of connectives, these methods severely undermine the coverage of EventKGs.	1
The lack of high-quality labelled corpora further exacerbates that problem.	1
In this paper, we propose a knowledge projection paradigm for event relation extraction: projecting discourse knowledge to narratives by exploiting the commonalities between them.	2
Specifically, we propose Multi-tier Knowledge Projection Network (MKPNet), which can leverage multi-tier discourse knowledge effectively for event relation extraction.	2
In this way, the labelled data requirement is significantly reduced, and implicit event relations can be effectively extracted.	4
Intrinsic experimental results show that MKPNet achieves the new state-of-the-art performance and extrinsic experimental results verify the value of the extracted event relations.	4
----------
Neural methods have been shown to achieve high performance in Named Entity Recognition (NER), but rely on costly high-quality labeled data for training, which is not always available across languages.	1
While previous works have shown that unlabeled data in a target language can be used to improve cross-lingual model performance, we propose a novel adversarial approach (AdvPicker) to better leverage such data and further improve results.	1+2
We design an adversarial learning framework in which an encoder learns entity domain knowledge from labeled source-language data and better shared features are captured via adversarial training - where a discriminator selects less language-dependent target-language data via similarity to the source language.	3
Experimental results on standard benchmark datasets well demonstrate that the proposed method benefits strongly from this data selection process and outperforms existing state-of-the-art methods; without requiring any additional external resources (e.g., gazetteers or via machine translation).	4
----------
Nowadays, fake news detection, which aims to verify whether a news document is trusted or fake, has become urgent and important.	1
Most existing methods rely heavily on linguistic and semantic features from the news content, and fail to effectively exploit external knowledge which could help determine whether the news document is trusted.	1
In this paper, we propose a novel end-to-end graph neural model called CompareNet, which compares the news to the knowledge base (KB) through entities for fake news detection.	2
Considering that fake news detection is correlated with topics, we also incorporate topics to enrich the news representation.	3
Specifically, we first construct a directed heterogeneous document graph for each news incorporating topics and entities.	3
Based on the graph, we develop a heterogeneous graph attention network for learning the topic-enriched news representation as well as the contextual entity representations that encode the semantics of the news content.	3
The contextual entity representations are then compared to the corresponding KB-based entity representations through a carefully designed entity comparison network, to capture the consistency between the news content and KB.	3
Finally, the topic-enriched news representation combining the entity comparison features is fed into a fake news classifier.	3
Experimental results on two benchmark datasets demonstrate that CompareNet significantly outperforms state-of-the-art methods.	4
----------
Named entity recognition (NER) remains challenging when entity mentions can be discontinuous.	1
Existing methods break the recognition process into several sequential steps.	1
In training, they predict conditioned on the golden intermediate results, while at inference relying on the model output of the previous steps, which introduces exposure bias.	1
To solve this problem, we first construct a segment graph for each sentence, in which each node denotes a segment (a continuous entity on its own, or a part of discontinuous entities), and an edge links two nodes that belong to the same entity.	3
The nodes and edges can be generated respectively in one stage with a grid tagging scheme and learned jointly using a novel architecture named Mac.	3
Then discontinuous NER can be reformulated as a non-parametric process of discovering maximal cliques in the graph and concatenating the spans in each clique.	3
Experiments on three benchmarks show that our method outperforms the state-of-the-art (SOTA) results, with up to 3.5 percentage points improvement on F1, and achieves 5x speedup over the SOTA model.	4
----------
Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems.	1
In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches.	1
Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning.	3
Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches.	4
Furthermore, our framework has the benefits of extensibility and transferability.	4
We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods.	4
As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3% increase in F1 score relative to previous SoTA.	4
Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.	4
----------
The scarcity of parallel data is a major obstacle for training high-quality machine translation systems for low-resource languages.	1
Fortunately, some low-resource languages are linguistically related or similar to high-resource languages; these related languages may share many lexical or syntactic structures.	1
In this work, we exploit this linguistic overlap to facilitate translating to and from a low-resource language with only monolingual data, in addition to any parallel data in the related high-resource language.	2
Our method, NMT-Adapt, combines denoising autoencoding, back-translation and adversarial objectives to utilize monolingual data for low-resource adaptation.	3
We experiment on 7 languages from three different language families and show that our technique significantly improves translation into low-resource language compared to other translation baselines.	4
----------
Bilingual lexicons map words in one language to their translations in another, and are typically induced by learning linear projections to align monolingual word embedding spaces.	1
In this paper, we show it is possible to produce much higher quality lexicons with methods that combine (1) unsupervised bitext mining and (2) unsupervised word alignment.	2
Directly applying a pipeline that uses recent algorithms for both subproblems significantly improves induced lexicon quality and further gains are possible by learning to filter the resulting lexical entries, with both unsupervised and semi-supervised schemes.	3
Our final model outperforms the state of the art on the BUCC 2020 shared task by 14 F1 points averaged over 12 language pairs, while also providing a more interpretable approach that allows for rich reasoning of word meaning in context.	4
Further analysis of our output and the standard reference lexicons suggests they are of comparable quality, and new benchmarks may be needed to measure further progress on this task.	4
----------
We present a simple yet effective approach to build multilingual speech-to-text (ST) translation through efficient transfer learning from a pretrained speech encoder and text decoder.	2
Our key finding is that a minimalistic LNA (LayerNorm and Attention) finetuning can achieve zero-shot crosslingual and cross-modality transfer ability by only finetuning 10 50% of the pretrained parameters.	4
This effectively leverages large pretrained models at low training cost such as wav2vec 2.0 for acoustic modeling, and mBART for multilingual text generation.	3
This sets a new state-of-the-art for 36 translation directions (and surpassing cascaded ST for 26 of them) on the large-scale multilingual ST benchmark CoVoST 2 (+6.4 BLEU on average for En-X directions and +6.7 BLEU for X-En directions).	4
Our approach demonstrates strong zero-shot performance in a many-to-many multilingual model (+5.6 BLEU on average across 28 non-English directions), making it an appealing approach for attaining high-quality speech translation with improved parameter and data efficiency.	4
----------
Learning contextual text embeddings that represent causal graphs has been useful in improving the performance of downstream tasks like causal treatment effect estimation.	1
However, existing causal embeddings which are trained to predict direct causal links, fail to capture other indirect causal links of the graph, thus leading to spurious correlations in downstream tasks.	1
In this paper, we define the faithfulness property of contextual embeddings to capture geometric distance-based properties of directed acyclic causal graphs.	2
By incorporating these faithfulness properties, we learn text embeddings that are 31.3% more faithful to human validated causal graphs with about 800K and 200K causal links and achieve 21.1% better Precision-Recall AUC in a link prediction fine-tuning task.	4
Further, in a crowdsourced causal question-answering task on Yahoo!	4
Answers with questions of the form “What causes X?”, our faithful embeddings achieved a precision of the first ranked answer (P@1) of 41.07%, outperforming the existing baseline by 10.2%.	4
----------
Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens.	1
What aspects of these contexts contribute to accurate model prediction?	1
We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia.	2
In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15% of the usable information.	4
Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.	4
----------
In this paper, we introduce Integrated Directional Gradients (IDG), a method for attributing importance scores to groups of features, indicating their relevance to the output of a neural network model for a given input.	2
The success of Deep Neural Networks has been attributed to their ability to capture higher level feature interactions.	1
Hence, in the last few years capturing the importance of these feature interactions has received increased prominence in ML interpretability literature.	1
In this paper, we formally define the feature group attribution problem and outline a set of axioms that any intuitive feature group attribution method should satisfy.	2
Earlier, cooperative game theory inspired axiomatic methods only borrowed axioms from solution concepts (such as Shapley value) for individual feature attributions and introduced their own extensions to model interactions.	1
In contrast, our formulation is inspired by axioms satisfied by characteristic functions as well as solution concepts in cooperative game theory literature.	3
We believe that characteristic functions are much better suited to model importance of groups compared to just solution concepts.	3
We demonstrate that our proposed method, IDG, satisfies all the axioms.	4
Using IDG we analyze two state-of-the-art text classifiers on three benchmark datasets for sentiment analysis.	3
Our experiments show that IDG is able to effectively capture semantic interactions in linguistic models via negations and conjunctions.	4
----------
Sentence embeddings are an important component of many natural language processing (NLP) systems.	1
Like word embeddings, sentence embeddings are typically learned on large text corpora and then transferred to various downstream tasks, such as clustering and retrieval.	1
Unlike word embeddings, the highest performing solutions for learning sentence embeddings require labelled data, limiting their usefulness to languages and domains where labelled data is abundant.	1
In this paper, we present DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations.	2
Inspired by recent advances in deep metric learning (DML), we carefully design a self-supervised objective for learning universal sentence embeddings that does not require labelled training data.	3
When used to extend the pretraining of transformer-based language models, our approach closes the performance gap between unsupervised and supervised pretraining for universal sentence encoders.	3
Importantly, our experiments suggest that the quality of the learned embeddings scale with both the number of trainable parameters and the amount of unlabelled training data.	3
Our code and pretrained models are publicly available and can be easily adapted to new domains or used to embed unseen text.	4
----------
Due to the scarcity of annotated data, Abstract Meaning Representation (AMR) research is relatively limited and challenging for languages other than English.	1
Upon the availability of English AMR dataset and English-to- X parallel datasets, in this paper we propose a novel cross-lingual pre-training approach via multi-task learning (MTL) for both zeroshot AMR parsing and AMR-to-text generation.	2
Specifically, we consider three types of relevant tasks, including AMR parsing, AMR-to-text generation, and machine translation.	3
We hope that knowledge gained while learning for English AMR parsing and text generation can be transferred to the counterparts of other languages.	3
With properly pretrained models, we explore four different finetuning methods, i.e., vanilla fine-tuning with a single task, one-for-all MTL fine-tuning, targeted MTL fine-tuning, and teacher-studentbased MTL fine-tuning.	3
Experimental results on AMR parsing and text generation of multiple non-English languages demonstrate that our approach significantly outperforms a strong baseline of pre-training approach, and greatly advances the state of the art.	4
In detail, on LDC2020T07 we have achieved 70.45%, 71.76%, and 70.80% in Smatch F1 for AMR parsing of German, Spanish, and Italian, respectively, while for AMR-to-text generation of the languages, we have obtained 25.69, 31.36, and 28.42 in BLEU respectively.	4
We make our code available on github https://github.com/xdqkid/XLPT-AMR.	6
----------
Despite the success of sequence-to-sequence (seq2seq) models in semantic parsing, recent work has shown that they fail in compositional generalization, i.e., the ability to generalize to new structures built of components observed during training.	1
In this work, we posit that a span-based parser should lead to better compositional generalization.	1
we propose SpanBasedSP, a parser that predicts a span tree over an input utterance, explicitly encoding how partial programs compose over spans in the input.	2
SpanBasedSP extends Pasupat et al.(2019) to be comparable to seq2seq models by (i) training from programs, without access to gold trees, treating trees as latent variables, (ii) parsing a class of non-projective trees through an extension to standard CKY.	3
On GeoQuery, SCAN and CLOSURE datasets, SpanBasedSP performs similarly to strong seq2seq baselines on random splits, but dramatically improves performance compared to baselines on splits that require compositional generalization: from 61.0 → 88.9 average accuracy.	3+4
----------
Sequence-to-sequence models excel at handling natural language variation, but have been shown to struggle with out-of-distribution compositional generalization.	1
This has motivated new specialized architectures with stronger compositional biases, but most of these approaches have only been evaluated on synthetically-generated datasets, which are not representative of natural language variation.	1
In this work we ask: can we develop a semantic parsing approach that handles both natural language variation and compositional generalization?	1
To better assess this capability, we propose new train and test splits of non-synthetic datasets.	2
We demonstrate that strong existing approaches do not perform well across a broad set of evaluations.	4
We also propose NQG-T5, a hybrid model that combines a high-precision grammar-based approach with a pre-trained sequence-to-sequence model.	2
It outperforms existing approaches across several compositional generalization challenges on non-synthetic data, while also being competitive with the state-of-the-art on standard evaluations.	4
While still far from solving this problem, our study highlights the importance of diverse evaluations and the open challenge of handling both compositional generalization and natural language variation in semantic parsing.	4
----------
We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena.	2
Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task.	3
We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes.	3
We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model ‘accuracy’ scores a la Marvin and Linzen (2018) about equal.	4
However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences.	4
Specifically, when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data.	4
These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.	4
----------
Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are.	1
Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more.	1
Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard.	1
Furthermore, these senses are often analyzed independently of the events that they modify.	1
This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al.(2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies.	1
We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events.	2
We show that detecting and classifying modal expressions is not only feasible, it also improves the detection of modal events in their own right.	4
----------
Part-of-Speech (POS) tags are routinely included as features in many NLP tasks.	1
However, the importance and usefulness of POS tags needs to be examined as NLP expands to low-resource languages because linguists who provide many annotated resources do not place priority on early identification and tagging of POS.	1
This paper describes an empirical study about the effect that POS tags have on two computational morphological tasks with the Transformer architecture.	2
Each task is tested twice on identical data except for the presence/absence of POS tags, using published data in ten high- to low-resource languages or unpublished linguistic field data in five low-resource languages.	3
We find that the presence or absence of POS tags does not have a significant bearing on performance.	4
In joint segmentation and glossing, the largest average difference is an .09 improvement in F1-scores by removing POS tags.	4
In reinflection, the greatest average difference is 1.2% in accuracy for published data and 5% for unpublished and noisy field data.	4
----------
Parsing spoken dialogue poses unique difficulties, including disfluencies and unmarked boundaries between sentence-like units.	1
Previous work has shown that prosody can help with parsing disfluent speech (Tran et al.2018), but has assumed that the input to the parser is already segmented into sentence-like units (SUs), which isn’t true in existing speech applications.	1
We investigate how prosody affects a parser that receives an entire dialogue turn as input (a turn-based model), instead of gold standard pre-segmented SUs (an SU-based model).	2
In experiments on the English Switchboard corpus, we find that when using transcripts alone, the turn-based model has trouble segmenting SUs, leading to worse parse performance than the SU-based model.	3
However, prosody can effectively replace gold standard SU boundaries: with prosody, the turn-based model performs as well as the SU-based model (91.38 vs. 91.06 F1 score, respectively), despite performing two tasks (SU segmentation and parsing) rather than one (parsing alone).	3+4
Analysis shows that pitch and intensity features are the most important for this corpus, since they allow the model to correctly distinguish an SU boundary from a speech disfluency – a distinction that the model otherwise struggles to make.	4
----------
Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal.	1
Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms.	1
We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution.	3
We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping.	2
We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.	4
----------
Knowledge Graph (KG) completion research usually focuses on densely connected benchmark datasets that are not representative of real KGs.	1
We curate two KG datasets that include biomedical and encyclopedic knowledge and use an existing commonsense KG dataset to explore KG completion in the more realistic setting where dense connectivity is not guaranteed.	3
We develop a deep convolutional network that utilizes textual entity representations and demonstrate that our model outperforms recent KG completion methods in this challenging setting.	3
We find that our model’s performance improvements stem primarily from its robustness to sparsity.	3
We then distill the knowledge from the convolutional network into a student network that re-ranks promising candidate entities.	3
This re-ranking stage leads to further improvements in performance and demonstrates the effectiveness of entity re-ranking for KG completion.	4
----------
Computing precise evidences, namely minimal sets of sentences that support or refute a given claim, rather than larger evidences is crucial in fact verification (FV), since larger evidences may contain conflicting pieces some of which support the claim while the other refute, thereby misleading FV.	1
Despite being important, precise evidences are rarely studied by existing methods for FV.	1
It is challenging to find precise evidences due to a large search space with lots of local optimums.	1
Inspired by the strong exploration ability of the deep Q-learning network (DQN), we propose a DQN-based approach to retrieval of precise evidences.	2
In addition, to tackle the label bias on Q-values computed by DQN, we design a post-processing strategy which seeks best thresholds for determining the true labels of computed evidences.	2
Experimental results confirm the effectiveness of DQN in computing precise evidences and demonstrate improvements in achieving accurate claim verification.	4
----------
In selective prediction, a classifier is allowed to abstain from making predictions on low-confidence examples.	1
Though this setting is interesting and important, selective prediction has rarely been examined in natural language processing (NLP) tasks.	1
To fill this void in the literature, we study in this paper selective prediction for NLP, comparing different models and confidence estimators.	2
We further propose a simple error regularization trick that improves confidence estimation without substantially increasing the computation budget.	2
We show that recent pre-trained transformer models simultaneously improve both model accuracy and confidence estimation effectiveness.	4
We also find that our proposed regularization improves confidence estimation and can be applied to other relevant scenarios, such as using classifier cascades for accuracy–efficiency trade-offs.	4
Source code for this paper can be found at https://github.com/castorini/transformers-selective.	6
----------
Deployed real-world machine learning applications are often subject to uncontrolled and even potentially malicious inputs.	1
Those out-of-domain inputs can lead to unpredictable outputs and sometimes catastrophic safety issues.	1
Prior studies on out-of-domain detection require in-domain task labels and are limited to supervised classification scenarios.	1
Our work tackles the problem of detecting out-of-domain samples with only unsupervised in-domain data.	2
We utilize the latent representations of pre-trained transformers and propose a simple yet effective method to transform features across all layers to construct out-of-domain detectors efficiently.	2
Two domain-specific fine-tuning approaches are further proposed to boost detection accuracy.	2
Our empirical evaluations of related methods on two datasets validate that our method greatly improves out-of-domain detection ability in a more general scenario.	4
----------
The advent of large pre-trained language models has given rise to rapid progress in the field of Natural Language Processing (NLP).	1
While the performance of these models on standard benchmarks has scaled with size, compression techniques such as knowledge distillation have been key in making them practical.	1
We present MATE-KD, a novel text-based adversarial training algorithm which improves the performance of knowledge distillation.	2
MATE-KD first trains a masked language model-based generator to perturb text by maximizing the divergence between teacher and student logits.	3
Then using knowledge distillation a student is trained on both the original and the perturbed training samples.	3
We evaluate our algorithm, using BERT-based models, on the GLUE benchmark and demonstrate that MATE-KD outperforms competitive adversarial learning and data augmentation baselines.	4
On the GLUE test set our 6 layer RoBERTa based model outperforms BERT-large.	4
----------
Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming.	1
Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance.	1
Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning.	2
We define the information gain of an example as the improvement on a validation metric after training on that example.	3
A secondary learner is then trained to approximate this quantity.	3
During fine-tuning, this learner selects informative examples and skips uninformative ones.	3
We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures.	4
For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning.	4
We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning.	2
The generality of our method leads us to propose a new paradigm for language model fine-tuning — we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.	3
----------
Text simplification reduces the language complexity of professional content for accessibility purposes.	1
End-to-end neural network models have been widely adopted to directly generate the simplified version of input text, usually functioning as a blackbox.	1
We show that text simplification can be decomposed into a compact pipeline of tasks to ensure the transparency and explainability of the process.	2
The first two steps in this pipeline are often neglected: 1) to predict whether a given piece of text needs to be simplified, and 2) if yes, to identify complex parts of the text.	3
The two tasks can be solved separately using either lexical or deep learning methods, or solved jointly.	3
Simply applying explainable complexity prediction as a preliminary step, the out-of-sample text simplification performance of the state-of-the-art, black-box simplification models can be improved by a large margin.	4
----------
Retrieving relevant contexts from a large corpus is a crucial step for tasks such as open-domain question answering and fact checking.	1
Although neural retrieval outperforms traditional methods like tf-idf and BM25, its performance degrades considerably when applied to out-of-domain data.	1
Driven by the question of whether a neural retrieval model can be _universal_ and perform robustly on a wide variety of problems, we propose a multi-task trained model.	2
Our approach not only outperforms previous methods in the few-shot setting, but also rivals specialised neural retrievers, even when in-domain training data is abundant.	3
With the help of our retriever, we improve existing models for downstream tasks and closely match or improve the state of the art on multiple benchmarks.	4
----------
NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words.	1
But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data?	2
To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks.	3
We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words.	3
We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test.	4
They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks.	4
The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.	4
----------
In Neural Machine Translation (and, more generally, conditional language modeling), the generation of a target token is influenced by two types of context: the source and the prefix of the target sequence.	1
While many attempts to understand the internal workings of NMT models have been made, none of them explicitly evaluates relative source and target contributions to a generation decision.	1
We argue that this relative contribution can be evaluated by adopting a variant of Layerwise Relevance Propagation (LRP).	2
Its underlying ‘conservation principle’ makes relevance propagation unique: differently from other methods, it evaluates not an abstract quantity reflecting token importance, but the proportion of each token’s influence.	3
We extend LRP to the Transformer and conduct an analysis of NMT models which explicitly evaluates the source and target relative contributions to the generation process.	3
We analyze changes in these contributions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process.	3
We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.	4
----------
Recent years have seen numerous NLP datasets introduced to evaluate the performance of fine-tuned models on natural language understanding tasks.	1
Recent results from large pretrained models, though, show that many of these datasets are largely saturated and unlikely to be able to detect further progress.	1
What kind of datasets are still effective at discriminating among strong models, and what kind of datasets should we expect to be able to detect future improvements?	2
To measure this uniformly across datasets, we draw on Item Response Theory and evaluate 29 datasets using predictions from 18 pretrained Transformer models on individual test examples.	3
We find that Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models.	4
We also observe span selection task format, which is used for QA datasets like QAMR or SQuAD2.0, is effective in differentiating between strong and weak models.	4
----------
A growing body of literature has focused on detailing the linguistic knowledge embedded in large, pretrained language models.	1
Existing work has shown that non-linguistic biases in models can drive model behavior away from linguistic generalizations.	1
We hypothesized that competing linguistic processes within a language, rather than just non-linguistic model biases, could obscure underlying linguistic knowledge.	2
We tested this claim by exploring a single phenomenon in four languages: English, Chinese, Spanish, and Italian.	3
While human behavior has been found to be similar across languages, we find cross-linguistic variation in model behavior.	1
We show that competing processes in a language act as constraints on model behavior and demonstrate that targeted fine-tuning can re-weight the learned constraints, uncovering otherwise dormant linguistic knowledge in models.	4
Our results suggest that models need to learn both the linguistic constraints in a language and their relative ranking, with mismatches in either producing non-human-like behavior.	4
----------
Interpretability is an important aspect of the trustworthiness of a model’s predictions.	1
Transformer’s predictions are widely explained by the attention weights, i.e., a probability distribution generated at its self-attention unit (head).	1
Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique.	1
A recent study showed theoretical justifications to this observation by proving the non-identifiability of attention weights.	1
For a given input to a head and its output, if the attention weights generated in it are unique, we call the weights identifiable.	1
In this work, we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights.	2
Ignored in the previous works, we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector.	1
However, the weights are still prone to be non-unique attentions that make them unfit for interpretation.	1
To tackle this issue, we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input.	4
We prove the applicability of such variations by providing empirical justifications on varied text classification tasks.	4
The implementations are available at https://github.com/declare-lab/identifiable-transformers.	6
----------
Natural Language Generation (NLG) is a key component in a task-oriented dialogue system, which converts the structured meaning representation (MR) to the natural language.	1
For large-scale conversational systems, where it is common to have over hundreds of intents and thousands of slots, neither template-based approaches nor model-based approaches are scalable.	1
Recently, neural NLGs started leveraging transfer learning and showed promising results in few-shot settings.	1
This paper proposes AugNLG, a novel data augmentation approach that combines a self-trained neural retrieval model with a few-shot learned NLU model, to automatically create MR-to-Text data from open-domain texts.	2
The proposed system mostly outperforms the state-of-the-art methods on the FewshotWOZ data in both BLEU and Slot Error Rate.	4
We further confirm improved results on the FewshotSGD data and provide comprehensive analysis results on key components of our system.	4
Our code and data are available at https://github.com/XinnuoXu/AugNLG.	6
----------
In this paper we implement and compare 7 different data augmentation strategies for the task of automatic scoring of children’s ability to understand others’ thoughts, feelings, and desires (or “mindreading”).	2
We recruit in-domain experts to re-annotate augmented samples and determine to what extent each strategy preserves the original rating.	3
We also carry out multiple experiments to measure how much each augmentation strategy improves the performance of automatic scoring systems.	3
To determine the capabilities of automatic systems to generalize to unseen data, we create UK-MIND-20 - a new corpus of children’s performance on tests of mindreading, consisting of 10,320 question-answer pairs.	3
We obtain a new state-of-the-art performance on the MIND-CA corpus, improving macro-F1-score by 6 points.	3
Results indicate that both the number of training examples and the quality of the augmentation strategies affect the performance of the systems.	4
The task-specific augmentations generally outperform task-agnostic augmentations.	4
Automatic augmentations based on vectors (GloVe, FastText) perform the worst.	4
We find that systems trained on MIND-CA generalize well to UK-MIND-20.	4
We demonstrate that data augmentation strategies also improve the performance on unseen data.	4
----------
Ideology of legislators is typically estimated by ideal point models from historical records of votes.	1
It represents legislators and legislation as points in a latent space and shows promising results for modeling voting behavior.	1
However, it fails to capture more specific attitudes of legislators toward emerging issues and is unable to model newly-elected legislators without voting histories.	1
In order to mitigate these two problems, we explore to incorporate both voting behavior and public statements on Twitter to jointly model legislators.	2
In addition, we propose a novel task, namely hashtag usage prediction to model the ideology of legislators on Twitter.	2
In practice, we construct a heterogeneous graph for the legislative context and use relational graph neural networks to learn the representation of legislators with the guidance of historical records of their voting and hashtag usage.	3
Experiment results indicate that our model yields significant improvements for the task of roll call vote prediction.	4
Further analysis further demonstrates that legislator representation we learned captures nuances in statements.	4
----------
Languages are dynamic systems: word usage may change over time, reflecting various societal factors.	1
However, all languages do not evolve identically: the impact of an event, the influence of a trend or thinking, can differ between communities.	1
In this paper, we propose to track these divergences by comparing the evolution of a word and its translation across two languages.	2
We investigate several methods of building time-varying and bilingual word embeddings, using contextualised and non-contextualised embeddings.	3
We propose a set of scenarios to characterize semantic divergence across two languages, along with a setup to differentiate them in a bilingual corpus.	2
We evaluate the different methods by generating a corpus of synthetic semantic change across two languages, English and French, before applying them to newspaper corpora to detect bilingual semantic divergence and provide qualitative insight for the task.	3
We conclude that BERT embeddings coupled with a clustering step lead to the best performance on synthetic corpora; however, the performance of CBOW embeddings is very competitive and more adapted to an exploratory analysis on a large corpus.	4
----------
Multilingual neural machine translation has shown the capability of directly translating between language pairs unseen in training, i.e.zero-shot translation.	1
Despite being conceptually attractive, it often suffers from low output quality.	1
The difficulty of generalizing to new translation directions suggests the model representations are highly specific to those language pairs seen in training.	1
We demonstrate that a main factor causing the language-specific representations is the positional correspondence to input tokens.	2
We show that this can be easily alleviated by removing residual connections in an encoder layer.	3
With this modification, we gain up to 18.5 BLEU points on zero-shot translation while retaining quality on supervised directions.	4
The improvements are particularly prominent between related languages, where our proposed model outperforms pivot-based translation.	4
Moreover, our approach allows easy integration of new languages, which substantially expands translation coverage.	4
By thorough inspections of the hidden layer outputs, we show that our approach indeed leads to more language-independent representations.	4
----------
Commonsense reasoning research has so far been limited to English.	1
We aim to evaluate and improve popular multilingual language models (ML-LMs) to help advance commonsense reasoning (CSR) beyond English.	2
We collect the Mickey corpus, consisting of 561k sentences in 11 different languages, which can be used for analyzing and improving ML-LMs.	3
We propose Mickey Probe, a language-general probing task for fairly evaluating the common sense of popular ML-LMs across different languages.	2
In addition, we also create two new datasets, X-CSQA and X-CODAH, by translating their English versions to 14 other languages, so that we can evaluate popular ML-LMs for cross-lingual commonsense reasoning.	3
To improve the performance beyond English, we propose a simple yet effective method — multilingual contrastive pretraining (MCP).	2
It significantly enhances sentence representations, yielding a large performance gain on both benchmarks (e.g., +2.7% accuracy for X-CSQA over XLM-R_L).	4
----------
Attention mechanisms have achieved substantial improvements in neural machine translation by dynamically selecting relevant inputs for different predictions.	1
However, recent studies have questioned the attention mechanisms’ capability for discovering decisive inputs.	1
In this paper, we propose to calibrate the attention weights by introducing a mask perturbation model that automatically evaluates each input’s contribution to the model outputs.	2
We increase the attention weights assigned to the indispensable tokens, whose removal leads to a dramatic performance decrease.	3
The extensive experiments on the Transformer-based translation have demonstrated the effectiveness of our model.	4
We further find that the calibrated attention weights are more uniform at lower layers to collect multiple information while more concentrated on the specific inputs at higher layers.	4
Detailed analyses also show a great need for calibration in the attention weights with high entropy where the model is unconfident about its decision.	4
----------
We propose a new architecture for adapting a sentence-level sequence-to-sequence transformer by incorporating multiple pre-trained document context signals and assess the impact on translation performance of (1) different pretraining approaches for generating these signals, (2) the quantity of parallel data for which document context is available, and (3) conditioning on source, target, or source and target contexts.	2
Experiments on the NIST Chinese-English, and IWSLT and WMT English-German tasks support four general conclusions: that using pre-trained context representations markedly improves sample efficiency, that adequate parallel data resources are crucial for learning to use document context, that jointly conditioning on multiple context representations outperforms any single representation, and that source context is more valuable for translation performance than target side context.	3+4
Our best multi-context model consistently outperforms the best existing context-aware transformers.	4
----------
Recent research in multilingual language models (LM) has demonstrated their ability to effectively handle multiple languages in a single model.	1
This holds promise for low web-resource languages (LRL) as multilingual models can enable transfer of supervision from high resource languages to LRLs.	1
However, incorporating a new language in an LM still remains a challenge, particularly for languages with limited corpora and in unseen scripts.	1
In this paper we argue that relatedness among languages in a language family may be exploited to overcome some of the corpora limitations of LRLs, and propose RelateLM.	2
We focus on Indian languages, and exploit relatedness along two dimensions: (1) script (since many Indic scripts originated from the Brahmic script), and (2) sentence structure.	3
RelateLM uses transliteration to convert the unseen script of limited LRL text into the script of a Related Prominent Language (RPL) (Hindi in our case).	3
While exploiting similar sentence structures, RelateLM utilizes readily available bilingual dictionaries to pseudo translate RPL text into LRL corpora.	3
Experiments on multiple real-world benchmark datasets provide validation to our hypothesis that using a related language as pivot, along with transliteration and pseudo translation based data augmentation, can be an effective way to adapt LMs for LRLs, rather than direct training or pivoting through English.	4
----------
The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community.	1
However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root.	1
While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees.	1
This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees.	1
Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1.	4
In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al.(1980).	2
Our simplification allows us to obtain a constant time speed-up over the original algorithm.	4
Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.	2
----------
Research on the application of NLP in symbol-based Augmentative and Alternative Communication (AAC) tools for improving social interaction support is scarce.	1
We contribute a novel method for generating context-related vocabulary from photographs of personally relevant events aimed at supporting people with language impairments in retelling their past experiences.	2
Performance was calculated with information retrieval concepts on the relevance of vocabulary generated for communicating a corpus of 9730 narrative phrases about events depicted in 1946 photographs.	3
In comparison to a baseline generation composed of frequent English words, our method generated vocabulary with a 4.6 gain in mean average precision, regardless of the level of contextual information in the input photographs, and 6.9 for photographs in which contextual information was extracted correctly.	4
We conclude by discussing how our findings provide insights for system optimization and usage.	4
----------
Emojis have become ubiquitous in digital communication, due to their visual appeal as well as their ability to vividly convey human emotion, among other factors.	1
This also leads to an increased need for systems and tools to operate on text containing emojis.	1
In this study, we assess this support by considering test sets of tweets with emojis, based on which we perform a series of experiments investigating the ability of prominent NLP and text processing tools to adequately process them.	2
In particular, we consider tokenization, part-of-speech tagging, dependency parsing, as well as sentiment analysis.	3
Our findings show that many systems still have notable shortcomings when operating on text containing emojis.	4
----------
Natural language processing techniques have demonstrated promising results in keyphrase generation.	1
However, one of the major challenges in neural keyphrase generation is processing long documents using deep neural networks.	1
Generally, documents are truncated before given as inputs to neural networks.	1
Consequently, the models may miss essential points conveyed in the target document.	1
To overcome this limitation, we propose SEG-Net, a neural keyphrase generation model that is composed of two major components, (1) a selector that selects the salient sentences in a document and (2) an extractor-generator that jointly extracts and generates keyphrases from the selected sentences.	2
SEG-Net uses Transformer, a self-attentive architecture, as the basic building block with a novel layer-wise coverage attention to summarize most of the points discussed in the document.	3
The experimental results on seven keyphrase generation benchmarks from scientific and web documents demonstrate that SEG-Net outperforms the state-of-the-art neural generative methods by a large margin.	4
----------
We propose a method for generating paraphrases of English questions that retain the original intent but use a different surface form.	2
Our model combines a careful choice of training objective with a principled information bottleneck, to induce a latent encoding space that disentangles meaning and form.	3
We train an encoder-decoder model to reconstruct a question from a paraphrase with the same meaning and an exemplar with the same surface form, leading to separated encoding spaces.	3
We use a Vector-Quantized Variational Autoencoder to represent the surface form as a set of discrete latent variables, allowing us to use a classifier to select a different surface form at test time.	3
Crucially, our method does not require access to an external source of target exemplars.	3
Extensive experiments and a human evaluation show that we are able to generate paraphrases with a better tradeoff between semantic preservation and syntactic novelty compared to previous methods.	4
----------
We present AggGen (pronounced ‘again’) a data-to-text model which re-introduces two explicit sentence planning stages into neural data-to-text systems: input ordering and input aggregation.	2
In contrast to previous work using sentence planning, our model is still end-to-end: AggGen performs sentence planning at the same time as generating text by learning latent alignments (via semantic facts) between input representation and target text.	3
Experiments on the WebNLG and E2E challenge data show that by using fact-based alignments our approach is more interpretable, expressive, robust to noise, and easier to control, while retaining the advantages of end-to-end systems in terms of fluency.	3+4
Our code is available at https://github.com/XinnuoXu/AggGen.	6
----------
Publicly available, large pretrained Language Models (LMs) generate text with remarkable quality, but only sequentially from left to right.	1
As a result, they are not immediately applicable to generation tasks that break the unidirectional assumption, such as paraphrasing or text-infilling, necessitating task-specific supervision.	1
In this paper, we present Reflective Decoding, a novel unsupervised algorithm that allows for direct application of unidirectional LMs to non-sequential tasks.	2
Our 2-step approach requires no supervision or even parallel corpora, only two off-the-shelf pretrained LMs in opposite directions: forward and backward.	3
First, in the contextualization step, we use LMs to generate ensembles of past and future contexts which collectively capture the input (e.g. the source sentence for paraphrasing).	3
Second, in the reflection step, we condition on these “context ensembles”, generating outputs that are compatible with them.	3
Comprehensive empirical results demonstrate that Reflective Decoding outperforms strong unsupervised baselines on both paraphrasing and abductive text infilling, significantly narrowing the gap between unsupervised and supervised methods.	4
Reflective Decoding surpasses multiple supervised baselines on various metrics including human evaluation.	4
----------
Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats.	1
One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source.	1
The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators.	1
In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism.	2
The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning.	3
However, it still lacks fidelity to the table contents.	3
The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency.	3
In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning.	4
----------
In this paper, we focus on the problem of citing sentence generation, which entails generating a short text to capture the salient information in a cited paper and the connection between the citing and cited paper.	2
We present BACO, a BAckground knowledge- and COntent-based framework for citing sentence generation, which considers two types of information: (1) background knowledge by leveraging structural information from a citation network; and (2) content, which represents in-depth information about what to cite and why to cite.	2
First, a citation network is encoded to provide background knowledge.	3
Second, we apply salience estimation to identify what to cite by estimating the importance of sentences in the cited paper.	3
During the decoding stage, both types of information are combined to facilitate the text generation, and then we conduct a joint training for the generator and citation function classification to make the model aware of why to cite.	3
Our experimental results show that our framework outperforms comparative baselines.	4
----------
Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities.	1
However, these features are obtained via open-domain toolkits that are dialog-agnostic or heavily relied on human annotations.	1
In this paper, we show how DialoGPT, a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT.	2
We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers.	3
Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-the-art performance on the SAMSum dataset.	4
----------
Recent pretrained language models “solved” many reading comprehension benchmarks, where questions are written with access to the evidence document.	1
However, datasets containing information-seeking queries where evidence documents are provided after the queries are written independently remain challenging.	1
We analyze why answering information-seeking queries is more challenging and where their prevalent unanswerabilities arise, on Natural Questions and TyDi QA.	3
Our controlled experiments suggest two headrooms – paragraph selection and answerability prediction, i.e. whether the paired evidence document contains the answer to the query or not.	2
When provided with a gold paragraph and knowing when to abstain from answering, existing models easily outperform a human annotator.	3
However, predicting answerability itself remains challenging.	3
We manually annotate 800 unanswerable examples across six languages on what makes them challenging to answer.	3
With this new data, we conduct per-category answerability prediction, revealing issues in the current dataset collection as well as task formulation.	4
Together, our study points to avenues for future research in information-seeking question answering, both for dataset creation and model development.	4
Our code and annotated data is publicly available at https://github.com/AkariAsai/unanswerable_qa.	6
----------
Users of medical question answering systems often submit long and detailed questions, making it hard to achieve high recall in answer retrieval.	1
To alleviate this problem, we propose a novel Multi-Task Learning (MTL) method with data augmentation for medical question understanding.	2
We first establish an equivalence between the tasks of question summarization and Recognizing Question Entailment (RQE) using their definitions in the medical domain.	3
Based on this equivalence, we propose a data augmentation algorithm to use just one dataset to optimize for both tasks, with a weighted MTL loss.	2
We introduce gradually soft parameter-sharing: a constraint for decoder parameters to be close, that is gradually loosened as we move to the highest layer.	3
We show through ablation studies that our proposed novelties improve performance.	3
Our method outperforms existing MTL methods across 4 datasets of medical question pairs, in ROUGE scores, RQE accuracy and human evaluation.	4
Finally, we show that our method fares better than single-task learning under 4 low-resource settings.	4
----------
A common issue in real-world applications of named entity recognition and classification (NERC) is the absence of annotated data for the target entity classes during training.	1
Zero-shot learning approaches address this issue by learning models from classes with training data that can predict classes without it.	1
This paper presents the first approach for zero-shot NERC, introducing novel architectures that leverage the fact that textual descriptions for many entity classes occur naturally.	2
We address the zero-shot NERC specific challenge that the not-an-entity class is not well defined as different entity classes are considered in training and testing.	3
For evaluation, we adapt two datasets, OntoNotes and MedMentions, emulating the difficulty of real-world zero-shot learning by testing models on the rarest entity classes.	3
Our proposed approach outperforms baselines adapted from machine reading comprehension and zero-shot text classification.	4
Furthermore, we assess the effect of different class descriptions for this task.	4
----------
Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words.	1
However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information.	1
Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters.	1
This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters.	2
Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding.	3
With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER.	4
The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method.	4
----------
As the sources of information that we consume everyday rapidly diversify, it is becoming increasingly important to develop NLP tools that help to evaluate the credibility of the information we receive.	1
A critical step towards this goal is to determine the factuality of events in text.	1
In this paper, we frame factuality assessment as a modal dependency parsing task that identifies the events and their sources, formally known as conceivers, and then determine the level of certainty that the sources are asserting with respect to the events.	2
We crowdsource the first large-scale data set annotated with modal dependency structures that consists of 353 Covid-19 related news articles, 24,016 events, and 2,938 conceivers.	3
We also develop the first modal dependency parser that jointly extracts events, conceivers and constructs the modal dependency structure of a text.	3
We evaluate the joint model against a pipeline model and demonstrate the advantage of the joint model in conceiver extraction and modal dependency structure construction when events and conceivers are automatically extracted.	4
We believe the dataset and the models will be a valuable resource for a whole host of NLP applications such as fact checking and rumor detection.	5
----------
The modeling of conversational context plays a vital role in emotion recognition from conversation (ERC).	1
In this paper, we put forward a novel idea of encoding the utterances with a directed acyclic graph (DAG) to better model the intrinsic structure within a conversation, and design a directed acyclic neural network, namely DAG-ERC, to implement this idea.	2
In an attempt to combine the strengths of conventional graph-based neural models and recurrence-based neural models, DAG-ERC provides a more intuitive way to model the information flow between long-distance conversation background and nearby context.	3
Extensive experiments are conducted on four ERC benchmarks with state-of-the-art models employed as baselines for comparison.	3
The empirical results demonstrate the superiority of this new model and confirm the motivation of the directed acyclic graph architecture for ERC.	4
----------
Models pre-trained on large-scale regular text corpora often do not work well for user-generated data where the language styles differ significantly from the mainstream text.	1
Here we present Context-Aware Rule Injection (CARI), an innovative method for formality style transfer (FST) by injecting multiple rules into an end-to-end BERT-based encoder and decoder model.	2
CARI is able to learn to select optimal rules based on context.	3
The intrinsic evaluation showed that CARI achieved the new highest performance on the FST benchmark dataset.	4
Our extrinsic evaluation showed that CARI can greatly improve the regular pre-trained models’ performance on several tweet sentiment analysis tasks.	4
Our contributions are as follows: 1.We propose a new method, CARI, to integrate rules for pre-trained language models.	4
CARI is context-aware and can trained end-to-end with the downstream NLP applications.	4
2.We have achieved new state-of-the-art results for FST on the benchmark GYAFC dataset.	4
3.We are the first to evaluate FST methods with extrinsic evaluation and specifically on sentiment classification tasks.	4
We show that CARI outperformed existing rule-based FST approaches for sentiment classification.	4
----------
Emotion detection in dialogues is challenging as it often requires the identification of thematic topics underlying a conversation, the relevant commonsense knowledge, and the intricate transition patterns between the affective states.	1
In this paper, we propose a Topic-Driven Knowledge-Aware Transformer to handle the challenges above.	2
We firstly design a topic-augmented language model (LM) with an additional layer specialized for topic detection.	3
The topic-augmented LM is then combined with commonsense statements derived from a knowledge base based on the dialogue contextual information.	3
Finally, a transformer-based encoder-decoder architecture fuses the topical and commonsense information, and performs the emotion label sequence prediction.	3
The model has been experimented on four datasets in dialogue emotion detection, demonstrating its superiority empirically over the existing state-of-the-art approaches.	4
Quantitative and qualitative results show that the model can discover topics which help in distinguishing emotion categories.	4
----------
Approaches to computational argumentation tasks such as stance detection and aspect detection have largely focused on the text of independent claims, losing out on potentially valuable context provided by the rest of the collection.	1
We introduce a general approach to these tasks motivated by syntopical reading, a reading process that emphasizes comparing and contrasting viewpoints in order to improve topic understanding.	2
To capture collection-level context, we introduce the syntopical graph, a data structure for linking claims within a collection.	2
A syntopical graph is a typed multi-graph where nodes represent claims and edges represent different possible pairwise relationships, such as entailment, paraphrase, or support.	3
Experiments applying syntopical graphs to the problems of detecting stance and aspects demonstrate state-of-the-art performance in each domain, significantly outperforming approaches that do not utilize collection-level information.	4
----------
The prevalence of the COVID-19 pandemic in day-to-day life has yielded large amounts of stance detection data on social media sites, as users turn to social media to share their views regarding various issues related to the pandemic, e.g.	1
stay at home mandates and wearing face masks when out in public.	1
We set out to make use of this data by collecting the stance expressed by Twitter users, with respect to topics revolving around the pandemic.	3
We annotate a new stance detection dataset, called COVID-19-Stance.	3
Using this newly annotated dataset, we train several established stance detection models to ascertain a baseline performance for this specific task.	3
To further improve the performance, we employ self-training and domain adaptation approaches to take advantage of large amounts of unlabeled data and existing stance detection datasets.	3
The dataset, code, and other resources are available on GitHub.	6
----------
Fact verification is a challenging task that requires simultaneously reasoning and aggregating over multiple retrieved pieces of evidence to evaluate the truthfulness of a claim.	1
Existing approaches typically (i) explore the semantic interaction between the claim and evidence at different granularity levels but fail to capture their topical consistency during the reasoning process, which we believe is crucial for verification; (ii) aggregate multiple pieces of evidence equally without considering their implicit stances to the claim, thereby introducing spurious information.	1
To alleviate the above issues, we propose a novel topic-aware evidence reasoning and stance-aware aggregation model for more accurate fact verification, with the following four key properties: 1) checking topical consistency between the claim and evidence; 2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring semantic similarity between the global topic information and the semantic representation of evidence; 4) aggregating evidence based on their implicit stances to the claim.	2
Extensive experiments conducted on the two benchmark datasets demonstrate the superiority of the proposed model over several state-of-the-art approaches for fact verification.	4
The source code can be obtained from https://github.com/jasenchn/TARSA.	6
----------
In conversation, uptake happens when a speaker builds on the contribution of their interlocutor by, for example, acknowledging, repeating or reformulating what they have said.	1
In education, teachers’ uptake of student contributions has been linked to higher student achievement.	1
Yet measuring and improving teachers’ uptake at scale is challenging, as existing methods require expensive annotation by experts.	1
We propose a framework for computationally measuring uptake, by (1) releasing a dataset of student-teacher exchanges extracted from US math classroom transcripts annotated for uptake by experts; (2) formalizing uptake as pointwise Jensen-Shannon Divergence (pJSD), estimated via next utterance classification; (3) conducting a linguistically-motivated comparison of different unsupervised measures and (4) correlating these measures with educational outcomes.	2
We find that although repetition captures a significant part of uptake, pJSD outperforms repetition-based baselines, as it is capable of identifying a wider range of uptake phenomena like question answering and reformulation.	4
We apply our uptake measure to three different educational datasets with outcome indicators.	3
Unlike baseline measures, pJSD correlates significantly with instruction quality in all three, providing evidence for its generalizability and for its potential to serve as an automated professional development tool for teachers.	4
----------
To defend against machine-generated fake news, an effective mechanism is urgently needed.	1
We contribute a novel benchmark for fake news detection at the knowledge element level, as well as a solution for this task which incorporates cross-media consistency checking to detect the fine-grained knowledge elements making news articles misinformative.	2
Due to training data scarcity, we also formulate a novel data synthesis method by manipulating knowledge elements within the knowledge graph to generate noisy training data with specific, hard to detect, known inconsistencies.	3
Our detection approach outperforms the state-of-the-art (up to 16.8% accuracy gain), and more critically, yields fine-grained explanations.	4
----------
To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues.	2
We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers.	4
We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.	4
----------
This paper is concerned with dialogue state tracking (DST) in a task-oriented dialogue system.	2
Building a DST module that is highly effective is still a challenging issue, although significant progresses have been made recently.	1
This paper proposes a new approach to dialogue state tracking, referred to as Seq2Seq-DU, which formalizes DST as a sequence-to-sequence problem.	2
Seq2Seq-DU employs two BERT-based encoders to respectively encode the utterances in the dialogue and the descriptions of schemas, an attender to calculate attentions between the utterance embeddings and the schema embeddings, and a decoder to generate pointers to represent the current state of dialogue.	3
Seq2Seq-DU has the following advantages.	4
It can jointly model intents, slots, and slot values; it can leverage the rich representations of utterances and schemas based on BERT; it can effectively deal with categorical and non-categorical slots, and unseen schemas.	4
In addition, Seq2Seq-DU can also be used in the NLU (natural language understanding) module of a dialogue system.	4
Experimental results on benchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1, WOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the existing methods.	4
----------
Learning discrete dialog structure graph from human-human dialogs yields basic insights into the structure of conversation, and also provides background knowledge to facilitate dialog generation.	1
However, this problem is less studied in open-domain dialogue.	1
In this paper, we conduct unsupervised discovery of discrete dialog structure from chitchat corpora, and then leverage it to facilitate coherent dialog generation in downstream systems.	2
To this end, we present an unsupervised model, Discrete Variational Auto-Encoder with Graph Neural Network (DVAE-GNN), to discover discrete hierarchical latent dialog states (at the level of both session and utterance) and their transitions from corpus as a dialog structure graph.	2
Then we leverage it as background knowledge to facilitate dialog management in a RL based dialog system.	3
Experimental results on two benchmark corpora confirm that DVAE-GNN can discover meaningful dialog structure graph, and the use of dialog structure as background knowledge can significantly improve multi-turn coherence.	4
----------
We study the learning of a matching model for dialogue response selection.	1
Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an “easy-to-difficult” scheme.	2
Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC).	3
In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate.	3
As for IC, it progressively strengthens the model’s ability in identifying the mismatching information between the dialogue context and a response candidate.	3
Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics.	4
----------
In this paper, we present a neural model for joint dropped pronoun recovery (DPR) and conversational discourse parsing (CDP) in Chinese conversational speech.	2
We show that DPR and CDP are closely related, and a joint model benefits both tasks.	4
We refer to our model as DiscProReco, and it first encodes the tokens in each utterance in a conversation with a directed Graph Convolutional Network (GCN).	3
The token states for an utterance are then aggregated to produce a single state for each utterance.	3
The utterance states are then fed into a biaffine classifier to construct a conversational discourse graph.	3
A second (multi-relational) GCN is then applied to the utterance states to produce a discourse relation-augmented representation for the utterances, which are then fused together with token states in each utterance as input to a dropped pronoun recovery layer.	3
The joint model is trained and evaluated on a new Structure Parsing-enhanced Dropped Pronoun Recovery (SPDPR) data set that we annotated with both two types of information.	3
Experimental results on the SPDPR dataset and other benchmarks show that DiscProReco significantly outperforms the state-of-the-art baselines of both tasks.	4
----------
Weak supervision has shown promising results in many natural language processing tasks, such as Named Entity Recognition (NER).	1
Existing work mainly focuses on learning deep NER models only with weak supervision, i.e., without any human annotation, and shows that by merely using weakly labeled data, one can achieve good performance, though still underperforms fully supervised NER with manually/strongly labeled data.	1
In this paper, we consider a more practical scenario, where we have both a small amount of strongly labeled data and a large amount of weakly labeled data.	2
Unfortunately, we observe that weakly labeled data does not necessarily improve, or even deteriorate the model performance (due to the extensive noise in the weak labels) when we train deep NER models over a simple or weighted combination of the strongly labeled and weakly labeled data.	3+4
To address this issue, we propose a new multi-stage computational framework – NEEDLE with three essential ingredients: (1) weak label completion, (2) noise-aware loss function, and (3) final fine-tuning over the strongly labeled data.	2
Through experiments on E-commerce query NER and Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise of the weak labels and outperforms existing methods.	4
In particular, we achieve new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74, BC5CDR-disease 90.69, NCBI-disease 92.28.	4
----------
Recently, there is an effort to extend fine-grained entity typing by using a richer and ultra-fine set of types, and labeling noun phrases including pronouns and nominal nouns instead of just named entity mentions.	1
A key challenge for this ultra-fine entity typing task is that human annotated data are extremely scarce, and the annotation ability of existing distant or weak supervision approaches is very limited.	1
To remedy this problem, in this paper, we propose to obtain training data for ultra-fine entity typing by using a BERT Masked Language Model (MLM).	2
Given a mention in a sentence, our approach constructs an input for the BERT MLM so that it predicts context dependent hypernyms of the mention, which can be used as type labels.	3
Experimental results demonstrate that, with the help of these automatically generated labels, the performance of an ultra-fine entity typing model can be improved substantially.	4
We also show that our approach can be applied to improve traditional fine-grained entity typing after performing simple type mapping.	4
----------
Recent advances in Named Entity Recognition (NER) show that document-level contexts can significantly improve model performance.	1
In many application scenarios, however, such contexts are not available.	1
In this paper, we propose to find external contexts of a sentence by retrieving and selecting a set of semantically relevant texts through a search engine, with the original sentence as the query.	2
We find empirically that the contextual representations computed on the retrieval-based input view, constructed through the concatenation of a sentence and its external contexts, can achieve significantly improved performance compared to the original input view based only on the sentence.	3
Furthermore, we can improve the model performance of both input views by Cooperative Learning, a training method that encourages the two input views to produce similar contextual representations or output label distributions.	3
Experiments show that our approach can achieve new state-of-the-art performance on 8 NER data sets across 5 domains.	4
----------
Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe?	1
In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse.	3
These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity’s current properties and relations, and can be manipulated with predictable effects on language generation.	3
Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.	4
----------
Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts.	1
To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models.	1
We investigate the magnitude of models’ preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures.	2
We uncover similarities and differences across architectures and model sizes—notably, that larger models do not necessarily learn stronger preferences.	4
We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence.	3
Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.	4
----------
NLP has a rich history of representing our prior understanding of language in the form of graphs.	1
Recent work on analyzing contextualized text representations has focused on hand-designed probe models to understand how and to what extent do these representations encode a particular linguistic phenomenon.	1
However, due to the inter-dependence of various phenomena and randomness of training probe models, detecting how these representations encode the rich information in these linguistic graphs remains a challenging problem.	1
In this paper, we propose a new information-theoretic probe, Bird’s Eye, which is a fairly simple probe method for detecting if and how these representations encode the information in these linguistic graphs.	2
Instead of using model performance, our probe takes an information-theoretic view of probing and estimates the mutual information between the linguistic graph embedded in a continuous space and the contextualized word representations.	3
Furthermore, we also propose an approach to use our probe to investigate localized linguistic information in the linguistic graphs using perturbation analysis.	2
We call this probing setup Worm’s Eye.	2
Using these probes, we analyze the BERT models on its ability to encode a syntactic and a semantic graph structure, and find that these models encode to some degree both syntactic as well as semantic information; albeit syntactic information to a greater extent.	4
----------
Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source.	1
In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms.	2
By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts.	4
Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage.	3+4
Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases.	4
----------
We study the problem of generating data poisoning attacks against Knowledge Graph Embedding (KGE) models for the task of link prediction in knowledge graphs.	2
To poison KGE models, we propose to exploit their inductive abilities which are captured through the relationship patterns like symmetry, inversion and composition in the knowledge graph.	2
Specifically, to degrade the model’s prediction confidence on target facts, we propose to improve the model’s prediction confidence on a set of decoy facts.	2
Thus, we craft adversarial additions that can improve the model’s prediction confidence on decoy facts through different inference patterns.	3
Our experiments demonstrate that the proposed poisoning attacks outperform state-of-art baselines on four KGE models for two publicly available datasets.	4
We also find that the symmetry pattern based attacks generalize across all model-dataset combinations which indicates the sensitivity of KGE models to this pattern.	4
----------
A common factor in bias measurement methods is the use of hand-curated seed lexicons, but there remains little guidance for their selection.	1
We gather seeds used in prior work, documenting their common sources and rationales, and in case studies of three English-language corpora, we enumerate the different types of social biases and linguistic features that, once encoded in the seeds, can affect subsequent bias measurements.	2+3
Seeds developed in one context are often re-used in other contexts, but documentation and evaluation remain necessary precursors to relying on seeds for sensitive measurements.	4
----------
Despite inextricable ties between race and language, little work has considered race in NLP research and development.	1
In this work, we survey 79 papers from the ACL anthology that mention race.	2
These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies.	2
However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature.	1
By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.	4
----------
Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations.	1
To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models.	1
Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable.	1
Do these intrinsic and extrinsic metrics correlate with each other?	1
We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions.	2
Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages.	4
We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data.	4
To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.	4
----------
This paper studies the relative importance of attention heads in Transformer-based models to aid their interpretability in cross-lingual and multi-lingual tasks.	2
Prior research has found that only a few attention heads are important in each mono-lingual Natural Language Processing (NLP) task and pruning the remaining heads leads to comparable or improved performance of the model.	1
However, the impact of pruning attention heads is not yet clear in cross-lingual and multi-lingual tasks.	1
Through extensive experiments, we show that (1) pruning a number of attention heads in a multi-lingual Transformer-based model has, in general, positive effects on its performance in cross-lingual and multi-lingual tasks and (2) the attention heads to be pruned can be ranked using gradients and identified with a few trial experiments.	4
Our experiments focus on sequence labeling tasks, with potential applicability on other cross-lingual and multi-lingual tasks.	3
For comprehensiveness, we examine two pre-trained multi-lingual models, namely multi-lingual BERT (mBERT) and XLM-R, on three tasks across 9 languages each.	3
We also discuss the validity of our findings and their extensibility to truly resource-scarce languages and other task settings.	4
----------
Effective adversary generation for neural machine translation (NMT) is a crucial prerequisite for building robust machine translation systems.	1
In this work, we investigate veritable evaluations of NMT adversarial attacks, and propose a novel method to craft NMT adversarial examples.	2
We first show the current NMT adversarial attacks may be improperly estimated by the commonly used mono-directional translation, and we propose to leverage the round-trip translation technique to build valid metrics for evaluating NMT adversarial attacks.	2
Our intuition is that an effective NMT adversarial example, which imposes minor shifting on the source and degrades the translation dramatically, would naturally lead to a semantic-destroyed round-trip translation result.	3
We then propose a promising black-box attack method called Word Saliency speedup Local Search (WSLS) that could effectively attack the mainstream NMT architectures.	3
Comprehensive experiments demonstrate that the proposed metrics could accurately evaluate the attack effectiveness, and the proposed WSLS could significantly break the state-of-art NMT models with small perturbation.	4
Besides, WSLS exhibits strong transferability on attacking Baidu and Bing online translators.	4
----------
Transfer learning has yielded state-of-the-art (SoTA) results in many supervised NLP tasks.	1
However, annotated data for every target task in every target language is rare, especially for low-resource languages.	1
We propose UXLA, a novel unsupervised data augmentation framework for zero-resource transfer learning scenarios.	2
In particular, UXLA aims to solve cross-lingual adaptation problems from a source language task distribution to an unknown target language task distribution, assuming no training label in the target language.	2
At its core, UXLA performs simultaneous self-training with data augmentation and unsupervised sample selection.	3
To show its effectiveness, we conduct extensive experiments on three diverse zero-resource cross-lingual transfer tasks.	3
UXLA achieves SoTA results in all the tasks, outperforming the baselines by a good margin.	4
With an in-depth framework dissection, we demonstrate the cumulative contributions of different components to its success.	4
----------
Recent work on non-autoregressive neural machine translation (NAT) aims at improving the efficiency by parallel decoding without sacrificing the quality.	1
However, existing NAT methods are either inferior to Transformer or require multiple decoding passes, leading to reduced speedup.	1
We propose the Glancing Language Model (GLM) for single-pass parallel generation models.	2
With GLM, we develop Glancing Transformer (GLAT) for machine translation.	2
With only single-pass parallel decoding, GLAT is able to generate high-quality translation with 8×-15× speedup.	3
Note that GLAT does not modify the network architecture, which is a training method to learn word interdependency.	3
Experiments on multiple WMT language directions show that GLAT outperforms all previous single pass non-autoregressive methods, and is nearly comparable to Transformer, reducing the gap to 0.25-0.9 BLEU points.	4
----------
Dense video event captioning aims to generate a sequence of descriptive captions for each event in a long untrimmed video.	1
Video-level context provides important information and facilities the model to generate consistent and less redundant captions between events.	1
In this paper, we introduce a novel Hierarchical Context-aware Network for dense video event captioning (HCN) to capture context from various aspects.	2
In detail, the model leverages local and global context with different mechanisms to jointly learn to generate coherent captions.	3
The local context module performs full interaction between neighbor frames and the global context module selectively attends to previous or future events.	3
According to our extensive experiment on both Youcook2 and Activitynet Captioning datasets, the video-level HCN model outperforms the event-level context-agnostic model by a large margin.	4
The code is available at https://github.com/KirkGuo/HCN.	6
----------
Generating image captions with user intention is an emerging need.	1
The recently published Localized Narratives dataset takes mouse traces as another input to the image captioning task, which is an intuitive and efficient way for a user to control what to describe in the image.	1
However, how to effectively employ traces to improve generation quality and controllability is still under exploration.	1
This paper aims to solve this problem by proposing a novel model called LoopCAG, which connects Contrastive constraints and Attention Guidance in a Loop manner, engaged explicit spatial and temporal constraints to the generating process.	2
Precisely, each generated sentence is temporally aligned to the corresponding trace sequence through a contrastive learning strategy.	3
Besides, each generated text token is supervised to attend to the correct visual objects under heuristic spatial attention guidance.	3
Comprehensive experimental results demonstrate that our LoopCAG model learns better correspondence among the three modalities (vision, language, and traces) and achieves SOTA performance on trace-controlled image captioning task.	4
Moreover, the controllability and explainability of LoopCAG are validated by analyzing spatial and temporal sensitivity during the generation process.	4
----------
We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language.	2
We factorize PIGLeT into a physical dynamics model, and a separate language model.	3
Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t.	3
We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning.	3
PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language.	4
Experimental results show that our model effectively learns world dynamics, along with how to communicate them.	4
It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%.	4
Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives.	4
We present comprehensive analysis showing room for future work.	4
----------
Neural entity typing models typically represent fine-grained entity types as vectors in a high-dimensional space, but such spaces are not well-suited to modeling these types’ complex interdependencies.	1
We study the ability of box embeddings, which embed concepts as d-dimensional hyperrectangles, to capture hierarchies of types even when these relationships are not defined explicitly in the ontology.	3
Our model represents both types and entity mentions as boxes.	3
Each mention and its context are fed into a BERT-based model to embed that mention in our box space; essentially, this model leverages typological clues present in the surface text to hypothesize a type representation for the mention.	3
Box containment can then be used to derive both the posterior probability of a mention exhibiting a given type and the conditional probability relations between types themselves.	3
We compare our approach with a vector-based typing model and observe state-of-the-art performance on several entity typing benchmarks.	2
In addition to competitive typing performance, our box-based model shows better performance in prediction consistency (predicting a supertype and a subtype together) and confidence (i.e., calibration), demonstrating that the box-based model captures the latent type hierarchies better than the vector-based model does.	4
----------
Recent pretraining models in Chinese neglect two important aspects specific to the Chinese language: glyph and pinyin, which carry significant syntax and semantic information for language understanding.	1
In this work, we propose ChineseBERT, which incorporates both the glyph and pinyin information of Chinese characters into language model pretraining.	2
The glyph embedding is obtained based on different fonts of a Chinese character, being able to capture character semantics from the visual features, and the pinyin embedding characterizes the pronunciation of Chinese characters, which handles the highly prevalent heteronym phenomenon in Chinese (the same character has different pronunciations with different meanings).	3
Pretrained on large-scale unlabeled Chinese corpus, the proposed ChineseBERT model yields significant performance boost over baseline models with fewer training steps.	3
The proposed model achieves new SOTA performances on a wide range of Chinese NLP tasks, including machine reading comprehension, natural language inference, text classification, sentence pair matching, and competitive performances in named entity recognition and word segmentation.	4
----------
Knowledge distillation has been proven to be effective in model acceleration and compression.	1
It transfers knowledge from a large neural network to a small one by using the large neural network predictions as targets of the small neural network.	1
But this way ignores the knowledge inside the large neural networks, e.g., parameters.	1
Our preliminary study as well as the recent success in pre-training suggests that transferring parameters are more effective in distilling knowledge.	1
In this paper, we propose Weight Distillation to transfer the knowledge in parameters of a large neural network to a small neural network through a parameter generator.	2
On the WMT16 En-Ro, NIST12 Zh-En, and WMT14 En-De machine translation tasks, our experiments show that weight distillation learns a small network that is 1.88 2.94x faster than the large network but with competitive BLEU performance.	4
When fixing the size of small networks, weight distillation outperforms knowledge distillation by 0.51 1.82 BLEU points.	4
----------
It is a common belief that training deep transformers from scratch requires large datasets.	1
Consequently, for small datasets, people usually use shallow and simple additional layers on top of pre-trained models during fine-tuning.	1
This work shows that this does not always need to be the case: with proper initialization and optimization, the benefits of very deep transformers can carry over to challenging tasks with small datasets, including Text-to-SQL semantic parsing and logical reading comprehension.	2+3
In particular, we successfully train 48 layers of transformers, comprising 24 fine-tuned layers from pre-trained RoBERTa and 24 relation-aware layers trained from scratch.	3
With fewer training steps and no task-specific pre-training, we obtain the state of the art performance on the challenging cross-domain Text-to-SQL parsing benchmark Spider.	4
We achieve this by deriving a novel Data dependent Transformer Fixed-update initialization scheme (DT-Fixup), inspired by the prior T-Fixup work.	4
Further error analysis shows that increasing depth can help improve generalization on small datasets for hard cases that require reasoning and structural understanding.	4
----------
Transformer-based language models (TLMs), such as BERT, ALBERT and GPT-3, have shown strong performance in a wide range of NLP tasks and currently dominate the field of NLP.	1
However, many researchers wonder whether these models can maintain their dominance forever.	1
Of course, we do not have answers now, but, as an attempt to find better neural architectures and training schemes, we pretrain a simple CNN using a GAN-style learning scheme and Wikipedia data, and then integrate it with standard TLMs.	3
We show that on the GLUE tasks, the combination of our pretrained CNN with ALBERT outperforms the original ALBERT and achieves a similar performance to that of SOTA.	4
Furthermore, on open-domain QA (Quasar-T and SearchQA), the combination of the CNN with ALBERT or RoBERTa achieved stronger performance than SOTA and the original TLMs.	4
We hope that this work provides a hint for developing a novel strong network architecture along with its training scheme.	5
Our source code and models are available at https://github.com/nict-wisdom/bertac.	6
----------
Existing software-based energy measurements of NLP models are not accurate because they do not consider the complex interactions between energy consumption and model execution.	1
We present IrEne, an interpretable and extensible energy prediction system that accurately predicts the inference energy consumption of a wide range of Transformer-based NLP models.	2
IrEne constructs a model tree graph that breaks down the NLP model into modules that are further broken down into low-level machine learning (ML) primitives.	3
IrEne predicts the inference energy consumption of the ML primitives as a function of generalizable features and fine-grained runtime resource usage.	3
IrEne then aggregates these low-level predictions recursively to predict the energy of each module and finally of the entire model.	3
Experiments across multiple Transformer models show IrEne predicts inference energy consumption of transformer models with an error of under 7% compared to the ground truth.	4
In contrast, existing energy models see an error of over 50%.	4
We also show how IrEne can be used to conduct energy bottleneck analysis and to easily evaluate the energy impact of different architectural choices.	4
We release the code and data at https://github.com/StonyBrookNLP/irene.	6
----------
The element of repetition in cyberbullying behavior has directed recent computational studies toward detecting cyberbullying based on a social media session.	1
In contrast to a single text, a session may consist of an initial post and an associated sequence of comments.	1
Yet, emerging efforts to enhance the performance of session-based cyberbullying detection have largely overlooked unintended social biases in existing cyberbullying datasets.	1
For example, a session containing certain demographic-identity terms (e.g., “gay” or “black”) is more likely to be classified as an instance of cyberbullying.	1
In this paper, we first show evidence of such bias in models trained on sessions collected from different social media platforms (e.g., Instagram).	2
We then propose a context-aware and model-agnostic debiasing strategy that leverages a reinforcement learning technique, without requiring any extra resources or annotations apart from a pre-defined set of sensitive triggers commonly used for identifying cyberbullying instances.	2
Empirical evaluations show that the proposed strategy can simultaneously alleviate the impacts of the unintended biases and improve the detection performance.	4
----------
Creating effective visualization is an important part of data analytics.	1
While there are many libraries for creating visualization, writing such code remains difficult given the myriad of parameters that users need to provide.	1
In this paper, we propose the new task of synthesizing visualization programs from a combination of natural language utterances and code context.	2
To tackle the learning problem, we introduce PlotCoder, a new hierarchical encoder-decoder architecture that models both the code context and the input utterance.	2
We use PlotCoder to first determine the template of the visualization code, followed by predicting the data to be plotted.	3
We use Jupyter notebooks containing visualization programs crawled from GitHub to train PlotCoder.	3
On a comprehensive set of test samples from those notebooks, we show that PlotCoder correctly predicts the plot type of about 70% samples, and synthesizes the correct programs for 35% samples, performing 3-4.5% better than the baselines.	4
----------
Heavily overparameterized language models such as BERT, XLNet and T5 have achieved impressive success in many NLP tasks.	1
However, their high model complexity requires enormous computation resources and extremely long training time for both pre-training and fine-tuning.	1
Many works have studied model compression on large NLP models, but only focusing on reducing inference time while still requiring an expensive training process.	1
Other works use extremely large batch sizes to shorten the pre-training time, at the expense of higher computational resource demands.	1
In this paper, inspired by the Early-Bird Lottery Tickets recently studied for computer vision tasks, we propose EarlyBERT, a general computationally-efficient training algorithm applicable to both pre-training and fine-tuning of large-scale language models.	2
By slimming the self-attention and fully-connected sub-layers inside a transformer, we are the first to identify structured winning tickets in the early stage of BERT training.	3
We apply those tickets towards efficient BERT training, and conduct comprehensive pre-training and fine-tuning experiments on GLUE and SQuAD downstream tasks.	3
Our results show that EarlyBERT achieves comparable performance to standard BERT, with 35 45% less training time.	4
Code is available at https://github.com/VITA-Group/EarlyBERT.	6
----------
Adapter-based tuning has recently arisen as an alternative to fine-tuning.	1
It works by adding light-weight adapter modules to a pretrained language model (PrLM) and only updating the parameters of adapter modules when learning on a downstream task.	1
As such, it adds only a few trainable parameters per new task, allowing a high degree of parameter sharing.	1
Prior studies have shown that adapter-based tuning often achieves comparable results to fine-tuning.	1
However, existing work only focuses on the parameter-efficient aspect of adapter-based tuning while lacking further investigation on its effectiveness.	1
In this paper, we study the latter.	1
We first show that adapter-based tuning better mitigates forgetting issues than fine-tuning since it yields representations with less deviation from those generated by the initial PrLM.	4
We then empirically compare the two tuning methods on several downstream NLP tasks and settings.	3
We demonstrate that 1) adapter-based tuning outperforms fine-tuning on low-resource and cross-lingual tasks; 2) it is more robust to overfitting and less sensitive to changes in learning rates.	4
----------
Data augmentation is an effective way to improve the performance of many neural text generation models.	1
However, current data augmentation methods need to define or choose proper data mapping functions that map the original samples into the augmented samples.	1
In this work, we derive an objective to formulate the problem of data augmentation on text generation tasks without any use of augmented data constructed by specific mapping functions.	2
Our proposed objective can be efficiently optimized and applied to popular loss functions on text generation tasks with a convergence rate guarantee.	5
Experiments on five datasets of two text generation tasks show that our approach can approximate or even surpass popular data augmentation methods.	4
----------
With the need of fast retrieval speed and small memory footprint, document hashing has been playing a crucial role in large-scale information retrieval.	1
To generate high-quality hashing code, both semantics and neighborhood information are crucial.	1
However, most existing methods leverage only one of them or simply combine them via some intuitive criteria, lacking a theoretical principle to guide the integration process.	1
In this paper, we encode the neighborhood information with a graph-induced Gaussian distribution, and propose to integrate the two types of information with a graph-driven generative model.	2
To deal with the complicated correlations among documents, we further propose a tree-structured approximation method for learning.	2
Under the approximation, we prove that the training objective can be decomposed into terms involving only singleton or pairwise documents, enabling the model to be trained as efficiently as uncorrelated ones.	3
Extensive experimental results on three benchmark datasets show that our method achieves superior performance over state-of-the-art methods, demonstrating the effectiveness of the proposed model for simultaneously preserving semantic and neighborhood information.	4
----------
The open-ended nature of visual captioning makes it a challenging area for evaluation.	1
The majority of proposed models rely on specialized training to improve human-correlation, resulting in limited adoption, generalizability, and explainabilty.	1
We introduce “typicality”, a new formulation of evaluation rooted in information theory, which is uniquely suited for problems lacking a definite ground truth.	2
Typicality serves as our framework to develop a novel semantic comparison, SPARCS, as well as referenceless fluency evaluation metrics.	3
Over the course of our analysis, two separate dimensions of fluency naturally emerge: style, captured by metric SPURTS, and grammar, captured in the form of grammatical outlier penalties.	3
Through extensive experiments and ablation studies on benchmark datasets, we show how these decomposed dimensions of semantics and fluency provide greater system-level insight into captioner differences.	3
Our proposed metrics along with their combination, SMURF, achieve state-of-the-art correlation with human judgment when compared with other rule-based evaluation metrics.	4
----------
The goal of database question answering is to enable natural language querying of real-life relational databases in diverse application domains.	1
Recently, large-scale datasets such as Spider and WikiSQL facilitated novel modeling techniques for text-to-SQL parsing, improving zero-shot generalization to unseen databases.	1
In this work, we examine the challenges that still prevent these techniques from practical deployment.	2
First, we present KaggleDBQA, a new cross-domain evaluation dataset of real Web databases, with domain-specific data types, original formatting, and unrestricted questions.	3
Second, we re-examine the choice of evaluation tasks for text-to-SQL parsers as applied in real-life settings.	3
Finally, we augment our in-domain evaluation task with database documentation, a naturally occurring source of implicit domain knowledge.	3
We show that KaggleDBQA presents a challenge to state-of-the-art zero-shot parsers but a more realistic evaluation setting and creative use of associated database documentation boosts their accuracy by over 13.2%, doubling their performance.	4
----------
The performance of fine-tuning pre-trained language models largely depends on the hyperparameter configuration.	1
In this paper, we investigate the performance of modern hyperparameter optimization methods (HPO) on fine-tuning pre-trained language models.	2
First, we study and report three HPO algorithms’ performances on fine-tuning two state-of-the-art language models on the GLUE dataset.	3
We find that using the same time budget, HPO often fails to outperform grid search due to two reasons: insufficient time budget and overfitting.	4
We propose two general strategies and an experimental procedure to systematically troubleshoot HPO’s failure cases.	2
By applying the procedure, we observe that HPO can succeed with more appropriate settings in the search space and time budget; however, in certain cases overfitting remains.	3
Finally, we make suggestions for future work.	4
Our implementation can be found in https://github.com/microsoft/FLAML/tree/main/flaml/nlp/	6
----------
Evaluation in NLP is usually done by comparing the scores of competing systems independently averaged over a common set of test instances.	1
In this work, we question the use of averages for aggregating evaluation scores into a final number used to decide which system is best, since the average, as well as alternatives such as the median, ignores the pairing arising from the fact that systems are evaluated on the same test instances.	2
We illustrate the importance of taking the instancelevel pairing of evaluation scores into account and demonstrate, both theoretically and empirically, the advantages of aggregation methods based on pairwise comparisons, such as the Bradley–Terry (BT) model, a mechanism based on the estimated probability that a given system scores better than another on the test set.	4
By re-evaluating 296 real NLP evaluation setups across four tasks and 18 evaluation metrics, we show that the choice of aggregation mechanism matters and yields different conclusions as to which systems are state of the art in about 30% of the setups.	4
To facilitate the adoption of pairwise evaluation, we release a practical tool for performing the full analysis of evaluation scores with the mean, median, BT, and two variants of BT (Elo and TrueSkill), alongside functionality for appropriate statistical testing.	4
----------
Despite pre-trained language models have proven useful for learning high-quality semantic representations, these models are still vulnerable to simple perturbations.	1
Recent works aimed to improve the robustness of pre-trained models mainly focus on adversarial training from perturbed examples with similar semantics, neglecting the utilization of different or even opposite semantics.	1
Different from the image processing field, the text is discrete and few word substitutions can cause significant semantic changes.	1
To study the impact of semantics caused by small perturbations, we conduct a series of pilot experiments and surprisingly find that adversarial training is useless or even harmful for the model to detect these semantic changes.	1
To address this problem, we propose Contrastive Learning with semantIc Negative Examples (CLINE), which constructs semantic negative examples unsupervised to improve the robustness under semantically adversarial attacking.	2+3
By comparing with similar and opposite semantic examples, the model can effectively perceive the semantic changes caused by small perturbations.	4
Empirical results show that our approach yields substantial improvements on a range of sentiment analysis, reasoning, and reading comprehension tasks.	4
And CLINE also ensures the compactness within the same semantics and separability across different semantics in sentence-level.	4
----------
Topic modeling has been widely used for discovering the latent semantic structure of documents, but most existing methods learn topics with a flat structure.	1
Although probabilistic models can generate topic hierarchies by introducing nonparametric priors like Chinese restaurant process, such methods have data scalability issues.	1
In this study, we develop a tree-structured topic model by leveraging nonparametric neural variational inference.	2
Particularly, the latent components of the stick-breaking process are first learned for each document, then the affiliations of latent components are modeled by the dependency matrices between network layers.	3
Utilizing this network structure, we can efficiently extract a tree-structured topic hierarchy with reasonable structure, low redundancy, and adaptable widths.	4
Experiments on real-world datasets validate the effectiveness of our method.	4
----------
Prior work infers the causation between events mainly based on the knowledge induced from the annotated causal event pairs.	1
However, additional evidence information intermediate to the cause and effect remains unexploited.	1
By incorporating such information, the logical law behind the causality can be unveiled, and the interpretability and stability of the causal reasoning system can be improved.	1
To facilitate this, we present an Event graph knowledge enhanced explainable CAusal Reasoning framework (ExCAR).	2
ExCAR first acquires additional evidence information from a large-scale causal event graph as logical rules for causal reasoning.	3
To learn the conditional probabilistic of logical rules, we propose the Conditional Markov Neural Logic Network (CMNLN) that combines the representation learning and structure learning of logical rules in an end-to-end differentiable manner.	2+3
Experimental results demonstrate that ExCAR outperforms previous state-of-the-art methods.	4
Adversarial evaluation shows the improved stability of ExCAR over baseline systems.	4
Human evaluation shows that ExCAR can achieve a promising explainable performance.	4
----------
Emotion category is usually divided into different ones by human beings, but it is indeed difficult to clearly distinguish and define the boundaries between different emotion categories.	1
The existing studies working on emotion detection usually focus on how to improve the performance of model prediction, in which emotions are represented with one-hot vectors.	1
However, emotion relations are ignored in one-hot representations.	1
In this article, we first propose a general framework to learn the distributed representations for emotion categories in emotion space from a given emotion classification dataset.	2
Furthermore, based on the soft labels predicted by the pre-trained neural network model, we derive a simple and effective algorithm.	3
Experiments have validated that the proposed representations in emotion space can express emotion relations much better than word vectors in semantic space.	4
----------
In this digital age, online users expect personalized content.	1
To cater to diverse group of audiences across online platforms it is necessary to generate multiple variants of same content with differing degree of characteristics (sentiment, style, formality, etc.).	1
Though text-style transfer is a well explored related area, it focuses on flipping the style attribute polarity instead of regulating a fine-grained attribute transfer.	1
In this paper we propose a hierarchical architecture for finer control over the at- tribute, preserving content using attribute dis- entanglement.	2
We demonstrate the effective- ness of the generative process for two different attributes with varied complexity, namely sentiment and formality.	4
With extensive experiments and human evaluation on five real-world datasets, we show that the framework can generate natural looking sentences with finer degree of control of intensity of a given attribute.	4
----------
Aspect-based Sentiment Analysis (ABSA) aims to identify the aspect terms, their corresponding sentiment polarities, and the opinion terms.	1
There exist seven subtasks in ABSA.	1
Most studies only focus on the subsets of these subtasks, which leads to various complicated ABSA models while hard to solve these subtasks in a unified framework.	1
In this paper, we redefine every subtask target as a sequence mixed by pointer indexes and sentiment class indexes, which converts all ABSA subtasks into a unified generative formulation.	2
Based on the unified formulation, we exploit the pre-training sequence-to-sequence model BART to solve all ABSA subtasks in an end-to-end framework.	3
Extensive experiments on four ABSA datasets for seven subtasks demonstrate that our framework achieves substantial performance gain and provides a real unified end-to-end solution for the whole ABSA subtasks, which could benefit multiple tasks.	4
----------
Task-oriented dialogue systems typically require manual annotation of dialogue slots in training data, which is costly to obtain.	1
We propose a method that eliminates this requirement: We use weak supervision from existing linguistic annotation models to identify potential slot candidates, then automatically identify domain-relevant slots by using clustering algorithms.	2+3
Furthermore, we use the resulting slot annotation to train a neural-network-based tagger that is able to perform slot tagging with no human intervention.	3
This tagger is trained solely on the outputs of our method and thus does not rely on any labeled data.	3
Our model demonstrates state-of-the-art performance in slot tagging without labeled training data on four different dialogue domains.	4
Moreover, we find that slot annotations discovered by our model significantly improve the performance of an end-to-end dialogue response generation model, compared to using no slot annotation at all.	4
----------
Intent classification is a major task in spoken language understanding (SLU).	1
Since most models are built with pre-collected in-domain (IND) training utterances, their ability to detect unsupported out-of-domain (OOD) utterances has a critical effect in practical use.	1
Recent works have shown that using extra data and labels can improve the OOD detection performance, yet it could be costly to collect such data.	1
This paper proposes to train a model with only IND data while supporting both IND intent classification and OOD detection.	2
Our method designs a novel domain-regularized module (DRM) to reduce the overconfident phenomenon of a vanilla classifier, achieving a better generalization in both cases.	3+4
Besides, DRM can be used as a drop-in replacement for the last layer in any neural network-based intent classifier, providing a low-cost strategy for a significant improvement.	4
The evaluation on four datasets shows that our method built on BERT and RoBERTa models achieves state-of-the-art performance against existing approaches and the strong baselines we created for the comparisons.	4
----------
Recent research considers few-shot intent detection as a meta-learning problem: the model is learning to learn from a consecutive set of small tasks named episodes.	1
In this work, we propose ProtAugment, a meta-learning algorithm for short texts classification (the intent detection task).	2
ProtAugment is a novel extension of Prototypical Networks, that limits overfitting on the bias introduced by the few-shots classification objective at each episode.	3
It relies on diverse paraphrasing: a conditional language model is first fine-tuned for paraphrasing, and diversity is later introduced at the decoding stage at each meta-learning episode.	3
The diverse paraphrasing is unsupervised as it is applied to unlabelled data, and then fueled to the Prototypical Network training objective as a consistency loss.	3
ProtAugment is the state-of-the-art method for intent detection meta-learning, at no extra labeling efforts and without the need to fine-tune a conditional language model on a given application domain.	4
----------
Most language understanding models in task-oriented dialog systems are trained on a small amount of annotated training data, and evaluated in a small set from the same distribution.	1
However, these models can lead to system failure or undesirable output when being exposed to natural language perturbation or variation in practice.	1
In this paper, we conduct comprehensive evaluation and analysis with respect to the robustness of natural language understanding models, and introduce three important aspects related to language understanding in real-world dialog systems, namely, language variety, speech characteristics, and noise perturbation.	2
We propose a model-agnostic toolkit LAUG to approximate natural language perturbations for testing the robustness issues in task-oriented dialog.	2
Four data augmentation approaches covering the three aspects are assembled in LAUG, which reveals critical robustness issues in state-of-the-art models.	4
The augmented dataset through LAUG can be used to facilitate future research on the robustness testing of language understanding in task-oriented dialog.	4
----------
Dialogue state tracking (DST) plays a key role in task-oriented dialogue systems to monitor the user’s goal.	1
In general, there are two strategies to track a dialogue state: predicting it from scratch and updating it from previous state.	1
The scratch-based strategy obtains each slot value by inquiring all the dialogue history, and the previous-based strategy relies on the current turn dialogue to update the previous dialogue state.	1
However, it is hard for the scratch-based strategy to correctly track short-dependency dialogue state because of noise; meanwhile, the previous-based strategy is not very useful for long-dependency dialogue state tracking.	1
Obviously, it plays different roles for the context information of different granularity to track different kinds of dialogue states.	1
Thus, in this paper, we will study and discuss how the context information of different granularity affects dialogue state tracking.	2
First, we explore how greatly different granularities affect dialogue state tracking.	3
Then, we further discuss how to combine multiple granularities for dialogue state tracking.	3
Finally, we apply the findings about context granularity to few-shot learning scenario.	3
Besides, we have publicly released all codes.	6
----------
Recently, there has been significant progress in studying neural networks to translate text descriptions into SQL queries.	1
Despite achieving good performance on some public benchmarks, existing text-to-SQL models typically rely on the lexical matching between words in natural language (NL) questions and tokens in table schemas, which may render the models vulnerable to attacks that break the schema linking mechanism.	1
In this work, we investigate the robustness of text-to-SQL models to synonym substitution.	2
In particular, we introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for text-to-SQL translation.	2
NL questions in Spider-Syn are modified from Spider, by replacing their schema-related words with manually selected synonyms that reflect real-world question paraphrases.	3
We observe that the accuracy dramatically drops by eliminating such explicit correspondence between NL questions and table schemas, even if the synonyms are not adversarially selected to conduct worst-case attacks.	3
Finally, we present two categories of approaches to improve the model robustness.	2
The first category of approaches utilizes additional synonym annotations for table schemas by modifying the model input, while the second category is based on adversarial training.	3
We demonstrate that both categories of approaches significantly outperform their counterparts without the defense, and the first category of approaches are more effective.	4
----------
In order to better understand the reason behind model behaviors (i.e., making predictions), most recent works have exploited generative models to provide complementary explanations.	1
However, existing approaches in NLP mainly focus on “WHY A” rather than contrastive “WHY A NOT B”, which is shown to be able to better distinguish confusing candidates and improve data efficiency in other research fields.	1
In this paper, we focus on generating contrastive explanations with counterfactual examples in NLI and propose a novel Knowledge-Aware Contrastive Explanation generation framework (KACE).	2
Specifically, we first identify rationales (i.e., key phrases) from input sentences, and use them as key perturbations for generating counterfactual examples.	3
After obtaining qualified counterfactual examples, we take them along with original examples and external knowledge as input, and employ a knowledge-aware generative pre-trained language model to generate contrastive explanations.	3
Experimental results show that contrastive explanations are beneficial to fit the scenarios by clarifying the difference between the predicted answer and other possible wrong ones.	4
Moreover, we train an NLI model enhanced with contrastive explanations and achieves an accuracy of 91.9% on SNLI, gaining improvements of 5.7% against ETPA (“Explain-Then-Predict-Attention”) and 0.6% against NILE (“WHY A”).	4
----------
Although BERT and its variants have reshaped the NLP landscape, it still remains unclear how best to derive sentence embeddings from such pre-trained Transformers.	1
In this work, we propose a contrastive learning method that utilizes self-guidance for improving the quality of BERT sentence representations.	2
Our method fine-tunes BERT in a self-supervised fashion, does not rely on data augmentation, and enables the usual [CLS] token embeddings to function as sentence vectors.	3
Moreover, we redesign the contrastive learning objective (NT-Xent) and apply it to sentence representation learning.	3
We demonstrate with extensive experiments that our approach is more effective than competitive baselines on diverse sentence-related tasks.	4
We also show it is efficient at inference and robust to domain shifts.	4
----------
This work aims to tackle the challenging heterogeneous graph encoding problem in the text-to-SQL task.	1
Previous methods are typically node-centric and merely utilize different weight matrices to parameterize edge types, which 1) ignore the rich semantics embedded in the topological structure of edges, and 2) fail to distinguish local and non-local relations for each node.	1
To this end, we propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying relational features without constructing meta-paths.	2
By virtue of the line graph, messages propagate more efficiently through not only connections between nodes, but also the topology of directed edges.	3
Furthermore, both local and non-local relations are integrated distinctively during the graph iteration.	3
We also design an auxiliary task called graph pruning to improve the discriminative capability of the encoder.	3
Our framework achieves state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the cross-domain text-to-SQL benchmark Spider at the time of writing.	4
----------
Multimodal pre-training models, such as LXMERT, have achieved excellent results in downstream tasks.	1
However, current pre-trained models require large amounts of training data and have huge model sizes, which make them impossible to apply in low-resource situations.	1
How to obtain similar or even better performance than a larger model under the premise of less pre-training data and smaller model size has become an important problem.	1
In this paper, we propose a new Multi-stage Pre-training (MSP) method, which uses information at different granularities from word, phrase to sentence in both texts and images to pre-train a model in stages.	2
We also design several different pre-training tasks suitable for the information granularity in different stage in order to efficiently capture the diverse knowledge from a limited corpus.	2
We take a Simplified LXMERT (LXMERT-S) which is with 45.9% parameters of the original LXMERT model and only 11.44% of the original pre-training data as the testbed of our MSP method.	3
Experimental results show that our method achieves comparable performance to the original LXMERT model in all downstream tasks, and even outperforms the original model in Image-Text Retrieval task.	4
----------
Document-level contextual information has shown benefits to text-based machine translation, but whether and how context helps end-to-end (E2E) speech translation (ST) is still under-studied.	1
We fill this gap through extensive experiments using a simple concatenation-based context-aware ST model, paired with adaptive feature selection on speech encodings for computational efficiency.	3
We investigate several decoding approaches, and introduce in-model ensemble decoding which jointly performs document- and sentence-level translation using the same model.	2
Our results on the MuST-C benchmark with Transformer demonstrate the effectiveness of context to E2E ST.	4
Compared to sentence-level ST, context-aware ST obtains better translation quality (+0.18-2.61 BLEU), improves pronoun and homophone translation, shows better robustness to (artificial) audio segmentation errors, and reduces latency and flicker to deliver higher quality for simultaneous translation.	4
----------
Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents.	1
We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework.	2
Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage.	3
Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks.	3
Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 to 0.8420), CORD (0.9493 to 0.9601), SROIE (0.9524 to 0.9781), Kleister-NDA (0.8340 to 0.8520), RVL-CDIP (0.9443 to 0.9564), and DocVQA (0.7295 to 0.8672).	4
----------
Existed pre-training methods either focus on single-modal tasks or multi-modal tasks, and cannot effectively adapt to each other.	1
They can only utilize single-modal data (i.e., text or image) or limited multi-modal data (i.e., image-text pairs).	1
In this work, we propose a UNIfied-MOdal pre-training architecture, namely UNIMO, which can effectively adapt to both single-modal and multi-modal understanding and generation tasks.	2
Large scale of free text corpus and image collections are utilized to improve the capability of visual and textual understanding, and cross-modal contrastive learning (CMCL) is leveraged to align the textual and visual information into a unified semantic space, over a corpus of image-text pairs augmented with related images and texts.	3
With the help of rich non-paired single-modal data, our model is able to learn more generalizable representations, by allowing textual knowledge and visual knowledge to enhance each other in the unified semantic space.	3
The experimental results show that UNIMO greatly improves the performance of several single-modal and multi-modal downstream tasks.	4
Our code and pre-trained models are public at https://github.com/PaddlePaddle/Research/tree/master/NLP/UNIMO.	6
----------
Multimodal fusion has been proved to improve emotion recognition performance in previous works.	1
However, in real-world applications, we often encounter the problem of missing modality, and which modalities will be missing is uncertain.	1
It makes the fixed multimodal fusion fail in such cases.	1
In this work, we propose a unified model, Missing Modality Imagination Network (MMIN), to deal with the uncertain missing modality problem.	2
MMIN learns robust joint multimodal representations, which can predict the representation of any missing modality given available modalities under different missing modality conditions.	3
Comprehensive experiments on two benchmark datasets demonstrate that the unified MMIN model significantly improves emotion recognition performance under both uncertain missing-modality testing conditions and full-modality ideal testing condition.	4
The code will be available at https://github.com/AIM3-RUC/MMIN.	6
----------
Encoder pre-training is promising in end-to-end Speech Translation (ST), given the fact that speech-to-translation data is scarce.	1
But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders.	1
For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences.	1
In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation.	2
Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence.	3
In this way, it is straightforward to incorporate the pre-trained models into the system.	3
Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge.	2
Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2.	4
To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available.	4
----------
In this paper, we study the task of graph-based constituent parsing in the setting that binarization is not conducted as a pre-processing step, where a constituent tree may consist of nodes with more than two children.	2
Previous graph-based methods on this setting typically generate hidden nodes with the dummy label inside the n-ary nodes, in order to transform the tree into a binary tree for prediction.	1
The limitation is that the hidden nodes break the sibling relations of the n-ary node’s children.	1
Consequently, the dependencies of such sibling constituents might not be accurately modeled and is being ignored.	1
To solve this limitation, we propose a novel graph-based framework, which is called “recursive semi-Markov model”.	1
The main idea is to utilize 1-order semi-Markov model to predict the immediate children sequence of a constituent candidate, which then recursively serves as a child candidate of its parent.	3
In this manner, the dependencies of sibling constituents can be described by 1-order transition features, which solves the above limitation.	3
Through experiments, the proposed framework obtains the F1 of 95.92% and 92.50% on the datasets of PTB and CTB 5.1 respectively.	4
Specially, the recursive semi-Markov model shows advantages in modeling nodes with more than two children, whose average F1 can be improved by 0.3-1.1 points in PTB and 2.3-6.8 points in CTB 5.1.	4
----------
Pretrained contextualized embeddings are powerful word representations for structured prediction tasks.	1
Recent work found that better word representations can be obtained by concatenating different types of embeddings.	1
However, the selection of embeddings to form the best concatenated representation usually varies depending on the task and the collection of candidate embeddings, and the ever-increasing number of embedding types makes it a more difficult problem.	1
In this paper, we propose Automated Concatenation of Embeddings (ACE) to automate the process of finding better concatenations of embeddings for structured prediction tasks, based on a formulation inspired by recent progress on neural architecture search.	2
Specifically, a controller alternately samples a concatenation of embeddings, according to its current belief of the effectiveness of individual embedding types in consideration for a task, and updates the belief based on a reward.	3
We follow strategies in reinforcement learning to optimize the parameters of the controller and compute the reward based on the accuracy of a task model, which is fed with the sampled concatenation as input and trained on a task dataset.	3
Empirical results on 6 tasks and 21 datasets show that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.	4
----------
In structured prediction problems, cross-lingual transfer learning is an efficient way to train quality models for low-resource languages, and further improvement can be obtained by learning from multiple source languages.	1
However, not all source models are created equal and some may hurt performance on the target language.	1
Previous work has explored the similarity between source and target sentences as an approximate measure of strength for different source models.	1
In this paper, we propose a multi-view framework, by leveraging a small number of labeled target sentences, to effectively combine multiple source models into an aggregated source view at different granularity levels (language, sentence, or sub-structure), and transfer it to a target view based on a task-specific model.	2
By encouraging the two views to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training.	3
Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data.	4
----------
Incorporating syntax into neural approaches in NLP has a multitude of practical and scientific benefits.	1
For instance, a language model that is syntax-aware is likely to be able to produce better samples; even a discriminative model like BERT with a syntax module could be used for core NLP tasks like unsupervised syntactic parsing.	1
Rapid progress in recent years was arguably spurred on by the empirical success of the Parsing-Reading-Predict architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM of (Shen et al., 2019).	1
Most notably, this is the first time neural approaches were able to successfully perform unsupervised syntactic parsing (evaluated by various metrics like F-1 score).	1
However, even heuristic (much less fully mathematical) understanding of why and when these architectures work is lagging severely behind.	1
In this work, we answer representational questions raised by the architectures in (Shen et al., 2018a, 2019), as well as some transition-based syntax-aware language models (Dyer et al., 2016): what kind of syntactic structure can current neural approaches to syntax represent?	2
Concretely, we ground this question in the sandbox of probabilistic context-free-grammars (PCFGs), and identify a key aspect of the representational power of these approaches: the amount and directionality of context that the predictor has access to when forced to make parsing decision.	3
We show that with limited context (either bounded, or unidirectional), there are PCFGs, for which these approaches cannot represent the max-likelihood parse; conversely, if the context is unlimited, they can represent the max-likelihood parse of any PCFG.	4
----------
Neural lexicalized PCFGs (L-PCFGs) have been shown effective in grammar induction.	1
However, to reduce computational complexity, they make a strong independence assumption on the generation of the child word and thus bilexical dependencies are ignored.	1
In this paper, we propose an approach to parameterize L-PCFGs without making implausible independence assumptions.	2
Our approach directly models bilexical dependencies and meanwhile reduces both learning and representation complexities of L-PCFGs.	3
Experimental results on the English WSJ dataset confirm the effectiveness of our approach in improving both running speed and unsupervised parsing performance.	4
----------
Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems.	1
However, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give Likert-type multi-level coherence scores, dubbed as “quantifiable”; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training.	1
To address these limitations, we propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards.	2
Specifically, QuantiDCE includes two training stages, Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning.	3
During MLR pre-training, a new MLR loss is proposed for enabling the model to learn the coarse judgement of coherence degrees.	3
Then, during KD fine-tuning, the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data.	3
To advocate the generalizability even with limited fine-tuning data, a novel KD regularization is introduced to retain the knowledge learned at the pre-training stage.	3
Experimental results show that the model trained by QuantiDCE presents stronger correlations with human judgements than the other state-of-the-art metrics.	4
----------
Statutory reasoning is the task of determining whether a legal statute, stated in natural language, applies to the text description of a case.	1
Prior work introduced a resource that approached statutory reasoning as a monolithic textual entailment problem, with neural baselines performing nearly at-chance.	1
To address this challenge, we decompose statutory reasoning into four types of language-understanding challenge problems, through the introduction of concepts and structure found in Prolog programs.	3
Augmenting an existing benchmark, we provide annotations for the four tasks, and baselines for three of them.	2
Models for statutory reasoning are shown to benefit from the additional structure, improving on prior baselines.	4
Further, the decomposition into subtasks facilitates finer-grained model diagnostics and clearer incremental progress.	4
----------
Entity Matching (EM) aims at recognizing entity records that denote the same real-world object.	1
Neural EM models learn vector representation of entity descriptions and match entities end-to-end.	1
Though robust, these methods require many annotated resources for training, and lack of interpretability.	1
In this paper, we propose a novel EM framework that consists of Heterogeneous Information Fusion (HIF) and Key Attribute Tree (KAT) Induction to decouple feature representation from matching decision.	2
Using self-supervised learning and mask mechanism in pre-trained language modeling, HIF learns the embeddings of noisy attribute values by inter-attribute attention with unlabeled data.	3
Using a set of comparison features and a limited amount of annotated data, KAT Induction learns an efficient decision tree that can be interpreted by generating entity matching rules whose structure is advocated by domain experts.	3
Experiments on 6 public datasets and 3 industrial datasets show that our method is highly efficient and outperforms SOTA EM models in most cases.	4
We will release the codes upon acceptance.	6
----------
Named entity recognition (NER) is a well-studied task in natural language processing.	1
Traditional NER research only deals with flat entities and ignores nested entities.	1
The span-based methods treat entity recognition as a span classification task.	1
Although these methods have the innate ability to handle nested NER, they suffer from high computational cost, ignorance of boundary information, under-utilization of the spans that partially match with entities, and difficulties in long entity recognition.	1
To tackle these issues, we propose a two-stage entity identifier.	2
First we generate span proposals by filtering and boundary regression on the seed spans to locate the entities, and then label the boundary-adjusted span proposals with the corresponding categories.	3
Our method effectively utilizes the boundary information of entities and partially matched spans during training.	4
Through boundary regression, entities of any length can be covered theoretically, which improves the ability to recognize long entities.	4
In addition, many low-quality seed spans are filtered out in the first stage, which reduces the time complexity of inference.	4
Experiments on nested NER datasets demonstrate that our proposed method outperforms previous state-of-the-art models.	4
----------
Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event.	1
Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks.	1
In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner.	2
Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning.	3
Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.	4
----------
Disease is one of the fundamental entities in biomedical research.	1
Recognizing such entities from biomedical text and then normalizing them to a standardized disease vocabulary offer a tremendous opportunity for many downstream applications.	1
Previous studies have demonstrated that joint modeling of the two sub-tasks has superior performance than the pipelined counterpart.	1
Although the neural joint model based on multi-task learning framework has achieved state-of-the-art performance, it suffers from the boundary inconsistency problem due to the separate decoding procedures.	1
Moreover, it ignores the rich information (e.g., the text surface form) of each candidate concept in the vocabulary, which is quite essential for entity normalization.	1
In this work, we propose a neural transition-based joint model to alleviate these two issues.	2
We transform the end-to-end disease recognition and normalization task as an action sequence prediction task, which not only jointly learns the model with shared representations of the input, but also jointly searches the output by state transitions in one search space.	3
Moreover, we introduce attention mechanisms to take advantage of the text surface form of each candidate concept for better normalization performance.	2
Experimental results conducted on two publicly available datasets show the effectiveness of the proposed method.	4
----------
Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type.	1
Most current methods to ED rely heavily on training instances, and almost ignore the correlation of event types.	1
Hence, they tend to suffer from data scarcity and fail to handle new unseen event types.	1
To address these problems, we formulate ED as a process of event ontology population: linking event instances to pre-defined event types in event ontology, and propose a novel ED framework entitled OntoED with ontology embedding.	2
We enrich event ontology with linkages among event types, and further induce more event-event correlations.	2
Based on the event ontology, OntoED can leverage and propagate correlation knowledge, particularly from data-rich to data-poor event types.	3
Furthermore, OntoED can be applied to new unseen event types, by establishing linkages to existing ones.	3
Experiments indicate that OntoED is more predominant and robust than previous approaches to ED, especially in data-scarce scenarios.	4
----------
Self-training has proven effective for improving NMT performance by augmenting model training with synthetic parallel data.	1
The common practice is to construct synthetic data based on a randomly sampled subset of large-scale monolingual data, which we empirically show is sub-optimal.	1
In this work, we propose to improve the sampling procedure by selecting the most informative monolingual sentences to complement the parallel data.	2
To this end, we compute the uncertainty of monolingual sentences using the bilingual dictionary extracted from the parallel data.	3
Intuitively, monolingual sentences with lower uncertainty generally correspond to easy-to-translate patterns which may not provide additional gains.	1
Accordingly, we design an uncertainty-based sampling strategy to efficiently exploit the monolingual data for self-training, in which monolingual sentences with higher uncertainty would be sampled with higher probability.	3
Experimental results on large-scale WMT English⇒German and English⇒Chinese datasets demonstrate the effectiveness of the proposed approach.	4
Extensive analyses suggest that emphasizing the learning on uncertain monolingual sentences by our approach does improve the translation quality of high-uncertainty sentences and also benefits the prediction of low-frequency words at the target side.	4
----------
Context-aware neural machine translation (NMT) remains challenging due to the lack of large-scale document-level parallel corpora.	1
To break the corpus bottleneck, in this paper we aim to improve context-aware NMT by taking the advantage of the availability of both large-scale sentence-level parallel dataset and source-side monolingual documents.	2
To this end, we propose two pre-training tasks.	2
One learns to translate a sentence from source language to target language on the sentence-level parallel dataset while the other learns to translate a document from deliberately noised to original on the monolingual documents.	3
Importantly, the two pre-training tasks are jointly and simultaneously learned via the same model, thereafter fine-tuned on scale-limited parallel documents from both sentence-level and document-level perspectives.	3
Experimental results on four translation tasks show that our approach significantly improves translation performance.	4
One nice property of our approach is that the fine-tuned model can be used to translate both sentences and documents.	4
----------
Although teacher forcing has become the main training paradigm for neural machine translation, it usually makes predictions only conditioned on past information, and hence lacks global planning for the future.	1
To address this problem, we introduce another decoder, called seer decoder, into the encoder-decoder framework during training, which involves future information in target predictions.	2
Meanwhile, we force the conventional decoder to simulate the behaviors of the seer decoder via knowledge distillation.	3
In this way, at test the conventional decoder can perform like the seer decoder without the attendance of it.	3
Experiment results on the Chinese-English, English-German and English-Romanian translation tasks show our method can outperform competitive baselines significantly and achieves greater improvements on the bigger data sets.	4
Besides, the experiments also prove knowledge distillation the best way to transfer knowledge from the seer decoder to the conventional decoder compared to adversarial learning and L2 regularization.	4
----------
Unsupervised machine translation, which utilizes unpaired monolingual corpora as training data, has achieved comparable performance against supervised machine translation.	1
However, it still suffers from data-scarce domains.	1
To address this issue, this paper presents a novel meta-learning algorithm for unsupervised neural machine translation (UNMT) that trains the model to adapt to another domain by utilizing only a small amount of training data.	2
We assume that domain-general knowledge is a significant factor in handling data-scarce domains.	1
Hence, we extend the meta-learning algorithm, which utilizes knowledge learned from high-resource domains, to boost the performance of low-resource UNMT.	3
Our model surpasses a transfer learning-based approach by up to 2-3 BLEU scores.	4
Extensive experimental results show that our proposed algorithm is pertinent for fast adaptation and consistently outperforms other baselines.	4
----------
Large-scale models for learning fixed-dimensional cross-lingual sentence representations like LASER (Artetxe and Schwenk, 2019b) lead to significant improvement in performance on downstream tasks.	1
However, further increases and modifications based on such large-scale models are usually impractical due to memory limitations.	1
In this work, we introduce a lightweight dual-transformer architecture with just 2 layers for generating memory-efficient cross-lingual sentence representations.	2
We explore different training tasks and observe that current cross-lingual training tasks leave a lot to be desired for this shallow architecture.	3
To ameliorate this, we propose a novel cross-lingual language model, which combines the existing single-word masked language model with the newly proposed cross-lingual token-level reconstruction task.	2
We further augment the training task by the introduction of two computationally-lite sentence-level contrastive learning tasks to enhance the alignment of cross-lingual sentence representation space, which compensates for the learning bottleneck of the lightweight transformer for generative tasks.	3
Our comparisons with competing models on cross-lingual sentence retrieval and multilingual document classification confirm the effectiveness of the newly proposed training tasks for a shallow model.	4
----------
Transformers are not suited for processing long documents, due to their quadratically increasing memory and time consumption.	1
Simply truncating a long document or applying the sparse attention mechanism will incur the context fragmentation problem or lead to an inferior modeling capability against comparable model sizes.	1
In this paper, we propose ERNIE-Doc, a document-level language pretraining model based on Recurrence Transformers.	2
Two well-designed techniques, namely the retrospective feed mechanism and the enhanced recurrence mechanism, enable ERNIE-Doc, which has a much longer effective context length, to capture the contextual information of a complete document.	3
We pretrain ERNIE-Doc to explicitly learn the relationships among segments with an additional document-aware segment-reordering objective.	3
Various experiments were conducted on both English and Chinese document-level tasks.	3
ERNIE-Doc improved the state-of-the-art language modeling result of perplexity to 16.8 on WikiText-103.	4
Moreover, it outperformed competitive pretraining models by a large margin on most language understanding tasks, such as text classification and question answering.	4
----------
Recently, knowledge distillation (KD) has shown great success in BERT compression.	1
Instead of only learning from the teacher’s soft label as in conventional KD, researchers find that the rich information contained in the hidden layers of BERT is conducive to the student’s performance.	1
To better exploit the hidden knowledge, a common practice is to force the student to deeply mimic the teacher’s hidden states of all the tokens in a layer-wise manner.	1
In this paper, however, we observe that although distilling the teacher’s hidden state knowledge (HSK) is helpful, the performance gain (marginal utility) diminishes quickly as more HSK is distilled.	2
To understand this effect, we conduct a series of analysis.	3
Specifically, we divide the HSK of BERT into three dimensions, namely depth, length and width.	3
We first investigate a variety of strategies to extract crucial knowledge for each single dimension and then jointly compress the three dimensions.	3
In this way, we show that 1) the student’s performance can be improved by extracting and distilling the crucial HSK, and 2) using a tiny fraction of HSK can achieve the same performance as extensive HSK distillation.	4
Based on the second finding, we further propose an efficient KD paradigm to compress BERT, which does not require loading the teacher during the training of student.	2
For two kinds of student models and computing devices, the proposed KD paradigm gives rise to training speedup of 2.7x 3.4x.	4
----------
Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks.	1
However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks.	1
In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models.	2
In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales.	3
When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions.	2
In the experiment, we tested Rational LAMOL on permutations of three datasets from the ERASER benchmark.	3
The results show that our proposed framework outperformed vanilla LAMOL on most permutations.	4
Furthermore, unsupervised rationale generation was able to consistently improve the overall LL performance from the baseline without relying on human-annotated rationales.	4
----------
Natural language processing (NLP) often faces the problem of data diversity such as different domains, themes, styles, and so on.	1
Therefore, a single language model (LM) is insufficient to learn all knowledge from diverse samples.	1
To solve this problem, we firstly propose an autoencoding topic model with a mixture prior (mATM) to perform clustering for the data, where the clusters defined in semantic space describes the data diversity.	1
Having obtained the clustering assignment for each sample, we develop the ensemble LM (EnsLM) with the technique of weight modulation.	2
Specifically, EnsLM contains a backbone that is adjusted by a few modulated weights to fit for different sample clusters.	3
As a result, the backbone learns the shared knowledge among all clusters while modulated weights extract the cluster-specific features.	3
EnsLM can be trained jointly with mATM with a flexible LM backbone.	3
We evaluate the effectiveness of both mATM and EnsLM on various tasks.	3
----------
Pre-trained language models like BERT are performant in a wide range of natural language tasks.	1
However, they are resource exhaustive and computationally expensive for industrial scenarios.	1
Thus, early exits are adopted at each layer of BERT to perform adaptive computation by predicting easier samples with the first few layers to speed up the inference.	1
In this work, to improve efficiency without performance drop, we propose a novel training scheme called Learned Early Exit for BERT (LeeBERT).	2
First, we ask each exit to learn from each other, rather than learning only from the last layer.	3
Second, the weights of different loss terms are learned, thus balancing off different objectives.	3
We formulate the optimization of LeeBERT as a bi-level optimization problem, and we propose a novel cross-level optimization (CLO) algorithm to improve the optimization results.	2
Experiments on the GLUE benchmark show that our proposed methods improve the performance of the state-of-the-art (SOTA) early exit methods for pre-trained models.	4
----------
We pioneer the first extractive summarization-based collaborative filtering model called ESCOFILT.	2
Our proposed model specifically produces extractive summaries for each item and user.	2
Unlike other types of explanations, summary-level explanations closely resemble real-life explanations.	1
The strength of ESCOFILT lies in the fact that it unifies representation and explanation.	4
In other words, extractive summaries both represent and explain the items and users.	4
Our model uniquely integrates BERT, K-Means embedding clustering, and multilayer perceptron to learn sentence embeddings, representation-explanations, and user-item interactions, respectively.	3
We argue that our approach enhances both rating prediction accuracy and user/item explainability.	4
Our experiments illustrate that ESCOFILT’s prediction accuracy is better than the other state-of-the-art recommender models.	4
Furthermore, we propose a comprehensive set of criteria that assesses the real-life explainability of explanations.	2
Our explainability study demonstrates the superiority of and preference for summary-level explanations over other explanation types.	4
----------
Chinese spelling correction (CSC) is a task to detect and correct spelling errors in texts.	1
CSC is essentially a linguistic problem, thus the ability of language understanding is crucial to this task.	1
In this paper, we propose a Pre-trained masked Language model with Misspelled knowledgE (PLOME) for CSC, which jointly learns how to understand language and correct spelling errors.	2
To this end, PLOME masks the chosen tokens with similar characters according to a confusion set rather than the fixed token “[MASK]” as in BERT.	3
Besides character prediction, PLOME also introduces pronunciation prediction to learn the misspelled knowledge on phonic level.	3
Moreover, phonological and visual similarity knowledge is important to this task.	3
PLOME utilizes GRU networks to model such knowledge based on characters’ phonics and strokes.	3
Experiments are conducted on widely used benchmarks.	3
Our method achieves superior performance against state-of-the-art approaches by a remarkable margin.	4
We release the source code and pre-trained model for further use by the community (https://github.com/liushulinle/PLOME).	6
----------
Medical report generation task, which targets to produce long and coherent descriptions of medical images, has attracted growing research interests recently.	1
Different from the general image captioning tasks, medical report generation is more challenging for data-driven neural models.	1
This is mainly due to 1) the serious data bias and 2) the limited medical data.	1
To alleviate the data bias and make best use of available data, we propose a Competence-based Multimodal Curriculum Learning framework (CMCL).	2
Specifically, CMCL simulates the learning process of radiologists and optimizes the model in a step by step manner.	3
Firstly, CMCL estimates the difficulty of each training instance and evaluates the competence of current model; 	3
Secondly, CMCL selects the most suitable batch of training instances considering current model competence.	3
By iterating above two steps, CMCL can gradually improve the model’s performance.	3
The experiments on the public IU-Xray and MIMIC-CXR datasets show that CMCL can be incorporated into existing models to improve their performance.	4
----------
Deep learning models for automatic readability assessment generally discard linguistic features traditionally used in machine learning models for the task.	1
We propose to incorporate linguistic features into neural network models by learning syntactic dense embeddings based on linguistic features.	2
To cope with the relationships between the features, we form a correlation graph among features and use it to learn their embeddings so that similar features will be represented by similar embeddings.	3
Experiments with six data sets of two proficiency levels demonstrate that our proposed methodology can complement BERT-only model to achieve significantly better performances for automatic readability assessment.	4
----------
Pre-trained language models have been applied to various NLP tasks with considerable performance gains.	1
However, the large model sizes, together with the long inference time, limit the deployment of such models in real-time applications.	1
One line of model compression approaches considers knowledge distillation to distill large teacher models into small student models.	1
Most of these studies focus on single-domain only, which ignores the transferable knowledge from other domains.	1
We notice that training a teacher with transferable knowledge digested across domains can achieve better generalization capability to help knowledge distillation.	1
Hence we propose a Meta-Knowledge Distillation (Meta-KD) framework to build a meta-teacher model that captures transferable knowledge across domains and passes such knowledge to students.	2
Specifically, we explicitly force the meta-teacher to capture transferable knowledge at both instance-level and feature-level from multiple domains, and then propose a meta-distillation algorithm to learn single-domain student models with guidance from the meta-teacher.	2
Experiments on public multi-domain NLP tasks show the effectiveness and superiority of the proposed Meta-KD framework.	4
Further, we also demonstrate the capability of Meta-KD in the settings where the training data is scarce.	4
----------
Unsupervised commonsense question answering is appealing since it does not rely on any labeled task data.	1
Among existing work, a popular solution is to use pre-trained language models to score candidate choices directly conditioned on the question or context.	1
However, such scores from language models can be easily affected by irrelevant factors, such as word frequencies, sentence structures, etc.	1
These distracting factors may not only mislead the model to choose a wrong answer but also make it oversensitive to lexical perturbations in candidate answers.	1
In this paper, we present a novel SEmantic-based Question Answering method (SEQA) for unsupervised commonsense question answering.	2
Instead of directly scoring each answer choice, our method first generates a set of plausible answers with generative models (e.g., GPT-2), and then uses these plausible answers to select the correct choice by considering the semantic similarity between each plausible answer and each choice.	3
We devise a simple, yet sound formalism for this idea and verify its effectiveness and robustness with extensive experiments.	3
We evaluate the proposed method on four benchmark datasets, and our method achieves the best results in unsupervised settings.	4
Moreover, when attacked by TextFooler with synonym replacement, SEQA demonstrates much less performance drops than baselines, thereby indicating stronger robustness.	4
----------
In several question answering benchmarks, pretrained models have reached human parity through fine-tuning on an order of 100,000 annotated questions and answers.	1
We explore the more realistic few-shot setting, where only a few hundred training examples are available, and observe that standard models perform poorly, highlighting the discrepancy between current pretraining objectives and question answering.	1
We propose a new pretraining scheme tailored for question answering: recurring span selection.	2
Given a passage with multiple sets of recurring spans, we mask in each set all recurring spans but one, and ask the model to select the correct span in the passage for each masked span.	3
Masked spans are replaced with a special token, viewed as a question representation, that is later used during fine-tuning to select the answer span.	3
The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.	4
----------
To date, most of recent work under the retrieval-reader framework for open-domain QA focuses on either extractive or generative reader exclusively.	1
In this paper, we study a hybrid approach for leveraging the strengths of both models.	2
We apply novel techniques to enhance both extractive and generative readers built upon recent pretrained neural language models, and find that proper training methods can provide large improvement over previous state-of-the-art models.	3
We demonstrate that a simple hybrid approach by combining answers from both readers can efficiently take advantages of extractive and generative answer inference strategies and outperforms single models as well as homogeneous ensembles.	4
Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively.	4
----------
Neural models have shown impressive performance gains in answering queries from natural language text.	1
However, existing works are unable to support database queries, such as “List/Count all female athletes who were born in 20th century”, which require reasoning over sets of relevant facts with operations such as join, filtering and aggregation.	1
We show that while state-of-the-art transformer models perform very well for small databases, they exhibit limitations in processing noisy data, numerical operations, and queries that aggregate facts.	1
We propose a modular architecture to answer these database-style queries over multiple spans from text and aggregating these at scale.	2
We evaluate the architecture using WikiNLDB, a novel dataset for exploring such queries.	3
Our architecture scales to databases containing thousands of facts whereas contemporary models are limited by how many facts can be encoded.	3
In direct comparison on small databases, our approach increases overall answer accuracy from 85% to 90%.	4
On larger databases, our approach retains its accuracy whereas transformer baselines could not encode the context.	4
----------
In Machine Translation, assessing the quality of a large amount of automatic translations can be challenging.	1
Automatic metrics are not reliable when it comes to high performing systems.	1
In addition, resorting to human evaluators can be expensive, especially when evaluating multiple systems.	1
To overcome the latter challenge, we propose a novel application of online learning that, given an ensemble of Machine Translation systems, dynamically converges to the best systems, by taking advantage of the human feedback available.	2
Our experiments on WMT’19 datasets show that our online approach quickly converges to the top-3 ranked systems for the language pairs considered, despite the lack of human feedback for many translations.	4
----------
In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance.	2
We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks.	3
We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference.	3
To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers.	3
We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance.	3
Our results show that languages that are adequately represented in the multilingual model’s vocabulary exhibit negligible performance decreases over their monolingual counterparts.	4
We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.	4
----------
Cross-lingual transfer has improved greatly through multi-lingual language model pretraining, reducing the need for parallel data and increasing absolute performance.	1
However, this progress has also brought to light the differences in performance across languages.	1
Specifically, certain language families and typologies seem to consistently perform worse in these models.	1
In this paper, we address what effects morphological typology has on zero-shot cross-lingual transfer for two tasks: Part-of-speech tagging and sentiment analysis.	2
We perform experiments on 19 languages from four language typologies (fusional, isolating, agglutinative, and introflexive) and find that transfer to another morphological type generally implies a higher loss than transfer to another language with the same morphological typology.	3+4
Furthermore, POS tagging is more sensitive to morphological typology than sentiment analysis and, on this task, models perform much better on fusional languages than on the other typologies.	4
----------
Generating code-switched text is a problem of growing interest, especially given the scarcity of corpora containing large volumes of real code-switched text.	1
In this work, we adapt a state-of-the-art neural machine translation model to generate Hindi-English code-switched sentences starting from monolingual Hindi sentences.	2
We outline a carefully designed curriculum of pretraining steps, including the use of synthetic code-switched text, that enable the model to generate high-quality code-switched text.	3
Using text generated from our model as data augmentation, we show significant reductions in perplexity on a language modeling task, compared to using text from other generative models of CS text.	3
We also show improvements using our text for a downstream code-switched natural language inference task.	3
Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics, where we show performance comparable (and sometimes even superior) to code-switched text obtained via crowd workers who are native Hindi speakers.	4
----------
It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks.	1
Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead.	1
In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion.	2
Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time.	4
In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh->En and En->De).	4
----------
Online misogyny, a category of online abusive language, has serious and harmful social consequences.	1
Automatic detection of misogynistic language online, while imperative, poses complicated challenges to both data gathering, data annotation, and bias mitigation, as this type of data is linguistically complex and diverse.	1
This paper makes three contributions in this area: Firstly, we describe the detailed design of our iterative annotation process and codebook.	2
Secondly, we present a comprehensive taxonomy of labels for annotating misogyny in natural written language, and finally, we introduce a high-quality dataset of annotated posts sampled from social media posts.	2
----------
Undermining the impact of hateful content with informed and non-aggressive responses, called counter narratives, has emerged as a possible solution for having healthier online communities.	1
Thus, some NLP studies have started addressing the task of counter narrative generation.	1
Although such studies have made an effort to build hate speech / counter narrative (HS/CN) datasets for neural generation, they fall short in reaching either high-quality and/or high-quantity.	1
In this paper, we propose a novel human-in-the-loop data collection methodology in which a generative language model is refined iteratively by using its own data from the previous loops to generate new training samples that experts review and/or post-edit.	2
Our experiments comprised several loops including diverse dynamic variations.	3
Results show that the methodology is scalable and facilitates diverse, novel, and cost-effective data collection.	4
To our knowledge, the resulting dataset is the only expert-based multi-target HS/CN dataset available to the community.	4
----------
Recent work has investigated the interesting question using pre-trained language models (PLMs) as knowledge bases for answering open questions.	1
However, existing work is limited in using small benchmarks with high test-train overlaps.	1
We construct a new dataset of closed-book QA using SQuAD, and investigate the performance of BART.	3
Experiments show that it is challenging for BART to remember training facts in high precision, and also challenging to answer closed-book questions even if relevant knowledge is retained.	4
Some promising directions are found, including decoupling the knowledge memorizing process and the QA finetune process, forcing the model to recall relevant knowledge when question answering.	4
----------
This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection (AS2) modules, which are core components of retrieval-based Question Answering (QA) systems.	2
Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers.	4
For this purpose, we build a three-way multi-classifier, which decides if an answer supports, refutes, or is neutral with respect to another one.	3
More specifically, our neural architecture integrates a state-of-the-art AS2 module with the multi-classifier, and a joint layer connecting all components.	3
We tested our models on WikiQA, TREC-QA, and a real-world dataset.	3
The results show that our models obtain the new state of the art in AS2.	4
----------
In open-domain question answering, questions are highly likely to be ambiguous because users may not know the scope of relevant topics when formulating them.	1
Therefore, a system needs to find possible interpretations of the question, and predict one or multiple plausible answers.	1
When multiple plausible answers are found, the system should rewrite the question for each answer to resolve the ambiguity.	1
In this paper, we present a model that aggregates and combines evidence from multiple passages to adaptively predict a single answer or a set of question-answer pairs for ambiguous questions.	2
In addition, we propose a novel round-trip prediction approach to iteratively generate additional interpretations that our model fails to find in the first pass, and then verify and filter out the incorrect question-answer pairs to arrive at the final disambiguated output.	2
Our model, named Refuel, achieves a new state-of-the-art performance on the AmbigQA dataset, and shows competitive performance on NQ-Open and TriviaQA.	4
The proposed round-trip prediction is a model-agnostic general approach for answering ambiguous open-domain questions, which improves our Refuel as well as several baseline models.	4
We release source code for our models and experiments at https://github.com/amzn/refuel-open-domain-qa.	6
----------
Conversational KBQA is about answering a sequence of questions related to a KB.	1
Follow-up questions in conversational KBQA often have missing information referring to entities from the conversation history.	1
In this paper, we propose to model these implied entities, which we refer to as the focal entities of the conversation.	2
We propose a novel graph-based model to capture the transitions of focal entities and apply a graph neural network to derive a probability distribution of focal entities for each question, which is then combined with a standard KBQA module to perform answer ranking.	2+3
Our experiments on two datasets demonstrate the effectiveness of our proposed method.	4
----------
This paper introduces the task of factual error correction: performing edits to a claim so that the generated rewrite is better supported by evidence.	2
This extends the well-studied task of fact verification by providing a mechanism to correct written texts that are refuted or only partially supported by evidence.	1
We demonstrate that it is feasible to train factual error correction systems from existing fact checking datasets which only contain labeled claims accompanied by evidence, but not the correction.	4
We achieve this by employing a two-stage distant supervision approach that incorporates evidence into masked claims when generating corrections.	3
Our approach, based on the T5 transformer and using retrieved evidence, achieved better results than existing work which used a pointer copy network and gold evidence, producing accurate factual error corrections for 5x more instances in human evaluation and a .125 increase in SARI score.	3+4
The evaluation is conducted on a dataset of 65,000 instances based on a recent fact verification shared task and we release it to enable further work on the task.	4
----------
We present algorithms for aligning components of Abstract Meaning Representation (AMR) graphs to spans in English sentences.	2
We leverage unsupervised learning in combination with heuristics, taking the best of both worlds from previous AMR aligners.	3
Our unsupervised models, however, are more sensitive to graph substructures, without requiring a separate syntactic parse.	3
Our approach covers a wider variety of AMR substructures than previously considered, achieves higher coverage of nodes and edges, and does so with higher accuracy.	4
We will release our LEAMR datasets and aligner for use in research on AMR parsing, generation, and evaluation.	4
----------
Natural language is compositional; the meaning of a sentence is a function of the meaning of its parts.	1
This property allows humans to create and interpret novel sentences, generalizing robustly outside their prior experience.	1
Neural networks have been shown to struggle with this kind of generalization, in particular performing poorly on tasks designed to assess compositional generalization (i.e. where training and testing distributions differ in ways that would be trivial for a compositional strategy to resolve).	1
Their poor performance on these tasks may in part be due to the nature of supervised learning which assumes training and testing data to be drawn from the same distribution.	1
We implement a meta-learning augmented version of supervised learning whose objective directly optimizes for out-of-distribution generalization.	2
We construct pairs of tasks for meta-learning by sub-sampling existing training data.	3
Each pair of tasks is constructed to contain relevant examples, as determined by a similarity metric, in an effort to inhibit models from memorizing their input.	3
Experimental results on the COGS and SCAN datasets show that our similarity-driven meta-learning can improve generalization performance.	4
----------
Large pre-trained models such as BERT are known to improve different downstream NLP tasks, even when such a model is trained on a generic domain.	1
Moreover, recent studies have shown that when large domain-specific corpora are available, continued pre-training on domain-specific data can further improve the performance of in-domain tasks.	1
However, this practice requires significant domain-specific data and computational resources which may not always be available.	1
In this paper, we aim to adapt a generic pretrained model with a relatively small amount of domain-specific data.	2
We demonstrate that by explicitly incorporating multi-granularity information of unseen and domain-specific words via the adaptation of (word based) n-grams, the performance of a generic pretrained model can be greatly improved.	4
Specifically, we introduce a Transformer-based Domain-aware N-gram Adaptor, T-DNA, to effectively learn and incorporate the semantic representation of different combinations of words in the new domain.	2
Experimental results illustrate the effectiveness of T-DNA on eight low-resource downstream tasks from four domains.	3
We show that T-DNA is able to achieve significant improvements compared to existing methods on most tasks using limited data with lower computational costs.	4
Moreover, further analyses demonstrate the importance and effectiveness of both unseen words and the information of different granularities.	4
Our code is available at https://github.com/shizhediao/T-DNA.	6
----------
Pre-trained Language Models (PLMs) have shown superior performance on various downstream Natural Language Processing (NLP) tasks.	1
However, conventional pre-training objectives do not explicitly model relational facts in text, which are crucial for textual understanding.	1
To address this issue, we propose a novel contrastive learning framework ERICA to obtain a deep understanding of the entities and their relations in text.	2
Specifically, we define two novel pre-training tasks to better understand entities and relations: (1) the entity discrimination task to distinguish which tail entity can be inferred by the given head entity and relation; (2) the relation discrimination task to distinguish whether two relations are close or not semantically, which involves complex relational reasoning.	2
Experimental results demonstrate that ERICA can improve typical PLMs (BERT and RoBERTa) on several language understanding tasks, including relation extraction, entity typing and question answering, especially under low-resource settings.	4
----------
The Emotion Cause Extraction (ECE) task aims to identify clauses which contain emotion-evoking information for a particular emotion expressed in text.	1
We observe that a widely-used ECE dataset exhibits a bias that the majority of annotated cause clauses are either directly before their associated emotion clauses or are the emotion clauses themselves.	1
Existing models for ECE tend to explore such relative position information and suffer from the dataset bias.	1
To investigate the degree of reliance of existing ECE models on clause relative positions, we propose a novel strategy to generate adversarial examples in which the relative position information is no longer the indicative feature of cause clauses.	2
We test the performance of existing models on such adversarial examples and observe a significant performance drop.	3
To address the dataset bias, we propose a novel graph-based method to explicitly model the emotion triggering paths by leveraging the commonsense knowledge to enhance the semantic dependencies between a candidate clause and an emotion clause.	2
Experimental results show that our proposed approach performs on par with the existing state-of-the-art methods on the original ECE dataset, and is more robust against adversarial attacks compared to existing models.	4
----------
Previous work on review summarization focused on measuring the sentiment toward the main aspects of the reviewed product or business, or on creating a textual summary.	1
These approaches provide only a partial view of the data: aspect-based sentiment summaries lack sufficient explanation or justification for the aspect rating, while textual summaries do not quantify the significance of each element, and are not well-suited for representing conflicting views.	1
Recently, Key Point Analysis (KPA) has been proposed as a summarization framework that provides both textual and quantitative summary of the main points in the data.	1
We adapt KPA to review data by introducing Collective Key Point Mining for better key point extraction; integrating sentiment analysis into KPA; identifying good key point candidates for review summaries; and leveraging the massive amount of available reviews and their metadata.	2+3
We show empirically that these novel extensions of KPA substantially improve its performance.	3
We demonstrate that promising results can be achieved without any domain-specific annotation, while human supervision can lead to further improvement.	4
----------
Structured sentiment analysis attempts to extract full opinion tuples from a text, but over time this task has been subdivided into smaller and smaller sub-tasks, e.g., target extraction or targeted polarity classification.	1
We argue that this division has become counterproductive and propose a new unified framework to remedy the situation.	2
We cast the structured sentiment problem as dependency graph parsing, where the nodes are spans of sentiment holders, targets and expressions, and the arcs are the relations between them.	3
We perform experiments on five datasets in four languages (English, Norwegian, Basque, and Catalan) and show that this approach leads to strong improvements over state-of-the-art baselines.	4
Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results.	4
----------
Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others.	1
In this work, we propose to improve cross-lingual fine-tuning with consistency regularization.	2
Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation.	3
In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set.	3
Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.	4
----------
Knowledge distillation (KD) is commonly used to construct synthetic data for training non-autoregressive translation (NAT) models.	1
However, there exists a discrepancy on low-frequency words between the distilled and the original data, leading to more errors on predicting low-frequency words.	1
To alleviate the problem, we directly expose the raw data into NAT by leveraging pretraining.	1
By analyzing directed alignments, we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source.	1
Accordingly, we propose reverse KD to rejuvenate more alignments for low-frequency target words.	2
To make the most of authentic and synthetic data, we combine these complementary approaches as a new training strategy for further boosting NAT performance.	3
We conduct experiments on five translation benchmarks over two advanced architectures.	3
Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words.	4
Encouragingly, our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English-German and WMT16 Romanian-English datasets, respectively.	4
Our code, data, and trained models are available at https://github.com/longyuewangdcu/RLFW-NAT.	6
----------
Document-level MT models are still far from satisfactory.	1
Existing work extend translation unit from single sentence to multiple sentences.	1
However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail.	1
In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training.	2
Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure.	2
As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source.	2+3
Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.	4
----------
The Neural Machine Translation (NMT) model is essentially a joint language model conditioned on both the source sentence and partial translation.	1
Therefore, the NMT model naturally involves the mechanism of the Language Model (LM) that predicts the next token only based on partial translation.	1
Despite its success, NMT still suffers from the hallucination problem, generating fluent but inadequate translations.	1
The main reason is that NMT pays excessive attention to the partial translation while neglecting the source sentence to some extent, namely overconfidence of the LM.	1
Accordingly, we define the Margin between the NMT and the LM, calculated by subtracting the predicted probability of the LM from that of the NMT model for each token.	1
The Margin is negatively correlated to the overconfidence degree of the LM.	1
Based on the property, we propose a Margin-based Token-level Objective (MTO) and a Margin-based Sentence-level Objective (MSO) to maximize the Margin for preventing the LM from being overconfident.	2
Experiments on WMT14 English-to-German, WMT19 Chinese-to-English, and WMT14 English-to-French translation tasks demonstrate the effectiveness of our approach, with 1.36, 1.50, and 0.63 BLEU improvements, respectively, compared to the Transformer baseline.	4
The human evaluation further verifies that our approaches improve translation adequacy as well as fluency.	4
----------
Emotional support is a crucial ability for many conversation scenarios, including social interactions, mental health support, and customer service chats.	1
Following reasonable procedures and using various support skills can help to effectively provide support.	1
However, due to the lack of a well-designed task and corpora of effective emotional support conversations, research on building emotional support into dialog systems remains lacking.	1
In this paper, we define the Emotional Support Conversation (ESC) task and propose an ESC Framework, which is grounded on the Helping Skills Theory.	2
We construct an Emotion Support Conversation dataset (ESConv) with rich annotation (especially support strategy) in a help-seeker and supporter mode.	3
To ensure a corpus of high-quality conversations that provide examples of effective emotional support, we take extensive effort to design training tutorials for supporters and several mechanisms for quality control during data collection.	3
Finally, we evaluate state-of-the-art dialog models with respect to the ability to provide emotional support.	3
Our results show the importance of support strategies in providing effective emotional support and the utility of ESConv in training more emotional support systems.	4
----------
Generating some appealing questions in open-domain conversations is an effective way to improve human-machine interactions and lead the topic to a broader or deeper direction.	1
To avoid dull or deviated questions, some researchers tried to utilize answer, the “future” information, to guide question generation.	1
However, they separate a post-question-answer (PQA) triple into two parts: post-question (PQ) and question-answer (QA) pairs, which may hurt the overall coherence.	1
Besides, the QA relationship is modeled as a one-to-one mapping that is not reasonable in open-domain conversations.	1
To tackle these problems, we propose a generative triple-wise model with hierarchical variations for open-domain conversational question generation (CQG).	2
Latent variables in three hierarchies are used to represent the shared background of a triple and one-to-many semantic mappings in both PQ and QA pairs.	3
Experimental results on a large-scale CQG dataset show that our method significantly improves the quality of questions in terms of fluency, coherence and diversity over competitive baselines.	4
----------
Neural dialogue generation models trained with the one-hot target distribution suffer from the over-confidence issue, which leads to poor generation diversity as widely reported in the literature.	1
Although existing approaches such as label smoothing can alleviate this issue, they fail to adapt to diverse dialog contexts.	1
In this paper, we propose an Adaptive Label Smoothing (AdaLabel) approach that can adaptively estimate a target label distribution at each time step for different contexts.	2
The maximum probability in the predicted distribution is used to modify the soft target distribution produced by a novel light-weight bi-directional decoder module.	3
The resulting target distribution is aware of both previous and future contexts and is adjusted to avoid over-training the dialogue model.	3
Our model can be trained in an endto-end manner.	3
Extensive experiments on two benchmark datasets show that our approach outperforms various competitive baselines in producing diverse responses.	4
----------
Out-of-scope intent detection is of practical importance in task-oriented dialogue systems.	1
Since the distribution of outlier utterances is arbitrary and unknown in the training stage, existing methods commonly rely on strong assumptions on data distribution such as mixture of Gaussians to make inference, resulting in either complex multi-step training procedures or hand-crafted rules such as confidence threshold selection for outlier detection.	1
In this paper, we propose a simple yet effective method to train an out-of-scope intent classifier in a fully end-to-end manner by simulating the test scenario in training, which requires no assumption on data distribution and no additional post-processing or threshold setting.	2
Specifically, we construct a set of pseudo outliers in the training stage, by generating synthetic outliers using inliner features via self-supervision and sampling out-of-scope sentences from easily available open-domain datasets.	3
The pseudo outliers are used to train a discriminative classifier that can be directly applied to and generalize well on the test task.	3
We evaluate our method extensively on four benchmark dialogue datasets and observe significant improvements over state-of-the-art approaches.	4
Our code has been released at https://github.com/liam0949/DCLOOS.	6
----------
Document-level event extraction aims to recognize event information from a whole piece of article.	1
Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model.	1
In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges.	2
For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions.	3
For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events.	3
Experiments on a large-scale dataset (Zheng et al, 2019) show GIT outperforms the previous methods by 2.8 F1.	4
Further analysis reveals is effective in extracting multiple correlated events and event arguments that scatter across the document.	4
----------
This paper presents a novel method for nested named entity recognition.	2
As a layered method, our method extends the prior second-best path recognition method by explicitly excluding the influence of the best path.	3
Our method maintains a set of hidden states at each time step and selectively leverages them to build a different potential function for recognition at each level.	3
In addition, we demonstrate that recognizing innermost entities first results in better performance than the conventional outermost entities first scheme.	4
We provide extensive experimental results on ACE2004, ACE2005, and GENIA datasets to show the effectiveness and efficiency of our proposed method.	4
----------
Modern models for event causality identification (ECI) are mainly based on supervised learning, which are prone to the data lacking problem.	1
Unfortunately, the existing NLP-related augmentation methods cannot directly produce available data required for this task.	1
To solve the data lacking problem, we introduce a new approach to augment training data for event causality identification, by iteratively generating new examples and classifying event causality in a dual learning framework.	2
On the one hand, our approach is knowledge guided, which can leverage existing knowledge bases to generate well-formed new sentences.	3
On the other hand, our approach employs a dual mechanism, which is a learnable augmentation framework, and can interactively adjust the generation process to generate task-related sentences.	3
Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show that 1) our method can augment suitable task-related training data for ECI; 2) our method outperforms previous methods on EventStoryLine and Causal-TimeBank (+2.5 and +2.1 points on F1 value respectively).	4
----------
Distantly supervision automatically generates plenty of training samples for relation extraction.	1
However, it also incurs two major problems: noisy labels and imbalanced training data.	1
Previous works focus more on reducing wrongly labeled relations (false positives) while few explore the missing relations that are caused by incompleteness of knowledge base (false negatives).	1
Furthermore, the quantity of negative labels overwhelmingly surpasses the positive ones in previous problem formulations.	1
In this paper, we first provide a thorough analysis of the above challenges caused by negative data.	2
Next, we formulate the problem of relation extraction into as a positive unlabeled learning task to alleviate false negative problem.	2
Thirdly, we propose a pipeline approach, dubbed ReRe, that first performs sentence classification with relational labels and then extracts the subjects/objects.	2
Experimental results show that the proposed method consistently outperforms existing approaches and remains excellent performance even learned with a large quantity of false positive samples.	4
Source code is available online at https://github.com/redreamality/RERE-relation-extraction.	6
----------
This paper studies a new problem setting of entity alignment for knowledge graphs (KGs).	2
Since KGs possess different sets of entities, there could be entities that cannot find alignment across them, leading to the problem of dangling entities.	1
As the first attempt to this problem, we construct a new dataset and design a multi-task learning framework for both entity alignment and dangling entity detection.	3
The framework can opt to abstain from predicting alignment for the detected dangling entities.	3
We propose three techniques for dangling entity detection that are based on the distribution of nearest-neighbor distances, i.e., nearest neighbor classification, marginal ranking and background ranking.	3
After detecting and removing dangling entities, an incorporated entity alignment model in our framework can provide more robust alignment for remaining entities.	3
Comprehensive experiments and analyses demonstrate the effectiveness of our framework.	4
We further discover that the dangling entity detection module can, in turn, improve alignment learning and the final performance.	4
The contributed resource is publicly available to foster further research.	6
----------
How does the input segmentation of pretrained language models (PLMs) affect their interpretations of complex words?	1
We present the first study investigating this question, taking BERT as the example PLM and focusing on its semantic representations of English derivatives.	2
We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words.	4
This hypothesis is confirmed by a series of semantic probing tasks on which DelBERT (Derivation leveraging BERT), a model with derivational input segmentation, substantially outperforms BERT with WordPiece segmentation.	3
Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.	4
----------
Analogies play a central role in human commonsense reasoning.	1
The ability to recognize analogies such as “eye is to seeing what ear is to hearing”, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language.	1
Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era.	1
In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets.	2
We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters.	4
Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models.	4
Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.	4
----------
This paper presents a multilingual study of word meaning representations in context.	2
We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy.	2
To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses.	4
A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context.	3
However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences.	3
Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.	3+4
----------
We propose to measure fine-grained domain relevance– the degree that a term is relevant to a broad (e.g., computer science) or narrow (e.g., deep learning) domain.	2
Such measurement is crucial for many downstream tasks in natural language processing.	1
To handle long-tail terms, we build a core-anchored semantic graph, which uses core terms with rich description information to bridge the vast remaining fringe terms semantically.	3
To support a fine-grained domain without relying on a matching corpus for supervision, we develop hierarchical core-fringe learning, which learns core and fringe terms jointly in a semi-supervised manner contextualized in the hierarchy of the domain.	3
To reduce expensive human efforts, we employ automatic annotation and hierarchical positive-unlabeled learning.	3
Our approach applies to big or small domains, covers head or tail terms, and requires little human effort.	3
Extensive experiments demonstrate that our methods outperform strong baselines and even surpass professional human performance.	4
----------
Open-domain dialog systems have a user-centric goal: to provide humans with an engaging conversation experience.	1
User engagement is one of the most important metrics for evaluating open-domain dialog systems, and could also be used as real-time feedback to benefit dialog policy learning.	1
Existing work on detecting user disengagement typically requires hand-labeling many dialog samples.	1
We propose HERALD, an efficient annotation framework that reframes the training data annotation process as a denoising problem.	2
Specifically, instead of manually labeling training samples, we first use a set of labeling heuristics to label training samples automatically.	3
We then denoise the weakly labeled data using the Shapley algorithm.	3
Finally, we use the denoised data to train a user engagement detector.	3
Our experiments show that HERALD improves annotation efficiency significantly and achieves 86% user disengagement detection accuracy in two dialog corpora.	4
----------
Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses.	1
Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed.	1
We propose a model that abstracts over values to focus prediction on type- and function-level context.	2
This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency.	3
Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime.	3
Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3% and 10.6% respectively in terms of absolute accuracy.	4
Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4% and 6.4%.	4
These results indicate that simple representations are key to effective generalization in conversational semantic parsing.	4
----------
Recently, various neural models for multi-party conversation (MPC) have achieved impressive improvements on a variety of tasks such as addressee recognition, speaker identification and response prediction.	1
However, these existing methods on MPC usually represent interlocutors and utterances individually and ignore the inherent complicated structure in MPC which may provide crucial interlocutor and utterance semantics and would enhance the conversation understanding process.	1
To this end, we present MPC-BERT, a pre-trained model for MPC understanding that considers learning who says what to whom in a unified model with several elaborated self-supervised tasks.	2
Particularly, these tasks can be generally categorized into (1) interlocutor structure modeling including reply-to utterance recognition, identical speaker searching and pointer consistency distinction, and (2) utterance semantics modeling including masked shared utterance restoration and shared node detection.	3
We evaluate MPC-BERT on three downstream tasks including addressee recognition, speaker identification and response selection.	3
Experimental results show that MPC-BERT outperforms previous methods by large margins and achieves new state-of-the-art performance on all three downstream tasks at two benchmarks.	4
----------
While Transformer-based text classifiers pre-trained on large volumes of text have yielded significant improvements on a wide range of computational linguistics tasks, their implementations have been unsuitable for live incremental processing thus far, operating only on the level of complete sentence inputs.	1
We address the challenge of introducing methods for word-by-word left-to-right incremental processing to Transformers such as BERT, models without an intrinsic sense of linear order.	2
We modify the training method and live decoding of non-incremental models to detect speech disfluencies with minimum latency and without pre-segmentation of dialogue acts.	3
We experiment with several decoding methods to predict the rightward context of the word currently being processed using a GPT-2 language model and apply a BERT-based disfluency detector to sequences, including predicted words.	3
We show our method of incrementalising Transformers maintains most of their high non-incremental performance while operating strictly incrementally.	3
We also evaluate our models’ incremental performance to establish the trade-off between incremental performance and final performance, using different prediction strategies.	3
We apply our system to incremental speech recognition results as they arrive into a live system and achieve state-of-the-art results in this setting.	4
----------
We propose NeuralWOZ, a novel dialogue collection framework that uses model-based dialogue simulation.	2
NeuralWOZ has two pipelined models, Collector and Labeler.	3
Collector generates dialogues from (1) user’s goal instructions, which are the user context and task constraints in natural language, and (2) system’s API call results, which is a list of possible query responses for user requests from the given knowledge base.	3
Labeler annotates the generated dialogue by formulating the annotation as a multiple-choice problem, in which the candidate labels are extracted from goal instructions and API call results.	3
We demonstrate the effectiveness of the proposed method in the zero-shot domain transfer learning for dialogue state tracking.	4
In the evaluation, the synthetic dialogue corpus generated from NeuralWOZ achieves a new state-of-the-art with improvements of 4.4% point joint goal accuracy on average across domains, and improvements of 5.7% point of zero-shot coverage against the MultiWOZ 2.1 dataset.	4
----------
The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes.	1
This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain & Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time).	2
Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study.	3
Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.	4
----------
Transformer-based language models pre-trained on large amounts of text data have proven remarkably successful in learning generic transferable linguistic representations.	1
Here we study whether structural guidance leads to more human-like systematic linguistic generalization in Transformer language models without resorting to pre-training on very large amounts of data.	2
We explore two general ideas.	2
The “Generative Parsing” idea jointly models the incremental parse and word sequence as part of the same sequence modeling task.	2
The “Structural Scaffold” idea guides the language model’s representation via additional structure loss that separately predicts the incremental constituency parse.	2
We train the proposed models along with a vanilla Transformer language model baseline on a 14 million-token and a 46 million-token subset of the BLLIP dataset, and evaluate models’ syntactic generalization performances on SG Test Suites and sized BLiMP.	3
Experiment results across two benchmarks suggest converging evidence that generative structural supervisions can induce more robust and humanlike linguistic generalization in Transformer language models without the need for data intensive pre-training.	4
----------
While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling.	1
This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities.	2
Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data.	4
This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.	4
----------
Most previous studies integrate cognitive language processing signals (e.g., eye-tracking or EEG data) into neural models of natural language processing (NLP) just by directly concatenating word embeddings with cognitive features, ignoring the gap between the two modalities (i.e., textual vs. cognitive) and noise in cognitive features.	1
In this paper, we propose a CogAlign approach to these issues, which learns to align textual neural representations to cognitive features.	2
In CogAlign, we use a shared encoder equipped with a modality discriminator to alternatively encode textual and cognitive inputs to capture their differences and commonalities.	3
Additionally, a text-aware attention mechanism is proposed to detect task-related information and to avoid using noise in cognitive features.	3
Experimental results on three NLP tasks, namely named entity recognition, sentiment analysis and relation extraction, show that CogAlign achieves significant improvements with multiple cognitive features over state-of-the-art models on public datasets.	4
Moreover, our model is able to transfer cognitive information to other datasets that do not have any cognitive processing signals.	4
----------
Despite their impressive performance in NLP, self-attention networks were recently proved to be limited for processing formal languages with hierarchical structure, such as Dyck-k, the language consisting of well-nested parentheses of k types.	1
This suggested that natural language can be approximated well with models that are too weak for formal languages, or that the role of hierarchy and recursion in natural language might be limited.	1
We qualify this implication by proving that self-attention networks can process Dyck-(k, D), the subset of Dyck-k with depth bounded by D, which arguably better captures the bounded hierarchical structure of natural language.	2
Specifically, we construct a hard-attention network with D+1 layers and O(log k) memory size (per token per layer) that recognizes Dyck-(k, D), and a soft-attention network with two layers and O(log k) memory size that generates Dyck-(k, D).	3
Experiments show that self-attention networks trained on Dyck-(k, D) generalize to longer inputs with near-perfect accuracy, and also verify the theoretical memory advantage of self-attention networks over recurrent networks.	4
----------
We present a novel approach to the problem of text style transfer.	2
Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time.	3
We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer.	3
As our label-free training results in a style vector space encoding many facets of style, we recast transfers as “targeted restyling” vector operations that adjust specific attributes of the input while preserving others.	3
We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data.	4
Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time.	4
----------
We describe an efficient hierarchical method to compute attention in the Transformer architecture.	2
The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity.	3
We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks.	3
Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark.	4
It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.	4
----------
The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context.	1
Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient.	1
We present LM-BFF—better few-shot fine-tuning of language models—a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples.	2
Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context.	3
Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression.	2
Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks.	4
Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.	4
----------
The Universal Trigger (UniTrigger) is a recently-proposed powerful adversarial textual attack method.	1
Utilizing a learning-based mechanism, UniTrigger generates a fixed phrase that, when added to any benign inputs, can drop the prediction accuracy of a textual neural network (NN) model to near zero on a target class.	1
To defend against this attack that can cause significant harm, in this paper, we borrow the “honeypot” concept from the cybersecurity community and propose DARCY, a honeypot-based defense framework against UniTrigger.	2
DARCY greedily searches and injects multiple trapdoors into an NN model to “bait and catch” potential attacks.	3
Through comprehensive experiments across four public datasets, we show that DARCY detects UniTrigger’s adversarial attacks with up to 99% TPR and less than 2% FPR in most cases, while maintaining the prediction accuracy (in F1) for clean inputs within a 1% margin.	4
We also demonstrate that DARCY with multiple trapdoors is also robust to a diverse set of attack scenarios with attackers’ varying levels of knowledge and skills.	4
We release the source code of DARCY at: https://github.com/lethaiq/ACL2021-DARCY-HoneypotDefenseNLP.	6
----------
Detecting rumors on social media is a very critical task with significant implications to the economy, public health, etc.	1
Previous works generally capture effective features from texts and the propagation structure.	1
However, the uncertainty caused by unreliable relations in the propagation structure is common and inevitable due to wily rumor producers and the limited collection of spread data.	1
Most approaches neglect it and may seriously limit the learning of features.	1
Towards this issue, this paper makes the first attempt to explore propagation uncertainty for rumor detection.	1
Specifically, we propose a novel Edge-enhanced Bayesian Graph Convolutional Network (EBGCN) to capture robust structural features.	2
The model adaptively rethinks the reliability of latent relations by adopting a Bayesian approach.	3
Besides, we design a new edge-wise consistency training framework to optimize the model by enforcing consistency on relations.	3
Experiments on three public benchmark datasets demonstrate that the proposed model achieves better performance than baseline methods on both rumor detection and early rumor detection tasks.	4
----------
Multi-label text classification is one of the fundamental tasks in natural language processing.	1
Previous studies have difficulties to distinguish similar labels well because they learn the same document representations for different labels, that is they do not explicitly extract label-specific semantic components from documents.	1
Moreover, they do not fully explore the high-order interactions among these semantic components, which is very helpful to predict tail labels.	1
In this paper, we propose a novel label-specific dual graph neural network (LDGN), which incorporates category information to learn label-specific components from documents, and employs dual Graph Convolution Network (GCN) to model complete and adaptive interactions among these components based on the statistical label co-occurrence and dynamic reconstruction graph in a joint way.	2+3
Experimental results on three benchmark datasets demonstrate that LDGN significantly outperforms the state-of-the-art models, and also achieves better performance with respect to tail labels.	4
----------
Topic models have been widely used to learn text representations and gain insight into document corpora.	1
To perform topic discovery, most existing neural models either take document bag-of-words (BoW) or sequence of tokens as input followed by variational inference and BoW reconstruction to learn topic-word distribution.	1
However, leveraging topic-word distribution for learning better features during document encoding has not been explored much.	1
To this end, we develop a framework TAN-NTM, which processes document as a sequence of tokens through a LSTM whose contextual outputs are attended in a topic-aware manner.	2
We propose a novel attention mechanism which factors in topic-word distribution to enable the model to attend on relevant words that convey topic related cues.	2
The output of topic attention module is then used to carry out variational inference.	3
We perform extensive ablations and experiments resulting in ~9-15 percentage improvement over score of existing SOTA topic models in NPMI coherence on several benchmark datasets - 20Newsgroups, Yelp Review Polarity and AGNews.	3
Further, we show that our method learns better latent document-topic features compared to existing topic models through improvement on two downstream tasks: document classification and topic guided keyphrase generation.	4
----------
This paper proposes an approach to cross-language sentence selection in a low-resource setting.	2
It uses data augmentation and negative sampling techniques on noisy parallel sentence data to directly learn a cross-lingual embedding-based query relevance model.	3
Results show that this approach performs as well as or better than multiple state-of-the-art machine translation + monolingual retrieval systems trained on the same parallel data.	4
Moreover, when a rationale training secondary objective is applied to encourage the model to match word alignment hints from a phrase-based statistical machine translation model, consistent improvements are seen across three language pairs (English-Somali, English-Swahili and English-Tagalog) over a variety of state-of-the-art baselines.	4
----------
Question answering (QA) systems for large document collections typically use pipelines that (i) retrieve possibly relevant documents, (ii) re-rank them, (iii) rank paragraphs or other snippets of the top-ranked documents, and (iv) select spans of the top-ranked snippets as exact answers.	1
Pipelines are conceptually simple, but errors propagate from one component to the next, without later components being able to revise earlier decisions.	1
We present an architecture for joint document and snippet ranking, the two middle stages, which leverages the intuition that relevant documents have good snippets and good snippets come from relevant documents.	2
The architecture is general and can be used with any neural text relevance ranker.	3
We experiment with two main instantiations of the architecture, based on POSIT-DRMM (PDRMM) and a BERT-based ranker.	3
Experiments on biomedical data from BIOASQ show that our joint models vastly outperform the pipelines in snippet retrieval, the main goal for QA, with fewer trainable parameters, also remaining competitive in document retrieval.	4
Furthermore, our joint PDRMM-based model is competitive with BERT-based models, despite using orders of magnitude fewer parameters.	4
These claims are also supported by human evaluation on two test batches of BIOASQ.	4
To test our key findings on another dataset, we modified the Natural Questions dataset so that it can also be used for document and snippet retrieval.	4
Our joint PDRMM-based model again outperforms the corresponding pipeline in snippet retrieval on the modified Natural Questions dataset, even though it performs worse than the pipeline in document retrieval.	4
We make our code and the modified Natural Questions dataset publicly available.	6
----------
Atomic clauses are fundamental text units for understanding complex sentences.	1
Identifying the atomic sentences within complex sentences is important for applications such as summarization, argument mining, discourse analysis, discourse parsing, and question answering.	1
Previous work mainly relies on rule-based methods dependent on parsing.	1
We propose a new task to decompose each complex sentence into simple sentences derived from the tensed clauses in the source, and a novel problem formulation as a graph edit task.	2
Our neural model learns to Accept, Break, Copy or Drop elements of a graph that combines word adjacency and grammatical dependencies.	3
The full processing pipeline includes modules for graph construction, graph editing, and sentence generation from the output graph.	3
We introduce DeSSE, a new dataset designed to train and evaluate complex sentence decomposition, and MinWiki, a subset of MinWikiSplit.	2
ABCD achieves comparable performance as two parsing baselines on MinWiki.	4
On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline.	4
Results include a detailed error analysis.	4
----------
Many Question-Answering (QA) datasets contain unanswerable questions, but their treatment in QA systems remains primitive.	1
Our analysis of the Natural Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion of unanswerable questions (~21%) can be explained based on the presence of unverifiable presuppositions.	1
Through a user preference study, we demonstrate that the oracle behavior of our proposed system—which provides responses based on presupposition failure—is preferred over the oracle behavior of existing QA systems.	1
Then, we present a novel framework for implementing such a system in three steps: presupposition generation, presupposition verification, and explanation generation, reporting progress on each.	2
Finally, we show that a simple modification of adding presuppositions and their verifiability to the input of a competitive end-to-end QA system yields modest gains in QA performance and unanswerability detection, demonstrating the promise of our approach.	4
----------
Text-level discourse rhetorical structure (DRS) parsing is known to be challenging due to the notorious lack of training data.	1
Although recent top-down DRS parsers can better leverage global document context and have achieved certain success, the performance is still far from perfect.	1
To our knowledge, all previous DRS parsers make local decisions for either bottom-up node composition or top-down split point ranking at each time step, and largely ignore DRS parsing from the global view point.	1
Obviously, it is not sufficient to build an entire DRS tree only through these local decisions.	1
In this work, we present our insight on evaluating the pros and cons of the entire DRS tree for global optimization.	2
Specifically, based on recent well-performing top-down frameworks, we introduce a novel method to transform both gold standard and predicted constituency trees into tree diagrams with two color channels.	2
After that, we learn an adversarial bot between gold and fake tree diagrams to estimate the generated DRS trees from a global perspective.	3
We perform experiments on both RST-DT and CDTB corpora and use the original Parseval for performance evaluation.	3
The experimental results show that our parser can substantially improve the performance when compared with previous state-of-the-art parsers.	4
----------
Discourse relations among arguments reveal logical structures of a debate conversation.	1
However, no prior work has explicitly studied how the sequence of discourse relations influence a claim’s impact.	1
This paper empirically shows that the discourse relations between two arguments along the context path are essential factors for identifying the persuasive power of an argument.	2
We further propose DisCOC to inject and fuse the sentence-level structural discourse information with contextualized features derived from large-scale language models.	2
Experimental results and extensive analysis show that the attention and gate mechanisms that explicitly model contexts and texts can indeed help the argument impact classification task defined by Durmus et al.(2019), and discourse structures among the context path of the claim to be classified can further boost the performance.	4
----------
This paper proposes a sophisticated neural architecture to incorporate bilingual dictionaries into Neural Machine Translation (NMT) models.	2
By introducing three novel components: Pointer, Disambiguator, and Copier, our method PDC achieves the following merits inherently compared with previous efforts: 	3
(1) Pointer leverages the semantic information from bilingual dictionaries, for the first time, to better locate source words whose translation in dictionaries can potentially be used; 	3
(2) Disambiguator synthesizes contextual information from the source view and the target view, both of which contribute to distinguishing the proper translation of a specific source word from multiple candidates in dictionaries; 	3
(3) Copier systematically connects Pointer and Disambiguator based on a hierarchical copy mechanism seamlessly integrated with Transformer, thereby building an end-to-end architecture that could avoid error propagation problems in alternative pipe-line methods.	3
The experimental results on Chinese-English and English-Japanese benchmarks demonstrate the PDC’s overall superiority and effectiveness of each component.	4
----------
Existing work in multilingual pretraining has demonstrated the potential of cross-lingual transferability by training a unified Transformer encoder for multiple languages.	1
However, much of this work only relies on the shared vocabulary and bilingual contexts to encourage the correlation across languages, which is loose and implicit for aligning the contextual representations between languages.	1
In this paper, we plug a cross-attention module into the Transformer encoder to explicitly build the interdependence between languages.	2+3
It can effectively avoid the degeneration of predicting masked words only conditioned on the context in its own language.	4
More importantly, when fine-tuning on downstream tasks, the cross-attention module can be plugged in or out on-demand, thus naturally benefiting a wider range of cross-lingual tasks, from language understanding to generation.	4
As a result, the proposed cross-lingual model delivers new state-of-the-art results on various cross-lingual understanding tasks of the XTREME benchmark, covering text classification, sequence labeling, question answering, and sentence retrieval.	4
For cross-lingual generation tasks, it also outperforms all existing cross-lingual models and state-of-the-art Transformer variants on WMT14 English-to-German and English-to-French translation datasets, with gains of up to 1 2 BLEU.	4
----------
The sentence is a fundamental unit of text processing.	1
Yet sentences in the wild are commonly encountered not in isolation, but unsegmented within larger paragraphs and documents.	1
Therefore, the first step in many NLP pipelines is sentence segmentation.	1
Despite its importance, this step is the subject of relatively little research.	1
There are no standard test sets or even methods for evaluation, leaving researchers and engineers without a clear footing for evaluating and selecting models for the task.	1
Existing tools have relatively small language coverage, and efforts to extend them to other languages are often ad hoc.	1
We introduce a modern context-based modeling approach that provides a solution to the problem of segmenting punctuated text in many languages, and show how it can be trained on noisily-annotated data.	2
We also establish a new 23-language multilingual evaluation set.	2
Our approach exceeds high baselines set by existing methods on prior English corpora (WSJ and Brown corpora), and also performs well on average on our new evaluation set.	4
We release our tool, ersatz, as open source.	6
----------
A good translation should not only translate the original content semantically, but also incarnate personal traits of the original text.	1
For a real-world neural machine translation (NMT) system, these user traits (e.g., topic preference, stylistic characteristics and expression habits) can be preserved in user behavior (e.g., historical inputs).	1
However, current NMT systems marginally consider the user behavior due to: 1) the difficulty of modeling user portraits in zero-shot scenarios, and 2) the lack of user-behavior annotated parallel dataset.	1
To fill this gap, we introduce a novel framework called user-driven NMT.	2
Specifically, a cache-based module and a user-driven contrastive learning method are proposed to offer NMT the ability to capture potential user traits from their historical inputs under a zero-shot learning fashion.	3
Furthermore, we contribute the first Chinese-English parallel corpus annotated with user behavior called UDT-Corpus.	2
Experimental results confirm that the proposed user-driven NMT can generate user-specific translations.	4
----------
Technical logbooks are a challenging and under-explored text type in automated event identification.	1
These texts are typically short and written in non-standard yet technical language, posing challenges to off-the-shelf NLP pipelines.	1
The granularity of issue types described in these datasets additionally leads to class imbalance, making it challenging for models to accurately predict which issue each logbook entry describes.	1
In this paper we focus on the problem of technical issue classification by considering logbook datasets from the automotive, aviation, and facilities maintenance domains.	2
We adapt a feedback strategy from computer vision for handling extreme class imbalance, which resamples the training data based on its error in the prediction process.	3
Our experiments show that with statistical significance this feedback strategy provides the best results for four different neural network models trained across a suite of seven different technical logbook datasets from distinct technical domains.	4
The feedback strategy is also generic and could be applied to any learning problem with substantial class imbalances.	4
----------
The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information.	1
However, a large amount of world’s knowledge is stored in structured databases, and need to be accessed using query languages such as SQL.	1
Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability.	1
In this paper, we propose a hybrid framework that takes both textual and tabular evidences as input and generates either direct answers or SQL queries depending on which form could better answer the question.	2
The generated SQL queries can then be executed on the associated databases to obtain the final answers.	2
To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks.	2
Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only takes homogeneous input by a large margin.	4
Specifically we achieve the state-of-the-art performance on OpenSQuAD dataset using a T5-base model.	4
In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.	4
----------
We propose Generation-Augmented Retrieval (GAR) for answering open-domain questions, which augments a query through text generation of heuristically discovered relevant contexts without external resources as supervision.	2
We demonstrate that the generated contexts substantially enrich the semantics of the queries and GAR with sparse representations (BM25) achieves comparable or better performance than state-of-the-art dense retrieval methods such as DPR.	4
We show that generating diverse contexts for a query is beneficial as fusing their results consistently yields better retrieval accuracy.	4
Moreover, as sparse and dense representations are often complementary, GAR can be easily combined with DPR to achieve even better performance.	4
GAR achieves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.	4
----------
While sophisticated neural-based models have achieved remarkable success in Visual Question Answering (VQA), these models tend to answer questions only according to superficial correlations between question and answer.	1
Several recent approaches have been developed to address this language priors problem.	1
However, most of them predict the correct answer according to one best output without checking the authenticity of answers.	1
Besides, they only explore the interaction between image and question, ignoring the semantics of candidate answers.	1
In this paper, we propose a select-and-rerank (SAR) progressive framework based on Visual Entailment.	2
Specifically, we first select the candidate answers relevant to the question or the image, then we rerank the candidate answers by a visual entailment task, which verifies whether the image semantically entails the synthetic statement of the question and each candidate answer.	3
Experimental results show the effectiveness of our proposed framework, which establishes a new state-of-the-art accuracy on VQA-CP v2 with a 7.55% improvement.	4
----------
Weakly supervised question answering usually has only the final answers as supervision signals while the correct solutions to derive the answers are not provided.	1
This setting gives rise to the spurious solution problem: there may exist many spurious solutions that coincidentally derive the correct answer, but training on such solutions can hurt model performance (e.g., producing wrong solutions or answers).	1
For example, for discrete reasoning tasks as on DROP, there may exist many equations to derive a numeric answer, and typically only one of them is correct.	1
Previous learning methods mostly filter out spurious solutions with heuristics or using model confidence, but do not explicitly exploit the semantic correlations between a question and its solution.	1
In this paper, to alleviate the spurious solution problem, we propose to explicitly exploit such semantic correlations by maximizing the mutual information between question-answer pairs and predicted solutions.	2
Extensive experiments on four question answering datasets show that our method significantly outperforms previous learning methods in terms of task performance and is more effective in training models to produce correct solutions.	4
----------
Questions of fairness, robustness, and transparency are paramount to address before deploying NLP systems.	1
Central to these concerns is the question of reliability: Can NLP systems reliably treat different demographics fairly and function correctly in diverse and noisy environments?	1
To address this, we argue for the need for reliability testing and contextualize it among existing work on improving accountability.	2
We show how adversarial attacks can be reframed for this goal, via a framework for developing reliability tests.	2
We argue that reliability testing — with an emphasis on interdisciplinary collaboration — will enable rigorous and targeted testing, and aid in the enactment and enforcement of industry standards.	4
----------
Mental health conditions remain underdiagnosed even in countries with common access to advanced medical care.	1
The ability to accurately and efficiently predict mood from easily collectible data has several important implications for the early detection, intervention, and treatment of mental health disorders.	1
One promising data source to help monitor human behavior is daily smartphone usage.	1
However, care must be taken to summarize behaviors without identifying the user through personal (e.g., personally identifiable information) or protected (e.g., race, gender) attributes.	1
In this paper, we study behavioral markers of daily mood using a recent dataset of mobile behaviors from adolescent populations at high risk of suicidal behaviors.	2
Using computational models, we find that language and multimodal representations of mobile typed text (spanning typed characters, words, keystroke timings, and app usage) are predictive of daily mood.	3
However, we find that models trained to predict mood often also capture private user identities in their intermediate representations.	3
To tackle this problem, we evaluate approaches that obfuscate user identity while remaining predictive.	3
By combining multimodal representations with privacy-preserving learning, we are able to push forward the performance-privacy frontier.	4
----------
Although parsing to Abstract Meaning Representation (AMR) has become very popular and AMR has been shown effective on the many sentence-level downstream tasks, little work has studied how to generate AMRs that can represent multi-sentence information.	1
We introduce the first end-to-end AMR coreference resolution model in order to build multi-sentence AMRs.	2
Compared with the previous pipeline and rule-based approaches, our model alleviates error propagation and it is more robust for both in-domain and out-domain situations.	3
Besides, the document-level AMRs obtained by our model can significantly improve over the AMRs generated by a rule-based method (Liu et al., 2015) on text summarization.	4
----------
Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly.	1
In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.	2
In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers.	3
Next, we gather datasets of morphosyntactic, semantic, and commonsense anomalies from psycholinguistic studies; 	3
we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer.	4
These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.	4
----------
Most of the recent work on personality detection from online posts adopts multifarious deep neural networks to represent the posts and builds predictive models in a data-driven manner, without the exploitation of psycholinguistic knowledge that may unveil the connections between one’s language use and his psychological traits.	1
In this paper, we propose a psycholinguistic knowledge-based tripartite graph network, TrigNet, which consists of a tripartite graph network and a BERT-based graph initializer.	2
The graph network injects structural psycholinguistic knowledge in LIWC, a computerized instrument for psycholinguistic analysis, by constructing a heterogeneous tripartite graph.	3
The initializer is employed to provide initial embeddings for the graph nodes.	3
To reduce the computational cost in graph learning, we further propose a novel flow graph attention network (GAT) that only transmits messages between neighboring parties in the tripartite graph.	3
Benefiting from the tripartite graph, TrigNet can aggregate post information from a psychological perspective, which is a novel way of exploiting domain knowledge.	3
Extensive experiments on two datasets show that TrigNet outperforms the existing state-of-art model by 3.47 and 2.10 points in average F1.	4
Moreover, the flow GAT reduces the FLOPS and Memory measures by 38% and 32%, respectively, in comparison to the original GAT in our setting.	4
----------
Correct natural language understanding requires computers to distinguish the literal and metaphorical senses of a word.	1
Recent neu- ral models achieve progress on verb metaphor detection by viewing it as sequence labeling.	1
In this paper, we argue that it is appropriate to view this task as relation classification between a verb and its various contexts.	2
We propose the Metaphor-relation BERT (Mr-BERT) model, which explicitly models the relation between a verb and its grammatical, sentential and semantic contexts.	2
We evaluate our method on the VUA, MOH-X and TroFi datasets.	3
Our method gets competitive results compared with state-of-the-art approaches.	4
----------
Pretraining and multitask learning are widely used to improve the speech translation performance.	1
In this study, we are interested in training a speech translation model along with an auxiliary text translation task.	1
We conduct a detailed analysis to understand the impact of the auxiliary task on the primary task within the multitask learning framework.	3
Our analysis confirms that multitask learning tends to generate similar decoder representations from different modalities and preserve more information from the pretrained text translation modules.	3+4
We observe minimal negative transfer effect between the two tasks and sharing more parameters is helpful to transfer knowledge from the text task to the speech task.	4
The analysis also reveals that the modality representation difference at the top decoder layers is still not negligible, and those layers are critical for the translation quality.	4
Inspired by these findings, we propose three methods to improve translation quality.	2
First, a parameter sharing and initialization strategy is proposed to enhance information sharing between the tasks.	2
Second, a novel attention-based regularization is proposed for the encoders and pulls the representations from different modalities closer.	2
Third, an online knowledge distillation is proposed to enhance the knowledge transfer from the text to the speech task.	2
Our experiments show that the proposed approach improves translation performance by more than 2 BLEU over a strong baseline and achieves state-of-the-art results on the MuST-C English-German, English-French and English-Spanish language pairs.	4
----------
Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems.	1
We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates.	2
The templates are prompted by a name of a social group followed by a cause-effect relation.	3
We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities.	3
We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.	4
----------
Active Learning (AL) has been successfully applied to Deep Learning in order to drastically reduce the amount of data required to achieve high performance.	1
Previous works have shown that lightweight architectures for Named Entity Recognition (NER) can achieve optimal performance with only 25% of the original training data.	1
However, these methods do not exploit the sequential nature of language and the heterogeneity of uncertainty within each instance, requiring the labelling of whole sentences.	1
Additionally, this standard method requires that the annotator has access to the full sentence when labelling.	1
In this work, we overcome these limitations by allowing the AL algorithm to query subsequences within sentences, and propagate their labels to other sentences.	2+3
We achieve highly efficient results on OntoNotes 5.0, only requiring 13% of the original training data, and CoNLL 2003, requiring only 27%.	4
This is an improvement of 39% and 37% compared to querying full sentences.	4
----------
In this paper, we detail the relationship between convolutions and self-attention in natural language tasks.	2
We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention.	2
Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework.	2
We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings.	3
To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.	4
----------
The rapid development of large pre-trained language models has greatly increased the demand for model compression techniques, among which quantization is a popular solution.	1
In this paper, we propose BinaryBERT, which pushes BERT quantization to the limit by weight binarization.	2
We find that a binary BERT is hard to be trained directly than a ternary counterpart due to its complex and irregular loss landscape.	1
Therefore, we propose ternary weight splitting, which initializes BinaryBERT by equivalently splitting from a half-sized ternary network.	2+3
The binary model thus inherits the good performance of the ternary one, and can be further enhanced by fine-tuning the new architecture after splitting.	4
Empirical results show that our BinaryBERT has only a slight performance drop compared with the full-precision model while being 24x smaller, achieving the state-of-the-art compression results on the GLUE and SQuAD benchmarks.	4
Code will be released.	6
----------
Distance based knowledge graph embedding methods show promising results on link prediction task, on which two topics have been widely studied: one is the ability to handle complex relations, such as N-to-1, 1-to-N and N-to-N, the other is to encode various relation patterns, such as symmetry/antisymmetry.	1
However, the existing methods fail to solve these two problems at the same time, which leads to unsatisfactory results.	1
To mitigate this problem, we propose PairRE, a model with paired vectors for each relation representation.	2
The paired vectors enable an adaptive adjustment of the margin in loss function to fit for different complex relations.	3
Besides, PairRE is capable of encoding three important relation patterns, symmetry/antisymmetry, inverse and composition.	3
Given simple constraints on relation representations, PairRE can encode subrelation further.	3
Experiments on link prediction benchmarks demonstrate the proposed key capabilities of PairRE.	4
Moreover, We set a new state-of-the-art on two knowledge graph datasets of the challenging Open Graph Benchmark.	4
----------
Hierarchical text classification is an important yet challenging task due to the complex structure of the label hierarchy.	1
Existing methods ignore the semantic relationship between text and labels, so they cannot make full use of the hierarchical information.	1
To this end, we formulate the text-label semantics relationship as a semantic matching problem and thus propose a hierarchy-aware label semantics matching network (HiMatch).	2
First, we project text semantics and label semantics into a joint embedding space.	3
We then introduce a joint embedding loss and a matching learning loss to model the matching relationship between the text semantics and the label semantics.	3
Our model captures the text-label semantics matching relationship among coarse-grained labels and fine-grained labels in a hierarchy-aware manner.	3
The experimental results on various benchmark datasets verify that our model achieves state-of-the-art results.	4
----------
Fine-tuning large pre-trained models with task-specific data has achieved great success in NLP.	1
However, it has been demonstrated that the majority of information within the self-attention networks is redundant and not utilized effectively during the fine-tuning stage.	1
This leads to inferior results when generalizing the obtained models to out-of-domain distributions.	1
To this end, we propose a simple yet effective data augmentation technique, HiddenCut, to better regularize the model and encourage it to learn more generalizable features.	2
Specifically, contiguous spans within the hidden space are dynamically and strategically dropped during training.	3
Experiments show that our HiddenCut method outperforms the state-of-the-art augmentation methods on the GLUE benchmark, and consistently exhibits superior generalization performances on out-of-distribution and challenging counterexamples.	4
We have publicly released our code at https://github.com/GT-SALT/HiddenCut.	6
----------
Generating open-domain conversational responses in the desired style usually suffers from the lack of parallel data in the style.	1
Meanwhile, using monolingual stylistic data to increase style intensity often leads to the expense of decreasing content relevance.	1
In this paper, we propose to disentangle the content and style in latent space by diluting sentence-level information in style representations.	2
Combining the desired style representation and a response content representation will then obtain a stylistic response.	3
Our approach achieves a higher BERT-based style intensity score and comparable BLEU scores, compared with baselines.	4
Human evaluation results show that our approach significantly improves style intensity and maintains content relevance.	4
----------
Although neural models have achieved competitive results in dialogue systems, they have shown limited ability in representing core semantics, such as ignoring important entities.	1
To this end, we exploit Abstract Meaning Representation (AMR) to help dialogue modeling.	3
Compared with the textual input, AMR explicitly provides core semantic knowledge and reduces data sparsity.	3
We develop an algorithm to construct dialogue-level AMR graphs from sentence-level AMRs and explore two ways to incorporate AMRs into dialogue systems.	2
Experimental results on both dialogue understanding and response generation tasks show the superiority of our model.	4
To our knowledge, we are the first to leverage a formal semantic representation into neural dialogue modeling.	4
----------
Recently, many studies are emerging towards building a retrieval-based dialogue system that is able to effectively leverage background knowledge (e.g., documents) when conversing with humans.	1
However, it is non-trivial to collect large-scale dialogues that are naturally grounded on the background documents, which hinders the effective and adequate training of knowledge selection and response matching.	1
To overcome the challenge, we consider decomposing the training of the knowledge-grounded response selection into three tasks including: 1) query-passage matching task; 2) query-dialogue history matching task; 3) multi-turn response matching task, and joint learning all these tasks in a unified pre-trained language model.	2
The former two tasks could help the model in knowledge selection and comprehension, while the last task is designed for matching the proper response with the given query and background knowledge (dialogue history).	3
By this means, the model can be learned to select relevant knowledge and distinguish proper response, with the help of ad-hoc retrieval corpora and a large number of ungrounded multi-turn dialogues.	3
Experimental results on two benchmarks of knowledge-grounded response selection indicate that our model can achieve comparable performance with several existing methods that rely on crowd-sourced data for training.	4
----------
Syntactic information, especially dependency trees, has been widely used by existing studies to improve relation extraction with better semantic guidance for analyzing the context information associated with the given entities.	1
However, most existing studies suffer from the noise in the dependency trees, especially when they are automatically generated, so that intensively leveraging dependency information may introduce confusions to relation classification and necessary pruning is of great importance in this task.	1
In this paper, we propose a dependency-driven approach for relation extraction with attentive graph convolutional networks (A-GCN).	2
In this approach, an attention mechanism upon graph convolutional networks is applied to different contextual words in the dependency tree obtained from an off-the-shelf dependency parser, to distinguish the importance of different word dependencies.	3
Consider that dependency types among words also contain important contextual guidance, which is potentially helpful for relation extraction, we also include the type information in A-GCN modeling.	3
Experimental results on two English benchmark datasets demonstrate the effectiveness of our A-GCN, which outperforms previous studies and achieves state-of-the-art performance on both datasets.	4
----------
Leaderboards are widely used in NLP and push the field forward.	1
While leaderboards are a straightforward ranking of NLP models, this simplicity can mask nuances in evaluation items (examples) and subjects (NLP models).	1
Rather than replace leaderboards, we advocate a re-imagining so that they better highlight if and where progress is made.	2
Building on educational testing, we create a Bayesian leaderboard model where latent subject skill and latent item difficulty predict correct responses.	3
Using this model, we analyze the ranking reliability of leaderboards.	3
Afterwards, we show the model can guide what to annotate, identify annotation errors, detect overfitting, and identify informative examples.	4
We conclude with recommendations for future benchmark tasks.	4
----------
Manual fact-checking does not scale well to serve the needs of the internet.	1
This issue is further compounded in non-English contexts.	1
In this paper, we discuss claim matching as a possible solution to scale fact-checking.	2
We define claim matching as the task of identifying pairs of textual messages containing claims that can be served with one fact-check.	2
We construct a novel dataset of WhatsApp tipline and public group messages alongside fact-checked claims that are first annotated for containing “claim-like statements” and then matched with potentially similar items and annotated for claim matching.	3
Our dataset contains content in high-resource (English, Hindi) and lower-resource (Bengali, Malayalam, Tamil) languages.	3
We train our own embedding model using knowledge distillation and a high-quality “teacher” model in order to address the imbalance in embedding quality between the low- and high-resource languages in our dataset.	3
We provide evaluations on the performance of our solution and compare with baselines and existing state-of-the-art multilingual embedding models, namely LASER and LaBSE.	3
We demonstrate that our performance exceeds LASER and LaBSE in all settings.	4
We release our annotated datasets, codebooks, and trained embedding model to allow for further research.	4
----------
While pre-training techniques are working very well in natural language processing, how to pre-train a decoder and effectively use it for neural machine translation (NMT) still remains a tricky issue.	1
The main reason is that the cross-attention module between the encoder and decoder cannot be pre-trained, and the combined encoder-decoder model cannot work well in the fine-tuning stage because the inputs of the decoder cross-attention come from unknown encoder outputs.	1
In this paper, we propose a better pre-training method for NMT by defining a semantic interface (SemFace) between the pre-trained encoder and the pre-trained decoder.	2
Specifically, we propose two types of semantic interfaces, including CL-SemFace which regards cross-lingual embeddings as an interface, and VQ-SemFace which employs vector quantized embeddings to constrain the encoder outputs and decoder inputs in the same language-independent space.	2
We conduct massive experiments on six supervised translation pairs and three unsupervised pairs.	3
Experimental results demonstrate that our proposed SemFace can effectively connect the pre-trained encoder and decoder, and achieves significant improvement by 3.7 and 1.5 BLEU points on the two tasks respectively compared with previous pre-training-based NMT models.	4
----------
The discrepancy between maximum likelihood estimation (MLE) and task measures such as BLEU score has been studied before for autoregressive neural machine translation (NMT) and resulted in alternative training algorithms (Ranzato et al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018).	1
However, MLE training remains the de facto approach for autoregressive NMT because of its computational efficiency and stability.	1
Despite this mismatch between the training objective and task measure, we notice that the samples drawn from an MLE-based trained NMT support the desired distribution – there are samples with much higher BLEU score comparing to the beam decoding output.	1
To benefit from this observation, we train an energy-based model to mimic the behavior of the task measure (i.e., the energy-based model assigns lower energy to samples with higher BLEU score), which is resulted in a re-ranking algorithm based on the samples drawn from NMT: energy-based re-ranking (EBR).	3
We use both marginal energy models (over target sentence) and joint energy models (over both source and target sentences).	3
Our EBR with the joint energy model consistently improves the performance of the Transformer-based NMT: +3.7 BLEU points on IWSLT’14 German-English, +3.37 BELU points on Sinhala-English, +1.4 BLEU points on WMT’16 English-German tasks.	4
----------
In recent years, we have seen a colossal effort in pre-training multilingual text encoders using large-scale corpora in many languages to facilitate cross-lingual transfer learning.	1
However, due to typological differences across languages, the cross-lingual transfer is challenging.	1
Nevertheless, language syntax, e.g., syntactic dependencies, can bridge the typological gap.	1
Previous works have shown that pre-trained multilingual encoders, such as mBERT (CITATION), capture language syntax, helping cross-lingual transfer.	2
This work shows that explicitly providing language syntax and training mBERT using an auxiliary objective to encode the universal dependency tree structure helps cross-lingual transfer.	3
We perform rigorous experiments on four NLP tasks, including text classification, question answering, named entity recognition, and task-oriented semantic parsing.	3
The experiment results show that syntax-augmented mBERT improves cross-lingual transfer on popular benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across all languages.	4
In the generalized transfer setting, the performance boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.	4
----------
We study the problem of building entity tagging systems by using a few rules as weak supervision.	1
Previous methods mostly focus on disambiguating entity types based on contexts and expert-provided rules, while assuming entity spans are given.	1
In this work, we propose a novel method TALLOR that bootstraps high-quality logical rules to train a neural tagger in a fully automated manner.	2
Specifically, we introduce compound rules that are composed from simple rules to increase the precision of boundary detection and generate more diverse pseudo labels.	2
We further design a dynamic label selection strategy to ensure pseudo label quality and therefore avoid overfitting the neural tagger.	3
Experiments on three datasets demonstrate that our method outperforms other weakly supervised methods and even rivals a state-of-the-art distantly supervised tagger with a lexicon of over 2,000 terms when starting from only 20 simple rules.	4
Our method can serve as a tool for rapidly building taggers in emerging domains and tasks.	4
Case studies show that learned rules can potentially explain the predicted entities.	4
----------
Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks.	1
However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task.	1
In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix.	2
Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”.	3
We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization.	3
We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.	4
----------
Recently, the sequence-to-sequence models have made remarkable progress on the task of keyphrase generation (KG) by concatenating multiple keyphrases in a predefined order as a target sequence during training.	1
However, the keyphrases are inherently an unordered set rather than an ordered sequence.	1
Imposing a predefined order will introduce wrong bias during training, which can highly penalize shifts in the order between keyphrases.	1
In this work, we propose a new training paradigm One2Set without predefining an order to concatenate the keyphrases.	2
To fit this paradigm, we propose a novel model that utilizes a fixed set of learned control codes as conditions to generate a set of keyphrases in parallel.	2
To solve the problem that there is no correspondence between each prediction and target during training, we propose a K-step label assignment mechanism via bipartite matching, which greatly increases the diversity and reduces the repetition rate of generated keyphrases.	2
The experimental results on multiple benchmarks demonstrate that our approach significantly outperforms the state-of-the-art methods.	4
----------
Recent years have witnessed various types of generative models for natural language generation (NLG), especially RNNs or transformer based sequence-to-sequence models, as well as variational autoencoder (VAE) and generative adversarial network (GAN) based models.	1
However, flow-based generative models, which achieve strong performance in image generation due to their invertibility and exact density estimation properties, have been less explored for NLG.	1
In this paper, we propose a flow-based language generation model by adapting previous flow generative models to language generation via continuous input embeddings, adapted affine coupling structures, and a novel architecture for autoregressive text generation.	2
We also apply our framework to Sequence-to-Sequence generation, including text- and video-based Question Generation (QG) and Neural Machine Translation (NMT), and data augmentation for Question Answering (QA).	3
We use our language flow model to provide extra input features for QG and NMT, which achieves improvements over the strong QG baselines on SQuAD and TVQA and NMT baseline on WMT16.	3+4
We also augment QA data with new context by injecting noise to the latent features of the language flow and show this augmentation leads to a large performance improvement from strong baselines on SQuAD and TVQA.	4
----------
Wikipedia abstract generation aims to distill a Wikipedia abstract from web sources and has met significant success by adopting multi-document summarization techniques.	1
However, previous works generally view the abstract as plain text, ignoring the fact that it is a description of a certain entity and can be decomposed into different topics.	1
In this paper, we propose a two-stage model TWAG that guides the abstract generation with topical information.	2
First, we detect the topic of each input paragraph with a classifier trained on existing Wikipedia articles to divide input documents into different topics.	3
Then, we predict the topic distribution of each abstract sentence, and decode the sentence from topic-aware representations with a Pointer-Generator network.	3
We evaluate our model on the WikiCatSum dataset, and the results show that TWAG outperforms various existing baselines and is capable of generating comprehensive abstracts.	4
----------
Syntactic structure is an important component of natural language text.	1
Recent top-performing models in Answer Sentence Selection (AS2) use self-attention and transfer learning, but not syntactic structure.	1
Tree structures have shown strong performance in tasks with sentence pair input like semantic relatedness.	1
We investigate whether tree structures can boost performance in AS2.	1
We introduce the Tree Aggregation Transformer: a novel recursive, tree-structured self-attention model for AS2.	2
The recursive nature of our model is able to represent all levels of syntactic parse trees with only one additional self-attention layer.	3
Without transfer learning, we establish a new state of the art on the popular TrecQA and WikiQA benchmark datasets.	3
Additionally, we evaluate our method on four Community Question Answering datasets, and find that tree-structured representations have limitations with noisy user-generated text.	3
We conduct probing experiments to evaluate how our models leverage tree structures across datasets.	3
Our findings show that the ability of tree-structured models to successfully absorb syntactic information is strongly correlated with a higher performance in AS2.	4
----------
Implicit Event Argument Extraction seeks to identify arguments that play direct or implicit roles in a given event.	1
However, most prior works focus on capturing direct relations between arguments and the event trigger.	1
The lack of reasoning ability brings many challenges to the extraction of implicit arguments.	1
In this work, we present a Frame-aware Event Argument Extraction (FEAE) learning framework to tackle this issue through reasoning in event frame-level scope.	2
The proposed method leverages related arguments of the expected one as clues to guide the reasoning process.	3
To bridge the gap between oracle knowledge used in the training phase and the imperfect related arguments in the test stage, we further introduce a curriculum knowledge distillation strategy to drive a final model that could operate without extra inputs through mimicking the behavior of a well-informed teacher model.	3
Experimental results demonstrate FEAE obtains new state-of-the-art performance on the RAMS dataset.	4
----------
Open relation extraction aims to cluster relation instances referring to the same underlying relation, which is a critical step for general relation extraction.	1
Current OpenRE models are commonly trained on the datasets generated from distant supervision, which often results in instability and makes the model easily collapsed.	1
In this paper, we revisit the procedure of OpenRE from a causal view.	2
By formulating OpenRE using a structural causal model, we identify that the above-mentioned problems stem from the spurious correlations from entities and context to the relation type.	3
To address this issue, we conduct Element Intervention, which intervene on the context and entities respectively to obtain the underlying causal effects of them.	3
We also provide two specific implementations of the interventions based on entity ranking and context contrasting.	3
Experimental results on unsupervised relation extraction datasets show our method to outperform previous state-of-the-art methods and is robust across different datasets.	4
----------
Automatic extraction of product attribute values is an important enabling technology in e-Commerce platforms.	1
This task is usually modeled using sequence labeling architectures, with several extensions to handle multi-attribute extraction.	1
One line of previous work constructs attribute-specific models, through separate decoders or entirely separate models.	1
However, this approach constrains knowledge sharing across different attributes.	1
Other contributions use a single multi-attribute model, with different techniques to embed attribute information.	1
But sharing the entire network parameters across all attributes can limit the model’s capacity to capture attribute-specific characteristics.	1
In this paper we present AdaTag, which uses adaptive decoding to handle extraction.	2
We parameterize the decoder with pretrained attribute embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module.	3
This allows for separate, but semantically correlated, decoders to be generated on the fly for different attributes.	3
This approach facilitates knowledge sharing, while maintaining the specificity of each attribute.	3
Our experiments on a real-world e-Commerce dataset show marked improvements over previous methods.	4
----------
Integrating extracted knowledge from the Web to knowledge graphs (KGs) can facilitate tasks like question answering.	1
We study relation integration that aims to align free-text relations in subject-relation-object extractions to relations in a target KG.	1
To address the challenge that free-text relations are ambiguous, previous methods exploit neighbor entities and relations for additional context.	1
However, the predictions are made independently, which can be mutually inconsistent.	1
We propose a two-stage Collective Relation Integration (CoRI) model, where the first stage independently makes candidate predictions, and the second stage employs a collective model that accesses all candidate predictions to make globally coherent predictions.	2
We further improve the collective model with augmented data from the portion of the target KG that is otherwise unused.	3
Experiment results on two datasets show that CoRI can significantly outperform the baselines, improving AUC from .677 to .748 and from .716 to .780, respectively.	4
----------
Streaming cross document entity coreference (CDC) systems disambiguate mentions of named entities in a scalable manner via incremental clustering.	1
Unlike other approaches for named entity disambiguation (e.g., entity linking), streaming CDC allows for the disambiguation of entities that are unknown at inference time.	1
Thus, it is well-suited for processing streams of data where new entities are frequently introduced.	1
Despite these benefits, this task is currently difficult to study, as existing approaches are either evaluated on datasets that are no longer available, or omit other crucial details needed to ensure fair comparison.	1
In this work, we address this issue by compiling a large benchmark adapted from existing free datasets, and performing a comprehensive evaluation of a number of novel and existing baseline models.	2
We investigate: how to best encode mentions, which clustering algorithms are most effective for grouping mentions, how models transfer to different domains, and how bounding the number of mentions tracked during inference impacts performance.	2
Our results show that the relative performance of neural and feature-based mention encoders varies across different domains, and in most cases the best performance is achieved using a combination of both approaches.	4
We also find that performance is minimally impacted by limiting the number of tracked mentions.	4
----------
Temporal Knowledge Graphs (TKGs) have been developed and used in many different areas.	1
Reasoning on TKGs that predicts potential facts (events) in the future brings great challenges to existing models.	1
When facing a prediction task, human beings usually search useful historical information (i.e., clues) in their memories and then reason for future meticulously.	1
Inspired by this mechanism, we propose CluSTeR to predict future facts in a two-stage manner, Clue Searching and Temporal Reasoning, accordingly.	2
Specifically, at the clue searching stage, CluSTeR learns a beam search policy via reinforcement learning (RL) to induce multiple clues from historical facts.	3
At the temporal reasoning stage, it adopts a graph convolution network based sequence method to deduce answers from clues.	3
Experiments on four datasets demonstrate the substantial advantages of CluSTeR compared with the state-of-the-art methods.	4
Moreover, the clues found by CluSTeR further provide interpretability for the results.	4
----------
Generating high-quality arguments, while being challenging, may benefit a wide range of downstream applications, such as writing assistants and argument search engines.	1
Motivated by the effectiveness of utilizing knowledge graphs for supporting general text generation tasks, this paper investigates the usage of argumentation-related knowledge graphs to control the generation of arguments.	2
In particular, we construct and populate three knowledge graphs, employing several compositions of them to encode various knowledge into texts of debate portals and relevant paragraphs from Wikipedia.	3
Then, the texts with the encoded knowledge are used to fine-tune a pre-trained text generation model, GPT-2.	3
We evaluate the newly created arguments manually and automatically, based on several dimensions important in argumentative contexts, including argumentativeness and plausibility.	3
The results demonstrate the positive impact of encoding the graphs’ knowledge into debate portal texts for generating arguments with superior quality than those generated without knowledge.	4
----------
Aspect Sentiment Triplet Extraction (ASTE) is the most recent subtask of ABSA which outputs triplets of an aspect target, its associated sentiment, and the corresponding opinion term.	1
Recent models perform the triplet extraction in an end-to-end manner but heavily rely on the interactions between each target word and opinion word.	1
Thereby, they cannot perform well on targets and opinions which contain multiple words.	1
Our proposed span-level approach explicitly considers the interaction between the whole spans of targets and opinions when predicting their sentiment relation.	1
Thus, it can make predictions with the semantics of whole spans, ensuring better sentiment consistency.	1
To ease the high computational cost caused by span enumeration, we propose a dual-channel span pruning strategy by incorporating supervision from the Aspect Term Extraction (ATE) and Opinion Term Extraction (OTE) tasks.	2
This strategy not only improves computational efficiency but also distinguishes the opinion and target spans more properly.	4
Our framework simultaneously achieves strong performance for the ASTE as well as ATE and OTE tasks.	4
In particular, our analysis shows that our span-level approach achieves more significant improvements over the baselines on triplets with multi-word targets or opinions.	4
----------
Modern neural machine translation (NMT) models have achieved competitive performance in standard benchmarks such as WMT.	1
However, there still exist significant issues such as robustness, domain generalization, etc.	1
In this paper, we study NMT models from the perspective of compositional generalization by building a benchmark dataset, CoGnition, consisting of 216k clean and consistent sentence pairs.	2
We quantitatively analyze effects of various factors using compound translation error rate, then demonstrate that the NMT model fails badly on compositional generalization, although it performs remarkably well under traditional metrics.	3+4
----------
Word alignment, which aims to align translationally equivalent words between source and target sentences, plays an important role in many natural language processing tasks.	1
Current unsupervised neural alignment methods focus on inducing alignments from neural machine translation models, which does not leverage the full context in the target sequence.	1
In this paper, we propose Mask-Align, a self-supervised word alignment model that takes advantage of the full context on the target side.	2
Our model masks out each target token and predicts it conditioned on both source and the remaining target tokens.	3
This two-step process is based on the assumption that the source token contributing most to recovering the masked target token should be aligned.	3
We also introduce an attention variant called leaky attention, which alleviates the problem of unexpected high cross-attention weights on special tokens such as periods.	3
Experiments on four language pairs show that our model outperforms previous unsupervised neural aligners and obtains new state-of-the-art results.	4
----------
Computer-aided translation (CAT), the use of software to assist a human translator in the translation process, has been proven to be useful in enhancing the productivity of human translators.	1
Autocompletion, which suggests translation results according to the text pieces provided by human translators, is a core function of CAT.	1
There are two limitations in previous research in this line.	1
First, most research works on this topic focus on sentence-level autocompletion (i.e., generating the whole translation as a sentence based on human input), but word-level autocompletion is under-explored so far.	1
Second, almost no public benchmarks are available for the autocompletion task of CAT.	1
This might be among the reasons why research progress in CAT is much slower compared to automatic MT.	1
In this paper, we propose the task of general word-level autocompletion (GWLAN) from a real-world CAT scenario, and construct the first public benchmark to facilitate research in this topic.	2
In addition, we propose an effective method for GWLAN and compare it with several strong baselines.	2
Experiments demonstrate that our proposed method can give significantly more accurate predictions than the baseline methods on our benchmark datasets.	4
----------
Distant supervision tackles the data bottleneck in NER by automatically generating training instances via dictionary matching.	1
Unfortunately, the learning of DS-NER is severely dictionary-biased, which suffers from spurious correlations and therefore undermines the effectiveness and the robustness of the learned models.	1
In this paper, we fundamentally explain the dictionary bias via a Structural Causal Model (SCM), categorize the bias into intra-dictionary and inter-dictionary biases, and identify their causes.	2
Based on the SCM, we learn de-biased DS-NER via causal interventions.	3
For intra-dictionary bias, we conduct backdoor adjustment to remove the spurious correlations introduced by the dictionary confounder.	3
For inter-dictionary bias, we propose a causal invariance regularizer which will make DS-NER models more robust to the perturbation of dictionaries.	2
Experiments on four datasets and three DS-NER models show that our method can significantly improve the performance of DS-NER.	4
----------
Research on overlapped and discontinuous named entity recognition (NER) has received increasing attention.	1
The majority of previous work focuses on either overlapped or discontinuous entities.	1
In this paper, we propose a novel span-based model that can recognize both overlapped and discontinuous entities jointly.	2
The model includes two major steps.	3
First, entity fragments are recognized by traversing over all possible text spans, thus, overlapped entities can be recognized.	3
Second, we perform relation classification to judge whether a given pair of entity fragments to be overlapping or succession.	3
In this way, we can recognize not only discontinuous entities, and meanwhile doubly check the overlapped entities.	4
As a whole, our model can be regarded as a relation extraction paradigm essentially.	5
Experimental results on multiple benchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly competitive for overlapped and discontinuous NER.	4
----------
We consider the problem of collectively detecting multiple events, particularly in cross-sentence settings.	1
The key to dealing with the problem is to encode semantic information and model event inter-dependency at a document-level.	1
In this paper, we reformulate it as a Seq2Seq task and propose a Multi-Layer Bidirectional Network (MLBiNet) to capture the document-level association of events and semantic information simultaneously.	2
Specifically, a bidirectional decoder is firstly devised to model event inter-dependency within a sentence when decoding the event tag vector sequence.	2
Secondly, an information aggregation module is employed to aggregate sentence-level semantic and event tag information.	3
Finally, we stack multiple bidirectional decoders and feed cross-sentence information, forming a multi-layer bidirectional tagging architecture to iteratively propagate information across sentences.	3
We show that our approach provides significant improvement in performance compared to the current state-of-the-art results.	4
----------
We study the problem of event coreference resolution (ECR) that seeks to group coreferent event mentions into the same clusters.	2
Deep learning methods have recently been applied for this task to deliver state-of-the-art performance.	1
However, existing deep learning models for ECR are limited in that they cannot exploit important interactions between relevant objects for ECR, e.g., context words and entity mentions, to support the encoding of document-level context.	1
In addition, consistency constraints between golden and predicted clusters of event mentions have not been considered to improve representation learning in prior deep learning models for ECR.	1
This work addresses such limitations by introducing a novel deep learning model for ECR.	2
At the core of our model are document structures to explicitly capture relevant objects for ECR.	4
Our document structures introduce diverse knowledge sources (discourse, syntax, semantics) to compute edges/interactions between structure nodes for document-level representation learning.	3
We also present novel regularization techniques based on consistencies of golden and predicted clusters for event mentions in documents.	4
Extensive experiments show that our model achieve state-of-the-art performance on two benchmark datasets.	4
----------
Relational triple extraction is critical to understanding massive text corpora and constructing large-scale knowledge graph, which has attracted increasing research interest.	1
However, existing studies still face some challenging issues, including information loss, error propagation and ignoring the interaction between entity and relation.	1
To intuitively explore the above issues and address them, in this paper, we provide a revealing insight into relational triple extraction from a stereoscopic perspective, which rationalizes the occurrence of these issues and exposes the shortcomings of existing methods.	2
Further, a novel model is proposed for relational triple extraction, which maps relational triples to a three-dimension (3-D) space and leverages three decoders to extract them, aimed at simultaneously handling the above issues.	2+3
A series of experiments are conducted on five public datasets, demonstrating that the proposed model outperforms the recent advanced baselines.	4
----------
Identifying causal relations of events is an important task in natural language processing area.	1
However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues.	1
Existing methods cannot handle well the problem, especially in the condition of lacking training data.	1
Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge.	1
Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task.	2
Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge.	2
To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning.	2
Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.	4
----------
Recent studies show that neural natural language processing (NLP) models are vulnerable to backdoor attacks.	1
Injected with backdoors, models perform normally on benign examples but produce attacker-specified predictions when the backdoor is activated, presenting serious security threats to real-world applications.	1
Since existing textual backdoor attacks pay little attention to the invisibility of backdoors, they can be easily detected and blocked.	1
In this work, we present invisible backdoors that are activated by a learnable combination of word substitution.	2
We show that NLP models can be injected with backdoors that lead to a nearly 100% attack success rate, whereas being highly invisible to existing defense strategies and even human inspections.	4
The results raise a serious alarm to the security of NLP models, which requires further research to be resolved.	5
All the data and code of this paper are released at https://github.com/thunlp/BkdAtk-LWS.	6
----------
The large size of pretrained networks makes them difficult to deploy for multiple tasks in storage-constrained settings.	1
Diff pruning enables parameter-efficient transfer learning that scales well with new tasks.	2
The approach learns a task-specific “diff” vector that extends the original pretrained parameters.	3
This diff vector is adaptively pruned during training with a differentiable approximation to the L0-norm penalty to encourage sparsity.	3
As the number of tasks increases, diff pruning remains parameter-efficient, as it requires storing only a small diff vector for each task.	3
Since it does not require access to all tasks during training, it is attractive in on-device deployment settings where tasks arrive in stream or even from different providers.	3
Diff pruning can match the performance of finetuned baselines on the GLUE benchmark while only modifying 0.5% of the pretrained model’s parameters per task and scales favorably in comparison to popular pruning approaches.	4
----------
Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined.	1
However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process.	1
In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes.	2
To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time.	3
Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.	4
----------
Zero-shot sequence labeling aims to build a sequence labeler without human-annotated datasets.	1
One straightforward approach is utilizing existing systems (source models) to generate pseudo-labeled datasets and train a target sequence labeler accordingly.	1
However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels.	1
In this paper, we propose a novel unified framework for zero-shot sequence labeling with minimum risk training and design a new decomposable risk function that models the relations between the predicted labels from the source models and the true labels.	2
By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning.	3
We propose a unified learning algorithm based on the expectation maximization (EM) algorithm.	2
We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets.	3
The results show that our approaches outperform state-of-the-art baseline systems.	4
----------
Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks.	1
A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model.	2
In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation.	2
Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task.	3
Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark.	3+4
Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.	4
----------
Sequence-to-sequence transduction is the core problem in language processing applications as diverse as semantic parsing, machine translation, and instruction following.	1
The neural network models that provide the dominant solution to these problems are brittle, especially in low-resource settings: they fail to generalize correctly or systematically from small datasets.	1
Past work has shown that many failures of systematic generalization arise from neural models’ inability to disentangle lexical phenomena from syntactic ones.	1
To address this, we augment neural decoders with a lexical translation mechanism that generalizes existing copy mechanisms to incorporate learned, decontextualized, token-level translation rules.	2+3
We describe how to initialize this mechanism using a variety of lexicon learning algorithms, and show that it improves systematic generalization on a diverse set of sequence modeling tasks drawn from cognitive science, formal semantics, and machine translation.	4
----------
Personalization of natural language generation plays a vital role in a large spectrum of tasks, such as explainable recommendation, review summarization and dialog systems.	1
In these tasks, user and item IDs are important identifiers for personalization.	1
Transformer, which is demonstrated with strong language modeling capability, however, is not personalized and fails to make use of the user and item IDs since the ID tokens are not even in the same semantic space as the words.	1
To address this problem, we present a PErsonalized Transformer for Explainable Recommendation (PETER), on which we design a simple and effective learning objective that utilizes the IDs to predict the words in the target explanation, so as to endow the IDs with linguistic meanings and to achieve personalized Transformer.	2
Besides generating explanations, PETER can also make recommendations, which makes it a unified model for the whole recommendation-explanation pipeline.	3
Extensive experiments show that our small unpretrained model outperforms fine-tuned BERT on the generation task, in terms of both effectiveness and efficiency, which highlights the importance and the nice utility of our design.	4
----------
Following each patient visit, physicians draft long semi-structured clinical summaries called SOAP notes.	1
While invaluable to clinicians and researchers, creating digital SOAP notes is burdensome, contributing to physician burnout.	1
In this paper, we introduce the first complete pipelines to leverage deep summarization models to generate these notes based on transcripts of conversations between physicians and patients.	2
After exploring a spectrum of methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an algorithm that (i) extracts important utterances relevant to each summary section; (ii) clusters together related utterances; and then (iii) generates one summary sentence per cluster.	2+3
Cluster2Sent outperforms its purely abstractive counterpart by 8 ROUGE-1 points, and produces significantly more factual and coherent sentences as assessed by expert human evaluators.	4
For reproducibility, we demonstrate similar benefits on the publicly available AMI dataset.	3
Our results speak to the benefits of structuring summaries into sections and annotating supporting evidence when constructing summarization corpora.	4
----------
We investigate the problem of Chinese Grammatical Error Correction (CGEC) and present a new framework named Tail-to-Tail (TtT) non-autoregressive sequence prediction to address the deep issues hidden in CGEC.	2
Considering that most tokens are correct and can be conveyed directly from source to target, and the error positions can be estimated and corrected based on the bidirectional context information, thus we employ a BERT-initialized Transformer Encoder as the backbone model to conduct information modeling and conveying.	1
Considering that only relying on the same position substitution cannot handle the variable-length correction cases, various operations such substitution, deletion, insertion, and local paraphrasing are required jointly.	1
Therefore, a Conditional Random Fields (CRF) layer is stacked on the up tail to conduct non-autoregressive sequence prediction by modeling the token dependencies.	1
Since most tokens are correct and easily to be predicted/conveyed to the target, then the models may suffer from a severe class imbalance issue.	1
To alleviate this problem, focal loss penalty strategies are integrated into the loss functions.	3
Moreover, besides the typical fix-length error correction datasets, we also construct a variable-length corpus to conduct experiments.	2
Experimental results on standard datasets, especially on the variable-length datasets, demonstrate the effectiveness of TtT in terms of sentence-level Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and Correction.	4
----------
An important risk that children face today is online grooming, where a so-called sexual predator establishes an emotional connection with a minor online with the objective of sexual abuse.	1
Prior work has sought to automatically identify grooming chats, but only after an incidence has already happened in the context of legal prosecution.	1
In this work, we instead investigate this problem from the point of view of prevention.	1
We define and study the task of early sexual predator detection (eSPD) in chats, where the goal is to analyze a running chat from its beginning and predict grooming attempts as early and as accurately as possible.	2
We survey existing datasets and their limitations regarding eSPD, and create a new dataset called PANC for more realistic evaluations.	3
We present strong baselines built on BERT that also reach state-of-the-art results for conventional SPD.	2
Finally, we consider coping with limited computational resources, as real-life applications require eSPD on mobile devices.	2
----------
Medical report generation is one of the most challenging tasks in medical image analysis.	1
Although existing approaches have achieved promising results, they either require a predefined template database in order to retrieve sentences or ignore the hierarchical nature of medical report generation.	1
To address these issues, we propose MedWriter that incorporates a novel hierarchical retrieval mechanism to automatically extract both report and sentence-level templates for clinically accurate report generation.	2
MedWriter first employs the Visual-Language Retrieval (VLR) module to retrieve the most relevant reports for the given images.	3
To guarantee the logical coherence between generated sentences, the Language-Language Retrieval (LLR) module is introduced to retrieve relevant sentences based on the previous generated description.	3
At last, a language decoder fuses image features and features from retrieved reports and sentences to generate meaningful medical reports.	3
We verified the effectiveness of our model by automatic evaluation and human evaluation on two datasets, i.e., Open-I and MIMIC-CXR.	4
----------
Hierarchical Text Classification (HTC) is a challenging task that categorizes a textual description within a taxonomic hierarchy.	1
Most of the existing methods focus on modeling the text.	1
Recently, researchers attempt to model the class representations with some resources (e.g., external dictionaries).	1
However, the concept shared among classes which is a kind of domain-specific and fine-grained information has been ignored in previous work.	1
In this paper, we propose a novel concept-based label embedding method that can explicitly represent the concept and model the sharing mechanism among classes for the hierarchical text classification.	2
Experimental results on two widely used datasets prove that the proposed model outperforms several state-of-the-art methods.	4
We release our complementary resources (concepts and definitions of classes) for these two datasets to benefit the research on HTC.	6
----------
Text-to-image retrieval is an essential task in cross-modal information retrieval, i.e., retrieving relevant images from a large and unlabelled dataset given textual queries.	1
In this paper, we propose VisualSparta, a novel (Visual-text Sparse Transformer Matching) model that shows significant improvement in terms of both accuracy and efficiency.	2
VisualSparta is capable of outperforming previous state-of-the-art scalable methods in MSCOCO and Flickr30K.	4
We also show that it achieves substantial retrieving speed advantages, i.e., for a 1 million image index, VisualSparta using CPU gets ~391X speedup compared to CPU vector search and ~5.4X speedup compared to vector search with GPU acceleration.	4
Experiments show that this speed advantage even gets bigger for larger datasets because VisualSparta can be efficiently implemented as an inverted index.	4
To the best of our knowledge, VisualSparta is the first transformer-based text-to-image retrieval model that can achieve real-time searching for large-scale datasets, with significant accuracy improvement compared to previous state-of-the-art methods.	4
----------
The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a large scale of in-domain relevance training signals, which are not always available in real-world ranking scenarios.	1
To democratize the benefits of Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method that generalizes Neu-IR models from label-rich source domains to few-shot target domains.	2
Drawing on source-domain massive relevance supervision, MetaAdaptRank contrastively synthesizes a large number of weak supervision signals for target domains and meta-learns to reweight these synthetic “weak” data based on their benefits to the target-domain ranking accuracy of Neu-IR models.	3
Experiments on three TREC benchmarks in the web, news, and biomedical domains show that MetaAdaptRank significantly improves the few-shot ranking accuracy of Neu-IR models.	4
Further analyses indicate that MetaAdaptRank thrives from both its contrastive weak data synthesis and meta-reweighted data selection.	4
The code and data of this paper can be obtained from https://github.com/thunlp/MetaAdaptRank.	6
----------
Semi-Supervised Text Classification (SSTC) mainly works under the spirit of self-training.	1
They initialize the deep classifier by training over labeled texts; and then alternatively predict unlabeled texts as their pseudo-labels and train the deep classifier over the mixture of labeled and pseudo-labeled texts.	1
Naturally, their performance is largely affected by the accuracy of pseudo-labels for unlabeled texts.	1
Unfortunately, they often suffer from low accuracy because of the margin bias problem caused by the large difference between representation distributions of labels in SSTC.	1
To alleviate this problem, we apply the angular margin loss, and perform Gaussian linear transformation to achieve balanced label angle variances, i.e., the variance of label angles of texts within the same label.	3
More accuracy of predicted pseudo-labels can be achieved by constraining all label angle variances balanced, where they are estimated over both labeled and pseudo-labeled texts during self-training loops.	3
With this insight, we propose a novel SSTC method, namely Semi-Supervised Text Classification with Balanced Deep representation Distributions (S2TC-BDD).	2
To evaluate S2TC-BDD, we compare it against the state-of-the-art SSTC methods.	3
Empirical results demonstrate the effectiveness of S2TC-BDD, especially when the labeled texts are scarce.	4
----------
Recently, the retrieval models based on dense representations have been gradually applied in the first stage of the document retrieval tasks, showing better performance than traditional sparse vector space models.	1
To obtain high efficiency, the basic structure of these models is Bi-encoder in most cases.	1
However, this simple structure may cause serious information loss during the encoding of documents since the queries are agnostic.	1
To address this problem, we design a method to mimic the queries to each of the documents by an iterative clustering process and represent the documents by multiple pseudo queries (i.e., the cluster centroids).	2
To boost the retrieval process using approximate nearest neighbor search library, we also optimize the matching function with a two-step score calculation procedure.	3
Experimental results on several popular ranking and QA datasets show that our model can achieve state-of-the-art results while still remaining high efficiency.	4
----------
Learning high-quality sentence representations benefits a wide range of natural language processing tasks.	1
Though BERT-based pre-trained language models achieve high performance on many downstream tasks, the native derived sentence representations are proved to be collapsed and thus produce a poor performance on the semantic textual similarity (STS) tasks.	1
In this paper, we present ConSERT, a Contrastive Framework for Self-Supervised SEntence Representation Transfer, that adopts contrastive learning to fine-tune BERT in an unsupervised and effective way.	2
By making use of unlabeled texts, ConSERT solves the collapse issue of BERT-derived sentence representations and make them more applicable for downstream tasks.	3
Experiments on STS datasets demonstrate that ConSERT achieves an 8% relative improvement over the previous state-of-the-art, even comparable to the supervised SBERT-NLI.	4
And when further incorporating NLI supervision, we achieve new state-of-the-art performance on STS tasks.	4
Moreover, ConSERT obtains comparable results with only 1000 samples available, showing its robustness in data scarcity scenarios.	4
----------
Due to the great potential in facilitating software development, code generation has attracted increasing attention recently.	1
Generally, dominant models are Seq2Tree models, which convert the input natural language description into a sequence of tree-construction actions corresponding to the pre-order traversal of an Abstract Syntax Tree (AST).	1
However, such a traversal order may not be suitable for handling all multi-branch nodes.	1
In this paper, we propose to equip the Seq2Tree model with a context-based Branch Selector, which is able to dynamically determine optimal expansion orders of branches for multi-branch nodes.	2
Particularly, since the selection of expansion orders is a non-differentiable multi-step operation, we optimize the selector through reinforcement learning, and formulate the reward function as the difference of model losses obtained through different expansion orders.	3
Experimental results and in-depth analysis on several commonly-used datasets demonstrate the effectiveness and generality of our approach.	4
We have released our code at https://github.com/DeepLearnXMU/CG-RL.	6
----------
Despite recent successes of large pre-trained language models in solving reasoning tasks, their inference capabilities remain opaque.	1
We posit that such models can be made more interpretable by explicitly generating interim inference rules, and using them to guide the generation of task-specific textual outputs.	1
In this paper we present Coins, a recursive inference framework that i) iteratively reads context sentences, ii) dynamically generates contextualized inference rules, encodes them, and iii) uses them to guide task-specific output generation.	2
We apply to a Narrative Story Completion task that asks a model to complete a story with missing sentences, to produce a coherent story with plausible logical connections, causal relationships, and temporal dependencies.	3
By modularizing inference and sentence generation steps in a recurrent model, we aim to make reasoning steps and their effects on next sentence generation transparent.	3
Our automatic and manual evaluations show that the model generates better story sentences than SOTA baselines, especially in terms of coherence.	4
We further demonstrate improved performance over strong pre-trained LMs in generating commonsense inference rules.	4
The recursive nature of holds the potential for controlled generation of longer sequences.	4
----------
Procedural text understanding aims at tracking the states (e.g., create, move, destroy) and locations of the entities mentioned in a given paragraph.	1
To effectively track the states and locations, it is essential to capture the rich semantic relations between entities, actions, and locations in the paragraph.	1
Although recent works have achieved substantial progress, most of them focus on leveraging the inherent constraints or incorporating external knowledge for state prediction.	1
The rich semantic relations in the given paragraph are largely overlooked.	1
In this paper, we propose a novel approach (REAL) to procedural text understanding, where we build a general framework to systematically model the entity-entity, entity-action, and entity-location relations using a graph neural network.	2
We further develop algorithms for graph construction, representation learning, and state and location tracking.	3
We evaluate the proposed approach on two benchmark datasets, ProPara, and Recipes.	3
The experimental results show that our method outperforms strong baselines by a large margin, i.e., 5.0% on ProPara and 3.2% on Recipes, illustrating the utility of semantic relations and the effectiveness of the graph-based reasoning model.	4
----------
Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms.	1
In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding.	2
Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation.	3
During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly.	3
Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.	4
----------
Despite the well-developed cut-edge representation learning for language, most language representation models usually focus on specific levels of linguistic units.	1
This work introduces universal language representation learning, i.e., embeddings of different levels of linguistic units or text with quite diverse lengths in a uniform vector space.	1
We propose the training objective MiSAD that utilizes meaningful n-grams extracted from large unlabeled corpus by a simple but effective algorithm for pre-trained language models.	2
Then we empirically verify that well designed pre-training scheme may effectively yield universal language representation, which will bring great convenience when handling multiple layers of linguistic objects in a unified way.	3
Especially, our model achieves the highest accuracy on analogy tasks in different language levels and significantly improves the performance on downstream tasks in the GLUE benchmark and a question answering dataset.	4
----------
Pre-trained language models (PrLMs) have demonstrated superior performance due to their strong ability to learn universal language representations from self-supervised pre-training.	1
However, even with the help of the powerful PrLMs, it is still challenging to effectively capture task-related knowledge from dialogue texts which are enriched by correlations among speaker-aware utterances.	1
In this work, we present SPIDER, Structural Pre-traIned DialoguE Reader, to capture dialogue exclusive features.	2
To simulate the dialogue-like features, we propose two training objectives in addition to the original LM objectives: 1) utterance order restoration, which predicts the order of the permuted utterances in dialogue context; 2) sentence backbone regularization, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets.	3
Experimental results on widely used dialogue benchmarks verify the effectiveness of the newly introduced self-supervised tasks.	4
----------
Pre-trained language models (PLMs) have achieved great success in natural language processing.	1
Most of PLMs follow the default setting of architecture hyper-parameters (e.g., the hidden dimension is a quarter of the intermediate dimension in feed-forward sub-networks) in BERT.	1
Few studies have been conducted to explore the design of architecture hyper-parameters in BERT, especially for the more efficient PLMs with tiny sizes, which are essential for practical deployment on resource-constrained devices.	1
In this paper, we adopt the one-shot Neural Architecture Search (NAS) to automatically search architecture hyper-parameters.	2
Specifically, we carefully design the techniques of one-shot learning and the search space to provide an adaptive and efficient development way of tiny PLMs for various latency constraints.	3
We name our method AutoTinyBERT and evaluate its effectiveness on the GLUE and SQuAD benchmarks.	2
The extensive experiments show that our method outperforms both the SOTA search-based baseline (NAS-BERT) and the SOTA distillation-based methods (such as DistilBERT, TinyBERT, MiniLM, and MobileBERT).	4
In addition, based on the obtained architectures, we propose a more efficient development method that is even faster than the development of a single PLM.	4
The source code and models will be publicly available upon publication.	6
----------
Due to recent pretrained multilingual representation models, it has become feasible to exploit labeled data from one language to train a cross-lingual model that can then be applied to multiple new languages.	1
In practice, however, we still face the problem of scarce labeled data, leading to subpar results.	1
In this paper, we propose a novel data augmentation strategy for better cross-lingual natural language inference by enriching the data to reflect more diversity in a semantically faithful way.	2
To this end, we propose two methods of training a generative model to induce synthesized examples, and then leverage the resulting data using an adversarial training regimen for more robustness.	2
In a series of detailed experiments, we show that this fruitful combination leads to substantial gains in cross-lingual inference.	4
----------
As high-quality labeled data is scarce, unsupervised sentence representation learning has attracted much attention.	1
In this paper, we propose a new framework with a two-branch Siamese Network which maximizes the similarity between two augmented views of each sentence.	2
Specifically, given one augmented view of the input sentence, the online network branch is trained by predicting the representation yielded by the target network of the same sentence under another augmented view.	3
Meanwhile, the target network branch is bootstrapped with a moving average of the online network.	3
The proposed method significantly outperforms other state-of-the-art unsupervised methods on semantic textual similarity (STS) and classification tasks.	4
It can be adopted as a post-training procedure to boost the performance of the supervised methods.	4
We further extend our method for learning multilingual sentence representations and demonstrate its effectiveness on cross-lingual STS tasks.	4
Our code is available at https://github.com/yanzhangnlp/BSL.	6
----------
Abductive reasoning aims at inferring the most plausible explanation for observed events, which would play critical roles in various NLP applications, such as reading comprehension and question answering.	1
To facilitate this task, a narrative text based abductive reasoning task 𝛼NLI is proposed, together with explorations about building reasoning framework using pretrained language models.	1
However, abundant event commonsense knowledge is not well exploited for this task.	1
To fill this gap, we propose a variational autoencoder based model ege-RoBERTa, which employs a latent variable to capture the necessary commonsense knowledge from event graph for guiding the abductive reasoning task.	2
Experimental results show that through learning the external event graph knowledge, our approach outperforms the baseline methods on the 𝛼NLI task.	4
----------
The uniform information density (UID) hypothesis, which posits that speakers behaving optimally tend to distribute information uniformly across a linguistic signal, has gained traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices.	1
In this work, we explore whether the UID hypothesis can be operationalized as an inductive bias for statistical language modeling.	2
Specifically, we augment the canonical MLE objective for training language models with a regularizer that encodes UID.	3
In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited.	3
Moreover, via an analysis of generated sequences, we find that UID-regularized language models have other desirable properties, e.g., they generate text that is more lexically diverse.	3
Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.	4
----------
Lately proposed Word Sense Disambiguation (WSD) systems have approached the estimated upper bound of the task on standard evaluation benchmarks.	1
However, these systems typically implement the disambiguation of words in a document almost independently, underutilizing sense and word dependency in context.	1
In this paper, we convert the nearly isolated decisions into interrelated ones by exposing senses in context when learning sense embeddings in a similarity-based Sense Aware Context Exploitation (SACE) architecture.	2
Meanwhile, we enhance the context embedding learning with selected sentences from the same document, rather than utilizing only the sentence where each ambiguous word appears.	3
Experiments on both English and multilingual WSD datasets have shown the effectiveness of our approach, surpassing previous state-of-the-art by large margins (3.7% and 1.2% respectively), especially on few-shot (14.3%) and zero-shot (35.9%) scenarios.	4
----------
Frame Identification (FI) is a fundamental and challenging task in frame semantic parsing.	1
The task aims to find the exact frame evoked by a target word in a given sentence.	1
It is generally regarded as a classification task in existing work, where frames are treated as discrete labels or represented using onehot embeddings.	1
However, the valuable knowledge about frames is neglected.	1
In this paper, we propose a Knowledge-Guided Frame Identification framework (KGFI) that integrates three types frame knowledge, including frame definitions, frame elements and frame-to-frame relations, to learn better frame representation, which guides the KGFI to jointly map target words and frames into the same embedding space and subsequently identify the best frame by calculating the dot-product similarity scores between the target word embedding and all of the frame embeddings.	2
The extensive experimental results demonstrate KGFI significantly outperforms the state-of-the-art methods on two benchmark datasets.	4
----------
The advent of contextual word embeddings — representations of words which incorporate semantic and syntactic information from their context—has led to tremendous improvements on a wide variety of NLP tasks.	1
However, recent contextual models have prohibitively high computational cost in many use-cases and are often hard to interpret.	1
In this work, we demonstrate that our proposed distillation method, which is a simple extension of CBOW-based training, allows to significantly improve computational efficiency of NLP applications, while outperforming the quality of existing static embeddings trained from scratch as well as those distilled from previously proposed methods.	4
As a side-effect, our approach also allows a fair comparison of both contextual and static embeddings via standard lexical evaluation tasks.	4
----------
A critical challenge faced by supervised word sense disambiguation (WSD) is the lack of large annotated datasets with sufficient coverage of words in their diversity of senses.	1
This inspired recent research on few-shot WSD using meta-learning.	1
While such work has successfully applied meta-learning to learn new word senses from very few examples, its performance still lags behind its fully-supervised counterpart.	1
Aiming to further close this gap, we propose a model of semantic memory for WSD in a meta-learning setting.	2
Semantic memory encapsulates prior experiences seen throughout the lifetime of the model, which aids better generalization in limited data settings.	3
Our model is based on hierarchical variational inference and incorporates an adaptive memory update rule via a hypernetwork.	3
We show our model advances the state of the art in few-shot WSD, supports effective learning in extremely data scarce (e.g. one-shot) scenarios and produces meaning prototypes that capture similar senses of distinct words.	4
----------
Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters.	1
Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal “decontextualized” word encoders even when fed input words “in isolation” (i.e., without any context).	4
Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LexFit) based on dual-encoder network structures.	2
Further, we show that LexFit can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without any readily available external knowledge.	4
Our evaluation over four established, structurally different lexical-level tasks in 8 languages indicates the superiority of LexFit-based WEs over standard static WEs (e.g., fastText) and WEs from vanilla LMs.	4
Other extensive experiments and ablation studies further profile the LexFit framework, and indicate best practices and performance variations across LexFit variants, languages, and lexical tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models.	5
----------
In this paper we present the first model for directly synthesizing fluent, natural-sounding spoken audio captions for images that does not require natural language text as an intermediate representation or source of supervision.	2
Instead, we connect the image captioning module and the speech synthesis module with a set of discrete, sub-word speech units that are discovered with a self-supervised visual grounding task.	3
We conduct experiments on the Flickr8k spoken caption dataset in addition to a novel corpus of spoken audio captions collected for the popular MSCOCO dataset, demonstrating that our generated captions also capture diverse visual semantics of the images they describe.	3
We investigate several different intermediate speech representations, and empirically find that the representation must satisfy several important properties to serve as drop-in replacements for text.	4
----------
Multimodal sentiment analysis is the challenging research area that attends to the fusion of multiple heterogeneous modalities.	1
The main challenge is the occurrence of some missing modalities during the multimodal fusion procedure.	1
However, the existing techniques require all modalities as input, thus are sensitive to missing modalities at predicting time.	1
In this work, the coupled-translation fusion network (CTFN) is firstly proposed to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities.	2
Specifically, the cyclic consistency constraint is presented to improve the translation performance, allowing us directly to discard decoder and only embraces encoder of Transformer.	3
This could contribute to a much lighter model.	3
Due to the couple learning, CTFN is able to conduct bi-direction cross-modality intercorrelation parallelly.	3
Based on CTFN, a hierarchical architecture is further established to exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods.	3
Moreover, the convolution block is utilized to further highlight explicit interactions among those translations.	3
For evaluation, CTFN was verified on two multimodal benchmarks with extensive ablation studies.	3
The experiments demonstrate that the proposed framework achieves state-of-the-art or often competitive performance.	4
Additionally, CTFN still maintains robustness when considering missing modality.	4
----------
In this work, we demonstrate that the contextualized word vectors derived from pretrained masked language model-based encoders share a common, perhaps undesirable pattern across layers.	2
Namely, we find cases of persistent outlier neurons within BERT and RoBERTa’s hidden state vectors that consistently bear the smallest or largest values in said vectors.	3
In an attempt to investigate the source of this information, we introduce a neuron-level analysis method, which reveals that the outliers are closely related to information captured by positional embeddings.	2
We also pre-train the RoBERTa-base models from scratch and find that the outliers disappear without using positional embeddings.	3
These outliers, we find, are the major cause of anisotropy of encoders’ raw vector spaces, and clipping them leads to increased similarity across vectors.	3
We demonstrate this in practice by showing that clipped vectors can more accurately distinguish word senses, as well as lead to better sentence embeddings when mean pooling.	4
In three supervised tasks, we find that clipping does not affect the performance.	4
----------
We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language.	2
To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained.	3
We provide a framework–paired with significance tests–for evaluating the fit of language models to these trends.	2
We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present).	4
Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy.	4
As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type–token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.	4
----------
The importance of explaining the outcome of a machine learning model, especially a black-box model, is widely acknowledged.	1
Recent approaches explain an outcome by identifying the contributions of input features to this outcome.	1
In environments involving large black-box models or complex inputs, this leads to computationally demanding algorithms.	1
Further, these algorithms often suffer from low stability, with explanations varying significantly across similar examples.	1
In this paper, we propose a Learning to Explain (L2E) approach that learns the behaviour of an underlying explanation algorithm simultaneously from all training examples.	2
Once the explanation algorithm is distilled into an explainer network, it can be used to explain new instances.	3
Our experiments on three classification tasks, which compare our approach to six explanation algorithms, show that L2E is between 5 and 7.5×10ˆ4 times faster than these algorithms, while generating more stable explanations, and having comparable faithfulness to the black-box model.	4
----------
Deep learning models have achieved great success on the task of Natural Language Inference (NLI), though only a few attempts try to explain their behaviors.	1
Existing explanation methods usually pick prominent features such as words or phrases from the input text.	1
However, for NLI, alignments among words or phrases are more enlightening clues to explain the model.	1
To this end, this paper presents AREC, a post-hoc approach to generate alignment rationale explanations for co-attention based models in NLI.	2
The explanation is based on feature selection, which keeps few but sufficient alignments while maintaining the same prediction of the target model.	3
Experimental results show that our method is more faithful and human-readable compared with many existing approaches.	4
We further study and re-evaluate three typical models through our explanation beyond accuracy, and propose a simple method that greatly improves the model robustness.	4
----------
This paper presents a novel pre-trained language models (PLM) compression approach based on the matrix product operator (short as MPO) from quantum many-body physics.	2
It can decompose an original matrix into central tensors (containing the core information) and auxiliary tensors (with only a small proportion of parameters).	3
With the decomposed MPO structure, we propose a novel fine-tuning strategy by only updating the parameters from the auxiliary tensors, and design an optimization algorithm for MPO-based approximation over stacked network architectures.	3
Our approach can be applied to the original or the compressed PLMs in a general way, which derives a lighter network and significantly reduces the parameters to be fine-tuned.	4
Extensive experiments have demonstrated the effectiveness of the proposed approach in model compression, especially the reduction in fine-tuning parameters (91% reduction on average).	4
The code to reproduce the results of this paper can be found at https://github.com/RUCAIBox/MPOP.	6
----------
In the recent advances of natural language processing, the scale of the state-of-the-art models and datasets is usually extensive, which challenges the application of sample-based explanation methods in many aspects, such as explanation interpretability, efficiency, and faithfulness.	1
In this work, for the first time, we can improve the interpretability of explanations by allowing arbitrary text sequences as the explanation unit.	2
On top of this, we implement a hessian-free method with a model faithfulness guarantee.	2
Finally, to compare our method with the others, we propose a semantic-based evaluation metric that can better align with humans’ judgment of explanations than the widely adopted diagnostic or re-training measures.	2
The empirical results on multiple real data sets demonstrate the proposed method’s superior performance to popular explanation techniques such as Influence Function or TracIn on semantic evaluation.	4
----------
We study the problem of leveraging the syntactic structure of text to enhance pre-trained models such as BERT and RoBERTa.	1
Existing methods utilize syntax of text either in the pre-training stage or in the fine-tuning stage, so that they suffer from discrepancy between the two stages.	1
Such a problem would lead to the necessity of having human-annotated syntactic information, which limits the application of existing methods to broader scenarios.	1
To address this, we present a model that utilizes the syntax of text in both pre-training and fine-tuning stages.	2
Our model is based on Transformer with a syntax-aware attention layer that considers the dependency tree of the text.	3
We further introduce a new pre-training task of predicting the syntactic distance among tokens in the dependency tree.	2
We evaluate the model on three downstream tasks, including relation classification, entity typing, and question answering.	3
Results show that our model achieves state-of-the-art performance on six public benchmark datasets.	4
We have two major findings.	4
First, we demonstrate that infusing automatically produced syntax of text improves pre-trained models.	4
Second, global syntactic distances among tokens bring larger performance gains compared to local head relations between contiguous tokens.	4
----------
Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge of source domain to the unlabeled target domain.	1
Existing methods typically require to learn to adapt the target model by exploiting the source data and sharing the network architecture across domains.	1
However, this pipeline makes the source data risky and is inflexible for deploying the target model.	1
This paper tackles a novel setting where only a trained source model is available and different network architectures can be adapted for target domain in terms of deployment environments.	2
We propose a generic framework named Cross-domain Knowledge Distillation (CdKD) without needing any source data.	2
CdKD matches the joint distributions between a trained source model and a set of target data during distilling the knowledge from the source model to the target domain.	3
As a type of important knowledge in the source domain, for the first time, the gradient information is exploited to boost the transfer performance.	3
Experiments on cross-domain text classification demonstrate that CdKD achieves superior performance, which verifies the effectiveness in this novel setting.	4
----------
Today’s text classifiers inevitably suffer from unintended dataset biases, especially the document-level label bias and word-level keyword bias, which may hurt models’ generalization.	1
Many previous studies employed data-level manipulations or model-level balancing mechanisms to recover unbiased distributions and thus prevent models from capturing the two types of biases.	1
Unfortunately, they either suffer from the extra cost of data collection/selection/annotation or need an elaborate design of balancing strategies.	1
Different from traditional factual inference in which debiasing occurs before or during training, counterfactual inference mitigates the influence brought by unintended confounders after training, which can make unbiased decisions with biased observations.	1
Inspired by this, we propose a model-agnostic text classification debiasing framework – Corsair, which can effectively avoid employing data manipulations or designing balancing mechanisms.	2
Concretely, Corsair first trains a base model on a training set directly, allowing the dataset biases ‘poison’ the trained model.	3
In inference, given a factual input document, Corsair imagines its two counterfactual counterparts to distill and mitigate the two biases captured by the poisonous model.	4
Extensive experiments demonstrate Corsair’s effectiveness, generalizability and fairness.	4
----------
User interest modeling is critical for personalized news recommendation.	1
Existing news recommendation methods usually learn a single user embedding for each user from their previous behaviors to represent their overall interest.	1
However, user interest is usually diverse and multi-grained, which is difficult to be accurately modeled by a single user embedding.	1
In this paper, we propose a news recommendation method with hierarchical user interest modeling, named HieRec.	2
Instead of a single user embedding, in our method each user is represented in a hierarchical interest tree to better capture their diverse and multi-grained interest in news.	3
We use a three-level hierarchy to represent 1) overall user interest; 2) user interest in coarse-grained topics like sports; and 3) user interest in fine-grained topics like football.	2
Moreover, we propose a hierarchical user interest matching framework to match candidate news with different levels of user interest for more accurate user interest targeting.	2
Extensive experiments on two real-world datasets validate our method can effectively improve the performance of user modeling for personalized news recommendation.	4
----------
Personalized news recommendation methods are widely used in online news services.	1
These methods usually recommend news based on the matching between news content and user interest inferred from historical behaviors.	1
However, these methods usually have difficulties in making accurate recommendations to cold-start users, and tend to recommend similar news with those users have read.	1
In general, popular news usually contain important information and can attract users with different interests.	1
Besides, they are usually diverse in content and topic.	1
Thus, in this paper we propose to incorporate news popularity information to alleviate the cold-start and diversity problems for personalized news recommendation.	2
In our method, the ranking score for recommending a candidate news to a target user is the combination of a personalized matching score and a news popularity score.	3
The former is used to capture the personalized user interest in news.	3
The latter is used to measure time-aware popularity of candidate news, which is predicted based on news content, recency, and real-time CTR using a unified framework.	3
Besides, we propose a popularity-aware user encoder to eliminate the popularity bias in user behaviors for accurate interest modeling.	2
Experiments on two real-world datasets show our method can effectively improve the accuracy and diversity for news recommendation.	4
----------
False claims that have been previously fact-checked can still spread on social media.	1
To mitigate their continual spread, detecting previously fact-checked claims is indispensable.	1
Given a claim, existing works focus on providing evidence for detection by reranking candidate fact-checking articles (FC-articles) retrieved by BM25.	1
However, these performances may be limited because they ignore the following characteristics of FC-articles: (1) claims are often quoted to describe the checked events, providing lexical information besides semantics; (2) sentence templates to introduce or debunk claims are common across articles, providing pattern information.	1
Models that ignore the two aspects only leverage semantic relevance and may be misled by sentences that describe similar but irrelevant events.	1
In this paper, we propose a novel reranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles using key sentences selected with event (lexical and semantic) and pattern information.	2
For event information, we propose a ROUGE-guided Transformer which is finetuned with regression of ROUGE.	2
For pattern information, we generate pattern vectors for matching with sentences.	3
By fusing event and pattern information, we select key sentences to represent an article and then predict if the article fact-checks the given claim using the claim, key sentences, and patterns.	3
Experiments on two real-world datasets show that MTM outperforms existing methods.	4
Human evaluation proves that MTM can capture key sentences for explanations.	4
----------
Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples.	1
We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks.	2
During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data.	3
In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data.	3
DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications.	3
Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.	4
----------
Increasing the input length has been a driver of progress in language modeling with transformers.	1
We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length.	2
First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity.	3+4
Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once.	3+4
Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results.	3+4
We show that these recurrent models also benefit from short input lengths.	4
Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters.	4
----------
Task variance regularization, which can be used to improve the generalization of Multi-task Learning (MTL) models, remains unexplored in multi-task text classification.	1
Accordingly, to fill this gap, this paper investigates how the task might be effectively regularized, and consequently proposes a multi-task learning method based on adversarial multi-armed bandit.	2
The proposed method, named BanditMTL, regularizes the task variance by means of a mirror gradient ascent-descent algorithm.	3
Adopting BanditMTL in the multi-task text classification context is found to achieve state-of-the-art performance.	3+4
The results of extensive experiments back up our theoretical analysis and validate the superiority of our proposals.	4
----------
In knowledge graph embedding, the theoretical relationship between the softmax cross-entropy and negative sampling loss functions has not been investigated.	1
This makes it difficult to fairly compare the results of the two different loss functions.	1
We attempted to solve this problem by using the Bregman divergence to provide a unified interpretation of the softmax cross-entropy and negative sampling loss functions.	2
Under this interpretation, we can derive theoretical findings for fair comparison.	4
Experimental results on the FB15k-237 and WN18RR datasets show that the theoretical findings are valid in practical settings.	4
----------
Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables.	1
The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text.	1
The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table x and the sentence y.	1
Specifically, in the training stage, a model can get a low empirical loss without understanding x and use spurious statistical cues instead.	1
In this paper, we propose a de-confounded variational encoder-decoder (DCVED) based on causal intervention, learning the objective p(y|do(x)).	2
Firstly, we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl’s do-calculus to alleviate the spurious correlations.	3
Secondly, to make the latent confounder meaningful, we propose a back-prediction process to predict the not-used entities but linguistically similar to the exactly selected ones.	3
Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best candidate sentence for the given table.	3
An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity.	4
----------
Recent researches have shown that large natural language processing (NLP) models are vulnerable to a kind of security threat called the Backdoor Attack.	1
Backdoor attacked models can achieve good performance on clean test sets but perform badly on those input sentences injected with designed trigger words.	1
In this work, we point out a potential problem of current backdoor attacking research: its evaluation ignores the stealthiness of backdoor attacks, and most of existing backdoor attacking methods are not stealthy either to system deployers or to system users.	2
To address this issue, we first propose two additional stealthiness-based metrics to make the backdoor attacking evaluation more credible.	2
We further propose a novel word-based backdoor attacking method based on negative data augmentation and modifying word embeddings, making an important step towards achieving stealthy backdoor attacking.	2
Experiments on sentiment analysis and toxic detection tasks show that our method is much stealthier while maintaining pretty good attacking performance.	4
Our code is available at https://github.com/lancopku/SOS.	6
----------
Crowdsourcing is regarded as one prospective solution for effective supervised learning, aiming to build large-scale annotated training data by crowd workers.	1
Previous studies focus on reducing the influences from the noises of the crowdsourced annotations for supervised models.	1
We take a different point in this work, regarding all crowdsourced annotations as gold-standard with respect to the individual annotators.	2
In this way, we find that crowdsourcing could be highly similar to domain adaptation, and then the recent advances of cross-domain methods can be almost directly applied to crowdsourcing.	2
Here we take named entity recognition (NER) as a study case, suggesting an annotator-aware representation learning model that inspired by the domain adaptation methods which attempt to capture effective domain-aware features.	2+3
We investigate both unsupervised and supervised crowdsourcing learning, assuming that no or only small-scale expert annotations are available.	3
Experimental results on a benchmark crowdsourced NER dataset show that our method is highly effective, leading to a new state-of-the-art performance.	4
In addition, under the supervised setting, we can achieve impressive performance gains with only a very small scale of expert annotations.	4
----------
Recent studies strive to incorporate various human rationales into neural networks to improve model performance, but few pay attention to the quality of the rationales.	1
Most existing methods distribute their models’ focus to distantly-labeled rationale words entirely and equally, while ignoring the potential important non-rationale words and not distinguishing the importance of different rationale words.	1
In this paper, we propose two novel auxiliary loss functions to make better use of distantly-labeled rationales, which encourage models to maintain their focus on important words beyond labeled rationales (PINs) and alleviate redundant training on non-helpful rationales (NoIRs).	2
Experiments on two representative classification tasks show that our proposed methods can push a classification model to effectively learn crucial clues from non-perfect rationales while maintaining the ability to spread its focus to other unlabeled important words, thus significantly outperform existing methods.	4
----------
QA models based on pretrained language models have achieved remarkable performance on various benchmark datasets.	1
However, QA models do not generalize well to unseen data that falls outside the training distribution, due to distributional shifts.	1
Data augmentation (DA) techniques which drop/replace words have shown to be effective in regularizing the model from overfitting to the training data.	1
Yet, they may adversely affect the QA tasks since they incur semantic changes that may lead to wrong answers for the QA task.	1
To tackle this problem, we propose a simple yet effective DA method based on a stochastic noise generator, which learns to perturb the word embedding of the input questions and context without changing their semantics.	2
We validate the performance of the QA models trained with our word embedding perturbation on a single source dataset, on five different target domains.	3
The results show that our method significantly outperforms the baseline DA methods.	4
Notably, the model trained with ours outperforms the model trained with more than 240K artificially generated QA pairs.	4
----------
Arguably, the visual perception of conversational agents to the physical world is a key way for them to exhibit the human-like intelligence.	1
Image-grounded conversation is thus proposed to address this challenge.	1
Existing works focus on exploring the multimodal dialog models that ground the conversation on a given image.	1
In this paper, we take a step further to study image-grounded conversation under a fully open-ended setting where no paired dialog and image are assumed available.	2
Specifically, we present Maria, a neural conversation agent powered by the visual world experiences which are retrieved from a large-scale image index.	2
Maria consists of three flexible components, i.e., text-to-image retriever, visual concept detector and visual-knowledge-grounded response generator.	3
The retriever aims to retrieve a correlated image to the dialog from an image index, while the visual concept detector extracts rich visual knowledge from the image.	3
Then, the response generator is grounded on the extracted visual knowledge and dialog context to generate the target response.	3
Extensive experiments demonstrate Maria outperforms previous state-of-the-art methods on automatic metrics and human evaluation, and can generate informative responses that have some visual commonsense of the physical world.	4
----------
Conversational dialogue systems (CDSs) are hard to evaluate due to the complexity of natural language.	1
Automatic evaluation of dialogues often shows insufficient correlation with human judgements.	1
Human evaluation is reliable but labor-intensive.	1
We introduce a human-machine collaborative framework, HMCEval, that can guarantee reliability of the evaluation outcomes with reduced human effort.	2
HMCEval casts dialogue evaluation as a sample assignment problem, where we need to decide to assign a sample to a human or a machine for evaluation.	3
HMCEval includes a model confidence estimation module to estimate the confidence of the predicted sample assignment, and a human effort estimation module to estimate the human effort should the sample be assigned to human evaluation, as well as a sample assignment execution module that finds the optimum assignment solution based on the estimated confidence and effort.	4
We assess the performance of HMCEval on the task of evaluating malevolence in dialogues.	3
The experimental results show that HMCEval achieves around 99% evaluation accuracy with half of the human effort spared, showing that HMCEval provides reliable evaluation outcomes while reducing human effort by a large amount.	4
----------
Conditional Variational AutoEncoder (CVAE) effectively increases the diversity and informativeness of responses in open-ended dialogue generation tasks through enriching the context vector with sampled latent variables.	1
However, due to the inherent one-to-many and many-to-one phenomena in human dialogues, the sampled latent variables may not correctly reflect the contexts’ semantics, leading to irrelevant and incoherent generated responses.	1
To resolve this problem, we propose Self-separated Conditional Variational AutoEncoder (abbreviated as SepaCVAE) that introduces group information to regularize the latent variables, which enhances CVAE by improving the responses’ relevance and coherence while maintaining their diversity and informativeness.	2
SepaCVAE actively divides the input data into groups, and then widens the absolute difference between data pairs from distinct groups, while narrowing the relative distance between data pairs in the same group.	3
Empirical results from automatic evaluation and detailed analysis demonstrate that SepaCVAE can significantly boost responses in well-established open-domain dialogue datasets.	4
----------
Conversational Question Simplification (CQS) aims to simplify self-contained questions into conversational ones by incorporating some conversational characteristics, e.g., anaphora and ellipsis.	1
Existing maximum likelihood estimation based methods often get trapped in easily learned tokens as all tokens are treated equally during training.	1
In this work, we introduce a Reinforcement Iterative Sequence Editing (RISE) framework that optimizes the minimum Levenshtein distance through explicit editing actions.	2
RISE is able to pay attention to tokens that are related to conversational characteristics.	3
To train RISE, we devise an Iterative Reinforce Training (IRT) algorithm with a Dynamic Programming based Sampling (DPS) process to improve exploration.	2
Experimental results on two benchmark datasets show that RISE significantly outperforms state-of-the-art methods and generalizes well on unseen data.	4
----------
Emotion recognition in conversation (ERC) is a crucial component in affective dialogue systems, which helps the system understand users’ emotions and generate empathetic responses.	1
However, most works focus on modeling speaker and contextual information primarily on the textual modality or simply leveraging multimodal information through feature concatenation.	1
In order to explore a more effective way of utilizing both multimodal and long-distance contextual information, we propose a new model based on multimodal fused graph convolutional network, MMGCN, in this work.	2
MMGCN can not only make use of multimodal dependencies effectively, but also leverage speaker information to model inter-speaker and intra-speaker dependency.	3
We evaluate our proposed model on two public benchmark datasets, IEMOCAP and MELD, and the results prove the effectiveness of MMGCN, which outperforms other SOTA methods by a significant margin under the multimodal conversation setting.	4
----------
A dialogue is essentially a multi-turn interaction among interlocutors.	1
Effective evaluation metrics should reflect the dynamics of such interaction.	1
Existing automatic metrics are focused very much on the turn-level quality, while ignoring such dynamics.	1
To this end, we propose DynaEval, a unified automatic evaluation framework which is not only capable of performing turn-level evaluation, but also holistically considers the quality of the entire dialogue.	2
In DynaEval, the graph convolutional network (GCN) is adopted to model a dialogue in totality, where the graph nodes denote each individual utterance and the edges represent the dependency between pairs of utterances.	3
A contrastive loss is then applied to distinguish well-formed dialogues from carefully constructed negative samples.	3
Experiments show that DynaEval significantly outperforms the state-of-the-art dialogue coherence model, and correlates strongly with human judgements across multiple dialogue evaluation aspects at both turn and dialogue level.	4
----------
A few approaches have been developed to improve neural machine translation (NMT) models with multiple passes of decoding.	1
However, their performance gains are limited because of lacking proper policies to terminate the multi-pass process.	1
To address this issue, we introduce a novel architecture of Rewriter-Evaluator.	3
Translating a source sentence involves multiple rewriting passes.	3
In every pass, a rewriter generates a new translation to improve the past translation.	3
Termination of this multi-pass process is determined by a score of translation quality estimated by an evaluator.	3
We also propose prioritized gradient descent (PGD) to jointly and efficiently train the rewriter and the evaluator.	2
Extensive experiments on three machine translation tasks show that our architecture notably improves the performances of NMT models and significantly outperforms prior methods.	4
An oracle experiment reveals that it can largely reduce performance gaps to the oracle policy.	4
Experiments confirm that the evaluator trained with PGD is more accurate than prior methods in determining proper numbers of rewriting.	4
----------
Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages.	1
However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge.	1
Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design.	1
To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages.	2
The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages.	3
Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.	4
----------
Multi-source sequence generation (MSG) is an important kind of sequence generation tasks that takes multiple sources, including automatic post-editing, multi-source translation, multi-document summarization, etc.	1
As MSG tasks suffer from the data scarcity problem and recent pretrained models have been proven to be effective for low-resource downstream tasks, transferring pretrained sequence-to-sequence models to MSG tasks is essential.	1
Although directly finetuning pretrained models on MSG tasks and concatenating multiple sources into a single long sequence is regarded as a simple method to transfer pretrained models to MSG tasks, we conjecture that the direct finetuning method leads to catastrophic forgetting and solely relying on pretrained self-attention layers to capture cross-source information is not sufficient.	1
Therefore, we propose a two-stage finetuning method to alleviate the pretrain-finetune discrepancy and introduce a novel MSG model with a fine encoder to learn better representations in MSG tasks.	2
Experiments show that our approach achieves new state-of-the-art results on the WMT17 APE task and multi-source translation task using the WMT14 test set.	4
When adapted to document-level translation, our framework outperforms strong baselines significantly.	4
----------
One of the main bottlenecks in developing discourse dependency parsers is the lack of annotated training data.	1
A potential solution is to utilize abundant unlabeled data by using unsupervised techniques, but there is so far little research in unsupervised discourse dependency parsing.	1
Fortunately, unsupervised syntactic dependency parsing has been studied by decades, which could potentially be adapted for discourse parsing.	1
In this paper, we propose a simple yet effective method to adapt unsupervised syntactic dependency parsing methodology for unsupervised discourse dependency parsing.	2
We apply the method to adapt two state-of-the-art unsupervised syntactic dependency parsing methods.	3
Experimental results demonstrate that our adaptation is effective.	4
Moreover, we extend the adapted methods to the semi-supervised and supervised setting and surprisingly, we find that they outperform previous methods specially designed for supervised discourse parsing.	4
Further analysis shows our adaptations result in superiority not only in parsing accuracy but also in time and space efficiency.	4
----------
We introduce a generic seq2seq parsing framework that casts constituency parsing problems (syntactic and discourse parsing) into a series of conditional splitting decisions.	2
Our parsing model estimates the conditional probability distribution of possible splitting points in a given text span and supports efficient top-down decoding, which is linear in number of nodes.	3
The conditional splitting formulation together with efficient beam search inference facilitate structural consistency without relying on expensive structured inference.	3
Crucially, for discourse analysis we show that in our formulation, discourse segmentation can be framed as a special case of parsing which allows us to perform discourse parsing without requiring segmentation as a pre-requisite.	3
Experiments show that our model achieves good results on the standard syntactic parsing tasks under settings with/without pre-trained representations and rivals state-of-the-art (SoTA) methods that are more computationally expensive than ours.	4
In discourse parsing, our method outperforms SoTA by a good margin.	4
----------
Unlike English letters, Chinese characters have rich and specific meanings.	1
Usually, the meaning of a word can be derived from its constituent characters in some way.	1
Several previous works on syntactic parsing propose to annotate shallow word-internal structures for better utilizing character-level information.	1
This work proposes to model the deep internal structures of Chinese words as dependency trees with 11 labels for distinguishing syntactic relationships.	2
First, based on newly compiled annotation guidelines, we manually annotate a word-internal structure treebank (WIST) consisting of over 30K multi-char words from Chinese Penn Treebank.	3
To guarantee quality, each word is independently annotated by two annotators and inconsistencies are handled by a third senior annotator.	3
Second, we present detailed and interesting analysis on WIST to reveal insights on Chinese word formation.	3
Third, we propose word-internal structure parsing as a new task, and conduct benchmark experiments using a competitive dependency parser.	3
Finally, we present two simple ways to encode word-internal structures, leading to promising gains on the sentence-level syntactic parsing task.	3+4
----------
Named Entity Recognition (NER) for low-resource languages is a both practical and challenging research problem.	1
This paper addresses zero-shot transfer for cross-lingual NER, especially when the amount of source-language training data is also limited.	2
The paper first proposes a simple but effective labeled sequence translation method to translate source-language training data to target languages and avoids problems such as word order change and entity span determination.	2
With the source-language data as well as the translated data, a generation-based multilingual data augmentation method is introduced to further increase diversity by generating synthetic labeled data in multiple languages.	3
These augmented data enable the language model based NER models to generalize better with both the language-specific features from the target-language synthetic data and the language-independent features from multilingual synthetic data.	4
An extensive set of experiments were conducted to demonstrate encouraging cross-lingual transfer performance of the new research on a wide variety of target languages.	3
----------
Lexicon information and pre-trained models, such as BERT, have been combined to explore Chinese sequence labeling tasks due to their respective strengths.	1
However, existing methods solely fuse lexicon features via a shallow and random initialized sequence layer and do not integrate them into the bottom layers of BERT.	1
In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese sequence labeling, which integrates external lexicon knowledge into BERT layers directly by a Lexicon Adapter layer.	2
Compared with existing methods, our model facilitates deep lexicon knowledge fusion at the lower layers of BERT.	3
Experiments on ten Chinese datasets of three tasks including Named Entity Recognition, Word Segmentation, and Part-of-Speech Tagging, show that LEBERT achieves state-of-the-art results.	4
----------
In recent years, math word problem solving has received considerable attention and achieved promising results, but previous methods rarely take numerical values into consideration.	1
Most methods treat the numerical values in the problems as number symbols, and ignore the prominent role of the numerical values in solving the problem.	1
In this paper, we propose a novel approach called NumS2T, which enhances math word problem solving performance by explicitly incorporating numerical values into a sequence-to-tree network.	2+3
In addition, a numerical properties prediction mechanism is used to capture the category and comparison information of numerals and measure their importance in global expressions.	3
Experimental results on the Math23K and APE datasets demonstrate that our model achieves better performance than existing state-of-the-art models.	4
----------
Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions.	1
Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks.	2
Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers.	3
Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: 	3
a) self-supervised number prediction task predicting both number quantity and number locations;	3
b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; 	3
c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; 	3
d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem’s part-of-speech generation to enhance the understanding ability of a solver.	3
Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples.	3
Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods.	4
----------
Recently, the performance of Pre-trained Language Models (PLMs) has been significantly improved by injecting knowledge facts to enhance their abilities of language understanding.	1
For medical domains, the background knowledge sources are especially useful, due to the massive medical terms and their complicated relations are difficult to understand in text.	1
In this work, we introduce SMedBERT, a medical PLM trained on large-scale medical corpora, incorporating deep structured semantic knowledge from neighbours of linked-entity.	2
In SMedBERT, the mention-neighbour hybrid attention is proposed to learn heterogeneous-entity information, which infuses the semantic representations of entity types into the homogeneous neighbouring entity structure.	3
Apart from knowledge integration as external features, we propose to employ the neighbors of linked-entities in the knowledge graph as additional global contexts of text mentions, allowing them to communicate via shared neighbors, thus enrich their semantic representations.	2+3
Experiments demonstrate that SMedBERT significantly outperforms strong baselines in various knowledge-intensive Chinese medical tasks.	4
It also improves the performance of other tasks such as question answering, question matching and natural language inference.	4
----------
When evaluating an article and the claims it makes, a critical reader must be able to assess where the information presented comes from, and whether the various claims are mutually consistent and support the conclusion.	1
This motivates the study of claim provenance, which seeks to trace and explain the origins of claims.	1
In this paper, we introduce new techniques to model and reason about the provenance of multiple interacting claims, including how to capture fine-grained information about the context.	2
Our solution hinges on first identifying the sentences that potentially contain important external information.	3
We then develop a query generator with our novel rank-aware cross attention mechanism, which aims at generating metadata for the source article, based on the context and the signals collected from a search engine.	3
This establishes relevant search queries, and it allows us to obtain source article candidates for each identified sentence and propose an ILP based algorithm to infer the best sources.	3
We experiment with a newly created evaluation dataset, Politi-Prov, based on fact-checking articles from www.politifact.com; our experimental results show that our solution leads to a significant improvement over baselines.	3+4
----------
Medical imaging plays a significant role in clinical practice of medical diagnosis, where the text reports of the images are essential in understanding them and facilitating later treatments.	1
By generating the reports automatically, it is beneficial to help lighten the burden of radiologists and significantly promote clinical automation, which already attracts much attention in applying artificial intelligence to medical domain.	1
Previous studies mainly follow the encoder-decoder paradigm and focus on the aspect of text generation, with few studies considering the importance of cross-modal mappings and explicitly exploit such mappings to facilitate radiology report generation.	1
In this paper, we propose a cross-modal memory networks (CMN) to enhance the encoder-decoder framework for radiology report generation, where a shared memory is designed to record the alignment between images and texts so as to facilitate the interaction and generation across modalities.	2
Experimental results illustrate the effectiveness of our proposed model, where state-of-the-art performance is achieved on two widely used benchmark datasets, i.e., IU X-Ray and MIMIC-CXR.	3
Further analyses also prove that our model is able to better align information from radiology images and texts so as to help generating more accurate reports in terms of clinical indicators.	4
----------
There is content such as hate speech, offensive, toxic or aggressive documents, which are perceived differently by their consumers.	1
They are commonly identified using classifiers solely based on textual content that generalize pre-agreed meanings of difficult problems.	1
Such models provide the same results for each user, which leads to high misclassification rate observable especially for contentious, aggressive documents.	1
Both document controversy and user nonconformity require new solutions.	1
Therefore, we propose novel personalized approaches that respect individual beliefs expressed by either user conformity-based measures or various embeddings of their previous text annotations.	2
We found that only a few annotations of most controversial documents are enough for all our personalization methods to significantly outperform classic, generalized solutions.	4
The more controversial the content, the greater the gain.	4
The personalized solutions may be used to efficiently filter unwanted aggressive content in the way adjusted to a given person.	4
----------
As more and more product reviews are posted in both text and images, Multimodal Review Analysis (MRA) becomes an attractive research topic.	1
Among the existing review analysis tasks, helpfulness prediction on review text has become predominant due to its importance for e-commerce platforms and online shops, i.e. helping customers quickly acquire useful product information.	1
This paper proposes a new task Multimodal Review Helpfulness Prediction (MRHP) aiming to analyze the review helpfulness from text and visual modalities.	2
Meanwhile, a novel Multi-perspective Coherent Reasoning method (MCR) is proposed to solve the MRHP task, which conducts joint reasoning over texts and images from both the product and the review, and aggregates the signals to predict the review helpfulness.	2
Concretely, we first propose a product-review coherent reasoning module to measure the intra- and inter-modal coherence between the target product and the review.	3
In addition, we also devise an intra-review coherent reasoning module to identify the coherence between the text content and images of the review, which is a piece of strong evidence for review helpfulness prediction.	3
To evaluate the effectiveness of MCR, we present two newly collected multimodal review datasets as benchmark evaluation resources for the MRHP task.	2
Experimental results show that our MCR method can lead to a performance increase of up to 8.5% as compared to the best performing text-only model.	4
The source code and datasets can be obtained from https://github.com/jhliu17/MCR.	6
----------
In this paper, we propose Shallow Aggressive Decoding (SAD) to improve the online inference efficiency of the Transformer for instantaneous Grammatical Error Correction (GEC).	2
SAD optimizes the online inference efficiency for GEC by two innovations: 1) it aggressively decodes as many tokens as possible in parallel instead of always decoding only one token in each step to improve computational parallelism; 2) it uses a shallow decoder instead of the conventional Transformer architecture with balanced encoder-decoder depth to reduce the computational cost during inference.	3
Experiments in both English and Chinese GEC benchmarks show that aggressive decoding could yield identical predictions to greedy decoding but with significant speedup for online inference.	3
Its combination with the shallow decoder could offer an even higher online inference speedup over the powerful Transformer baseline without quality loss.	3
Not only does our approach allow a single model to achieve the state-of-the-art results in English GEC benchmarks: 66.4 F0.5 in the CoNLL-14 and 72.9 F0.5 in the BEA-19 test set with an almost 10x online inference speedup over the Transformer-big model, but also it is easily adapted to other languages.	4
Our code is available at https://github.com/AutoTemp/Shallow-Aggressive-Decoding.	6
----------
The ICD coding task aims at assigning codes of the International Classification of Diseases in clinical notes.	1
Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.	1
However, existing works either ignore the long-tail of code frequency or the noisy clinical notes.	1
To address the above issues, we propose an Interactive Shared Representation Network with Self-Distillation Mechanism.	2
Specifically, an interactive shared representation network targets building connections among codes while modeling the co-occurrence, consequently alleviating the long-tail problem.	3
Moreover, to cope with the noisy text issue, we encourage the model to focus on the clinical note’s noteworthy part and extract valuable information through a self-distillation learning mechanism.	3
Experimental results on two MIMIC datasets demonstrate the effectiveness of our method.	4
----------
Chinese Spelling Check (CSC) is a challenging task due to the complex characteristics of Chinese characters.	1
Statistics reveal that most Chinese spelling errors belong to phonological or visual errors.	1
However, previous methods rarely utilize phonological and morphological knowledge of Chinese characters or heavily rely on external resources to model their similarities.	1
To address the above issues, we propose a novel end-to-end trainable model called PHMOSpell, which promotes the performance of CSC with multi-modal information.	2
Specifically, we derive pinyin and glyph representations for Chinese characters from audio and visual modalities respectively, which are integrated into a pre-trained language model by a well-designed adaptive gating mechanism.	3
To verify its effectiveness, we conduct comprehensive experiments and ablation tests.	3
Experimental results on three shared benchmarks demonstrate that our model consistently outperforms previous state-of-the-art models.	4
----------
This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels.	2
Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability.	1
In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions.	3
To this end, we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain.	2
A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method.	3
----------
Table-to-text generation aims at automatically generating natural text to help people conveniently obtain salient information in tables.	1
Although neural models for table-to-text have achieved remarkable progress, some problems are still overlooked.	1
Previous methods cannot deduce the factual results from the entity’s (player or team) performance and the relations between entities.	1
To solve this issue, we first build an entity graph from the input tables and introduce a reasoning module to perform reasoning on the graph.	3
Moreover, there are different relations (e.g., the numeric size relation and the importance relation) between records in different dimensions.	3
And these relations may contribute to the data-to-text generation.	3
However, it is hard for a vanilla encoder to capture these.	3
Consequently, we propose to utilize two auxiliary tasks, Number Ranking (NR) and Importance Ranking (IR), to supervise the encoder to capture the different relations.	2
Experimental results on ROTOWIRE and RW-FG show that our method not only has a good generalization but also outperforms previous methods on several metrics: BLEU, Content Selection, Content Ordering.	4
----------
The multimodality problem has become a major challenge of existing non-autoregressive generation (NAG) systems.	1
A common solution often resorts to sequence-level knowledge distillation by rebuilding the training dataset through autoregressive generation (hereinafter known as “teacher AG”).	1
The success of such methods may largely depend on a latent assumption, i.e., the teacher AG is superior to the NAG model.	1
However, in this work, we experimentally reveal that this assumption does not always hold for the text generation tasks like text summarization and story ending generation.	1
To provide a feasible solution to the multimodality problem of NAG, we propose incorporating linguistic structure (Part-of-Speech sequence in particular) into NAG inference instead of relying on teacher AG.	2
More specifically, the proposed POS-constrained Parallel Decoding (POSPD) method aims at providing a specific POS sequence to constrain the NAG model during decoding.	2
Our experiments demonstrate that POSPD consistently improves NAG models on four text generation tasks to a greater extent compared to knowledge distillation.	4
This observation validates the necessity of exploring the alternatives for sequence-level knowledge distillation.	5
----------
A well-known limitation in pretrain-finetune paradigm lies in its inflexibility caused by the one-size-fits-all vocabulary.This potentially weakens the effect when applying pretrained models into natural language generation (NLG) tasks, especially for the subword distributions between upstream and downstream tasks with significant discrepancy.	1
Towards approaching this problem, we extend the vanilla pretrain-finetune pipeline with an extra embedding transfer step.	2
Specifically, a plug-and-play embedding generator is introduced to produce the representation of any input token, according to pre-trained embeddings of its morphologically similar ones.	3
Thus, embeddings of mismatch tokens in downstream tasks can also be efficiently initialized.We conduct experiments on a variety of NLG tasks under the pretrain-finetune fashion.	3
Experimental results and extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models.	4
----------
Transformer-based models have achieved state-of-the-art results in a wide range of natural language processing (NLP) tasks including document summarization.	1
Typically these systems are trained by fine-tuning a large pre-trained model to the target task.	1
One issue with these transformer-based models is that they do not scale well in terms of memory and compute requirements as the input length grows.	1
Thus, for long document summarization, it can be challenging to train or fine-tune these models.	1
In this work, we exploit large pre-trained transformer-based models and address long-span dependencies in abstractive summarization using two methods: local self-attention; and explicit content selection.	2
These approaches are compared on a range of network configurations.	3
Experiments are carried out on standard long-span summarization tasks, including Spotify Podcast, arXiv, and PubMed datasets.	3
We demonstrate that by combining these methods, we can achieve state-of-the-art results on all three tasks in the ROUGE scores.	4
Moreover, without a large-scale GPU card, our approach can achieve comparable or better results than existing approaches.	4
----------
In the field of dialogue summarization, due to the lack of training data, it is often difficult for supervised summary generation methods to learn vital information from dialogue context with limited data.	1
Several attempts on unsupervised summarization for text by leveraging semantic information solely or auto-encoder strategy (i.e., sentence compression), it however cannot be adapted to the dialogue scene due to the limited words in utterances and huge gap between the dialogue and its summary.	1
In this study, we propose a novel unsupervised strategy to address this challenge, which roots from the hypothetical foundation that a superior summary approximates a replacement of the original dialogue, and they are roughly equivalent for auxiliary (self-supervised) tasks, e.g., dialogue generation.	2
The proposed strategy RepSum is applied to generate both extractive and abstractive summary with the guidance of the followed nˆth utterance generation and classification tasks.	3
Extensive experiments on various datasets demonstrate the superiority of the proposed model compared with the state-of-the-art methods.	4
----------
Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text.	1
In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases.	2
Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure.	2
Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary.	3
Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks.	4
----------
Given a set of related publications, related work section generation aims to provide researchers with an overview of the specific research area by summarizing these works and introducing them in a logical order.	1
Most of existing related work generation models follow the inflexible extractive style, which directly extract sentences from multiple original papers to form a related work discussion.	1
Hence, in this paper, we propose a Relation-aware Related work Generator (RRG), which generates an abstractive related work from the given multiple scientific papers in the same research area.	2
Concretely, we propose a relation-aware multi-document encoder that relates one document to another according to their content dependency in a relation graph.	2
The relation graph and the document representation are interacted and polished iteratively, complementing each other in the training process.	3
We also contribute two public datasets composed of related work sections and their corresponding papers.	2
Extensive experiments on the two datasets show that the proposed model brings substantial improvements over several strong baselines.	4
We hope that this work will promote advances in related work generation task.	5
----------
Professional summaries are written with document-level information, such as the theme of the document, in mind.	1
This is in contrast with most seq2seq decoders which simultaneously learn to focus on salient content, while deciding what to generate, at each decoding step.	1
With the motivation to narrow this gap, we introduce Focus Attention Mechanism, a simple yet effective method to encourage decoders to proactively generate tokens that are similar or topical to the input document.	2
Further, we propose a Focus Sampling method to enable generation of diverse summaries, an area currently understudied in summarization.	2
When evaluated on the BBC extreme summarization task, two state-of-the-art models augmented with Focus Attention generate summaries that are closer to the target and more faithful to their input documents, outperforming their vanilla counterparts on ROUGE and multiple faithfulness measures.	4
We also empirically demonstrate that Focus Sampling is more effective in generating diverse and faithful summaries than top-k or nucleus sampling-based decoding methods.	4
----------
The availability of large-scale datasets has driven the development of neural models that create generic summaries from single or multiple documents.	1
In this work we consider query focused summarization (QFS), a task for which training data in the form of queries, documents, and summaries is not readily available.	2
We propose to decompose QFS into (1) query modeling (i.e., finding supportive evidence within a set of documents for a query) and (2) conditional language modeling (i.e., summary generation).	2
We introduce MaRGE, a Masked ROUGE Regression framework for evidence estimation and ranking which relies on a unified representation for summaries and queries, so that summaries in generic data can be converted into proxy queries for learning a query model.	3
Experiments across QFS benchmarks and query types show that our model achieves state-of-the-art performance despite learning from weak supervision.	4
----------
This paper studies the bias problem of multi-hop question answering models, of answering correctly without correct reasoning.	2
One way to robustify these models is by supervising to not only answer right, but also with right reasoning chains.	1
An existing direction is to annotate reasoning chains to train models, requiring expensive additional annotations.	1
In contrast, we propose a new approach to learn evidentiality, deciding whether the answer prediction is supported by correct evidences, without such annotations.	2
Instead, we compare counterfactual changes in answer confidence with and without evidence sentences, to generate “pseudo-evidentiality” annotations.	3
We validate our proposed model on an original set and challenge set in HotpotQA, showing that our method is accurate and robust in multi-hop reasoning.	4
----------
Dense passage retrieval has been shown to be an effective approach for information retrieval tasks such as open domain question answering.	1
Under this paradigm, a dual-encoder model is learned to encode questions and passages separately into vector representations, and all the passage vectors are then pre-computed and indexed, which can be efficiently retrieved by vector space search during inference time.	1
In this paper, we propose a new contrastive learning method called Cross Momentum Contrastive learning (xMoCo), for learning a dual-encoder model for question-passage matching.	2
Our method efficiently maintains a large pool of negative samples like the original MoCo, and by jointly optimizing question-to-passage and passage-to-question matching tasks, enables using separate encoders for questions and passages.	3
We evaluate our method on various open-domain question answering dataset, and the experimental results show the effectiveness of the proposed method.	4
----------
One of the main challenges in conversational question answering (CQA) is to resolve the conversational dependency, such as anaphora and ellipsis.	1
However, existing approaches do not explicitly train QA models on how to resolve the dependency, and thus these models are limited in understanding human dialogues.	1
In this paper, we propose a novel framework, ExCorD (Explicit guidance on how to resolve Conversational Dependency) to enhance the abilities of QA models in comprehending conversational context.	2
ExCorD first generates self-contained questions that can be understood without the conversation history, then trains a QA model with the pairs of original and self-contained questions using a consistency-based regularizer.	3
In our experiments, we demonstrate that ExCorD significantly improves the QA models’ performance by up to 1.2 F1 on QuAC, and 5.2 F1 on CANARD, while addressing the limitations of the existing approaches.	4
----------
Video Question Answering is a task which requires an AI agent to answer questions grounded in video.	1
This task entails three key challenges: (1) understand the intention of various questions, (2) capturing various elements of the input video (e.g., object, action, causality), and (3) cross-modal grounding between language and vision information.	1
We propose Motion-Appearance Synergistic Networks (MASN), which embed two cross-modal features grounded on motion and appearance information and selectively utilize them depending on the question’s intentions.	2
MASN consists of a motion module, an appearance module, and a motion-appearance fusion module.	3
The motion module computes the action-oriented cross-modal joint representations, while the appearance module focuses on the appearance aspect of the input video.	3
Finally, the motion-appearance fusion module takes each output of the motion module and the appearance module as input, and performs question-guided fusion.	3
As a result, MASN achieves new state-of-the-art performance on the TGIF-QA and MSVD-QA datasets.	4
We also conduct qualitative analysis by visualizing the inference results of MASN.	3
----------
We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources.	1
Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model.	1
To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way.	2
CHMM enhances the classic hidden Markov model with the contextual representation power of pre-trained language models.	4
Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations.	3
We further refine CHMM with an alternate-training approach (CHMM-ALT).	3
It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER’s output is regarded as an additional weak source to train the CHMM in return.	3
Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.	4
----------
The journey of reducing noise from distant supervision (DS) generated training data has been started since the DS was first introduced into the relation extraction (RE) task.	1
For the past decade, researchers apply the multi-instance learning (MIL) framework to find the most reliable feature from a bag of sentences.	1
Although the pattern of MIL bags can greatly reduce DS noise, it fails to represent many other useful sentence features in the datasets.	1
In many cases, these sentence features can only be acquired by extra sentence-level human annotation with heavy costs.	1
Therefore, the performance of distantly supervised RE models is bounded.	1
In this paper, we go beyond typical MIL framework and propose a novel contrastive instance learning (CIL) framework.	2
Specifically, we regard the initial MIL as the relational triple encoder and constraint positive pairs against negative pairs for each instance.	3
Experiments demonstrate the effectiveness of our proposed framework, with significant improvements over the previous methods on NYT10, GDS and KBP.	4
----------
Distant supervision for relation extraction provides uniform bag labels for each sentence inside the bag, while accurate sentence labels are important for downstream applications that need the exact relation type.	1
Directly using bag labels for sentence-level training will introduce much noise, thus severely degrading performance.	1
In this work, we propose the use of negative training (NT), in which a model is trained using complementary labels regarding that “the instance does not belong to these complementary labels”.	2
Since the probability of selecting a true label as a complementary label is low, NT provides less noisy information.	3
Furthermore, the model trained with NT is able to separate the noisy data from the training data.	3
Based on NT, we propose a sentence-level framework, SENT, for distant relation extraction.	2
SENT not only filters the noisy data to construct a cleaner dataset, but also performs a re-labeling process to transform the noisy data into useful training data, thus further benefiting the model’s performance.	3
Experimental results show the significant improvement of the proposed method over previous methods on sentence-level evaluation and de-noise effect.	4
----------
Medical named entity recognition (NER) and normalization (NEN) are fundamental for constructing knowledge graphs and building QA systems.	1
Existing implementations for medical NER and NEN are suffered from the error propagation between the two tasks.	1
The mispredicted mentions from NER will directly influence the results of NEN.	1
Therefore, the NER module is the bottleneck of the whole system.	1
Besides, the learnable features for both tasks are beneficial to improving the model performance.	1
To avoid the disadvantages of existing models and exploit the generalized representation across the two tasks, we design an end-to-end progressive multi-task learning model for jointly modeling medical NER and NEN in an effective way.	2
There are three level tasks with progressive difficulty in the framework.	3
The progressive tasks can reduce the error propagation with the incremental task settings which implies the lower level tasks gain the supervised signals other than errors from the higher level tasks to improve their performances.	3
Besides, the context features are exploited to enrich the semantic information of entity mentions extracted by NER.	3
The performance of NEN profits from the enhanced entity mention features.	3
The standard entities from knowledge bases are introduced into the NER module for extracting corresponding entity mentions correctly.	3
The empirical results on two publicly available medical literature datasets demonstrate the superiority of our method over nine typical methods.	4
----------
Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction.	1
Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency.	1
In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC).	2
Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity.	3
Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples.	4
The source code has been submitted as the supplementary material and will be made publicly available after the blind review.	6
----------
Few-shot Named Entity Recognition (NER) exploits only a handful of annotations to iden- tify and classify named entity mentions.	1
Pro- totypical network shows superior performance on few-shot NER.	1
However, existing prototyp- ical methods fail to differentiate rich seman- tics in other-class words, which will aggravate overfitting under few shot scenario.	1
To address the issue, we propose a novel model, Mining Undefined Classes from Other-class (MUCO), that can automatically induce different unde- fined classes from the other class to improve few-shot NER.	2
With these extra-labeled unde- fined classes, our method will improve the dis- criminative ability of NER classifier and en- hance the understanding of predefined classes with stand-by semantic knowledge.	3
Experi- mental results demonstrate that our model out- performs five state-of-the-art models in both 1- shot and 5-shots settings on four NER bench- marks.	4
We will release the code upon acceptance.	6
The source code is released on https: //github.com/shuaiwa16/OtherClassNER.git.	6
----------
Compared to the general news domain, information extraction (IE) from biomedical text requires much broader domain knowledge.	1
However, many previous IE methods do not utilize any external knowledge during inference.	1
Due to the exponential growth of biomedical publications, models that do not go beyond their fixed set of parameters will likely fall behind.	1
Inspired by how humans look up relevant information to comprehend a scientific text, we present a novel framework that utilizes external knowledge for joint entity and relation extraction named KECI (Knowledge-Enhanced Collective Inference).	2
Given an input text, KECI first constructs an initial span graph representing its initial understanding of the text.	3
It then uses an entity linker to form a knowledge graph containing relevant background knowledge for the the entity mentions in the text.	3
To make the final predictions, KECI fuses the initial span graph and the knowledge graph into a more refined graph using an attention mechanism.	3
KECI takes a collective approach to link mention spans to entities by integrating global relational information into local representations using graph convolutional networks.	3
Our experimental results show that the framework is highly effective, achieving new state-of-the-art results in two different benchmark datasets: BioRelEx (binding interaction detection) and ADE (adverse drug event extraction).	4
For example, KECI achieves absolute improvements of 4.59% and 4.91% in F1 scores over the state-of-the-art on the BioRelEx entity and relation extraction tasks	4
----------
Biomedical Information Extraction from scientific literature presents two unique and non-trivial challenges.	1
First, compared with general natural language texts, sentences from scientific papers usually possess wider contexts between knowledge elements.	1
Moreover, comprehending the fine-grained scientific entities and events urgently requires domain-specific background knowledge.	1
In this paper, we propose a novel biomedical Information Extraction (IE) model to tackle these two challenges and extract scientific entities and events from English research papers.	2
We perform Abstract Meaning Representation (AMR) to compress the wide context to uncover a clear semantic structure for each complex sentence.	3
Besides, we construct the sentence-level knowledge graph from an external knowledge base and use it to enrich the AMR graph to improve the model’s understanding of complex scientific concepts.	3
We use an edge-conditioned graph attention network to encode the knowledge-enriched AMR graph for biomedical IE tasks.	3
Experiments on the GENIA 2011 dataset show that the AMR and external knowledge have contributed 1.8% and 3.0% absolute F-score gains respectively.	4
In order to evaluate the impact of our approach on real-world problems that involve topic-specific fine-grained knowledge elements, we have also created a new ontology and annotated corpus for entity and event extraction for the COVID-19 scientific literature, which can serve as a new benchmark for the biomedical IE community.	3+4
----------
Event Detection (ED) aims to recognize mentions of events (i.e., event triggers) and their types in text.	1
Recently, several ED datasets in various domains have been proposed.	1
However, the major limitation of these resources is the lack of enough training data for individual event types which hinders the efficient training of data-hungry deep learning models.	1
To overcome this issue, we propose to exploit the powerful pre-trained language model GPT-2 to generate training samples for ED.	2
To prevent the noises inevitable in automatically generated data from hampering training process, we propose to exploit a teacher-student architecture in which the teacher is supposed to learn anchor knowledge from the original data.	2+3
The student is then trained on combination of the original and GPT-generated data while being led by the anchor knowledge from the teacher.	3
Optimal transport is introduced to facilitate the anchor knowledge-based guidance between the two networks.	3
We evaluate the proposed model on multiple ED benchmark datasets, gaining consistent improvement and establishing state-of-the-art results for ED.	4
----------
Event extraction (EE) has considerably benefited from pre-trained language models (PLMs) by fine-tuning.	1
However, existing pre-training methods have not involved modeling event characteristics, resulting in the developed EE models cannot take full advantage of large-scale unsupervised data.	1
To this end, we propose CLEVE, a contrastive pre-training framework for EE to better learn event knowledge from large unsupervised data and their semantic structures (e.g. AMR) obtained with automatic parsers.	2
CLEVE contains a text encoder to learn event semantics and a graph encoder to learn event structures respectively.	3
Specifically, the text encoder learns event semantic representations by self-supervised contrastive learning to represent the words of the same events closer than those unrelated words; the graph encoder learns event structure representations by graph contrastive pre-training on parsed event-related semantic structures.	3
The two complementary representations then work together to improve both the conventional supervised EE and the unsupervised “liberal” EE, which requires jointly extracting events and discovering event schemata without any annotated data.	3
Experiments on ACE 2005 and MAVEN datasets show that CLEVE achieves significant improvements, especially in the challenging unsupervised setting.	4
The source code and pre-trained checkpoints can be obtained from https://github.com/THU-KEG/CLEVE.	6
----------
Document-level event extraction (DEE) is indispensable when events are described throughout a document.	1
We argue that sentence-level extractors are ill-suited to the DEE task where event arguments always scatter across sentences and multiple events may co-exist in a document.	1
It is a challenging task because it requires a holistic understanding of the document and an aggregated ability to assemble arguments across multiple sentences.	1
In this paper, we propose an end-to-end model, which can extract structured events from a document in a parallel manner.	2
Specifically, we first introduce a document-level encoder to obtain the document-aware representations.	3
Then, a multi-granularity non-autoregressive decoder is used to generate events in parallel.	3
Finally, to train the entire model, a matching loss function is proposed, which can bootstrap a global optimization.	3
The empirical results on the widely used DEE dataset show that our approach significantly outperforms current state-of-the-art methods in the challenging DEE task.	4
Code will be available at https://github.com/HangYang-NLP/DE-PPN.	6
----------
Large pre-trained language models achieve state-of-the-art results when fine-tuned on downstream NLP tasks.	1
However, they almost exclusively focus on text-only representation, while neglecting cell-level layout information that is important for form image understanding.	1
In this paper, we propose a new pre-training approach, StructuralLM, to jointly leverage cell and layout information from scanned documents.	2
Specifically, we pre-train StructuralLM with two new designs to make the most of the interactions of cell and layout information: 1) each cell as a semantic unit; 2) classification of cell positions.	3
The pre-trained StructuralLM achieves new state-of-the-art results in different types of downstream tasks, including form understanding (from 78.95 to 85.14), document visual question answering (from 72.59 to 83.94) and document image classification (from 94.43 to 96.08).	4
----------
Aspect-based sentiment analysis is a fine-grained sentiment classification task.	1
Recently, graph neural networks over dependency trees have been explored to explicitly model connections between aspects and opinion words.	1
However, the improvement is limited due to the inaccuracy of the dependency parsing results and the informal expressions and complexity of online reviews.	1
To overcome these challenges, in this paper, we propose a dual graph convolutional networks (DualGCN) model that considers the complementarity of syntax structures and semantic correlations simultaneously.	2
Particularly, to alleviate dependency parsing errors, we design a SynGCN module with rich syntactic knowledge.	3
To capture semantic correlations, we design a SemGCN module with self-attention mechanism.	3
Furthermore, we propose orthogonal and differential regularizers to capture semantic correlations between words precisely by constraining attention scores in the SemGCN module.	2
The orthogonal regularizer encourages the SemGCN to learn semantically correlated words with less overlap for each word.	3
The differential regularizer encourages the SemGCN to learn semantic features that the SynGCN fails to capture.	3
Experimental results on three public datasets show that our DualGCN model outperforms state-of-the-art methods and verify the effectiveness of our model.	4
----------
Aspect category detection (ACD) in sentiment analysis aims to identify the aspect categories mentioned in a sentence.	1
In this paper, we formulate ACD in the few-shot learning scenario.	2
However, existing few-shot learning approaches mainly focus on single-label predictions.	1
These methods can not work well for the ACD task since a sentence may contain multiple aspect categories.	1
Therefore, we propose a multi-label few-shot learning method based on the prototypical network.	2
To alleviate the noise, we design two effective attention mechanisms.	3
The support-set attention aims to extract better prototypes by removing irrelevant aspects.	3
The query-set attention computes multiple prototype-specific representations for each query instance, which are then used to compute accurate distances with the corresponding prototypes.	3
To achieve multi-label inference, we further learn a dynamic threshold per instance by a policy network.	3
Extensive experimental results on three datasets demonstrate that the proposed method significantly outperforms strong baselines.	4
----------
Argument pair extraction (APE) is a research task for extracting arguments from two passages and identifying potential argument pairs.	1
Prior research work treats this task as a sequence labeling problem and a binary classification problem on two passages that are directly concatenated together, which has a limitation of not fully utilizing the unique characteristics and inherent relations of two different passages.	1
This paper proposes a novel attention-guided multi-layer multi-cross encoding scheme to address the challenges.	2
The new model processes two passages with two individual sequence encoders and updates their representations using each other’s representations through attention.	3
In addition, the pair prediction part is formulated as a table-filling problem by updating the representations of two sequences’ Cartesian product.	3
Furthermore, an auxiliary attention loss is introduced to guide each argument to align to its paired argument.	3
An extensive set of experiments show that the new model significantly improves the APE performance over several alternatives.	4
----------
The goal of argumentation mining is to automatically extract argumentation structures from argumentative texts.	1
Most existing methods determine argumentative relations by exhaustively enumerating all possible pairs of argument components, which suffer from low efficiency and class imbalance.	1
Moreover, due to the complex nature of argumentation, there is, so far, no universal method that can address both tree and non-tree structured argumentation.	1
Towards these issues, we propose a neural transition-based model for argumentation mining, which incrementally builds an argumentation graph by generating a sequence of actions, avoiding inefficient enumeration operations.	2
Furthermore, our model can handle both tree and non-tree structured argumentation without introducing any structural constraints.	4
Experimental results show that our model achieves the best performance on two public datasets of different structures.	4
----------
This work presents Keep it Simple (KiS), a new approach to unsupervised text simplification which learns to balance a reward across three properties: fluency, salience and simplicity.	2
We train the model with a novel algorithm to optimize the reward (k-SCST), in which the model proposes several candidate simplifications, computes each candidate’s reward, and encourages candidates that outperform the mean reward.	3
Finally, we propose a realistic text comprehension task as an evaluation method for text simplification.	2
When tested on the English news domain, the KiS model outperforms strong supervised baselines by more than 4 SARI points, and can help people complete a comprehension task an average of 18% faster while retaining accuracy, when compared to the original text.	4
----------
Generating long and coherent text is an important but challenging task, particularly for open-ended language generation tasks such as story generation.	1
Despite the success in modeling intra-sentence coherence, existing generation models (e.g., BART) still struggle to maintain a coherent event sequence throughout the generated text.	1
We conjecture that this is because of the difficulty for the decoder to capture the high-level semantics and discourse structures in the context beyond token-level co-occurrence.	1
In this paper, we propose a long text generation model, which can represent the prefix sentences at sentence level and discourse level in the decoding process.	2
To this end, we propose two pretraining objectives to learn the representations by predicting inter-sentence semantic similarity and distinguishing between normal and shuffled sentence orders.	2
Extensive experiments show that our model can generate more coherent texts than state-of-the-art baselines.	4
----------
We study the task of long-form opinion text generation, which faces at least two distinct challenges.	2
First, existing neural generation models fall short of coherence, thus requiring efficient content planning.	2
Second, diverse types of information are needed to guide the generator to cover both subjective and objective content.	2
To this end, we propose DYPLOC, a generation framework that conducts dynamic planning of content while generating the output based on a novel design of mixed language models.	2
To enrich the generation with diverse content, we further propose to use large pre-trained models to predict relevant concepts and to generate claims.	2
We experiment with two challenging tasks on newly collected datasets: (1) argument generation with Reddit ChangeMyView, and (2) writing articles using New York Times’ Opinion section.	3
Automatic evaluation shows that our model significantly outperforms competitive comparisons.	4
Human judges further confirm that our generations are more coherent with richer content.	4
----------
We present BERTGen, a novel, generative, decoder-only model which extends BERT by fusing multimodal and multilingual pre-trained models VL-BERT and M-BERT, respectively.	2
BERTGen is auto-regressively trained for language generation tasks, namely image captioning, machine translation and multimodal machine translation, under a multi-task setting.	3
With a comprehensive set of evaluations, we show that BERTGen outperforms many strong baselines across the tasks explored.	4
We also show BERTGen’s ability for zero-shot language generation, where it exhibits competitive performance to supervised counterparts.	4
Finally, we conduct ablation studies which demonstrate that BERTGen substantially benefits from multi-tasking and effectively transfers relevant inductive biases from the pre-trained models.	4
----------
Neural Machine Translation (NMT) models achieve state-of-the-art performance on many translation benchmarks.	1
As an active research field in NMT, knowledge distillation is widely applied to enhance the model’s performance by transferring teacher model’s knowledge on each training sample.	1
However, previous work rarely discusses the different impacts and connections among these samples, which serve as the medium for transferring teacher knowledge.	1
In this paper, we design a novel protocol that can effectively analyze the different impacts of samples by comparing various samples’ partitions.	2
Based on above protocol, we conduct extensive experiments and find that the teacher’s knowledge is not the more, the better.	3
Knowledge over specific samples may even hurt the whole performance of knowledge distillation.	3
Finally, to address these issues, we propose two simple yet effective strategies, i.e., batch-level and global-level selections, to pick suitable samples for distillation.	2
We evaluate our approaches on two large-scale machine translation tasks, WMT’14 English-German and WMT’19 Chinese-English.	3
Experimental results show that our approaches yield up to +1.28 and +0.89 BLEU points improvements over the Transformer baseline, respectively.	4
----------
Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated.	1
However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time.	1
In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models.	2
Using this metric, we measure how much document-level machine translation systems use particular varieties of context.	3
We find that target context is referenced more than source context, and that including more context has a diminishing affect on results.	3
We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models.	3
Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.	4
----------
Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings.	1
Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption.	1
In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision.	2
Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them.	3
To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary.	3
Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task.	4
----------
We show that margin-based bitext mining in a multilingual sentence space can be successfully scaled to operate on monolingual corpora of billions of sentences.	2
We use 32 snapshots of a curated common crawl corpus (Wenzel et al, 2019) totaling 71 billion unique sentences.	3
Using one unified approach for 90 languages, we were able to mine 10.8 billion parallel sentences, out of which only 2.9 billions are aligned with English.	3
We illustrate the capability of our scalable mining system to create high quality training sets from one language to any other by training hundreds of different machine translation models and evaluating them on the many-to-many TED benchmark.	3+4
Further, we evaluate on competitive translation benchmarks such as WMT and WAT.	3
Using only mined bitext, we set a new state of the art for a single system on the WMT’19 test set for English-German/Russian/Chinese.	4
In particular, our English/German and English/Russian systems outperform the best single ones by over 4 BLEU points and are on par with best WMT’19 systems, which train on the WMT training data and augment it with backtranslation.	4
We also achieve excellent results for distant languages pairs like Russian/Japanese, outperforming the best submission at the 2020 WAT workshop.	4
All of the mined bitext will be freely available.	6
----------
Despite transformers’ impressive accuracy, their computational cost is often prohibitive to use with limited computational resources.	1
Most previous approaches to improve inference efficiency require a separate model for each possible computational budget.	1
In this paper, we extend PoWER-BERT (Goyal et al., 2020) and propose Length-Adaptive Transformer that can be used for various inference scenarios after one-shot training.	2
We train a transformer with LengthDrop, a structural variant of dropout, which stochastically determines a sequence length at each layer.	3
We then conduct a multi-objective evolutionary search to find a length configuration that maximizes the accuracy and minimizes the efficiency metric under any given computational budget.	3
Additionally, we significantly extend the applicability of PoWER-BERT beyond sequence-level classification into token-level classification with Drop-and-Restore process that drops word-vectors temporarily in intermediate layers and restores at the last layer if necessary.	3
We empirically verify the utility of the proposed approach by demonstrating the superior accuracy-efficiency trade-off under various setups, including span-based question answering and text classification.	4
Code is available at https://github.com/clovaai/lengthadaptive-transformer.	6
----------
Transformer-based pre-trained language models like BERT, though powerful in many tasks, are expensive in both memory and computation, due to their large number of parameters.	1
Previous works show that some parameters in these models can be pruned away without severe accuracy drop.	1
However, these redundant features contribute to a comprehensive understanding of the training data and removing them weakens the model’s representation ability.	1
In this paper, we propose GhostBERT, which generates more features with very cheap operations from the remaining features.	2
In this way, GhostBERT has similar memory and computational cost as the pruned model, but enjoys much larger representation power.	4
The proposed ghost module can also be applied to unpruned BERT models to enhance their performance with negligible additional parameters and computation.	4
Empirical results on the GLUE benchmark on three backbone models (i.e., BERT, RoBERTa and ELECTRA) verify the efficacy of our proposed method.	4
----------
The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of ”lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model.	1
In this paper, we study such a collection of tickets, which is referred to as ”winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models.	2
We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model.	4
In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold.	4
We refer to the tickets on the threshold as ”super tickets”.	4
We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced.	4
Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score.	4
We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.	4
----------
Learning disentangled representations of textual data is essential for many natural language tasks such as fair classification, style transfer and sentence generation, among others.	1
The existent dominant approaches in the context of text data either rely on training an adversary (discriminator) that aims at making attribute values difficult to be inferred from the latent code or rely on minimising variational bounds of the mutual information between latent code and the value attribute.	1
However, the available methods suffer of the impossibility to provide a fine-grained control of the degree (or force) of disentanglement.	1
In contrast to adversarial methods, which are remarkably simple, although the adversary seems to be performing perfectly well during the training phase, after it is completed a fair amount of information about the undesired attribute still remains.	1
This paper introduces a novel variational upper bound to the mutual information between an attribute and the latent code of an encoder.	2
Our bound aims at controlling the approximation error via the Renyi’s divergence, leading to both better disentangled representations and in particular, a precise control of the desirable degree of disentanglement than state-of-the-art methods proposed for textual data.	4
Furthermore, it does not suffer from the degeneracy of other losses in multi-class scenarios.	4
We show the superiority of this method on fair classification and on textual style transfer tasks.	4
Additionally, we provide new insights illustrating various trade-offs in style transfer when attempting to learn disentangled representations and quality of the generated sentence.	5
----------
Beam search is a go-to strategy for decoding neural sequence models.	1
The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates.	1
Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word.	1
Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired.	1
To address this issue, we propose a reformulation of beam search, which we call determinantal beam search.	2
Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions.	1
By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process.	3
In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model.	3
We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.	4
----------
Graph convolutional network (GCN) has become popular in various natural language processing (NLP) tasks with its superiority in long-term and non-consecutive word interactions.	1
However, existing single-hop graph reasoning in GCN may miss some important non-consecutive dependencies.	1
In this study, we define the spectral graph convolutional network with the high-order dynamic Chebyshev approximation (HDGCN), which augments the multi-hop graph reasoning by fusing messages aggregated from direct and long-term dependencies into one convolutional layer.	2+3
To alleviate the over-smoothing in high-order Chebyshev approximation, a multi-vote-based cross-attention (MVCAttn) with linear computation complexity is also proposed.	2
The empirical results on four transductive and inductive NLP tasks and the ablation study verify the efficacy of the proposed model.	3
----------
Typing every character in a text message may require more time or effort than strictly necessary.	1
Skipping spaces or other characters may be able to speed input and reduce a user’s physical input effort.	1
This can be particularly important for people with motor impairments.	1
In a large crowdsourced study, we found workers frequently abbreviated text by omitting mid-word vowels.	1
We designed a recognizer optimized for expanding noisy abbreviated input where users often omit spaces and mid-word vowels.	2
We show using neural language models for selecting conversational-style training text and for rescoring the recognizer’s n-best sentences improved accuracy.	3
On noisy touchscreen data collected from hundreds of users, we found accurate abbreviated input was possible even if a third of characters was omitted.	4
Finally, in a study where users had to dwell for a second on each key, sentence abbreviated input was competitive with a conventional keyboard with word predictions.	4
After practice, users wrote abbreviated sentences at 9.6 words-per-minute versus word input at 9.9 words-per-minute.	4
----------
Behavior of deep neural networks can be inconsistent between different versions.	1
Regressions during model update are a common cause of concern that often over-weigh the benefits in accuracy or efficiency gain.	1
This work focuses on quantifying, reducing and analyzing regression errors in the NLP model updates.	2
Using negative flip rate as regression measure, we show that regression has a prevalent presence across tasks in the GLUE benchmark.	4
We formulate the regression-free model updates into a constrained optimization problem, and further reduce it into a relaxed form which can be approximately optimized through knowledge distillation training method.	3
We empirically analyze how model ensemble reduces regression.	3
Finally, we conduct CheckList behavioral testing to understand the distribution of regressions across linguistic phenomena, and the efficacy of ensemble and distillation methods.	3
----------
In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions.	1
Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle.	1
However, despite ADC’s intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models.	1
In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model).	2
Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets.	4
Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research.	4
----------
Open-domain question answering can be reformulated as a phrase retrieval problem, without the need for processing documents on-demand during inference (Seo et al., 2019).	1
However, current phrase retrieval models heavily depend on sparse representations and still underperform retriever-reader approaches.	1
In this work, we show for the first time that we can learn dense representations of phrases alone that achieve much stronger performance in open-domain QA.	2
We present an effective method to learn phrase representations from the supervision of reading comprehension tasks, coupled with novel negative sampling methods.	2
We also propose a query-side fine-tuning strategy, which can support transfer learning and reduce the discrepancy between training and inference.	2
On five popular open-domain QA datasets, our model DensePhrases improves over previous phrase retrieval models by 15%-25% absolute accuracy and matches the performance of state-of-the-art retriever-reader models.	4
Our model is easy to parallelize due to pure dense representations and processes more than 10 questions per second on CPUs.	4
Finally, we directly use our pre-indexed dense phrase representations for two slot filling tasks, showing the promise of utilizing DensePhrases as a dense knowledge base for downstream tasks.	3+4
----------
Recent work on training neural retrievers for open-domain question answering (OpenQA) has employed both supervised and unsupervised approaches.	1
However, it remains unclear how unsupervised and supervised methods can be used most effectively for neural retrievers.	1
In this work, we systematically study retriever pre-training.	2
We first propose an approach of unsupervised pre-training with the Inverse Cloze Task and masked salient spans, followed by supervised finetuning using question-context pairs.	2
This approach leads to absolute gains of 2+ points over the previous best result in the top-20 retrieval accuracy on Natural Questions and TriviaQA datasets.	4
We next explore two approaches for end-to-end training of the reader and retriever components in OpenQA models, which differ in the manner the reader ingests the retrieved documents.	2
Our experiments demonstrate the effectiveness of these approaches as we obtain state-of-the-art results.	4
On the Natural Questions dataset, we obtain a top-20 retrieval accuracy of 84%, an improvement of 5 points over the recent DPR model.	4
We also achieve good results on answer extraction, outperforming recent models like REALM and RAG by 3+ points.	4
----------
Although automated metrics are commonly used to evaluate NLG systems, they often correlate poorly with human judgements.	1
Newer metrics such as BERTScore have addressed many weaknesses in prior metrics such as BLEU and ROUGE, which rely on n-gram matching.	1
These newer methods, however, are still limited in that they do not consider the generation context, so they cannot properly reward generated text that is correct but deviates from the given reference.	1
In this paper, we propose Language Model Augmented Relevance Score (MARS), a new context-aware metric for NLG evaluation.	2
MARS leverages off-the-shelf language models, guided by reinforcement learning, to create augmented references that consider both the generation context and available human references, which are then used as additional references to score generated text.	3
Compared with seven existing metrics in three common NLG tasks, MARS not only achieves higher correlation with human reference judgements, but also differentiates well-formed candidates from adversarial samples to a larger degree.	4
----------
Despite recent advances in natural language generation, it remains challenging to control attributes of generated text.	1
We propose DExperts: Decoding-time Experts, a decoding-time method for controlled text generation that combines a pretrained language model with “expert” LMs and/or “anti-expert” LMs in a product of experts.	2
Intuitively, under the ensemble, tokens only get high probability if they are considered likely by the experts, and unlikely by the anti-experts.	3
We apply DExperts to language detoxification and sentiment-controlled generation, where we outperform existing controllable generation methods on both automatic and human evaluations.	4
Moreover, because DExperts operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3.	4
Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering.	5
----------
While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions.	1
We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences.	2+3
We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.	4
----------
Generating metaphors is a difficult task as it requires understanding nuanced relationships between abstract concepts.	1
In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs.	2
Guided by conceptual metaphor theory, we propose to control the generation process by encoding conceptual mappings between cognitive domains to generate meaningful metaphoric expressions.	2
To achieve this, we develop two methods: 1) using FrameNet-based embeddings to learn mappings between domains and applying them at the lexical level (CM-Lex), and 2) deriving source/target pairs to train a controlled seq-to-seq generation model (CM-BART).	3
We assess our methods through automatic and human evaluation for basic metaphoricity and conceptual metaphor presence.	3
We show that the unsupervised CM-Lex model is competitive with recent deep learning metaphor generation systems, and CM-BART outperforms all other models both in automatic and human evaluations.	4
----------
Wet laboratory protocols (WLPs) are critical for conveying reproducible procedures in biological research.	1
They are composed of instructions written in natural language describing the step-wise processing of materials by specific actions.	1
This process flow description for reagents and materials synthesis in WLPs can be captured by material state transfer graphs (MSTGs), which encode global temporal and causal relationships between actions.	1
Here, we propose methods to automatically generate a MSTG for a given protocol by extracting all action relationships across multiple sentences.	2
We also note that previous corpora and methods focused primarily on local intra-sentence relationships between actions and entities and did not address two critical issues: (i) resolution of implicit arguments and (ii) establishing long-range dependencies across sentences.	1
We propose a new model that incrementally learns latent structures and is better suited to resolving inter-sentence relations and implicit arguments.	2
This model draws upon a new corpus WLP-MSTG which was created by extending annotations in the WLP corpora for inter-sentence relations and implicit arguments.	3
Our model achieves an F1 score of 54.53% for temporal and causal relations in protocols from our corpus, which is a significant improvement over previous models - DyGIE++:28.17%; spERT:27.81%.	4
We make our annotated WLP-MSTG corpus available to the research community.	6
----------
To translate large volumes of text in a globally connected world, more and more translators are integrating machine translation (MT) and post-editing (PE) into their translation workflows to generate publishable quality translations.	1
While this process has been shown to save time and reduce errors, the task of translation is changing from mostly text production from scratch to fixing errors within useful but partly incorrect MT output.	1
This is affecting the interface design of translation tools, where better support for text editing tasks is required.	1
Here, we present the first study that investigates the usefulness of mid-air hand gestures in combination with the keyboard (GK) for text editing in PE of MT.	2
Guided by a gesture elicitation study with 14 freelance translators, we develop a prototype supporting mid-air hand gestures for cursor placement, text selection, deletion, and reordering.	2
These gestures combined with the keyboard facilitate all editing types required for PE.	4
An evaluation of the prototype shows that the average editing duration of GK is only slightly slower than the standard mouse and keyboard (MK), even though participants are very familiar with the latter, and relative novices to the former.	4
Furthermore, the qualitative analysis shows positive attitudes towards hand gestures for PE, especially when manipulating single words.	4
----------
Structured information is an important knowledge source for automatic verification of factual claims.	1
Nevertheless, the majority of existing research into this task has focused on textual data, and the few recent inquiries into structured data have been for the closed-domain setting where appropriate evidence for each claim is assumed to have already been retrieved.	1
In this paper, we investigate verification over structured data in the open-domain setting, introducing a joint reranking-and-verification model which fuses evidence documents in the verification component.	2
Our open-domain model achieves performance comparable to the closed-domain state-of-the-art on the TabFact dataset, and demonstrates performance gains from the inclusion of multiple tables as well as a significant improvement over a heuristic retrieval baseline.	4
----------
Monolingual word alignment is important for studying fine-grained editing operations (i.e., deletion, addition, and substitution) in text-to-text generation tasks, such as paraphrase generation, text simplification, neutralizing biased language, etc.	1
In this paper, we present a novel neural semi-Markov CRF alignment model, which unifies word and phrase alignments through variable-length spans.	2
We also create a new benchmark with human annotations that cover four different text genres to evaluate monolingual word alignment models in more realistic settings.	2
Experimental results show that our proposed model outperforms all previous approaches for monolingual word alignment as well as a competitive QA-based baseline, which was previously only applied to bilingual data.	4
Our model demonstrates good generalizability to three out-of-domain datasets and shows great utility in two downstream applications: automatic text simplification and sentence pair classification tasks.	4
----------
A commonly observed problem with the state-of-the art abstractive summarization models is that the generated summaries can be factually inconsistent with the input documents.	1
The fact that automatic summarization may produce plausible-sounding yet inaccurate summaries is a major concern that limits its wide application.	1
In this paper we present an approach to address factual consistency in summarization.	2
We first propose an efficient automatic evaluation metric to measure factual consistency; next, we propose a novel learning algorithm that maximizes the proposed metric during model training.	2
Through extensive experiments, we confirm that our method is effective in improving factual consistency and even overall quality of the summaries, as judged by both automatic metrics and human evaluation.	4
----------
Parallel cross-lingual summarization data is scarce, requiring models to better use the limited available cross-lingual resources.	1
Existing methods to do so often adopt sequence-to-sequence networks with multi-task frameworks.	1
Such approaches apply multiple decoders, each of which is utilized for a specific task.	1
However, these independent decoders share no parameters, hence fail to capture the relationships between the discrete phrases of summaries in different languages, breaking the connections in order to transfer the knowledge of the high-resource languages to low-resource languages.	1
To bridge these connections, we propose a novel Multi-Task framework for Cross-Lingual Abstractive Summarization (MCLAS) in a low-resource setting.	2
Employing one unified decoder to generate the sequential concatenation of monolingual and cross-lingual summaries, MCLAS makes the monolingual summarization task a prerequisite of the CLS task.	3
In this way, the shared decoder learns interactions involving alignments and summary patterns across languages, which encourages attaining knowledge transfer.	3
Experiments on two CLS datasets demonstrate that our model significantly outperforms three baseline models in both low-resource and full-dataset scenarios.	4
Moreover, in-depth analysis on the generated summaries and attention heads verifies that interactions are learned well using MCLAS, which benefits the CLS task under limited parallel resources.	4
----------
Humans create things for a reason.	1
Ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc.	1
The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language.	1
For example, if someone says “She borrowed the book” then you would assume that she intends to read the book, or if someone asks “Can I use your knife?” then you would assume that they need to cut something.	1
In this paper, we introduce a new NLP task of learning the prototypical uses for human-made physical objects.	2
We use frames from FrameNet to represent a set of common functions for objects, and describe a manually annotated data set of physical objects labeled with their prototypical function.	3
We also present experimental results for this task, including BERT-based models that use predictions from masked patterns as well as artifact sense definitions from WordNet and frame definitions from FrameNet.	3
----------
Linguistic probing of pretrained Transformer-based language models (LMs) revealed that they encode a range of syntactic and semantic properties of a language.	1
However, they are still prone to fall back on superficial cues and simple heuristics to solve downstream tasks, rather than leverage deeper linguistic information.	1
In this paper, we target a specific facet of linguistic knowledge, the interplay between verb meaning and argument structure.	2
We investigate whether injecting explicit information on verbs’ semantic-syntactic behaviour improves the performance of pretrained LMs in event extraction tasks, where accurate verb processing is paramount.	2
Concretely, we impart the verb knowledge from curated lexical resources into dedicated adapter modules (verb adapters), allowing it to complement, in downstream tasks, the language knowledge obtained during LM-pretraining.	3
We first demonstrate that injecting verb knowledge leads to performance gains in English event extraction.	4
We then explore the utility of verb adapters for event extraction in other languages: we investigate 1) zero-shot language transfer with multilingual Transformers and 2) transfer via (noisy automatic) translation of English verb-based lexical knowledge.	3+4
Our results show that the benefits of verb knowledge injection indeed extend to other languages, even when relying on noisily translated lexical knowledge.	4
----------
Static word embeddings that represent words by a single vector cannot capture the variability of word meaning in different linguistic and extralinguistic contexts.	1
Building on prior work on contextualized and dynamic word embeddings, we introduce dynamic contextualized word embeddings that represent words as a function of both linguistic and extralinguistic context.	2
Based on a pretrained language model (PLM), dynamic contextualized word embeddings model time and social space jointly, which makes them attractive for a range of NLP tasks involving semantic variability.	4
We highlight potential application scenarios by means of qualitative and quantitative analyses on four English datasets.	4
----------
While there is a large amount of research in the field of Lexical Semantic Change Detection, only few approaches go beyond a standard benchmark evaluation of existing models.	1
In this paper, we propose a shift of focus from change detection to change discovery, i.e., discovering novel word senses over time from the full corpus vocabulary.	2
By heavily fine-tuning a type-based and a token-based approach on recently published German data, we demonstrate that both models can successfully be applied to discover new words undergoing meaning change.	4
Furthermore, we provide an almost fully automated framework for both evaluation and discovery.	4
----------
In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers.	2
Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots.	1
By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition.	3
In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40% of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge.	4
The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40% of the chatbots.	4
When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77% of them, and more than 0.1 in 39% of the chatbots.	4
----------
To improve the coherence and knowledge retrieval capabilities of non-task-oriented dialogue systems, recent Transformer-based models aim to integrate fixed background context.	1
This often comes in the form of knowledge graphs, and the integration is done by creating pseudo utterances through paraphrasing knowledge triples, added into the accumulated dialogue context.	1
However, the context length is fixed in these architectures, which restricts how much background or dialogue context can be kept.	1
In this work, we propose a more concise encoding for background context structured in the form of knowledge graphs, by expressing the graph connections through restrictions on the attention weights.	2
The results of our human evaluation show that this encoding reduces space requirements without negative effects on the precision of reproduction of knowledge and perceived consistency.	4
Further, models trained with our proposed context encoding generate dialogues that are judged to be more comprehensive and interesting.	4
----------
Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines.	1
Recently, many approaches have been devoted to perceiving conversational context by deep learning models.	1
However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues.	1
In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective.	2
Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues.	3
The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking.	3
Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.	4
----------
When collecting annotations and labeled data from humans, a standard practice is to use inter-rater reliability (IRR) as a measure of data goodness (Hallgren, 2012).	1
Metrics such as Krippendorff’s alpha or Cohen’s kappa are typically required to be above a threshold of 0.6 (Landis and Koch, 1977).	1
These absolute thresholds are unreasonable for crowdsourced data from annotators with high cultural and training variances, especially on subjective topics.	1
We present a new alternative to interpreting IRR that is more empirical and contextualized.	2
It is based upon benchmarking IRR against baseline measures in a replication, one of which is a novel cross-replication reliability (xRR) measure based on Cohen’s (1960) kappa.	3
We call this approach the xRR framework.	4
We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework.	3
We argue this framework can be used to measure the quality of crowdsourced datasets.	4
----------
Pre-trained language models (LMs) are currently integral to many natural language processing systems.	1
Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training.	1
We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT.	2
To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation.	2
ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions.	3+4
When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets).	4
Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large ( 3.4x larger size).	4
Our models are publicly available at https://github.com/UBC-NLP/marbert and ARLUE will be released through the same repository.	6
----------
We present ReadOnce Transformers, an approach to convert a transformer-based model into one that can build an information-capturing, task-independent, and compressed representation of text.	2
The resulting representation is reusable across different examples and tasks, thereby requiring a document shared across many examples or tasks to only be read once.	3
This leads to faster training and evaluation of models.	3
Additionally, we extend standard text-to-text transformer models to Representation+Text-to-text models, and evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and long-document summarization.	3
Our one-time computed representation results in a 2x-5x speedup compared to standard text-to-text models, while the compression also allows existing language models to handle longer documents without the need for designing new pre-trained models.	4
----------
Models of narrative schema knowledge have proven useful for a range of event-related tasks, but they typically do not capture the temporal relationships between events.	1
We propose a single model that addresses both temporal ordering, sorting given events into the order they occurred, and event infilling, predicting new events which fit into an existing temporally-ordered sequence.	2
We use a BART-based conditional generation model that can capture both temporality and common event co-occurrence, meaning it can be flexibly applied to different tasks in this space.	3
Our model is trained as a denoising autoencoder: we take temporally-ordered event sequences, shuffle them, delete some events, and then attempt to recover the original event sequence.	3
This task teaches the model to make inferences given incomplete knowledge about the events in an underlying scenario.	3
On the temporal ordering task, we show that our model is able to unscramble event sequences from existing datasets without access to explicitly labeled temporal training data, outperforming both a BERT-based pairwise model and a BERT-based pointer network.	4
On event infilling, human evaluation shows that our model is able to generate events that fit better temporally into the input events when compared to GPT-2 story completion models.	4
----------
The wanton spread of hate speech on the internet brings great harm to society and families.	1
It is urgent to establish and improve automatic detection and active avoidance mechanisms for hate speech.	1
While there exist methods for hate speech detection, they stereotype words and hence suffer from inherently biased training.	1
In other words, getting more affective features from other affective resources will significantly affect the performance of hate speech detection.	1
In this paper, we propose a hate speech detection framework based on sentiment knowledge sharing.	2
While extracting the affective features of the target sentence itself, we make better use of the sentiment features from external resources, and finally fuse features from different feature extraction units to detect hate speech.	3
Experimental results on two public datasets demonstrate the effectiveness of our model.	4
----------
We propose a transition-based bubble parser to perform coordination structure identification and dependency-based syntactic analysis simultaneously.	2
Bubble representations were proposed in the formal linguistics literature decades ago; they enhance dependency trees by encoding coordination boundaries and internal relationships within coordination structures explicitly.	1
In this paper, we introduce a transition system and neural models for parsing these bubble-enhanced structures.	2
Experimental results on the English Penn Treebank and the English GENIA corpus show that our parsers beat previous state-of-the-art approaches on the task of coordination structure prediction, especially for the subset of sentences with complex coordination structures.	4
----------
Recent years have seen the paradigm shift of Named Entity Recognition (NER) systems from sequence labeling to span prediction.	1
Despite its preliminary effectiveness, the span prediction model’s architectural bias has not been fully understood.	1
In this paper, we first investigate the strengths and weaknesses when the span prediction model is used for named entity recognition compared with the sequence labeling framework and how to further improve it, which motivates us to make complementary advantages of systems based on different paradigms.	2
We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems’ outputs.	2
We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners.	3
We make all codes and datasets available: https://github.com/neulab/spanner, as well as an online system demo: http://spanner.sh.	6
Our model also has been deployed into the ExplainaBoard platform, which allows users to flexibly perform a system combination of top-scoring systems in an interactive way: http://explainaboard.nlpedia.ai/leaderboard/task-ner/.	6
----------
There are two major classes of natural language grammars — the dependency grammar that models one-to-one correspondences between words and the constituency grammar that models the assembly of one or several corresponded words.	1
While previous unsupervised parsing methods mostly focus on only inducing one class of grammars, we introduce a novel model, StructFormer, that can induce dependency and constituency structure at the same time.	2
To achieve this, we propose a new parsing framework that can jointly generate a constituency tree and dependency graph.	3
Then we integrate the induced dependency relations into the transformer, in a differentiable manner, through a novel dependency-constrained self-attention mechanism.	3
Experimental results show that our model can achieve strong results on unsupervised constituency parsing, unsupervised dependency parsing, and masked language modeling at the same time.	4
----------
Decipherment of historical ciphers is a challenging problem.	1
The language of the target plaintext might be unknown, and ciphertext can have a lot of noise.	1
State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known.	1
We propose an end-to-end multilingual model for solving simple substitution ciphers.	2
We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise.	4
----------
While it has been shown that Neural Machine Translation (NMT) is highly sensitive to noisy parallel training samples, prior work treats all types of mismatches between source and target as noise.	1
As a result, it remains unclear how samples that are mostly equivalent but contain a small number of semantically divergent tokens impact NMT training.	1
To close this gap, we analyze the impact of different types of fine-grained semantic divergences on Transformer models.	3
We show that models trained on synthetic divergences output degenerated text more frequently and are less confident in their predictions.	4
Based on these findings, we introduce a divergent-aware NMT framework that uses factors to help NMT recover from the degradation caused by naturally occurring divergences, improving both translation quality and model calibration on EN-FR tasks.	2
----------
Human evaluations are typically considered the gold standard in natural language generation, but as models’ fluency improves, how well can evaluators detect and judge machine-generated text?	1
We run a study assessing non-experts’ ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes).	2
We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level.	4
We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators’ accuracy improved up to 55%, it did not significantly improve across the three domains.	4
Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.	4
----------
This paper presents the first large-scale meta-evaluation of machine translation (MT).	2
We annotated MT evaluations conducted in 769 research papers published from 2010 to 2020.	3
Our study shows that practices for automatic MT evaluation have dramatically changed during the past decade and follow concerning trends.	4
An increasing number of MT evaluations exclusively rely on differences between BLEU scores to draw conclusions, without performing any kind of statistical significance testing nor human evaluation, while at least 108 metrics claiming to be better than BLEU have been proposed.	4
MT evaluations in recent papers tend to copy and compare automatic metric scores from previous work to claim the superiority of a method or an algorithm without confirming neither exactly the same training, validating, and testing data have been used nor the metric scores are comparable.	1
Furthermore, tools for reporting standardized metric scores are still far from being widely adopted by the MT community.	1
After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.	4
----------
Prior work has proved that Translation Memory (TM) can boost the performance of Neural Machine Translation (NMT).	1
In contrast to existing work that uses bilingual corpus as TM and employs source-side similarity search for memory retrieval, we propose a new framework that uses monolingual memory and performs learnable memory retrieval in a cross-lingual manner.	2
Our framework has unique advantages.	4
First, the cross-lingual memory retriever allows abundant monolingual data to be TM.	4
Second, the memory retriever and NMT model can be jointly optimized for the ultimate translation goal.	4
Experiments show that the proposed method obtains substantial improvements.	3
Remarkably, it even outperforms strong TM-augmented NMT baselines using bilingual TM.	4
Owning to the ability to leverage monolingual data, our model also demonstrates effectiveness in low-resource and domain adaptation scenarios.	4
----------
Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime.	1
Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples?	1
In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon.	2
We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space.	4
For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC.	4
Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness.	4
Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.	4
----------
The choice of token vocabulary affects the performance of machine translation.	1
This paper aims to figure out what is a good vocabulary and whether we can find the optimal vocabulary without trial training.	2
To answer these questions, we first provide an alternative understanding of vocabulary from the perspective of information theory.	3
It motivates us to formulate the quest of vocabularization – finding the best token dictionary with a proper size – as an optimal transport (OT) problem.	3
We propose VOLT, a simple and efficient solution without trial training.	2
Empirical results show that VOLT beats widely-used vocabularies in diverse scenarios, including WMT-14 English-German translation, TED bilingual translation, and TED multilingual translation.	4
For example, VOLT achieves 70% vocabulary size reduction and 0.5 BLEU gain on English-German translation.	4
Also, compared to BPE-search, VOLT reduces the search time from 384 GPU hours to 30 GPU hours on English-German translation.	4
Codes are available at https://github.com/Jingjing-NLP/VOLT.	6
----------
Finding attackable sentences in an argument is the first step toward successful refutation in argumentation.	1
We present a first large-scale analysis of sentence attackability in online arguments.	1
We analyze driving reasons for attacks in argumentation and identify relevant characteristics of sentences.	2
We demonstrate that a sentence’s attackability is associated with many of these characteristics regarding the sentence’s content, proposition types, and tone, and that an external knowledge source can provide useful information about attackability.	4
Building on these findings, we demonstrate that machine learning models can automatically detect attackable sentences in arguments, significantly better than several baselines and comparably well to laypeople.	3+4
----------
Argumentation accommodates various rhetorical devices, such as questions, reported speech, and imperatives.	1
These rhetorical tools usually assert argumentatively relevant propositions rather implicitly, so understanding their true meaning is key to understanding certain arguments properly.	1
However, most argument mining systems and computational linguistics research have paid little attention to implicitly asserted propositions in argumentation.	1
In this paper, we examine a wide range of computational methods for extracting propositions that are implicitly asserted in questions, reported speech, and imperatives in argumentation.	2+3
By evaluating the models on a corpus of 2016 U.S. presidential debates and online commentary, we demonstrate the effectiveness and limitations of the computational models.	4
Our study may inform future research on argument mining and the semantics of these rhetorical devices in argumentation.	4
----------
When summarizing a collection of views, arguments or opinions on some topic, it is often desirable not only to extract the most salient points, but also to quantify their prevalence.	1
Work on multi-document summarization has traditionally focused on creating textual summaries, which lack this quantitative aspect.	1
Recent work has proposed to summarize arguments by mapping them to a small set of expert-generated key points, where the salience of each key point corresponds to the number of its matching arguments.	1
The current work advances key point analysis in two important respects: first, we develop a method for automatic extraction of key points, which enables fully automatic analysis, and is shown to achieve performance comparable to a human expert.	2
Second, we demonstrate that the applicability of key point analysis goes well beyond argumentation data.	2
Using models trained on publicly available argumentation datasets, we achieve promising results in two additional domains: municipal surveys and user reviews.	3+4
An additional contribution is an in-depth evaluation of argument-to-key point matching models, where we substantially outperform previous results.	4
----------
Social media platforms have become an essential venue for online deliberation where users discuss arguments, debate, and form opinions.	1
In this paper, we propose an unsupervised method to detect the stance of argumentative claims with respect to a topic.	2
Most related work focuses on topic-specific supervised models that need to be trained for every emergent debate topic.	3
To address this limitation, we propose a topic independent approach that focuses on a frequently encountered class of arguments, specifically, on arguments from consequences.	2
We do this by extracting the effects that claims refer to, and proposing a means for inferring if the effect is a good or bad consequence.	3
Our experiments provide promising results that are comparable to, and in particular regards even outperform BERT.	4
Furthermore, we publish a novel dataset of arguments relating to consequences, annotated with Amazon Mechanical Turk.	4
----------
The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems.	1
This paper demonstrates that, while choice of metric is important, the nature of the references is also critical.	2
We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics.	3
Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias.	3
Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references.	3+4
We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods.To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective.	4+5
----------
The term translationese has been used to describe features of translated text, and in this paper, we provide detailed analysis of potential adverse effects of translationese on machine translation evaluation.	1+2
Our analysis shows differences in conclusions drawn from evaluations that include translationese in test data compared to experiments that tested only with text originally composed in that language.	3
For this reason we recommend that reverse-created test data be omitted from future machine translation test sets.	3
In addition, we provide a re-evaluation of a past machine translation evaluation claiming human-parity of MT.	4
One important issue not previously considered is statistical power of significance tests applied to comparison of human and machine translation.	4
Since the very aim of past evaluations was investigation of ties between human and MT systems, power analysis is of particular importance, to avoid, for example, claims of human parity simply corresponding to Type II error resulting from the application of a low powered test.	4
We provide detailed analysis of tests used in such evaluations to provide an indication of a suitable minimum sample size for future studies.	5
----------
Many valid translations exist for a given sentence, yet machine translation (MT) is trained with a single reference translation, exacerbating data sparsity in low-resource settings.	1
We introduce Simulated Multiple Reference Training (SMRT), a novel MT training method that approximates the full space of possible translations by sampling a paraphrase of the reference sentence from a paraphraser and training the MT model to predict the paraphraser’s distribution over possible tokens.	2+3
We demonstrate the effectiveness of SMRT in low-resource settings when translating to English, with improvements of 1.2 to 7.0 BLEU.	2
We also find SMRT is complementary to back-translation.	4
----------
We frame the task of machine translation evaluation as one of scoring machine translation output with a sequence-to-sequence paraphraser, conditioned on a human reference.	1
We propose training the paraphraser as a multilingual NMT system, treating paraphrasing as a zero-shot translation task (e.g., Czech to Czech).	2
This results in the paraphraser’s output mode being centered around a copy of the input sequence, which represents the best case scenario where the MT system output matches a human reference.	3
Our method is simple and intuitive, and does not require human judgements for training.	4
Our single model (trained in 39 languages) outperforms or statistically ties with all prior metrics on the WMT 2019 segment-level shared metrics task in all languages (excluding Gujarati where the model had no training data).	4
We also explore using our model for the task of quality estimation as a metric—conditioning on the source instead of the reference—and find that it significantly outperforms every submission to the WMT 2019 shared task on quality estimation in every language pair.	3+4
----------
Recent work by Clark et al. (2020) shows that transformers can act as “soft theorem provers” by answering questions over explicitly provided knowledge in natural language.	1
In our work, we take a step closer to emulating formal theorem provers, by proposing PRover, an interpretable transformer-based model that jointly answers binary questions over rule-bases and generates the corresponding proofs.	1+2
Our model learns to predict nodes and edges corresponding to proof graphs in an efficient constrained training paradigm.	3
During inference, a valid proof, satisfying a set of global constraints is generated.	3
We conduct experiments on synthetic, hand-authored, and human-paraphrased rule-bases to show promising results for QA and proof generation, with strong generalization performance.	3
First, PRover generates proofs with an accuracy of 87%, while retaining or improving performance on the QA task, compared to RuleTakers (up to 6% improvement on zero-shot evaluation).	3+4
Second, when trained on questions requiring lower depths of reasoning, it generalizes significantly better to higher depths (up to 15% improvement).	3+4
Third, PRover obtains near perfect QA accuracy of 98% using only 40% of the training data.	3+4
However, generating proofs for questions requiring higher depths of reasoning becomes challenging, and the accuracy drops to 65% for “depth 5”, indicating significant scope for future work.	5
----------
The aim of all Question Answering (QA) systems is to generalize to unseen questions.	1
Current supervised methods are reliant on expensive data annotation.	1
Moreover, such annotations can introduce unintended annotator bias, making systems focus more on the bias than the actual task.	1
This work proposes Knowledge Triplet Learning (KTL), a self-supervised task over knowledge graphs.	2
We propose heuristics to create synthetic graphs for commonsense and scientific knowledge.	2
We propose using KTL to perform zero-shot question answering, and our experiments show considerable improvements over large pre-trained transformer language models.	3+4
----------
Deep learning models for linguistic tasks require large training datasets, which are expensive to create.	1
As an alternative to the traditional approach of creating new instances by repeating the process of creating one instance, we propose doing so by first collecting a set of seed examples and then applying human-driven natural perturbations (as opposed to rule-based machine perturbations), which often change the gold label as well.	2+3
Such perturbations have the advantage of being relatively easier (and hence cheaper) to create than writing out completely new examples.	1
Further, they help address the issue that even models achieving human-level scores on NLP datasets are known to be considerably sensitive to small changes in input.	1
To evaluate the idea, we consider a recent question-answering dataset (BOOLQ) and study our approach as a function of the perturbation cost ratio, the relative cost of perturbing an existing question vs. creating a new one from scratch.	2
We find that when natural perturbations are moderately cheaper to create (cost ratio under 60%), it is more effective to use them for training BOOLQ models: such models exhibit 9% higher robustness and 4.5% stronger generalization, while retaining performance on the original BOOLQ dataset.	4
----------
Deep pre-trained contextualized encoders like BERT demonstrate remarkable performance on a range of downstream tasks.	1
A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training.	1
While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms.	1
Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data.	1
Can the choice of formalism affect probing results?	1
To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics.	2
We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism.	2+3
Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.	5
----------
To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations.	1
Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations.	1
For example, they do not substantially favour pretrained representations over randomly initialized ones.	1
Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks.	1
To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size.	1
Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL).	2
With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data.	3
Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations.	3
In addition to probe quality, the description length evaluates “the amount of effort” needed to achieve the quality.	3
This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality.	4
We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding.	4
We show that these methods agree in results and are more informative and stable than the standard probes.	5
----------
Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks.	1
Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it.	1
In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted.	2+3
To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal.	3
We then probe fastText and BERT for various morphosyntactic attributes across 36 languages.	3
We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.	4
----------
One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding.	1
However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning.	1
With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during finetuning.	2+3
We pretrain RoBERTa from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa_BASE.	3
We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones.	4
Eventually, with about 30B words of pretraining data, RoBERTa_BASE does consistently demonstrate a linguistic bias with some regularity.	5
We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.	4
----------
The neural attention mechanism plays an important role in many natural language processing applications.	1
In particular, multi-head attention extends single-head attention by allowing a model to jointly attend information from different perspectives.	2
However, without explicit constraining, multi-head attention may suffer from attention collapse, an issue that makes different heads extract similar attentive features, thus limiting the model’s representation power.	4
In this paper, for the first time, we provide a novel understanding of multi-head attention from a Bayesian perspective.	4
Based on the recently developed particle-optimization sampling techniques, we propose a non-parametric approach that explicitly improves the repulsiveness in multi-head attention and consequently strengthens model’s expressiveness.	3
Remarkably, our Bayesian interpretation provides theoretical inspirations on the not-well-understood questions: why and how one uses multi-head attention.	4
Extensive experiments on various attention models and applications demonstrate that the proposed repulsive attention can improve the learned feature diversity, leading to more informative representations with consistent performance improvement on multiple tasks.	4
----------
Syntactic parsers have dominated natural language understanding for decades.	1
Yet, their syntactic interpretations are losing centrality in downstream tasks due to the success of large-scale textual representation learners.	1
In this paper, we propose KERMIT (Kernel-inspired Encoder with Recursive Mechanism for Interpretable Trees) to embed symbolic syntactic parse trees into artificial neural networks and to visualize how syntax is used in inference.	2+3
We experimented with KERMIT paired with two state-of-the-art transformer-based universal sentence encoders (BERT and XLNet) and we showed that KERMIT can indeed boost their performance by effectively embedding human-coded universal syntactic representations in neural networks	3+4
----------
Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks.	1
In this paper, we present a new Transformer architecture, “Extended Transformer Construction” (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs.	2
To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens.	3
We also show that combining global-local attention with relative position encodings and a “Contrastive Predictive Coding” (CPC) pre-training objective allows ETC to encode structured inputs.	4
We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.	3
----------
We introduce Electric, an energy-based cloze model for representation learning over text.	2
Like BERT, it is a conditional generative model of tokens given their contexts.	1
However, Electric does not use masking or output a full distribution over tokens that could occur in a context.	1
Instead, it assigns a scalar energy score to each input token indicating how likely it is given its context.	1
We train Electric using an algorithm based on noise-contrastive estimation and elucidate how this learning objective is closely related to the recently proposed ELECTRA pre-training method.	2+3
Electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text: it re-ranks speech recognition n-best lists better than language models and much faster than masked language models.	3+4
Furthermore, it offers a clearer and more principled view of what ELECTRA learns during pre-training.	4
----------
Pre-trained Transformers are now ubiquitous in natural language processing, but despite their high end-task performance, little is known empirically about whether they are calibrated.	1
Specifically, do these models’ posterior probabilities provide an accurate empirical measure of how likely the model is to be correct on a given example?	1
We focus on BERT and RoBERTa in this work, and analyze their calibration across three tasks: natural language inference, paraphrase detection, and commonsense reasoning.	2
For each task, we consider in-domain as well as challenging out-of-domain settings, where models face more examples they should be uncertain about.	3
We show that: (1) when used out-of-the-box, pre-trained models are calibrated in-domain, and compared to baselines, their calibration error out-of-domain can be as much as 3.5x lower; (2) temperature scaling is effective at further reducing calibration error in-domain, and using label smoothing to deliberately increase empirical uncertainty helps calibrate posteriors out-of-domain.	4
----------
Linguistic steganography studies how to hide secret messages in natural language cover texts.	1
Traditional methods aim to transform a secret message into an innocent text via lexical substitution or syntactical modification.	1
Recently, advances in neural language models (LMs) enable us to directly generate cover text conditioned on the secret message.	1
In this study, we present a new linguistic steganography method which encodes secret messages using self-adjusting arithmetic coding based on a neural language model.	2
We formally analyze the statistical imperceptibility of this method and empirically show it outperforms the previous state-of-the-art methods on four datasets by 15.3% and 38.9% in terms of bits/word and KL metrics, respectively.	3
Finally, human evaluations show that 51% of generated cover texts can indeed fool eavesdroppers.	3
----------
Machine learning models are trained to find patterns in data.	1
NLP models can inadvertently learn socially undesirable patterns when training on gender biased text.	1
In this work, we propose a novel, general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker.	2+3
Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information.	3
In addition, we collect a new, crowdsourced evaluation benchmark.	3
Distinguishing between gender bias along multiple dimensions enables us to train better and more fine-grained gender bias classifiers.	3
We show our classifiers are valuable for a variety of applications, like controlling for gender bias in generative models, detecting gender bias in arbitrary text, and classifying text as offensive based on its genderedness.	4
----------
Since obtaining a perfect training dataset (i.e., a dataset which is considerably large, unbiased, and well-representative of unseen cases) is hardly possible, many real-world text classifiers are trained on the available, yet imperfect, datasets.	1
These classifiers are thus likely to have undesirable properties.	1
For instance, they may have biases against some sub-populations or may not work effectively in the wild due to overfitting.	1
In this paper, we propose FIND – a framework which enables humans to debug deep learning text classifiers by disabling irrelevant hidden features.	2+3
Experiments show that by using FIND, humans can improve CNN text classifiers which were trained under different types of imperfect datasets (including datasets with biases and datasets with dissimilar train-test distributions).	4+5
----------
A frequent pattern in customer care conversations is the agents responding with appropriate webpage URLs that address users’ needs.	1
We study the task of predicting the documents that customer care agents can use to facilitate users’ needs.	2
We also introduce a new public dataset which supports the aforementioned problem.	3
Using this dataset and two others, we investigate state-of-the art deep learning (DL) and information retrieval (IR) models for the task.	3
Additionally, we analyze the practicality of such systems in terms of inference time complexity.	3
Our show that an hybrid IR+DL approach provides the best of both worlds.	4
----------
While humans process language incrementally, the best language encoders currently used in NLP do not.	1
Both bidirectional LSTMs and Transformers assume that the sequence that is to be encoded is available in full, to be processed either forwards and backwards (BiLSTMs) or as a whole (Transformers).	1
We investigate how they behave under incremental interfaces, when partial output must be provided based on partial input seen up to a certain time step, which may happen in interactive systems.	2
We test five models on various NLU datasets and compare their performance using three incremental evaluation metrics.	1
The results support the possibility of using bidirectional encoders in incremental mode while retaining most of their non-incremental quality.	3
The “omni-directional” BERT model, which achieves better non-incremental performance, is impacted more by the incremental access.	4
This can be alleviated by adapting the training regime (truncated training), or the testing procedure, by delaying the output until some right context is available or by incorporating hypothetical right contexts generated by a language model like GPT-2.	3
----------
We propose a generative framework for joint sequence labeling and sentence-level classification.	2
Our model performs multiple sequence labeling tasks at once using a single, shared natural language output space.	3
Unlike prior discriminative methods, our model naturally incorporates label semantics and shares knowledge across tasks.	3
Our framework general purpose, performing well on few-shot learning, low resource, and high resource tasks.	4
We demonstrate these advantages on popular named entity recognition, slot labeling, and intent classification benchmarks.	4
We set a new state-of-the-art for few-shot slot labeling, improving substantially upon the previous 5-shot (75.0% to 90.9%) and 1-shot (70.4% to 81.0%) state-of-the-art results.	4
Furthermore, our model generates large improvements (46.27% to 63.83%) in low resource slot labeling over a BERT baseline by incorporating label semantics.	4
We also maintain competitive results on high resource tasks, performing within two points of the state-of-the-art on all tasks and setting a new state-of-the-art on the SNIPS dataset.	4
----------
Existing open-domain dialog models are generally trained to minimize the perplexity of target human responses.	1
However, some human replies are more engaging than others, spawning more followup interactions.	1
Current conversational models are increasingly capable of producing turns that are context-relevant, but in order to produce compelling agents, these models need to be able to predict and optimize for turns that are genuinely engaging.	2
We leverage social media feedback data (number of replies and upvotes) to build a large-scale training dataset for feedback prediction.	3
To alleviate possible distortion between the feedback and engagingness, we convert the ranking problem to a comparison of response pairs which involve few confounding factors.	4
We trained DialogRPT, a set of GPT-2 based models on 133M pairs of human feedback data and the resulting ranker outperformed several baselines.	4
Particularly, our ranker outperforms the conventional dialog perplexity baseline with a large margin on predicting Reddit feedback.	3
We finally combine the feedback prediction models and a human-like scoring model to rank the machine-generated dialog responses.	3
Crowd-sourced human evaluation shows that our ranking method correlates better with real human preferences than baseline models.	4
----------
We propose test suite accuracy to approximate semantic accuracy for Text-to-SQL models.	2
Our method distills a small test suite of databases that achieves high code coverage for the gold query from a large number of randomly generated databases.	3
At evaluation time, it computes the denotation accuracy of the predicted queries on the distilled test suite, hence calculating a tight upper-bound for semantic accuracy efficiently.	3
We use our proposed method to evaluate 21 models submitted to the Spider leader board and manually verify that our method is always correct on 100 examples.	3
In contrast, the current Spider metric leads to a 2.5% false negative rate on average and 8.1% in the worst case, indicating that test suite accuracy is needed.	4
Our implementation, along with distilled test suites for eleven Text-to-SQL datasets, is publicly available.	4
----------
In this paper, we propose Cross-Thought, a novel approach to pre-training sequence encoder, which is instrumental in building reusable sequence embeddings for large-scale NLP tasks such as question answering.	2
Instead of using the original signals of full sentences, we train a Transformer-based sequence encoder over a large set of short sequences, which allows the model to automatically select the most useful information for predicting masked words.	3
Experiments on question answering and textual entailment tasks demonstrate that our pre-trained encoder can outperform state-of-the-art encoders trained with continuous sentence signals as well as traditional masked language modeling baselines.	4
Our proposed approach also achieves new state of the art on HotpotQA (full-wiki setting) by improving intermediate information retrieval performance.	4
----------
We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort.	2
Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations.	3
It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech.	3
It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences.	3
We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9% when tested on natural questions, which is only 6.4% lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers.	4+5
To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset.	4
AutoQA achieves 69.8% answer accuracy, 16.4% higher than the state-of-the-art zero-shot models and only 5.2% lower than the same model trained with human data.	4
----------
Multi-document summarization (MDS) aims at producing a good-quality summary for several related documents.	1
In this paper, we propose a spectral-based hypothesis, which states that the goodness of summary candidate is closely linked to its so-called spectral impact.	2
Here spectral impact considers the perturbation to the dominant eigenvalue of affinity matrix when dropping the summary candidate from the document cluster.	3
The hypothesis is validated by three theoretical perspectives: semantic scaling, propagation dynamics and matrix perturbation.	3
According to the hypothesis, we formulate the MDS task as the combinatorial optimization of spectral impact and propose an accelerated greedy solution based on a surrogate of spectral impact.	3
The evaluation results on various datasets demonstrate: (1) The performance of the summary candidate is positively correlated with its spectral impact, which accords with our hypothesis; (2) Our spectral-based method has a competitive result as compared to state-of-the-art MDS systems.	4
----------
Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years.	1
However, gaps still exist between summaries produced by automatic summarizers and human professionals.	1
Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level, we consult the Multidimensional Quality Metric (MQM) and quantify 8 major sources of errors on 10 representative summarization models manually.	2+3
Primarily, we find that 1) under similar settings, extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual-consistency; 2) milestone techniques such as copy, coverage and hybrid extractive/abstractive methods do bring specific improvements but also demonstrate limitations; 3) pre-training techniques, and in particular sequence-to-sequence pre-training, are highly effective for improving text summarization, with BART giving the best results.	4+5
----------
Unsupervised methods are promising for abstractive textsummarization in that the parallel corpora is not required.	1
However, their performance is still far from being satisfied, therefore research on promising solutions is on-going.	1
In this paper, we propose a new approach based on Q-learning with an edit-based summarization.	2
The method combines two key modules to form an Editorial Agent and Language Model converter (EALM).	3
The agent predicts edit actions (e.t., delete, keep, and replace), and then the LM converter deterministically generates a summary on the basis of the action signals.	3
Q-learning is leveraged to train the agent to produce proper edit actions.	3
Experimental results show that EALM delivered competitive performance compared with the previous encoder-decoder-based methods, even with truly zero paired data (i.e., no validation set).	4
Defining the task as Q-learning enables us not only to develop a competitive method but also to make the latest techniques in reinforcement learning available for unsupervised summarization.	4
We also conduct qualitative analysis, providing insights into future study on unsupervised summarizers.	5
----------
Abstractive document summarization is a comprehensive task including document understanding and summary generation, in which area Transformer-based models have achieved the state-of-the-art performance.	1
Compared with Transformers, topic models are better at learning explicit document semantics, and hence could be integrated into Transformers to further boost their performance.	2
To this end, we rearrange and explore the semantics learned by a topic model, and then propose a topic assistant (TA) including three modules.	3
TA is compatible with various Transformer-based models and user-friendly since i) TA is a plug-and-play model that does not break any structure of the original Transformer network, making users easily fine-tune Transformer+TA based on a well pre-trained model; ii) TA only introduces a small number of extra parameters.	3
Experimental results on three datasets demonstrate that TA is able to improve the performance of several Transformer-based models.	4
----------
Existing language model compression methods mostly use a simple L_2 loss to distill knowledge in the intermediate representations of a large BERT model to a smaller one.	1
Although widely used, this objective by design assumes that all the dimensions of hidden representations are independent, failing to capture important structural knowledge in the intermediate layers of the teacher network.	1
To achieve better distillation efficacy, we propose Contrastive Distillation on Intermediate Representations (CoDIR), a principled knowledge distillation framework where the student is trained to distill knowledge through intermediate layers of the teacher via a contrastive objective.	2+3
By learning to distinguish positive sample from a large set of negative samples, CoDIR facilitates the student’s exploitation of rich information in teacher’s hidden layers.	3
CoDIR can be readily applied to compress large-scale language models in both pre-training and finetuning stages, and achieves superb performance on the GLUE benchmark, outperforming state-of-the-art compression methods.	4
----------
Transformer-based pre-training models like BERT have achieved remarkable performance in many natural language processing tasks.	1
However, these models are both computation and memory expensive, hindering their deployment to resource-constrained devices.	1
In this work, we propose TernaryBERT, which ternarizes the weights in a fine-tuned BERT model.	2
Specifically, we use both approximation-based and loss-aware ternarization methods and empirically investigate the ternarization granularity of different parts of BERT.	3
Moreover, to reduce the accuracy degradation caused by lower capacity of low bits, we leverage the knowledge distillation technique in the training process.	3
Experiments on the GLUE benchmark and SQuAD show that our proposed TernaryBERT outperforms the other BERT quantization methods, and even achieves comparable performance as the full-precision model while being 14.9x smaller.	4
----------
Self-supervised pre-training of transformer models has revolutionized NLP applications.	1
Such pre-training with language modeling objectives provides a useful initial point for parameters that generalize well to new tasks with fine-tuning.	1
However, fine-tuning is still data inefficient — when there are few labeled examples, accuracy can be low.	1
Data efficiency can be improved by optimizing pre-training directly for future fine-tuning with few examples; this can be treated as a meta-learning problem.	1
However, standard meta-learning techniques require many training tasks in order to generalize; unfortunately, finding a diverse set of such supervised tasks is usually difficult.	2
This paper proposes a self-supervised approach to generate a large, rich, meta-learning task distribution from unlabeled text.	2+3
This is achieved using a cloze-style objective, but creating separate multi-class classification tasks by gathering tokens-to-be blanked from among only a handful of vocabulary terms.	3
This yields as many unique meta-training tasks as the number of subsets of vocabulary terms.	3
We meta-train a transformer model on this distribution of tasks using a recent meta-learning framework.	3
On 17 NLP tasks, we show that this meta-training leads to better few-shot generalization than language-model pre-training followed by finetuning.	4
Furthermore, we show how the self-supervised tasks can be combined with supervised tasks for meta-learning, providing substantial accuracy gains over previous supervised meta-learning.	4
----------
Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning.	1
State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time.	1
However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed.	1
In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion.	2
To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation.	3
Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning.	4
We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.	5
----------
Multilingual contextual embeddings have demonstrated state-of-the-art performance in zero-shot cross-lingual transfer learning, where multilingual BERT is fine-tuned on one source language and evaluated on a different target language.	1
However, published results for mBERT zero-shot accuracy vary as much as 17 points on the MLDoc classification task across four papers.	1
We show that the standard practice of using English dev accuracy for model selection in the zero-shot setting makes it difficult to obtain reproducible results on the MLDoc and XNLI tasks.	2
English dev accuracy is often uncorrelated (or even anti-correlated) with target language accuracy, and zero-shot performance varies greatly at different points in the same fine-tuning run and between different fine-tuning runs.	4
These reproducibility issues are also present for other tasks with different pre-trained embeddings (e.g., MLQA with XLM-R).	4
We recommend providing oracle scores alongside zero-shot results: still fine-tune using English data, but choose a checkpoint with the target dev set.	4
Reporting this upper bound makes results more consistent by avoiding arbitrarily bad checkpoints.	4
----------
We present a novel supervised word alignment method based on cross-language span prediction.	2
We first formalize a word alignment problem as a collection of independent predictions from a token in the source sentence to a span in the target sentence.	3
Since this step is equivalent to a SQuAD v2.0 style question answering task, we solve it using the multilingual BERT, which is fine-tuned on manually created gold word alignment data.	3
It is nontrivial to obtain accurate alignment from a set of independently predicted spans.	1
We greatly improved the word alignment accuracy by adding to the question the source token’s context and symmetrizing two directional predictions.	4
In experiments using five word alignment datasets from among Chinese, Japanese, German, Romanian, French, and English, we show that our proposed method significantly outperformed previous supervised and unsupervised word alignment methods without any bitexts for pretraining.	3+4
For example, we achieved 86.7 F1 score for the Chinese-English data, which is 13.3 points higher than the previous state-of-the-art supervised method.	4
----------
Despite its original goal to jointly learn to align and translate, prior researches suggest that Transformer captures poor word alignments through its attention mechanism.	1
In this paper, we show that attention weights do capture accurate word alignments and propose two novel word alignment induction methods Shift-Att and Shift-AET.	2
The main idea is to induce alignments at the step when the to-be-aligned target token is the decoder input rather than the decoder output as in previous work.	3
Shift-Att is an interpretation method that induces alignments from the attention weights of Transformer and does not require parameter update or architecture change.	4
Shift-AET extracts alignments from an additional alignment module which is tightly integrated into Transformer and trained in isolation with supervision from symmetrized Shift-Att alignments.	3
Experiments on three publicly available datasets demonstrate that both methods perform better than their corresponding neural baselines and Shift-AET significantly outperforms GIZA++ by 1.4-4.8 AER points.	4
----------
Cherokee is a highly endangered Native American language spoken by the Cherokee people.	1
The Cherokee culture is deeply embedded in its language.	1
However, there are approximately only 2,000 fluent first language Cherokee speakers remaining in the world and the number is declining every year.	1
To help save this endangered language, we introduce ChrEn, a Cherokee-English parallel dataset, to facilitate machine translation research between Cherokee and English.	2
Compared to some popular machine translation language pairs, ChrEn is extremely low-resource, only containing 14k sentence pairs in total.	3
We split our parallel data in ways that facilitate both in-domain and out-of-domain evaluation.	3
We also collect 5k Cherokee monolingual data to enable semi-supervised learning.	3
Besides these datasets, we propose several Cherokee-English and English-Cherokee machine translation systems.	3
We compare SMT (phrase-based) versus NMT (RNN-based and Transformer-based) systems; supervised versus semi-supervised (via language model, back-translation, and BERT/Multilingual-BERT) methods; as well as transfer learning versus multilingual joint training with 4 other languages.	3
Our best results are 15.8/12.7 BLEU for in-domain and 6.5/5.0 BLEU for out-of-domain Chr-En/EnChr translations, respectively; and we hope that our dataset and systems will encourage future work by the community for Cherokee language revitalization.	4+5
----------
Despite their prevalence in society, social biases are difficult to identify, primarily because human judgements in this domain can be unreliable.	1
We take an unsupervised approach to identifying gender bias against women at a comment level and present a model that can surface text likely to contain bias.	2
Our main challenge is forcing the model to focus on signs of implicit bias, rather than other artifacts in the data.	3
Thus, our methodology involves reducing the influence of confounds through propensity matching and adversarial learning.	3
Our analysis shows how biased comments directed towards female politicians contain mixed criticisms, while comments directed towards other female public figures focus on appearance and sexualization.	3
Ultimately, our work offers a way to capture subtle biases in various domains without relying on subjective human judgements.	4+5
----------
Offering condolence is a natural reaction to hearing someone’s distress.	1
Individuals frequently express distress in social media, where some communities can provide support.	2
However, not all condolence is equal—trite responses offer little actual support despite their good intentions.	3
Here, we develop computational tools to create a massive dataset of 11.4M expressions of distress and 2.8M corresponding offerings of condolence in order to examine the dynamics of condolence online.	3
Our study reveals widespread disparity in what types of distress receive supportive condolence rather than just engagement.	4
Building on studies from social psychology, we analyze the language of condolence and develop a new dataset for quantifying the empathy in a condolence using appraisal theory.	4
Finally, we demonstrate that the features of condolence individuals find most helpful online differ substantially in their features from those seen in interpersonal settings.	4
----------
Legislator preferences are typically represented as measures of general ideology estimated from roll call votes on legislation, potentially masking important nuances in legislators’ political attitudes.	1
In this paper we introduce a method of measuring more specific legislator attitudes using an alternative expression of preferences: tweeting.	2
Specifically, we present an embedding-based model for predicting the frequency and sentiment of legislator tweets.	3
To illustrate our method, we model legislators’ attitudes towards President Donald Trump as vector embeddings that interact with embeddings for Trump himself constructed using a neural network from the text of his daily tweets.	3
We demonstrate the predictive performance of our model on tweets authored by members of the U.S. House and Senate related to the president from November 2016 to February 2018.	4
We further assess the quality of our learned representations for legislators by comparing to traditional measures of legislator preferences.	4
----------
We present the task of modeling information propagation in literature, in which we seek to identify pieces of information passing from character A to character B to character C, only given a description of their activity in text.	2
We describe a new pipeline for measuring information propagation in this domain and publish a new dataset for speaker attribution, enabling the evaluation of an important component of this pipeline on a wider range of literary texts than previously studied.	2
Using this pipeline, we analyze the dynamics of information propagation in over 5,000 works of fiction, finding that information flows through characters that fill structural holes connecting different communities, and that characters who are women are depicted as filling this role much more frequently than characters who are men.	3+4
----------
Social norms—the unspoken commonsense rules about acceptable social behavior—are crucial in understanding the underlying causes and intents of people’s actions in narratives.	1
For example, underlying an action such as “wanting to call cops on my neighbor” are social norms that inform our conduct, such as “It is expected that you report crimes.” We present SOCIAL CHEMISTRY, a new conceptual formalism to study people’s everyday social norms and moral judgments over a rich spectrum of real life situations described in natural language.	1+2
We introduce SOCIAL-CHEM-101, a large-scale corpus that catalogs 292k rules-of-thumb such as “It is rude to run a blender at 5am” as the basic conceptual units.	3
Each rule-of-thumb is further broken down with 12 different dimensions of people’s judgments, including social judgments of good and bad, moral foundations, expected cultural pressure, and assumed legality, which together amount to over 4.5 million annotations of categorical labels and free-text descriptions.	3
Comprehensive empirical results based on state-of-the-art neural models demonstrate that computational modeling of social norms is a promising research direction.	4
Our model framework, Neural Norm Transformer, learns and generalizes SOCIAL-CHEM-101 to successfully reason about previously unseen situations, generating relevant (and potentially novel) attribute-aware social rules-of-thumb.	3
----------
The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments.	1
Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation.	1
To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner.	2+3
Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zero-shot learning setting).	3+4
----------
Event schemas can guide our understanding and ability to make predictions with respect to what might happen next.	1
We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story.	2
We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas.	3
We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph.	3
Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas.	4
Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.	3+4
----------
Understanding natural language involves recognizing how multiple event mentions structurally and temporally interact with each other.	1
In this process, one can induce event complexes that organize multi-granular events with temporal order and membership relations interweaving among them.	1
Due to the lack of jointly labeled data for these relational phenomena and the restriction on the structures they articulate, we propose a joint constrained learning framework for modeling event-event relations.	2+3
Specifically, the framework enforces logical constraints within and across multiple temporal and subevent relations of events by converting these constraints into differentiable learning objectives.	3
We show that our joint constrained learning approach effectively compensates for the lack of jointly labeled data, and outperforms SOTA methods on benchmarks for both temporal relation extraction and event hierarchy construction, replacing a commonly used but more expensive global inference process.	3+4
We also present a promising case study to show the effectiveness of our approach to inducing event complexes on an external corpus.	5
----------
Conventional approaches to event detection usually require a fixed set of pre-defined event types.	1
Such a requirement is often challenged in real-world applications, as new events continually occur.	1
Due to huge computation cost and storage budge, it is infeasible to store all previous data and re-train the model with all previous data and new data, every time new events arrive.	1
We formulate such challenging scenarios as incremental event detection, which requires a model to learn new classes incrementally without performance degradation on previous classes.	1
However, existing incremental learning methods cannot handle semantic ambiguity and training data imbalance problems between old and new classes in the task of incremental event detection.	1
In this paper, we propose a Knowledge Consolidation Network (KCN) to address the above issues.	2
Specifically, we devise two components, prototype enhanced retrospection and hierarchical distillation, to mitigate the adverse effects of semantic ambiguity and class imbalance, respectively.	3
Experimental results demonstrate the effectiveness of the proposed method, outperforming the state-of-the-art model by 19% and 13.4% of whole F1 score on ACE benchmark and TAC KBP benchmark, respectively.	4
----------
Most previous event extraction studies assume a set of target event types and corresponding event annotations are given, which could be very expensive.	1
In this paper, we work on a new task of semi-supervised event type induction, aiming to automatically discover a set of unseen types from a given corpus by leveraging annotations available for a few seen types.	2+3
We design a Semi-Supervised Vector Quantized Variational Autoencoder framework to automatically learn a discrete latent type representation for each seen and unseen type and optimize them using seen type event annotations.	3
A variational autoencoder is further introduced to enforce the reconstruction of each event mention conditioned on its latent type distribution.	3
Experiments show that our approach can not only achieve state-of-the-art performance on supervised event detection but also discover high-quality new event types.	4
----------
Despite the success of generative pre-trained language models on a series of text generation tasks, they still suffer in cases where reasoning over underlying commonsense knowledge is required during generation.	1
Existing approaches that integrate commonsense knowledge into generative pre-trained language models simply transfer relational knowledge by post-training on individual knowledge triples while ignoring rich connections within the knowledge graph.	1
We argue that exploiting both the structural and semantic information of the knowledge graph facilitates commonsense-aware text generation.	1
In this paper, we propose Generation with Multi-Hop Reasoning Flow (GRF) that enables pre-trained models with dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense knowledge graph.	2
We empirically show that our model outperforms existing baselines on three text generation tasks that require reasoning over commonsense knowledge.	4
We also demonstrate the effectiveness of the dynamic multi-hop reasoning module with reasoning paths inferred by the model that provide rationale to the generation.	4
----------
Modern NLP defines the task of style transfer as modifying the style of a given sentence without appreciably changing its semantics, which implies that the outputs of style transfer systems should be paraphrases of their inputs.	1
However, many existing systems purportedly designed for style transfer inherently warp the input’s meaning through attribute transfer, which changes semantic properties such as sentiment.	1
In this paper, we reformulate unsupervised style transfer as a paraphrase generation problem, and present a simple methodology based on fine-tuning pretrained language models on automatically generated paraphrase data.	2
Despite its simplicity, our method significantly outperforms state-of-the-art style transfer systems on both human and automatic evaluations.	4
We also survey 23 style transfer papers and discover that existing automatic metrics can be easily gamed and propose fixed variants.	4
Finally, we pivot to a more real-world style transfer setting by collecting a large dataset of 15M sentences in 11 diverse styles, which we use for an in-depth analysis of our system.	3+4
----------
Court’s view generation is a novel but essential task for legal AI, aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation.	1
While prior text-to-text natural language generation (NLG) approaches can be used to address this problem, neglecting the confounding bias from the data generation mechanism can limit the model performance, and the bias may pollute the learning outcomes.	1
In this paper, we propose a novel Attentional and Counterfactual based Natural Language Generation (AC-NLG) method, consisting of an attentional encoder and a pair of innovative counterfactual decoders.	2
The attentional encoder leverages the plaintiff’s claim and fact description as input to learn a claim-aware encoder from which the claim-related information in fact description can be emphasized.	3
The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgment-discriminative court’s views (both supportive and non-supportive views) by incorporating with a synergistic judgment predictive model.	3
Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics.	4
----------
Pre-trained Transformers have enabled impressive breakthroughs in generating long and fluent text, yet their outputs are often “rambling” without coherently arranged content.	1
In this work, we present a novel content-controlled text generation framework, PAIR, with planning and iterative refinement, which is built upon a large model, BART.	2
We first adapt the BERT model to automatically construct the content plans, consisting of keyphrase assignments and their corresponding sentence-level positions.	3
The BART model is employed for generation without modifying its structure.	3
We then propose a refinement algorithm to gradually enhance the generation quality within the sequence-to-sequence framework.	3
Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements.	3
In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning.	3
----------
Abductive and counterfactual reasoning, core abilities of everyday human cognition, require reasoning about what might have happened at time t, while conditioning on multiple contexts from the relative past and future.	1
However, simultaneous incorporation of past and future contexts using generative language models (LMs) can be challenging, as they are trained either to condition only on the past context or to perform narrowly scoped text-infilling.	1
In this paper, we propose DeLorean, a new unsupervised decoding algorithm that can flexibly incorporate both the past and future contexts using only off-the-shelf, left-to-right language models and no supervision.	2+3
The key intuition of our algorithm is incorporating the future through back-propagation, during which, we only update the internal representation of the output while fixing the model parameters.	3
By alternating between forward and backward propagation, DeLorean can decode the output representation that reflects both the left and right contexts.	3
We demonstrate that our approach is general and applicable to two nonmonotonic reasoning tasks: abductive text generation and counterfactual story revision, where DeLorean outperforms a range of unsupervised and some supervised methods, based on automatic and human evaluation.	3+4
----------
Learning to fuse vision and language information and representing them is an important research problem with many applications.	1
Recent progresses have leveraged the ideas of pre-training (from language modeling) and attention layers in Transformers to learn representation from datasets containing images aligned with linguistic expressions that describe the images.	1
In this paper, we propose learning representations from a set of implied, visually grounded expressions between image and text, automatically mined from those datasets.	2
In particular, we use denotation graphs to represent how specific concepts (such as sentences describing images) can be linked to abstract and generic concepts (such as short phrases) that are also visually grounded.	3
This type of generic-to-specific relations can be discovered using linguistic analysis tools.	4
We propose methods to incorporate such relations into learning representation.	4
We show that state-of-the-art multimodal learning models can be further improved by leveraging automatically harvested structural relations.	4
The representations lead to stronger empirical results on downstream tasks of cross-modal image retrieval, referring expression, and compositional attribute-object recognition.	4
Both our codes and the extracted denotation graphs on the Flickr30K and the COCO datasets are publically available on https://sha-lab.github.io/DG.	6
----------
Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering.	1
However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data.	1
We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task.	2
This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure.	3
For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation.	4
Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions.	4
We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model.	4+5
----------
While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting.	1
As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization.	1
In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge.	2
Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer).	3
Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions.	4
MUTANT establishes a new state-of-the-art accuracy on VQA-CP with a 10.57% improvement.	4
Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.	5
----------
Dialogue systems play an increasingly important role in various aspects of our daily life.	1
It is evident from recent research that dialogue systems trained on human conversation data are biased.	2
In particular, they can produce responses that reflect people’s gender prejudice.	3
Many debiasing methods have been developed for various NLP tasks, such as word embedding.	3
However, they are not directly applicable to dialogue systems because they are likely to force dialogue models to generate similar responses for different genders.	4
This greatly degrades the diversity of the generated responses and immensely hurts the performance of the dialogue models.	4
In this paper, we propose a novel adversarial learning framework Debiased-Chat to train dialogue models free from gender bias while keeping their performance.	5
Extensive experiments on two real-world conversation datasets show that our framework significantly reduces gender bias in dialogue models while maintaining the response quality.	4
----------
We explore the task of improving persona consistency of dialogue agents.	2
Recent models tackling consistency often train with additional Natural Language Inference (NLI) labels or attach trained extra modules to the generative agent for maintaining consistency.	1
However, such additional labels and training can be demanding.	1
Also, we find even the best-performing persona-based agents are insensitive to contradictory words.	2
Inspired by social cognition and pragmatics, we endow existing dialogue agents with public self-consciousness on the fly through an imaginary listener.	3
Our approach, based on the Rational Speech Acts framework (Frank and Goodman, 2012), can enforce dialogue agents to refrain from uttering contradiction.	3
We further extend the framework by learning the distractor selection, which has been usually done manually or randomly.	3
Results on Dialogue NLI (Welleck et al., 2019) and PersonaChat (Zhang et al., 2018) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models.	4
Moreover, we show that it can be generalized to improve context-consistency beyond persona in dialogues.	4
----------
The underlying difference of linguistic patterns between general text and task-oriented dialogue makes existing pre-trained language models less useful in practice.	1
In this work, we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling.	2
To better model dialogue behavior during pre-training, we incorporate user and system tokens into the masked language modeling.	3
We propose a contrastive objective function to simulate the response selection task.	3
Our pre-trained task-oriented dialogue BERT (TOD-BERT) outperforms strong baselines like BERT on four downstream task-oriented dialogue applications, including intention recognition, dialogue state tracking, dialogue act prediction, and response selection.	4
We also show that TOD-BERT has a stronger few-shot ability that can mitigate the data scarcity problem for task-oriented dialogue.	4
----------
Large-scale dialogue datasets have recently become available for training neural dialogue agents.	1
However, these datasets have been reported to contain a non-negligible number of unacceptable utterance pairs.	1
In this paper, we propose a method for scoring the quality of utterance pairs in terms of their connectivity and relatedness.	2
The proposed scoring method is designed based on findings widely shared in the dialogue and linguistics research communities.	3
We demonstrate that it has a relatively good correlation with the human judgment of dialogue quality.	4
Furthermore, the method is applied to filter out potentially unacceptable utterance pairs from a large-scale noisy dialogue corpus to ensure its quality.	4
We experimentally confirm that training data filtered by the proposed method improves the quality of neural dialogue agents in response generation.	5
----------
Analyzing the evolution of dialects remains a challenging problem because contact phenomena hinder the application of the standard tree model.	1
Previous statistical approaches to this problem resort to admixture analysis, where each dialect is seen as a mixture of latent ancestral populations.	1
However, such ancestral populations are hardly interpretable in the context of the tree model.	1
In this paper, we propose a probabilistic generative model that represents latent factors as geographical distributions.	2
We argue that the proposed model has higher affinity with the tree model because a tree can alternatively be represented as a set of geographical distributions.	5
Experiments involving synthetic and real data suggest that the proposed method is both quantitatively and qualitatively superior to the admixture model.	4
----------
Word class flexibility refers to the phenomenon whereby a single word form is used across different grammatical categories.	1
Extensive work in linguistic typology has sought to characterize word class flexibility across languages, but quantifying this phenomenon accurately and at scale has been fraught with difficulties.	1
We propose a principled methodology to explore regularity in word class flexibility.	2
Our method builds on recent work in contextualized word embeddings to quantify semantic shift between word classes (e.g., noun-to-verb, verb-to-noun), and we apply this method to 37 languages.	3
We find that contextualized embeddings not only capture human judgment of class variation within words in English, but also uncover shared tendencies in class flexibility across languages.	4
Specifically, we find greater semantic variation when flexible lemmas are used in their dominant word class, supporting the view that word class flexibility is a directional process.	3
Our work highlights the utility of deep contextualized models in linguistic typology.	5
----------
Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming.	1
Moreover, why deep models help NMT is an open question.	1
In this paper, we investigate the behavior of a well-tuned deep Transformer system.	2
We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly.	4
This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models.	3
In this way, we successfully train a Transformer system with a 54-layer encoder.	3
Experimental results on WMT’16 English-German and WMT’14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks.	4
The code is publicly available at https://github.com/libeineu/SDT-Training.	6
----------
We propose an efficient inference procedure for non-autoregressive machine translation that iteratively refines translation purely in the continuous space.	2
Given a continuous latent variable model for machine translation (Shu et al., 2020), we train an inference network to approximate the gradient of the marginal log probability of the target sentence, using the latent variable instead.	3
This allows us to use gradient-based optimization to find the target sentence at inference time that approximately maximizes its marginal probability.	3
As each refinement step only involves computation in the latent space of low dimensionality (we use 8 in our experiments), we avoid computational overhead incurred by existing non-autoregressive inference procedures that often refine in token space.	4
We compare our approach to a recently proposed EM-like inference procedure (Shu et al., 2020) that optimizes in a hybrid space, consisting of both discrete and continuous variables.	3
We evaluate our approach on WMT’14 En→De, WMT’16 Ro→En and IWSLT’16 De→En, and observe two advantages over the EM-like inference: (1) it is computationally efficient, i.e. each refinement step is twice as fast, and (2) it is more effective, resulting in higher marginal probabilities and BLEU scores with the same number of refinement steps.	3
On WMT’14 En→De, for instance, our approach is able to decode 6.2 times faster than the autoregressive model with minimal degradation to translation quality (0.9 BLEU).	3
----------
With the growth of computing power neural machine translation (NMT) models also grow accordingly and become better.	1
However, they also become harder to deploy on edge devices due to memory constraints.	1
To cope with this problem, a common practice is to distill knowledge from a large and accurately-trained teacher network (T) into a compact student network (S).	1
Although knowledge distillation (KD) is useful in most cases, our study shows that existing KD techniques might not be suitable enough for deep NMT engines, so we propose a novel alternative.	2+3
In our model, besides matching T and S predictions we have a combinatorial mechanism to inject layer-level supervision from T to S. In this paper, we target low-resource settings and evaluate our translation engines for Portuguese→English, Turkish→English, and English→German directions.	3
Students trained using our technique have 50% fewer parameters and can still deliver comparable results to those of 12-layer teachers.	4
----------
While monolingual data has been shown to be useful in improving bilingual neural machine translation (NMT), effectively and efficiently leveraging monolingual data for Multilingual NMT (MNMT) systems is a less explored area.	1
In this work, we propose a multi-task learning (MTL) framework that jointly trains the model with the translation task on bitext data and two denoising tasks on the monolingual data.	2
We conduct extensive empirical studies on MNMT systems with 10 language pairs from WMT datasets.	3
We show that the proposed approach can effectively improve the translation quality for both high-resource and low-resource languages with large margin, achieving significantly better results than the individual bilingual models.	4
We also demonstrate the efficacy of the proposed approach in the zero-shot setup for language pairs without bitext training data.	4
Furthermore, we show the effectiveness of MTL over pre-training approaches for both NMT and cross-lingual transfer learning NLU tasks; the proposed approach outperforms massive scale models trained on single task.	4
----------
There exists a token imbalance phenomenon in natural language as different tokens appear with different frequencies, which leads to different learning difficulties for tokens in Neural Machine Translation (NMT).	1
The vanilla NMT model usually adopts trivial equal-weighted objectives for target tokens with different frequencies and tends to generate more high-frequency tokens and less low-frequency tokens compared with the golden token distribution.	1
However, low-frequency tokens may carry critical semantic information that will affect the translation quality once they are neglected.	1
In this paper, we explored target token-level adaptive objectives based on token frequencies to assign appropriate weights for each target token during training.	2
We aimed that those meaningful but relatively low-frequency words could be assigned with larger weights in objectives to encourage the model to pay more attention to these tokens.	3
Our method yields consistent improvements in translation quality on ZH-EN, EN-RO, and EN-DE translation tasks, especially on sentences that contain more low-frequency tokens where we can get 1.68, 1.02, and 0.52 BLEU increases compared with baseline, respectively.	3
Further analyses show that our method can also improve the lexical diversity of translation.	3
----------
Transformer models achieve remarkable success in Neural Machine Translation.	1
Many efforts have been devoted to deepening the Transformer by stacking several units (i.e., a combination of Multihead Attentions and FFN) in a cascade, while the investigation over multiple parallel units draws little attention.	1
In this paper, we propose the Multi-Unit Transformer (MUTE) , which aim to promote the expressiveness of the Transformer by introducing diverse and complementary units.	2
Specifically, we use several parallel units and show that modeling with multiple units improves model performance and introduces diversity.	4
Further, to better leverage the advantage of the multi-unit setting, we design biased module and sequential dependency that guide and encourage complementariness among different units.	3
Experimental results on three machine translation tasks, the NIST Chinese-to-English, WMT’14 English-to-German and WMT’18 Chinese-to-English, show that the MUTE models significantly outperform the Transformer-Base, by up to +1.52, +1.90 and +1.10 BLEU points, with only a mild drop in inference speed (about 3.1%).	4
In addition, our methods also surpass the Transformer-Big model, with only 54% of its parameters.	4
These results demonstrate the effectiveness of the MUTE, as well as its efficiency in both the inference process and parameter usage.	4
----------
Modern neural machine translation (NMT) models employ a large number of parameters, which leads to serious over-parameterization and typically causes the underutilization of computational resources.	1
In response to this problem, we empirically investigate whether the redundant parameters can be reused to achieve better performance.	2
Experiments and analyses are systematically conducted on different datasets and NMT architectures.	3
We show that: 1) the pruned parameters can be rejuvenated to improve the baseline model by up to +0.8 BLEU points; 2) the rejuvenated parameters are reallocated to enhance the ability of modeling low-level lexical information.	4
----------
In this work, we introduce a novel local autoregressive translation (LAT) mechanism into non-autoregressive translation (NAT) models so as to capture local dependencies among target outputs.	2
Specifically, for each target decoding position, instead of only one token, we predict a short sequence of tokens in an autoregressive way.	3
We further design an efficient merging algorithm to align and merge the output pieces into one final output sequence.	3
We integrate LAT into the conditional masked language model (CMLM) (Ghazvininejad et al.,2019) and similarly adopt iterative decoding.	3
Empirical results on five translation tasks show that compared with CMLM, our method achieves comparable or better performance with fewer decoding iterations, bringing a 2.5x speedup.	4
Further analysis indicates that our method reduces repeated translations and performs better at longer sentences.	3
Our code will be released to the public.	6
----------
Recent studies have proven that the training of neural machine translation (NMT) can be facilitated by mimicking the learning process of humans.	1
Nevertheless, achievements of such kind of curriculum learning rely on the quality of artificial schedule drawn up with the handcrafted features, e.g. sentence length or word rarity.	1
We ameliorate this procedure with a more flexible manner by proposing self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step.	2+3
Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.	4
----------
Many document-level neural machine translation (NMT) systems have explored the utility of context-aware architecture, usually requiring an increasing number of parameters and computational complexity.	1
However, few attention is paid to the baseline model.	1
In this paper, we research extensively the pros and cons of the standard transformer in document-level translation, and find that the auto-regressive property can simultaneously bring both the advantage of the consistency and the disadvantage of error accumulation.	2
Therefore, we propose a surprisingly simple long-short term masking self-attention on top of the standard transformer to both effectively capture the long-range dependence and reduce the propagation of errors.	4
We examine our approach on the two publicly available document-level datasets.	4
We can achieve a strong result in BLEU and capture discourse phenomena.	4
----------
Despite the improvement of translation quality, neural machine translation (NMT) often suffers from the lack of diversity in its generation.	1
In this paper, we propose to generate diverse translations by deriving a large number of possible models with Bayesian modelling and sampling models from them for inference.	2
The possible models are obtained by applying concrete dropout to the NMT model and each of them has specific confidence for its prediction, which corresponds to a posterior model distribution under specific training data in the principle of Bayesian modeling.	3
With variational inference, the posterior model distribution can be approximated with a variational distribution, from which the final models for inference are sampled.	3
We conducted experiments on Chinese-English and English-German translation tasks and the results shows that our method makes a better trade-off between diversity and accuracy.	4
----------
This paper presents two strong methods, CTC and Imputer, for non-autoregressive machine translation that model latent alignments with dynamic programming.	2+3
We revisit CTC for machine translation and demonstrate that a simple CTC model can achieve state-of-the-art for single-step non-autoregressive machine translation, contrary to what prior work indicates.	3+4
In addition, we adapt the Imputer model for non-autoregressive machine translation and demonstrate that Imputer with just 4 generation steps can match the performance of an autoregressive Transformer baseline.	4
Our latent alignment models are simpler than many existing non-autoregressive translation baselines; for example, we do not require target length prediction or re-scoring with an autoregressive model.	4
On the competitive WMT’14 En→De task, our CTC model achieves 25.7 BLEU with a single generation step, while Imputer achieves 27.5 BLEU with 2 generation steps, and 28.0 BLEU with 4 generation steps.	4
This compares favourably to the autoregressive Transformer baseline at 27.8 BLEU.	4
----------
Many extractive question answering models are trained to predict start and end positions of answers.	1
The choice of predicting answers as positions is mainly due to its simplicity and effectiveness.	1
In this study, we hypothesize that when the distribution of the answer positions is highly skewed in the training set (e.g., answers lie only in the k-th sentence of each passage), QA models predicting answers as positions can learn spurious positional cues and fail to give answers in different positions.	1+2
We first illustrate this position bias in popular extractive QA models such as BiDAF and BERT and thoroughly examine how position bias propagates through each layer of BERT.	3
To safely deliver position information without position bias, we train models with various de-biasing methods including entropy regularization and bias ensembling.	3
Among them, we found that using the prior distribution of answer positions as a bias model is very effective at reducing position bias, recovering the performance of BERT from 37.48% to 81.64% when trained on a biased SQuAD dataset.	4
----------
BERT-era question answering systems have recently achieved impressive performance on several question-answering (QA) tasks.	1
These systems are based on representations that have been pre-trained on self-supervised tasks such as word masking and sentence entailment, using massive amounts of data.	1
Nevertheless, additional pre-training closer to the end-task, such as training on synthetic QA pairs, has been shown to improve performance.	1
While recent work has considered augmenting labelled data and leveraging large unlabelled datasets to generate synthetic QA data, directly adapting to target data has received little attention.	1
In this paper we investigate the iterative generation of synthetic QA pairs as a way to realize unsupervised self adaptation.	2
Motivated by the success of the roundtrip consistency method for filtering generated QA pairs, we present iterative generalizations of the approach, which maximize an approximation of a lower bound on the probability of the adaptation data.	3
By adapting on synthetic QA pairs generated on the target data, our method is able to improve QA systems significantly, using an order of magnitude less synthetic data and training computation than existing augmentation approaches.	3
----------
AMR-to-text generation aims to recover a text containing the same meaning as an input AMR graph.	1
Current research develops increasingly powerful graph encoders to better represent AMR graphs, with decoders based on standard language modeling being used to generate outputs.	1
We propose a decoder that back predicts projected AMR graphs on the target sentence during text generation.	2
As the result, our outputs can better preserve the input meaning than standard decoders.	4
Experiments on two AMR benchmarks show the superiority of our model over the previous state-of-the-art system based on graph Transformer.	4
----------
Generating long form narratives such as stories and procedures from multiple modalities has been a long standing dream for artificial intelligence.	1
In this regard, there is often crucial subtext that is derived from the surrounding contexts.	1
The general seq2seq training methods render the models shorthanded while attempting to bridge the gap between these neighbouring contexts.	1
In this paper, we tackle this problem by using infilling techniques involving prediction of missing steps in a narrative while generating textual descriptions from a sequence of images.	2+3
We also present a new large scale visual procedure telling (ViPT) dataset with a total of 46,200 procedures and around 340k pairwise images and textual descriptions that is rich in such contextual dependencies.	4
Generating steps using infilling technique demonstrates the effectiveness in visual procedures with more coherent texts.	4
We conclusively show a METEOR score of 27.51 on procedures which is higher than the state-of-the-art on visual storytelling.	4
We also demonstrate the effects of interposing new text with missing images during inference.	4
The code and the dataset will be publicly available at https://visual-narratives.github.io/Visual-Narratives/.	6
----------
We propose a new task in the area of computational creativity: acrostic poem generation in English.	2
Acrostic poems are poems that contain a hidden message; typically, the first letter of each line spells out a word or short phrase.	1
We define the task as a generation task with multiple constraints: given an input word, 1) the initial letters of each line should spell out the provided word, 2) the poem’s semantics should also relate to it, and 3) the poem should conform to a rhyming scheme.	3
We further provide a baseline model for the task, which consists of a conditional neural language model in combination with a neural rhyming model.	4
Since no dedicated datasets for acrostic poem generation exist, we create training data for our task by first training a separate topic prediction model on a small set of topic-annotated poems and then predicting topics for additional poems.	3
Our experiments show that the acrostic poems generated by our baseline are received well by humans and do not lose much quality due to the additional constraints.	4
Last, we confirm that poems generated by our model are indeed closely related to the provided prompts, and that pretraining on Wikipedia can boost performance.	3
----------
Named Entity Recognition (NER) is one of the first stages in deep language understanding yet current NER models heavily rely on human-annotated data.	1
In this work, to alleviate the dependence on labeled data, we propose a Local Additivity based Data Augmentation (LADA) method for semi-supervised NER, in which we create virtual samples by interpolating sequences close to each other.	2
Our approach has two variations: Intra-LADA and Inter-LADA, where Intra-LADA performs interpolations among tokens within one sentence, and Inter-LADA samples different sentences to interpolate.	3
Through linear additions between sampled training data, LADA creates an infinite amount of labeled data and improves both entity and context learning.	3
We further extend LADA to the semi-supervised setting by designing a novel consistency loss for unlabeled data.	3
Experiments conducted on two NER benchmarks demonstrate the effectiveness of our methods over several strong baselines.	4
We have publicly released our code at https://github.com/GT-SALT/LADA	6
----------
Language models have emerged as a central component across NLP, and a great deal of progress depends on the ability to cheaply adapt them (e.g., through finetuning) to new domains and tasks.	1
A language model’s vocabulary—typically selected before training and permanently fixed later—affects its size and is part of what makes it resistant to such adaptation.	1
Prior work has used compositional input embeddings based on surface forms to ameliorate this issue.	1
In this work, we go one step beyond and propose a fully compositional output embedding layer for language models, which is further grounded in information from a structured lexicon (WordNet), namely semantically related words and free-text definitions.	2+3
To our knowledge, the result is the first word-level language model with a size that does not depend on the training vocabulary.	4
We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-the-art output embedding methods and adaptation approaches.	4
Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words.	4
----------
Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples.	1
Data augmentation is a common method used to prevent overfitting and improve OOD generalization.	1
However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold.	1
We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold.	2+3
We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models.	3
In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8% on OOD Amazon reviews, 1.8% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.	3
----------
For many real-world classification problems, e.g., sentiment classification, most existing machine learning methods are biased towards the majority class when the Imbalance Ratio (IR) is high.	1
To address this problem, we propose a set convolution (SetConv) operation and an episodic training strategy to extract a single representative for each class, so that classifiers can later be trained on a balanced class distribution.	2+3
We prove that our proposed algorithm is permutation-invariant despite the order of inputs, and experiments on multiple large-scale benchmark text datasets show the superiority of our proposed framework when compared to other SOTA methods.	4+5
----------
Existing work on augmenting question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model’s prediction rationale.	1
In this paper, we propose a novel knowledge-aware approach that equips pre-trained language models (PTLMs) has with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN).	2
It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs.	3
The proposed reasoning module unifies path-based reasoning methods and graph neural networks to achieve better interpretability and scalability.	3
We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released.	3+4
----------
This paper designs a Monolingual Lexicon Induction task and observes that two factors accompany the degraded accuracy of bilingual lexicon induction for rare words.	2
First, a diminishing margin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency.	3
Based on the observation, we further propose two methods to address these two factors, respectively.	3
The larger issue is hubness.	4
Addressing that improves induction accuracy significantly, especially for low-frequency words.	4
----------
The introduction of VAE provides an efficient framework for the learning of generative models, including generative topic models.	1
However, when the topic model is a Latent Dirichlet Allocation (LDA) model, a central technique of VAE, the reparameterization trick, fails to be applicable.	1
This is because no reparameterization form of Dirichlet distributions is known to date that allows the use of the reparameterization trick.	1
In this work, we propose a new method, which we call Rounded Reparameterization Trick (RRT), to reparameterize Dirichlet distributions for the learning of VAE-LDA models.	2
This method, when applied to a VAE-LDA model, is shown experimentally to outperform the existing neural topic models on several benchmark datasets and on a synthetic dataset.	3+4
----------
Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization.	1
To mitigate this issue, we propose a regularized fine-tuning method.	2
Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold.	3
Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration.	4
(2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data.	4
Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets.	4+5
Our code can be found at https://github.com/Lingkai-Kong/Calibrated-BERT-Fine-Tuning.	6
----------
The hidden Markov model (HMM) is a fundamental tool for sequence modeling that cleanly separates the hidden state from the emission structure.	1
However, this separation makes it difficult to fit HMMs to large datasets in modern NLP, and they have fallen out of use due to very poor performance compared to fully observed models.	1
This work revisits the challenge of scaling HMMs to language modeling datasets, taking ideas from recent approaches to neural modeling.	1
We propose methods for scaling HMMs to massive state spaces while maintaining efficient exact inference, a compact parameterization, and effective regularization.	2
Experiments show that this approach leads to models that are much more accurate than previous HMMs and n-gram-based methods, making progress towards the performance of state-of-the-art NN models.	3+4
----------
Natural Language Processing (NLP) tasks are usually performed word by word on textual inputs.	1
We can use arbitrary symbols to represent the linguistic meaning of a word and use these symbols as inputs.	2
As “alternatives” to a text representation, we introduce Soundex, MetaPhone, NYSIIS, logogram to NLP, and develop fixed-output-length coding and its extension using Huffman coding.	3
Each of those codings combines different character/digital sequences and constructs a new vocabulary based on codewords.	3
We find that the integration of those codewords with text provides more reliable inputs to Neural-Network-based NLP systems through redundancy than text-alone inputs.	4
Experiments demonstrate that our approach outperforms the state-of-the-art models on the application of machine translation, language modeling, and part-of-speech tagging.	4
The source code is available at https://github.com/abdulrafae/coding_nmt.	6
----------
Typically, machine learning systems solve new tasks by training on thousands of examples.	1
In contrast, humans can solve new tasks by reading some instructions, with perhaps an example or two.	1
To take a step toward closing this gap, we introduce a framework for developing NLP systems that solve new tasks after reading their descriptions, synthesizing prior work in this area.	2
We instantiate this frame- work with a new English language dataset, ZEST, structured for task-oriented evaluation on unseen tasks.	3
Formulating task descriptions as questions, we ensure each is general enough to apply to many possible inputs, thus comprehensively evaluating a model’s ability to solve each task.	3
Moreover, the dataset’s structure tests specific types of systematic generalization.	3
We find that the state-of-the-art T5 model achieves a score of 12% on ZEST, leaving a significant challenge for NLP researchers.	4
----------
Existing approaches for named entity recognition suffer from data sparsity problems when conducted on short and informal texts, especially user-generated social media content.	1
Semantic augmentation is a potential way to alleviate this problem.	1
Given that rich semantic information is implicitly preserved in pre-trained word embeddings, they are potential ideal resources for semantic augmentation.	1
In this paper, we propose a neural-based approach to NER for social media texts where both local (from running text) and augmented semantics are taken into account.	2
In particular, we obtain the augmented semantic information from a large-scale corpus, and propose an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.	3
Extensive experiments are performed on three benchmark datasets collected from English and Chinese social media platforms, where the results demonstrate the superiority of our approach to previous studies across all three datasets.	4
----------
The prevalent use of social media enables rapid spread of rumors on a massive scale, which leads to the emerging need of automatic rumor verification (RV).	1
A number of previous studies focus on leveraging stance classification to enhance RV with multi-task learning (MTL) methods.	1
However, most of these methods failed to employ pre-trained contextualized embeddings such as BERT, and did not exploit inter-task dependencies by using predicted stance labels to improve the RV task.	1
Therefore, in this paper, to extend BERT to obtain thread representations, we first propose a Hierarchical Transformer, which divides each long thread into shorter subthreads, and employs BERT to separately represent each subthread, followed by a global Transformer layer to encode all the subthreads.	2+3
We further propose a Coupled Transformer Module to capture the inter-task interactions and a Post-Level Attention layer to use the predicted stance labels for RV, respectively.	2+3
Experiments on two benchmark datasets show the superiority of our Coupled Hierarchical Transformer model over existing MTL approaches.	4
----------
Social media sites like Twitter possess the potential to complement surveys that measure political opinions and, more specifically, political actors’ approval.	1
However, new challenges related to the reliability and validity of social-media-based estimates arise.	1
Various sentiment analysis and stance detection methods have been developed and used in previous research to measure users’ political opinions based on their content on social media.	1
In this work, we attempt to gauge the efficacy of untargeted sentiment, targeted sentiment, and stance detection methods in labeling various political actors’ approval by benchmarking them across several datasets.	2
We also contrast the performance of these pretrained methods that can be used in an off-the-shelf (OTS) manner against a set of models trained on minimal custom data.	3
We find that OTS methods have low generalizability on unseen and familiar targets, while low-resource custom models are more robust.	4
Our work sheds light on the strengths and limitations of existing methods proposed for understanding politicians’ approval from tweets.	5
----------
Machine reading comprehension (MRC) has achieved significant progress on the open domain in recent years, mainly due to large-scale pre-trained language models.	1
However, it performs much worse in specific domains such as the medical field due to the lack of extensive training data and professional structural knowledge neglect.	1
As an effort, we first collect a large scale medical multi-choice question dataset (more than 21k instances) for the National Licensed Pharmacist Examination in China.	3
It is a challenging medical examination with a passing rate of less than 14.2% in 2018.	3
Then we propose a novel reading comprehension model KMQA, which can fully exploit the structural medical knowledge (i.e., medical knowledge graph) and the reference medical plain text (i.e., text snippets retrieved from reference books).	2
The experimental results indicate that the KMQA outperforms existing competitive models with a large margin and passes the exam with 61.8% accuracy rate on the test set.	4
----------
Medical imaging is frequently used in clinical practice and trials for diagnosis and treatment.	1
Writing imaging reports is time-consuming and can be error-prone for inexperienced radiologists.	1
Therefore, automatically generating radiology reports is highly desired to lighten the workload of radiologists and accordingly promote clinical automation, which is an essential task to apply artificial intelligence to the medical domain.	1
In this paper, we propose to generate radiology reports with memory-driven Transformer, where a relational memory is designed to record key information of the generation process and a memory-driven conditional layer normalization is applied to incorporating the memory into the decoder of Transformer.	2
Experimental results on two prevailing radiology report datasets, IU X-Ray and MIMIC-CXR, show that our proposed approach outperforms previous models with respect to both language generation metrics and clinical evaluations.	4
Particularly, this is the first work reporting the generation results on MIMIC-CXR to the best of our knowledge.	4
Further analyses also demonstrate that our approach is able to generate long reports with necessary medical terms as well as meaningful image-text attention mappings.	4
----------
Existing approaches to disfluency detection heavily depend on human-annotated data.	1
Numbers of data augmentation methods have been proposed to alleviate the dependence on labeled data.	1
However, current augmentation approaches such as random insertion or repetition fail to resemble training corpus well and usually resulted in unnatural and limited types of disfluencies.	1
In this work, we propose a simple Planner-Generator based disfluency generation model to generate natural and diverse disfluent texts as augmented data, where the Planner decides on where to insert disfluent segments and the Generator follows the prediction to generate corresponding disfluent segments.	2+3
We further utilize this augmented data for pretraining and leverage it for the task of disfluency detection.	3
Experiments demonstrated that our two-stage disfluency generation model outperforms existing baselines; those disfluent sentences generated significantly aided the task of disfluency detection and led to state-of-the-art performance on Switchboard corpus.	4
----------
Clinical trials provide essential guidance for practicing Evidence-Based Medicine, though often accompanying with unendurable costs and risks.	1
To optimize the design of clinical trials, we introduce a novel Clinical Trial Result Prediction (CTRP) task.	2
In the CTRP framework, a model takes a PICO-formatted clinical trial proposal with its background as input and predicts the result, i.e. how the Intervention group compares with the Comparison group in terms of the measured Outcome in the studied Population.	3
While structured clinical evidence is prohibitively expensive for manual collection, we exploit large-scale unstructured sentences from medical literature that implicitly contain PICOs and results as evidence.	3
Specifically, we pre-train a model to predict the disentangled results from such implicit evidence and fine-tune the model with limited data on the downstream datasets.	3
Experiments on the benchmark Evidence Integration dataset show that the proposed model outperforms the baselines by large margins, e.g., with a 10.7% relative gain over BioBERT in macro-F1.	4
Moreover, the performance improvement is also validated on another dataset composed of clinical trials related to COVID-19.	4
----------
Clinical prediction models often use structured variables and provide outcomes that are not readily interpretable by clinicians.	1
Further, free-text medical notes may contain information not immediately available in structured variables.	1
We propose a hierarchical CNN-transformer model with explicit attention as an interpretable, multi-task clinical language model, which achieves an AUROC of 0.75 and 0.78 on sepsis and mortality prediction, respectively.	2
We also explore the relationships between learned features from structured and unstructured variables using projection-weighted canonical correlation analysis.	3
Finally, we outline a protocol to evaluate model usability in a clinical decision support context.	3
From domain-expert evaluations, our model generates informative rationales that have promising real-life applications.	3
----------
Medical entity normalization, which links medical mentions in the text to entities in knowledge bases, is an important research topic in medical natural language processing.	1
In this paper, we focus on Chinese medical procedure entity normalization.	2
However, nonstandard Chinese expressions and combined procedures present challenges in our problem.	3
The existing strategies relying on the discriminative model are poorly to cope with normalizing combined procedure mentions.	3
We propose a sequence generative framework to directly generate all the corresponding medical procedure entities.	3
we adopt two strategies: category-based constraint decoding and category-based model refining to avoid unrealistic results.	3
The method is capable of linking entities when a mention contains multiple procedure concepts and our comprehensive experiments demonstrate that the proposed model can achieve remarkable improvements over existing baselines, particularly significant in the case of multi-implication Chinese medical procedures.	4+5
----------
The extraction of labels from radiology text reports enables large-scale training of medical imaging models.	1
Existing approaches to report labeling typically rely either on sophisticated feature engineering based on medical domain knowledge or manual annotations by experts.	1
In this work, we introduce a BERT-based approach to medical image report labeling that exploits both the scale of available rule-based systems and the quality of expert annotations.	2
We demonstrate superior performance of a biomedically pretrained BERT model first trained on annotations of a rule-based labeler and then finetuned on a small set of expert annotations augmented with automated backtranslation.	4
We find that our final model, CheXbert, is able to outperform the previous best rules-based labeler with statistical significance, setting a new SOTA for report labeling on one of the largest datasets of chest x-rays.	4
----------
Computational and cognitive studies of event understanding suggest that identifying, comprehending, and predicting events depend on having structured representations of a sequence of events and on conceptualizing (abstracting) its components into (soft) event categories.	1
Thus, knowledge about a known process such as “buying a car” can be used in the context of a new but analogous process such as “buying a house”.	1
Nevertheless, most event understanding work in NLP is still at the ground level and does not consider abstraction.	1
In this paper, we propose an Analogous Process Structure Induction (APSI) framework, which leverages analogies among processes and conceptualization of sub-event instances to predict the whole sub-event sequence of previously unseen open-domain processes.	2+3
As our experiments and analysis indicate, APSI supports the generation of meaningful sub-event sequences for unseen processes and can help predict missing events.	3+4
----------
We introduce Sentence-level Language Modeling, a new pre-training objective for learning a discourse language representation in a fully self-supervised manner.	2
Recent pre-training methods in NLP focus on learning either bottom or top-level language representations: contextualized word representations derived from language model objectives at one extreme and a whole sequence representation learned by order classification of two given textual segments at the other.	1
However, these models are not directly encouraged to capture representations of intermediate-size structures that exist in natural languages such as sentences and the relationships among them.	1
To that end, we propose a new approach to encourage learning of a contextualized sentence-level representation by shuffling the sequence of input sentences and training a hierarchical transformer model to reconstruct the original ordering.	2
Through experiments on downstream tasks such as GLUE, SQuAD, and DiscoEval, we show that this feature of our model improves the performance of the original BERT by large margins.	4
----------
Detecting fine-grained differences in content conveyed in different languages matters for cross-lingual NLP and multilingual corpora analysis, but it is a challenging machine learning problem since annotation is expensive and hard to scale.	1
This work improves the prediction and annotation of fine-grained semantic divergences.	2
We introduce a training strategy for multilingual BERT models by learning to rank synthetic divergent examples of varying granularity.	2
We evaluate our models on the Rationalized English-French Semantic Divergences, a new dataset released with this work, consisting of English-French sentence-pairs annotated with semantic divergence classes and token-level rationales.	3
Learning to rank helps detect fine-grained sentence-level divergences more accurately than a strong sentence-level similarity model, while token-level predictions have the potential of further distinguishing between coarse and fine-grained divergences.	4
----------
Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences.	1
Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific.	1
We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors.	2
Our proposed approach differs from past work on semantic sentence encoding in two ways.	3
First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model’s posterior to predict sentence embeddings for monolingual data at test time.	3
Second, we use high-capacity transformers as both data generating distributions and inference networks – contrasting with most past work on sentence embeddings.	3
In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations.	4
Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.	5
----------
Abstract Meaning Representation (AMR) is a graph-based semantic formalism where the nodes are concepts and edges are relations among them.	1
Most of AMR parsing methods require alignment between the nodes of the graph and the words of the sentence.	1
However, this alignment is not provided by manual annotations and available automatic aligners focus only on the English language, not performing well for other languages.	1
Aiming to fulfill this gap, we developed an alignment method for the Portuguese language based on a more semantically matched word-concept pair.	2+3
We performed both intrinsic and extrinsic evaluations and showed that our alignment approach outperforms the alignment strategies developed for English, improving AMR parsers, and achieving competitive results with a parser designed for the Portuguese language.	3+4
----------
BERT is inefficient for sentence-pair tasks such as clustering or semantic search as it needs to evaluate combinatorially many sentence pairs which is very time-consuming.	1
Sentence BERT (SBERT) attempted to solve this challenge by learning semantically meaningful representations of single sentences, such that similarity comparison can be easily accessed.	1
However, SBERT is trained on corpus with high-quality labeled sentence pairs, which limits its application to tasks where labeled data is extremely scarce.	1
In this paper, we propose a lightweight extension on top of BERT and a novel self-supervised learning objective based on mutual information maximization strategies to derive meaningful sentence embeddings in an unsupervised manner.	2+3
Unlike SBERT, our method is not restricted by the availability of labeled data, such that it can be applied on different domain-specific corpus.	3
Experimental results show that the proposed method significantly outperforms other unsupervised sentence embedding baselines on common semantic textual similarity (STS) tasks and downstream supervised tasks.	4
It also outperforms SBERT in a setting where in-domain labeled data is not available, and achieves performance competitive with supervised methods on various tasks.	4
----------
Phrase alignment is the basis for modelling sentence pair interactions, such as paraphrase and textual entailment recognition.	1
Most phrase alignments are compositional processes such that an alignment of a phrase pair is constructed based on the alignments of their child phrases.	1
Nonetheless, studies have revealed that non-compositional alignments involving long-distance phrase reordering are prevalent in practice.	1
We address the phrase alignment problem by combining an unordered tree mapping algorithm and phrase representation modelling that explicitly embeds the similarity distribution in the sentences onto powerful contextualized representations.	2+3
Experimental results demonstrate that our method effectively handles compositional and non-compositional global phrase alignments.	4
Our method significantly outperforms that used in a previous study and achieves a performance competitive with that of experienced human annotators.	5
----------
Verifying fact on semi-structured evidence like tables requires the ability to encode structural information and perform symbolic reasoning.	1
Pre-trained language models trained on natural language could not be directly applied to encode tables, because simply linearizing tables into sequences will lose the cell alignment information.	1
To better utilize pre-trained transformers for table representation, we propose a Structure-Aware Transformer (SAT), which injects the table structural information into the mask of the self-attention layer.	2+3
A method to combine symbolic and linguistic reasoning is also explored for this task.	3
Our method outperforms baseline with 4.93% on TabFact, a large scale table verification dataset.	4
----------
Document-level relation extraction aims to extract relations among entities within a document.	1
Different from sentence-level relation extraction, it requires reasoning over multiple sentences across paragraphs.	1
In this paper, we propose Graph Aggregation-and-Inference Network (GAIN), a method to recognize such relations for long paragraphs.	2
GAIN constructs two graphs, a heterogeneous mention-level graph (MG) and an entity-level graph (EG).	3
The former captures complex interaction among different mentions and the latter aggregates mentions underlying for the same entities.	3
Based on the graphs we propose a novel path reasoning mechanism to infer relations between entities.	4
Experiments on the public dataset, DocRED, show GAIN achieves a significant performance improvement (2.85 on F1) over the previous state-of-the-art.	4
Our code is available at https://github.com/PKUnlp-icler/GAIN.	6
----------
Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts.	1
Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem.	1
In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC).	2
Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results.	1+2
This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC.	3
The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods.	4+5
ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8% in F1 for event argument extraction with only 1% data, compared with 2.2% of the previous method.	4+5
iii) Our model also fits with zero-shot scenarios, achieving 37.0% and 16% in F1 on two datasets without using any EE training data.	4
----------
Event detection (ED), which means identifying event trigger words and classifying event types, is the first and most fundamental step for extracting event knowledge from plain text.	1
Most existing datasets exhibit the following issues that limit further development of ED: (1) Data scarcity.	1
Existing small-scale datasets are not sufficient for training and stably benchmarking increasingly sophisticated modern neural methods.	1
(2) Low coverage.	1
Limited event types of existing datasets cannot well cover general-domain events, which restricts the applications of ED models.	1
To alleviate these problems, we present a MAssive eVENt detection dataset (MAVEN), which contains 4,480 Wikipedia documents, 118,732 event mention instances, and 168 event types.	2+3
MAVEN alleviates the data scarcity problem and covers much more general event types.	3
We reproduce the recent state-of-the-art ED models and conduct a thorough evaluation on MAVEN.	3
The experimental results show that existing ED methods cannot achieve promising results on MAVEN as on the small datasets, which suggests that ED in the real world remains a challenging task and requires further research efforts.	4+5
We also discuss further directions for general domain ED with empirical analyses.	5
The source code and dataset can be obtained from https://github.com/THU-KEG/MAVEN-dataset.	1
----------
Knowledge Graph (KG) alignment is to match entities in different KGs, which is important to knowledge fusion and integration.	1
Recently, a number of embedding-based approaches for KG alignment have been proposed and achieved promising results.	1
These approaches first embed entities in low-dimensional vector spaces, and then obtain entity alignments by computations on their vector representations.	1
Although continuous improvements have been achieved by recent work, the performances of existing approaches are still not satisfactory.	1
In this work, we present a new approach that directly learns embeddings of entity-pairs for KG alignment.	2
Our approach first generates a pair-wise connectivity graph (PCG) of two KGs, whose nodes are entity-pairs and edges correspond to relation-pairs; it then learns node (entity-pair) embeddings of the PCG, which are used to predict equivalent relations of entities.	3
To get desirable embeddings, a convolutional neural network is used to generate similarity features of entity-pairs from their attributes; and a graph neural network is employed to propagate the similarity features and get the final embeddings of entity-pairs.	3
Experiments on five real-world datasets show that our approach can achieve the state-of-the-art KG alignment results.	4+5
----------
Few-shot Knowledge Graph (KG) completion is a focus of current research, where each task aims at querying unseen facts of a relation given its few-shot reference entity pairs.	1
Recent attempts solve this problem by learning static representations of entities and references, ignoring their dynamic properties, i.e., entities may exhibit diverse roles within task relations, and references may make different contributions to queries.	1
This work proposes an adaptive attentional network for few-shot KG completion by learning adaptive entity and reference representations.	2
Specifically, entities are modeled by an adaptive neighbor encoder to discern their task-oriented roles, while references are modeled by an adaptive query-aware aggregator to differentiate their contributions.	3
Through the attention mechanism, both entities and references can capture their fine-grained semantic meanings, and thus render more expressive representations.	3
This will be more predictive for knowledge acquisition in the few-shot scenario.	3
Evaluation in link prediction on two public datasets shows that our approach achieves new state-of-the-art results with different few-shot sizes.	4+5
The source code is available at https://github.com/JiaweiSheng/FAAN.	6
----------
In this paper, we integrate span-related information into pre-trained encoder for entity relation extraction task.	1+2
Instead of using general-purpose sentence encoder (e.g., existing universal pre-trained models), we introduce a span encoder and a span pair encoder to the pre-training network, which makes it easier to import intra-span and inter-span information into the pre-trained model.	3
To learn the encoders, we devise three customized pre-training objectives from different perspectives, which target on tokens, spans, and span pairs.	3
In particular, a span encoder is trained to recover a random shuffling of tokens in a span, and a span pair encoder is trained to predict positive pairs that are from the same sentences and negative pairs that are from different sentences using contrastive loss.	3
Experimental results show that the proposed pre-training method outperforms distantly supervised pre-training, and achieves promising performance on two entity relation extraction benchmark datasets (ACE05, SciERC).	4
----------
Named entity recognition and relation extraction are two important fundamental problems.	1
Joint learning algorithms have been proposed to solve both tasks simultaneously, and many of them cast the joint task as a table-filling problem.	1
However, they typically focused on learning a single encoder (usually learning representation in the form of a table) to capture information required for both tasks within the same space.	1
We argue that it can be beneficial to design two distinct encoders to capture such two different types of information in the learning process.	2
In this work, we propose the novel table-sequence encoders where two different encoders – a table encoder and a sequence encoder are designed to help each other in the representation learning process.	2+3
Our experiments confirm the advantages of having two encoders over one encoder.	4
On several standard datasets, our model shows significant improvements over existing approaches.	4
----------
Generative models for Information Retrieval, where ranking of documents is viewed as the task of generating a query from a document’s language model, were very successful in various IR tasks in the past.	1
However, with the advent of modern deep neural networks, attention has shifted to discriminative ranking functions that model the semantic similarity of documents and queries instead.	1
Recently, deep generative models such as GPT2 and BART have been shown to be excellent text generators, but their effectiveness as rankers have not been demonstrated yet.	1
In this work, we revisit the generative framework for information retrieval and show that our generative approaches are as effective as state-of-the-art semantic similarity-based discriminative models for the answer selection task.	2+3
Additionally, we demonstrate the effectiveness of unlikelihood losses for IR.	4
----------
Topic models are a useful analysis tool to uncover the underlying themes within document collections.	1
The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pre-trained word embeddings while incorporating document information for weighted clustering and reranking top words.	1+2
We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA.	3
The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.	4
----------
While neural sequence learning methods have made significant progress in single-document summarization (SDS), they produce unsatisfactory results on multi-document summarization (MDS).	1
We observe two major challenges when adapting SDS advances to MDS: (1) MDS involves larger search space and yet more limited training data, setting obstacles for neural methods to learn adequate representations; (2) MDS needs to resolve higher information redundancy among the source documents, which SDS methods are less effective to handle.	1
To close the gap, we present RL-MMR, Maximal Margin Relevance-guided Reinforcement Learning for MDS, which unifies advanced neural SDS methods and statistical measures used in classical MDS.	2+3
RL-MMR casts MMR guidance on fewer promising candidates, which restrains the search space and thus leads to better representation learning.	4
Additionally, the explicit redundancy measure in MMR helps the neural representation of the summary to better capture redundancy.	4
Extensive experiments demonstrate that RL-MMR achieves state-of-the-art performance on benchmark MDS datasets.	4
In particular, we show the benefits of incorporating MMR into end-to-end learning when adapting SDS to MDS in terms of both learning effectiveness and efficiency.	4
----------
Topic models are often used to identify human-interpretable topics to help make sense of large document collections.	1
We use knowledge distillation to combine the best attributes of probabilistic topic models and pretrained transformers.	2+3
Our modular method can be straightforwardly applied with any neural topic model to improve topic quality, which we demonstrate using two models having disparate architectures, obtaining state-of-the-art topic coherence.	4
We show that our adaptable framework not only improves performance in the aggregate over all estimated topics, as is commonly reported, but also in head-to-head comparisons of aligned topics.	4
----------
Topic models have been prevailing for many years on discovering latent semantics while modeling long documents.	1
However, for short texts they generally suffer from data sparsity because of extremely limited word co-occurrences; thus tend to yield repetitive or trivial topics with low quality.	1
In this paper, to address this issue, we propose a novel neural topic model in the framework of autoencoding with a new topic distribution quantization approach generating peakier distributions that are more appropriate for modeling short texts.	2+3
Besides the encoding, to tackle this issue in terms of decoding, we further propose a novel negative sampling decoder learning from negative samples to avoid yielding repetitive topics.	2+3
We observe that our model can highly improve short text topic modeling performance.	4
Through extensive experiments on real-world datasets, we demonstrate our model can outperform both strong traditional and neural baselines under extreme data sparsity scenes, producing high-quality topics.	4+5
----------
Open-domain Keyphrase extraction (KPE) on the Web is a fundamental yet complex NLP task with a wide range of practical applications within the field of Information Retrieval.	1
In contrast to other document types, web page designs are intended for easy navigation and information finding.	1
Effective designs encode within the layout and formatting signals that point to where the important information can be found.	1
In this work, we propose a modeling approach that leverages these multi-modal signals to aid in the KPE task.	2+3
In particular, we leverage both lexical and visual features (e.g., size, font, position) at the micro-level to enable effective strategy induction and meta-level features that describe pages at a macro-level to aid in strategy selection.	3
Our evaluation demonstrates that a combination of effective strategy induction and strategy selection within this approach for the KPE task outperforms state-of-the-art models.	4+5
A qualitative post-hoc analysis illustrates how these features function within the model.	5
----------
Most existing approaches to disfluency detection heavily rely on human-annotated corpora, which is expensive to obtain in practice.	1
There have been several proposals to alleviate this issue with, for instance, self-supervised learning techniques, but they still require human-annotated corpora.	1
In this work, we explore the unsupervised learning paradigm which can potentially work with unlabeled text corpora that are cheaper and easier to obtain.	2
Our model builds upon the recent work on Noisy Student Training, a semi-supervised learning approach that extends the idea of self-training.	3
Experimental results on the commonly used English Switchboard test set show that our approach achieves competitive performance compared to the previous state-of-the-art supervised systems using contextualized word embeddings (e.g. BERT and ELECTRA).	4
----------
The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language.	1
Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often black-box, with very limited interpretability.	1
In this paper we propose, which dynamically adjusts weights between input modalities and output representations differently for each input sample.	2
Multimodal routing can identify relative importance of both individual modalities and cross-modality factors.	4
Moreover, the weight assignment by routing allows us to interpret modality-prediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.	4
----------
Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript).	1
Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs.	1
Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise.	1
To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module.	2+3
Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance.	4
Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures.	4
Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.	4
----------
Video-grounded dialogues are very challenging due to (i) the complexity of videos which contain both spatial and temporal variations, and (ii) the complexity of user utterances which query different segments and/or different objects in videos over multiple dialogue turns.	1
However, existing approaches to video-grounded dialogues often focus on superficial temporal-level visual cues, but neglect more fine-grained spatial signals from videos.	1
To address this drawback, we proposed Bi-directional Spatio-Temporal Learning (BiST), a vision-language neural framework for high-resolution queries in videos based on textual cues.	2
Specifically, our approach not only exploits both spatial and temporal-level information, but also learns dynamic information diffusion between the two feature spaces through spatial-to-temporal and temporal-to-spatial reasoning.	3
The bidirectional strategy aims to tackle the evolving semantics of user queries in the dialogue setting.	3
The retrieved visual cues are used as contextual information to construct relevant responses to the users.	3
Our empirical results and comprehensive qualitative analysis show that BiST achieves competitive performance and generates reasonable responses on a large-scale AVSD benchmark.	4
We also adapt our BiST models to the Video QA setting, and substantially outperform prior approaches on the TGIF-QA benchmark.	4
----------
Building an end-to-end conversational agent for multi-domain task-oriented dialogues has been an open challenge for two main reasons.	1
First, tracking dialogue states of multiple domains is non-trivial as the dialogue agent must obtain complete states from all relevant domains, some of which might have shared slots among domains as well as unique slots specifically for one domain only.	1
Second, the dialogue agent must also process various types of information across domains, including dialogue context, dialogue states, and database, to generate natural responses to users.	1
Unlike the existing approaches that are often designed to train each module separately, we propose “UniConv” - a novel unified neural architecture for end-to-end conversational systems in multi-domain task-oriented dialogues, which is designed to jointly train (i) a Bi-level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently, and (ii) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously.	2+3
We conduct comprehensive experiments in dialogue state tracking, context-to-text, and end-to-end settings on the MultiWOZ2.1 benchmark, achieving superior performance over competitive baselines.	4
----------
End-to-end task-oriented dialogue systems aim to generate system responses directly from plain text inputs.	1
There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history.	1
In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue.	2+3
To effectively leverage the structural information in dialogue history, we propose a new recurrent cell architecture which allows representation learning on graphs.	3
To exploit the relations between entities in KBs, the model combines multi-hop reasoning ability based on the graph structure.	3
Experimental results show that the proposed model achieves consistent improvement over state-of-the-art models on two different task-oriented dialogue datasets.	4
----------
Inducing a meaningful structural representation from one or a set of dialogues is a crucial but challenging task in computational linguistics.	1
Advancement made in this area is critical for dialogue system design and discourse analysis.	1
It can also be extended to solve grammatical inference.	1
In this work, we propose to incorporate structured attention layers into a Variational Recurrent Neural Network (VRNN) model with discrete latent states to learn dialogue structure in an unsupervised fashion.	1+2
Compared to a vanilla VRNN, structured attention enables a model to focus on different parts of the source sentence embeddings while enforcing a structural inductive bias.	4
Experiments show that on two-party dialogue datasets, VRNN with structured attention learns semantic structures that are similar to templates used to generate this dialogue corpus.	4
While on multi-party dialogue datasets, our model learns an interactive structure demonstrating its capability of distinguishing speakers or addresses, automatically disentangling dialogues without explicit human annotation.	4+5
----------
In the past few years, audiences from different fields witness the achievements of sequence-to-sequence models (e.g., LSTM+attention, Pointer Generator Networks and Transformer) to enhance dialogue content generation.	1
While content fluency and accuracy often serve as the major indicators for model training, dialogue logics, carrying critical information for some particular domains, are often ignored.	1
Take customer service and court debate dialogue as examples, compatible logics can be observed across different dialogue instances, and this information can provide vital evidence for utterance generation.	1
In this paper, we propose a novel network architecture - Cross Copy Networks (CCN) to explore the current dialog context and similar dialogue instances’ logical structure simultaneously.	1+2
Experiments with two tasks, court debate and customer service content generation, proved that the proposed algorithm is superior to existing state-of-art content generation models.	4
----------
Multi-turn response selection is a task designed for developing dialogue agents.	1
The performance on this task has a remarkable improvement with pre-trained language models.	1
However, these models simply concatenate the turns in dialogue history as the input and largely ignore the dependencies between the turns.	1
In this paper, we propose a dialogue extraction algorithm to transform a dialogue history into threads based on their dependency relations.	2
Each thread can be regarded as a self-contained sub-dialogue.	3
We also propose Thread-Encoder model to encode threads and candidates into compact representations by pre-trained Transformers and finally get the matching score through an attention layer.	4
The experiments show that dependency relations are helpful for dialogue context understanding, and our model outperforms the state-of-the-art baselines on both DSTC7 and DSTC8*, with competitive results on UbuntuV2.	4
----------
The dependencies between system and user utterances in the same turn and across different turns are not fully considered in existing multidomain dialogue state tracking (MDST) models.	1
In this study, we argue that the incorporation of these dependencies is crucial for the design of MDST and propose Parallel Interactive Networks (PIN) to model these dependencies.	2
Specifically, we integrate an interactive encoder to jointly model the in-turn dependencies and cross-turn dependencies.	3
The slot-level context is introduced to extract more expressive features for different slots.	4
And a distributed copy mechanism is utilized to selectively copy words from historical system utterances or historical user utterances.	4
Empirical studies demonstrated the superiority of the proposed PIN model.	5
----------
Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system.	1
In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling.	1+2
Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model.	1+2
Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to x10.77).	4
In-depth analysis show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots.	5
----------
Decisions of complex models for language understanding can be explained by limiting the inputs they are provided to a relevant subsequence of the original text — a rationale.	1
Models that condition predictions on a concise rationale, while being more interpretable, tend to be less accurate than models that are able to use the entire context.	1
In this paper, we show that it is possible to better manage the trade-off between concise explanations and high task accuracy by optimizing a bound on the Information Bottleneck (IB) objective.	1+2
Our approach jointly learns an explainer that predicts sparse binary masks over input sentences without explicit supervision, and an end-task predictor that considers only the residual sentences.	3
Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior.	3
Experiments on the ERASER benchmark demonstrate significant gains over previous work for both task performance and agreement with human rationales.	4
Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples with gold masks) can close the performance gap with a model that uses the full input.	4
----------
Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks.	1
However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations.	1
To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs).	1+2
CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age.	3
In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping.	3
The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups.	3
We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs.	4+5
As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.	5
----------
Machine learning techniques have been widely used in natural language processing (NLP).	1
However, as revealed by many recent studies, machine learning models often inherit and amplify the societal biases in data.	1
Various metrics have been proposed to quantify biases in model predictions.	1
In particular, several of them evaluate disparity in model performance between protected groups and advantaged groups in the test corpus.	1
However, we argue that evaluating bias at the corpus level is not enough for understanding how biases are embedded in a model.	2
In fact, a model with similar aggregated performance between different groups on the entire data may behave differently on instances in a local region.	3
To analyze and detect such local bias, we propose LOGAN, a new bias detection technique based on clustering.	2+3
Experiments on toxicity classification and object classification tasks show that LOGAN identifies bias in a local region and allows us to better analyze the biases in model predictions.	4+5
----------
Recurrent neural networks empirically generate natural language with high syntactic fidelity.	1
However, their success is not well-understood theoretically.	1
We provide theoretical insight into this success, proving in a finite-precision setting that RNNs can efficiently generate bounded hierarchical languages that reflect the scaffolding of natural language syntax.	2+3
We introduce Dyck-(k,m), the language of well-nested brackets (of k types) and m-bounded nesting depth, reflecting the bounded memory needs and long-distance dependencies of natural language syntax.	3
The best known results use O(km⁄2) memory (hidden units) to generate these languages.	3
We prove that an RNN with O(m log k) hidden units suffices, an exponential reduction in memory, by an explicit construction.	3+4
Finally, we show that no algorithm, even with unbounded computation, can suffice with o(m log k) hidden units.	3+4
----------
We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”.	1
We find that the same biases exist in recent language models like BERT.	4
While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models.	4
We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.	2
----------
Humans acquire language continually with much more limited access to data samples at a time, as compared to contemporary NLP systems.	1
To study this human-like language acquisition ability, we present VisCOLL, a visually grounded language learning task, which simulates the continual acquisition of compositional phrases from streaming visual scenes.	2
In the task, models are trained on a paired image-caption stream which has shifting object distribution; while being constantly evaluated by a visually-grounded masked language prediction task on held-out test sets.	3
VisCOLL compounds the challenges of continual learning (i.e., learning from continuously shifting data distribution) and compositional generalization (i.e., generalizing to novel compositions).	3
To facilitate research on VisCOLL, we construct two datasets, COCO-shift and Flickr-shift, and benchmark them using different continual learning methods.	3
Results reveal that SoTA continual learning approaches provide little to no improvements on VisCOLL, since storing examples of all possible compositions is infeasible.	4
We conduct further ablations and analysis to guide future work.	5
----------
Phrase localization is a task that studies the mapping from textual phrases to regions of an image.	1
Given difficulties in annotating phrase-to-object datasets at scale, we develop a Multimodal Alignment Framework (MAF) to leverage more widely-available caption-image datasets, which can then be used as a form of weak supervision.	1
We first present algorithms to model phrase-object relevance by leveraging fine-grained visual representations and visually-aware language representations.	2+3
By adopting a contrastive objective, our method uses information in caption-image pairs to boost the performance in weakly-supervised scenarios.	3+4
Experiments conducted on the widely-adopted Flickr30k dataset show a significant improvement over existing weakly-supervised methods.	3+4
With the help of the visually-aware language representations, we can also improve the previous best unsupervised result by 5.56%.	4
We conduct ablation studies to show that both our novel model and our weakly-supervised strategies significantly contribute to our strong results.	5
----------
Images can give us insights into the contextual meanings of words, but current image-text grounding approaches require detailed annotations.	1
Such granular annotation is rare, expensive, and unavailable in most domain-specific contexts.	1
In contrast, unlabeled multi-image, multi-sentence documents are abundant.	1
Can lexical grounding be learned from such documents, even though they have significant lexical and visual overlap?	1
Working with a case study dataset of real estate listings, we demonstrate the challenge of distinguishing highly correlated grounded terms, such as “kitchen” and “bedroom”, and introduce metrics to assess this document similarity.	2+3
We present a simple unsupervised clustering-based method that increases precision and recall beyond object detection and image tagging baselines when evaluated on labeled subsets of the dataset.	3+4
The proposed method is particularly effective for local contextual meanings of a word, for example associating “granite” with countertops in the real estate dataset and with rocky landscapes in a Wikipedia dataset.	5
----------
We present HERO, a novel framework for large-scale video+language omni-representation learning.	1+2
HERO encodes multimodal inputs in a hierarchical structure, where local context of a video frame is captured by a Cross-modal Transformer via multimodal fusion, and global video context is captured by a Temporal Transformer.	3
In addition to standard Masked Language Modeling (MLM) and Masked Frame Modeling (MFM) objectives, we design two new pre-training tasks: (i) Video-Subtitle Matching (VSM), where the model predicts both global and local temporal alignment; and (ii) Frame Order Modeling (FOM), where the model predicts the right order of shuffled video frames.	3
HERO is jointly trained on HowTo100M and large-scale TV datasets to gain deep understanding of complex social dynamics with multi-character interactions.	4+5
Comprehensive experiments demonstrate that HERO achieves new state of the art on multiple benchmarks over Text-based Video/Video-moment Retrieval, Video Question Answering (QA), Video-and-language Inference and Video Captioning tasks across different domains.	5
We also introduce two new challenging benchmarks How2QA and How2R for Video QA and Retrieval, collected from diverse video content over multimodalities.	5
----------
Humans learn language by listening, speaking, writing, reading, and also, via interaction with the multimodal real world.	1
Existing language pre-training frameworks show the effectiveness of text-only self-supervision while we explore the idea of a visually-supervised language model in this paper.	1
We find that the main reason hindering this exploration is the large divergence in magnitude and distributions between the visually-grounded language datasets and pure-language corpora.	1
Therefore, we develop a technique named “vokenization” that extrapolates multimodal alignments to language-only data by contextually mapping language tokens to their related images (which we call “vokens”).	2+3
The “vokenizer” is trained on relatively small image captioning datasets and we then apply it to generate vokens for large language corpora.	3+4
Trained with these contextually generated vokens, our visually-supervised language models show consistent improvements over self-supervised alternatives on multiple pure-language tasks such as GLUE, SQuAD, and SWAG.	4
----------
Large-scale dissemination of disinformation online intended to mislead or deceive the general population is a major societal problem.	1
Rapid progression in image, video, and natural language generative models has only exacerbated this situation and intensified our need for an effective defense mechanism.	1
While existing approaches have been proposed to defend against neural fake news, they are generally constrained to the very limited setting where articles only have text and metadata such as the title and authors.	1
In this paper, we introduce the more realistic and challenging task of defending against machine-generated news that also includes images and captions.	2
To identify the possible weaknesses that adversaries can exploit, we create a NeuralNews dataset which is comprised of 4 different types of generated articles as well as conduct a series of human user study experiments based on this dataset.	3
Coupled with providing a relatively effective approach based on detecting visual-semantic inconsistencies, the valuable insights gleaned from our user study experiments and, consequently, this paper will serve as an effective first line of defense and a valuable reference for future work in defending against machine-generated disinformation.	5
----------
Aspect term extraction (ATE) aims to extract aspect terms from a review sentence that users have expressed opinions on.	1
Existing studies mostly focus on designing neural sequence taggers to extract linguistic features from the token level.	1
However, since the aspect terms and context words usually exhibit long-tail distributions, these taggers often converge to an inferior state without enough sample exposure.	1
In this paper, we propose to tackle this problem by correlating words with each other through soft prototypes.	2+3
These prototypes, generated by a soft retrieval process, can introduce global knowledge from internal or external data and serve as the supporting evidence for discovering the aspect terms.	5
Our proposed model is a general framework and can be combined with almost all sequence taggers.	5
Experiments on four SemEval datasets show that our model boosts the performance of three typical ATE methods by a large margin.	5
----------
Unlike other domains, medical texts are inevitably accompanied by private information, so sharing or copying these texts is strictly restricted.	1
However, training a medical relation extraction model requires collecting these privacy-sensitive texts and storing them on one machine, which comes in conflict with privacy protection.	1
In this paper, we propose a privacy-preserving medical relation extraction model based on federated learning, which enables training a central model with no single piece of private local data being shared or exchanged.	1+2
Though federated learning has distinct advantages in privacy protection, it suffers from the communication bottleneck, which is mainly caused by the need to upload cumbersome local parameters.	3
To overcome this bottleneck, we leverage a strategy based on knowledge distillation.	3
Such a strategy uses the uploaded predictions of ensemble local models to train the central model without requiring uploading local parameters.	3
Experiments on three publicly available medical relation extraction datasets demonstrate the effectiveness of our method.	4+5
----------
Product attribute values are essential in many e-commerce scenarios, such as customer service robots, product recommendations, and product retrieval.	1
While in the real world, the attribute values of a product are usually incomplete and vary over time, which greatly hinders the practical applications.	1
In this paper, we propose a multimodal method to jointly predict product attributes and extract values from textual product descriptions with the help of the product images.	2
We argue that product attributes and values are highly correlated, e.g., it will be easier to extract the values on condition that the product attributes are given.	3
Thus, we jointly model the attribute prediction and value extraction tasks from multiple aspects towards the interactions between attributes and values.	3
Moreover, product images have distinct effects on our tasks for different product attributes and values.	4
Thus, we selectively draw useful visual information from product images to enhance our model.	3+4
We annotate a multimodal product attribute value dataset that contains 87,194 instances, and the experimental results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task.	3+4
Our code and dataset are available at https://github.com/jd-aig/JAVE.	6
----------
Existing OIE (Open Information Extraction) algorithms are independent of each other such that there exist lots of redundant works; the featured strategies are not reusable and not adaptive to new tasks.	1
This paper proposes a new pipeline to build OIE systems, where an Open-domain Information eXpression (OIX) task is proposed to provide a platform for all OIE strategies.	2
The OIX is an OIE friendly expression of a sentence without information loss.	4
The generation procedure of OIX contains shared works of OIE algorithms so that OIE strategies can be developed on the platform of OIX as inference operations focusing on more critical problems.	4
Based on the same platform of OIX, the OIE strategies are reusable, and people can select a set of strategies to assemble their algorithm for a specific task so that the adaptability may be significantly increased.	4
This paper focuses on the task of OIX and propose a solution – Open Information Annotation (OIA).	4
OIA is a predicate-function-argument annotation for sentences.	4
We label a data set of sentence-OIA pairs and propose a dependency-based rule system to generate OIA annotations from sentences.	4
The evaluation results reveal that learning the OIA from a sentence is a challenge owing to the complexity of natural language sentences, and it is worthy of attracting more attention from the research community.	4+5
----------
We consider retrofitting structure-aware Transformer language model for facilitating end tasks by proposing to exploit syntactic distance to encode both the phrasal constituency and dependency connection into the language model.	1+2
A middle-layer structural learning strategy is leveraged for structure integration, accomplished with main semantic task training under multi-task learning scheme.	3
Experimental results show that the retrofitted structure-aware Transformer language model achieves improved perplexity, meanwhile inducing accurate syntactic phrases.	4+5
By performing structure-aware fine-tuning, our model achieves significant improvements for both semantic- and syntactic-dependent tasks.	4+5
----------
AMR-to-text generation is used to transduce Abstract Meaning Representation structures (AMR) into text.	1
A key challenge in this task is to efficiently learn effective graph representations.	1
Previously, Graph Convolution Networks (GCNs) were used to encode input AMRs, however, vanilla GCNs are not able to capture non-local information and additionally, they follow a local (first-order) information aggregation scheme.	1
To account for these issues, larger and deeper GCN models are required to capture more complex interactions.	1
In this paper, we introduce a dynamic fusion mechanism, proposing Lightweight Dynamic Graph Convolutional Networks (LDGCNs) that capture richer non-local interactions by synthesizing higher order information from the input graphs.	2+3
We further develop two novel parameter saving strategies based on the group graph convolutions and weight tied convolutions to reduce memory usage and model complexity.	3
With the help of these strategies, we are able to train a model with fewer parameters while maintaining the model capacity.	4
Experiments demonstrate that LDGCNs outperform state-of-the-art models on two benchmark datasets for AMR-to-text generation with significantly fewer parameters.	4
----------
Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results.	1
Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate.	1
This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question?	1
We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy.	1+2
We find that beam search enforces uniform information density in text, a property motivated by cognitive science.	4
We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models.	4
Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.	4
----------
Latent structure models are a powerful tool for modeling language data: they can mitigate the error propagation and annotation bottleneck in pipeline systems, while simultaneously uncovering linguistic insights about the data.	1
One challenge with end-to-end training of these models is the argmax operation, which has null gradient.	1
In this paper, we focus on surrogate gradients, a popular strategy to deal with this problem.	2
We explore latent structure learning through the angle of pulling back the downstream learning objective.	3
In this paradigm, we discover a principled motivation for both the straight-through estimator (STE) as well as the recently-proposed SPIGOT – a variant of STE for structured models.	4
Our perspective leads to new algorithms in the same family.	4
We empirically compare the known and the novel pulled-back estimators against the popular alternatives, yielding new insight for practitioners and revealing intriguing failure cases.	4+5
----------
Recent work raises concerns about the use of standard splits to compare natural language processing models.	1
We propose a Bayesian statistical model comparison technique which uses k-fold cross-validation across multiple data sets to estimate the likelihood that one model will outperform the other, or that the two will produce practically equivalent results.	2+3
We use this technique to rank six English part-of-speech taggers across two data sets and three evaluation metrics.	3
----------
Previous studies have shown that hierarchical multi-task learning (MTL) can utilize task dependencies by stacking encoders and outperform democratic MTL.	1
However, stacking encoders only considers the dependencies of feature representations and ignores the label dependencies in logically dependent tasks.	1
Furthermore, how to properly utilize the labels remains an issue due to the cascading errors between tasks.	1
In this paper, we view logically dependent MTL from the perspective of causal inference and suggest a mediation assumption instead of the confounding assumption in conventional MTL models.	2
We propose a model including two key mechanisms: label transfer (LT) for each task to utilize the labels of all its lower-level tasks, and Gumbel sampling (GS) to deal with cascading errors.	2
In the field of causal inference, GS in our model is essentially a counterfactual reasoning process, trying to estimate the causal effect between tasks and utilize it to improve MTL.	3
We conduct experiments on two English datasets and one Chinese dataset.	3
Experiment results show that our model achieves state-of-the-art on six out of seven subtasks and improves predictions’ consistency.	4
----------
We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning.	2+3
Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred.	4+5
Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks.	4
Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy.	4
This confirms that masking can be utilized as an efficient alternative to finetuning.	5
----------
Document-level neural machine translation has yielded attractive improvements.	1
However, majority of existing methods roughly use all context sentences in a fixed scope.	1
They neglect the fact that different source sentences need different sizes of context.	1
To address this problem, we propose an effective approach to select dynamic context so that the document-level translation model can utilize the more useful selected context sentences to produce better translations.	2+3
Specifically, we introduce a selection module that is independent of the translation module to score each candidate context sentence.	3
Then, we propose two strategies to explicitly select a variable number of context sentences and feed them into the translation module.	3
We train the two modules end-to-end via reinforcement learning.	3
A novel reward is proposed to encourage the selection and utilization of dynamic context sentences.	3
Experiments demonstrate that our approach can select adaptive context sentences for different source sentences, and significantly improves the performance of document-level translation methods.	4+5
----------
Large-scale training datasets lie at the core of the recent success of neural machine translation (NMT) models.	1
However, the complex patterns and potential noises in the large-scale data make training NMT models difficult.	1
In this work, we explore to identify the inactive training examples which contribute less to the model performance, and show that the existence of inactive examples depends on the data distribution.	2+3
We further introduce data rejuvenation to improve the training of NMT models on large-scale datasets by exploiting inactive examples.	3
The proposed framework consists of three phases.	3
First, we train an identification model on the original training data, and use it to distinguish inactive examples and active examples by their sentence-level output probabilities.	3
Then, we train a rejuvenation model on the active examples, which is used to re-label the inactive examples with forward- translation.	3
Finally, the rejuvenated examples and the active examples are combined to train the final NMT model.	3
Experimental results on WMT14 English-German and English-French datasets show that the proposed data rejuvenation consistently and significantly improves performance for several strong NMT models.	4
Extensive analyses reveal that our approach stabilizes and accelerates the training process of NMT models, resulting in final models with better generalization capability.	4+5
----------
Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training.	1
We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model.	2
Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data.	3+4
We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset.	4
Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation.	4
We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En.	4+5
----------
Balancing accuracy and latency is a great challenge for simultaneous translation.	1
To achieve high accuracy, the model usually needs to wait for more streaming text before translation, which results in increased latency.	1
However, keeping low latency would probably hurt accuracy.	1
Therefore, it is essential to segment the ASR output into appropriate units for translation.	1
Inspired by human interpreters, we propose a novel adaptive segmentation policy for simultaneous translation.	2
The policy learns to segment the source text by considering possible translations produced by the translation model, maintaining consistency between the segmentation and translation.	4
Experimental results on Chinese-English and German-English translation show that our method achieves a better accuracy-latency trade-off over recently proposed state-of-the-art methods.	4
----------
Recent emergence of multilingual pre-training language model (mPLM) has enabled breakthroughs on various downstream cross-lingual transfer (CLT) tasks.	1
However, mPLM-based methods usually involve two problems: (1) simply fine-tuning may not adapt general-purpose multilingual representations to be task-aware on low-resource languages; (2) ignore how cross-lingual adaptation happens for downstream tasks.	1
To address the issues, we propose a meta graph learning (MGL) method.	2
Unlike prior works that transfer from scratch, MGL can learn to cross-lingual transfer by extracting meta-knowledge from historical CLT experiences (tasks), making mPLM insensitive to low-resource languages.	4
Besides, for each CLT task, MGL formulates its transfer process as information propagation over a dynamic graph, where the geometric structure can automatically capture intrinsic language relationships to explicitly guide cross-lingual transfer.	4
Empirically, extensive experiments on both public and real-world datasets demonstrate the effectiveness of the MGL method.	4+5
----------
Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality.	1
However, cross-language interference and restrained model capacity remain major obstacles.	1
To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules.	2+3
This approach enables to learn adapters via language embeddings while sharing model parameters across languages.	4
It also allows for an easy but effective integration of existing linguistic typology features into the parsing network.	4
The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, showing the success of the proposed adaptation approach.	5
Our in-depth analyses show that soft parameter sharing via typological features is key to this success.	5
----------
Conditional random fields (CRF) for label decoding has become ubiquitous in sequence labeling tasks.	1
However, the local label dependencies and inefficient Viterbi decoding have always been a problem to be solved.	1
In this work, we introduce a novel two-stage label decoding framework to model long-term label dependencies, while being much more computationally efficient.	2
A base model first predicts draft labels, and then a novel two-stream self-attention model makes refinements on these draft predictions based on long-range label dependencies, which can achieve parallel decoding for a faster prediction.	3+4
In addition, in order to mitigate the side effects of incorrect draft labels, Bayesian neural networks are used to indicate the labels with a high probability of being wrong, which can greatly assist in preventing error propagation.	3+4
The experimental results on three sequence labeling benchmarks demonstrated that the proposed method not only outperformed the CRF-based methods but also greatly accelerated the inference process.	4
----------
Building an effective adversarial attacker and elaborating on countermeasures for adversarial attacks for natural language processing (NLP) have attracted a lot of research in recent years.	1
However, most of the existing approaches focus on classification problems.	1
In this paper, we investigate attacks and defenses for structured prediction tasks in NLP.	2
Besides the difficulty of perturbing discrete words and the sentence fluency problem faced by attackers in any NLP tasks, there is a specific challenge to attackers of structured prediction models: the structured output of structured prediction models is sensitive to small perturbations in the input.	3
To address these problems, we propose a novel and unified framework that learns to attack a structured prediction model using a sequence-to-sequence model with feedbacks from multiple reference models of the same structured prediction task.	3+4
Based on the proposed attack, we further reinforce the victim model with adversarial training, making its prediction more robust and accurate.	3
We evaluate the proposed framework in dependency parsing and part-of-speech tagging.	3
Automatic and human evaluations show that our proposed framework succeeds in both attacking state-of-the-art structured prediction models and boosting them with adversarial training.	4
----------
Aspect Sentiment Triplet Extraction (ASTE) is the task of extracting the triplets of target entities, their associated sentiment, and opinion spans explaining the reason for the sentiment.	1
Existing research efforts mostly solve this problem using pipeline approaches, which break the triplet extraction process into several stages.	1
Our observation is that the three elements within a triplet are highly related to each other, and this motivates us to build a joint model to extract such triplets using a sequence tagging approach.	1
However, how to effectively design a tagging approach to extract the triplets that can capture the rich interactions among the elements is a challenging research question.	1
In this work, we propose the first end-to-end model with a novel position-aware tagging scheme that is capable of jointly extracting the triplets.	1+2
Our experimental results on several existing datasets show that jointly capturing elements in the triplet using our approach leads to improved performance over the existing approaches.	3+4
We also conducted extensive experiments to investigate the model effectiveness and robustness.	4
----------
Simultaneous machine translation (SiMT) aims to translate a continuous input text stream into another language with the lowest latency and highest quality possible.	1
The translation thus has to start with an incomplete source text, which is read progressively, creating the need for anticipation.	1
In this paper, we seek to understand whether the addition of visual information can compensate for the missing source context.	2+3
To this end, we analyse the impact of different multimodal approaches and visual features on state-of-the-art SiMT frameworks.	3
Our results show that visual context is helpful and that visually-grounded models based on explicit object region information are much better than commonly used global features, reaching up to 3 BLEU points improvement under low latency scenarios.	4
Our qualitative analysis illustrates cases where only the multimodal systems are able to translate correctly from English into gender-marked languages, as well as deal with differences in word order, such as adjective-noun placement between English and French.	4+5
----------
In order to simulate human language capacity, natural language processing systems must be able to reason about the dynamics of everyday situations, including their possible causes and effects.	1
Moreover, they should be able to generalise the acquired world knowledge to new languages, modulo cultural differences.	1
Advances in machine reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks.	1
Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, which includes resource-poor languages like Eastern Apurímac Quechua and Haitian Creole.	2
We evaluate a range of state-of-the-art models on this novel dataset, revealing that the performance of current methods based on multilingual pretraining and zero-shot fine-tuning falls short compared to translation-based transfer.	3+4
Finally, we propose strategies to adapt multilingual models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline.	5
The XCOPA dataset is freely available at github.com/cambridgeltl/xcopa.	6
----------
Performance in cross-lingual NLP tasks is impacted by the (dis)similarity of languages at hand: e.g., previous work has suggested there is a connection between the expected success of bilingual lexicon induction (BLI) and the assumption of (approximate) isomorphism between monolingual embedding spaces.	1
In this work we present a large-scale study focused on the correlations between monolingual embedding space similarity and task performance, covering thousands of language pairs and four different tasks: BLI, parsing, POS tagging and MT.	2
We hypothesize that statistics of the spectrum of each monolingual embedding space indicate how well they can be aligned.	3
We then introduce several isomorphism measures between two embedding spaces, based on the relevant statistics of their individual spectra.	3
We empirically show that (1) language similarity scores derived from such spectral isomorphism measures are strongly associated with performance observed in different cross-lingual tasks, and (2) our spectral-based measures consistently outperform previous standard isomorphism measures, while being computationally more tractable and easier to interpret.	3+4
Finally, our measures capture complementary information to typologically driven language distance measures, and the combination of measures from the two families yields even higher task performance correlations.	3+4
----------
Sparse language vectors from linguistic typology databases and learned embeddings from tasks like multilingual machine translation have been investigated in isolation, without analysing how they could benefit from each other’s language characterisation.	1
We propose to fuse both views using singular vector canonical correlation analysis and study what kind of information is induced from each source.	2+3
By inferring typological features and language phylogenies, we observe that our representations embed typology and strengthen correlations with language relationships.	3+4
We then take advantage of our multi-view language vector space for multilingual machine translation, where we achieve competitive overall translation accuracy in tasks that require information about language similarities, such as language clustering and ranking candidates for multilingual transfer.	3+4
With our method, we can easily project and assess new languages without expensive retraining of massive multilingual or ranking models, which are major disadvantages of related approaches.	5
----------
Product-related question answering platforms nowadays are widely employed in many E-commerce sites, providing a convenient way for potential customers to address their concerns during online shopping.	1
However, the misinformation in the answers on those platforms poses unprecedented challenges for users to obtain reliable and truthful product information, which may even cause a commercial loss in E-commerce business.	1
To tackle this issue, we investigate to predict the veracity of answers in this paper and introduce AnswerFact, a large scale fact checking dataset from product question answering forums.	2
Each answer is accompanied by its veracity label and associated evidence sentences, providing a valuable testbed for evidence-based fact checking tasks in QA settings.	3
We further propose a novel neural model with tailored evidence ranking components to handle the concerned answer veracity prediction problem.	3
Extensive experiments are conducted with our proposed model and various existing fact checking methods, showing that our method outperforms all baselines on this task.	3+4
----------
Extractive QA models have shown very promising performance in predicting the correct answer to a question for a given passage.	1
However, they sometimes result in predicting the correct answer text but in a context irrelevant to the given question.	1
This discrepancy becomes especially important as the number of occurrences of the answer text in a passage increases.	1
To resolve this issue, we propose BLANC (BLock AttentioN for Context prediction) based on two main ideas: context prediction as an auxiliary task in multi-task learning manner, and a block attention method that learns the context prediction task.	2+3
With experiments on reading comprehension, we show that BLANC outperforms the state-of-the-art QA models, and the performance gap increases as the number of answer text occurrences increases.	4+5
We also conduct an experiment of training the models using SQuAD and predicting the supporting facts on HotpotQA and show that BLANC outperforms all baseline models in this zero-shot setting.	4+5
----------
Document interpretation and dialog understanding are the two major challenges for conversational machine reading.	1
In this work, we propose “Discern”, a discourse-aware entailment reasoning network to strengthen the connection and enhance the understanding of both document and dialog.	1+2
Specifically, we split the document into clause-like elementary discourse units (EDU) using a pre-trained discourse segmentation model, and we train our model in a weakly-supervised manner to predict whether each EDU is entailed by the user feedback in a conversation.	3
Based on the learned EDU and entailment representations, we either reply to the user our final decision “yes/no/irrelevant” of the initial question, or generate a follow-up question to inquiry more information.	3
Our experiments on the ShARC benchmark (blind, held-out test set) show that Discern achieves state-of-the-art results of 78.3% macro-averaged accuracy on decision making and 64.0 BLEU1 on follow-up question generation.	4
Code and models are released at https://github.com/Yifan-Gao/Discern.	6
----------
Deepfake detection, the task of automatically discriminating machine-generated text, is increasingly critical with recent advances in natural language generative models.	1
Existing approaches to deepfake detection typically represent documents with coarse-grained representations.	1
However, they struggle to capture factual structures of documents, which is a discriminative factor between machine-generated and human-written text according to our statistical analysis.	1
To address this, we propose a graph-based model that utilizes the factual structure of a document for deepfake detection of text.	2+3
Our approach represents the factual structure of a given document as an entity graph, which is further utilized to learn sentence representations with a graph neural network.	3
Sentence representations are then composed to a document representation for making predictions, where consistent relations between neighboring sentences are sequentially modeled.	3
Results of experiments on two public deepfake datasets show that our approach significantly improves strong base models built with RoBERTa.	4
Model analysis further indicates that our model can distinguish the difference in the factual structure between machine-generated text and human-written text.	4
----------
We study the zero-shot transfer capabilities of text matching models on a massive scale, by self-supervised training on 140 source domains from community question answering forums in English.	2+3
We investigate the model performances on nine benchmarks of answer selection and question similarity tasks, and show that all 140 models transfer surprisingly well, where the large majority of models substantially outperforms common IR baselines.	3+4
We also demonstrate that considering a broad selection of source domains is crucial for obtaining the best zero-shot transfer performances, which contrasts the standard procedure that merely relies on the largest and most similar domains.	4
In addition, we extensively study how to best combine multiple source domains.	4
We propose to incorporate self-supervised with supervised multi-task learning on all available source domains.	4
Our best zero-shot transfer model considerably outperforms in-domain BERT and the previous state of the art on six benchmarks.	4
Fine-tuning of our model with in-domain data results in additional large gains and achieves the new state of the art on all nine benchmarks.	3+4
----------
Abstract Meaning Representation (AMR) is a popular formalism of natural language that represents the meaning of a sentence as a semantic graph.	1
It is agnostic about how to derive meanings from strings and for this reason it lends itself well to the encoding of semantics across languages.	1
However, cross-lingual AMR parsing is a hard task, because training data are scarce in languages other than English and the existing English AMR parsers are not directly suited to being used in a cross-lingual setting.	1
In this work we tackle these two problems so as to enable cross-lingual AMR parsing: we explore different transfer learning techniques for producing automatic AMR annotations across languages and develop a cross-lingual AMR parser, XL-AMR.	1+2
This can be trained on the produced data and does not rely on AMR aligners or source-copy mechanisms as is commonly the case in English AMR parsing.	3
The results of XL-AMR significantly surpass those previously reported in Chinese, German, Italian and Spanish.	4
Finally we provide a qualitative analysis which sheds light on the suitability of AMR across languages.	4
We release XL-AMR at github.com/SapienzaNLP/xl-amr.	6
----------
In the literature, the research on abstract meaning representation (AMR) parsing is much restricted by the size of human-curated dataset which is critical to build an AMR parser with good performance.	1
To alleviate such data size restriction, pre-trained models have been drawing more and more attention in AMR parsing.	1
However, previous pre-trained models, like BERT, are implemented for general purpose which may not work as expected for the specific task of AMR parsing.	1
In this paper, we focus on sequence-to-sequence (seq2seq) AMR parsing and propose a seq2seq pre-training approach to build pre-trained models in both single and joint way on three relevant tasks, i.e., machine translation, syntactic parsing, and AMR parsing itself.	2+3
Moreover, we extend the vanilla fine-tuning method to a multi-task learning fine-tuning method that optimizes for the performance of AMR parsing while endeavors to preserve the response of pre-trained models.	3+4
Extensive experimental results on two English benchmark datasets show that both the single and joint pre-trained models significantly improve the performance (e.g., from 71.5 to 80.2 on AMR 2.0), which reaches the state of the art.	4
The result is very encouraging since we achieve this with seq2seq models rather than complex models.	5
We make our code and model available at https:// github.com/xdqkid/S2S-AMR-Parser.	6
----------
The task of automatic hate-speech and offensive language detection in social media content is of utmost importance due to its implications in unprejudiced society concerning race, gender, or religion.	1
Existing research in this area, however, is mainly focused on the English language, limiting the applicability to particular demographics.	1
Despite its prevalence, Roman Urdu (RU) lacks language resources, annotated datasets, and language models for this task.	1
In this study, we: (1) Present a lexicon of hateful words in RU, (2) Develop an annotated dataset called RUHSOLD consisting of 10,012 tweets in RU with both coarse-grained and fine-grained labels of hate-speech and offensive language, (3) Explore the feasibility of transfer learning of five existing embedding models to RU, (4) Propose a novel deep learning architecture called CNN-gram for hate-speech and offensive language detection and compare its performance with seven current baseline approaches on RUHSOLD dataset, and (5) Train domain-specific embeddings on more than 4.7 million tweets and make them publicly available.	2+3
We conclude that transfer learning is more beneficial as compared to training embedding from scratch and that the proposed model exhibits greater robustness as compared to the baselines.	4
----------
Work on bias in hate speech typically aims to improve classification performance while relatively overlooking the quality of the data.	1
We examine selection bias in hate speech in a language and label independent fashion.	2
We first use topic models to discover latent semantics in eleven hate speech corpora, then, we present two bias evaluation metrics based on the semantic similarity between topics and search words frequently used to build corpora.	3
We discuss the possibility of revising the data collection process by comparing datasets and analyzing contrastive case studies.	4+5
----------
In the computational detection of cyberbullying, existing work largely focused on building generic classifiers that rely exclusively on text analysis of social media sessions.	1
Despite their empirical success, we argue that a critical missing piece is the model explainability, i.e., why a particular piece of media session is detected as cyberbullying.	1
In this paper, therefore, we propose a novel deep model, HEterogeneous Neural Interaction Networks (HENIN), for explainable cyberbullying detection.	2
HENIN contains the following components: a comment encoder, a post-comment co-attention sub-network, and session-session and post-post interaction extractors.	3
Extensive experiments conducted on real datasets exhibit not only the promising performance of HENIN, but also highlight evidential comments so that one can understand why a media session is identified as cyberbullying.	4
----------
Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data.	1
We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques.	2+3
We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features.	3
The dataset is expected to advance sarcasm detection research.	5
Our method can be adapted to other affective computing domains, thus opening up new research opportunities.	5
----------
Self-supervised neural machine translation (SSNMT) jointly learns to identify and select suitable training data from comparable (rather than parallel) corpora and to translate, in a way that the two tasks support each other in a virtuous circle.	1
In this study, we provide an in-depth analysis of the sampling choices the SSNMT model makes during training.	2
We show how, without it having been told to do so, the model self-selects samples of increasing (i) complexity and (ii) task-relevance in combination with (iii) performing a denoising curriculum.	4
We observe that the dynamics of the mutual-supervision signals of both system internal representation types are vital for the extraction and translation performance.	4
We show that in terms of the Gunning-Fog Readability index, SSNMT starts extracting and learning from Wikipedia data suitable for high school students and quickly moves towards content suitable for first year undergraduate students.	5
----------
Applying the Transformer architecture on the character level usually requires very deep architectures that are difficult and slow to train.	1
These problems can be partially overcome by incorporating a segmentation into tokens in the model.	1
We show that by initially training a subword model and then finetuning it on characters, we can obtain a neural machine translation model that works at the character level without requiring token segmentation.	2+3
We use only the vanilla 6-layer Transformer Base architecture.	3
Our character-level models better capture morphological phenomena and show more robustness to noise at the expense of somewhat worse overall translation quality.	4
Our study is a significant step towards high-performance and easy to train character-based models that are not extremely large.	5
----------
Multilingual transformer models like mBERT and XLM-RoBERTa have obtained great improvements for many NLP tasks on a variety of languages.	1
However, recent works also showed that results from high-resource languages could not be easily transferred to realistic, low-resource scenarios.	1
In this work, we study trends in performance for different amounts of available resources for the three African languages Hausa, isiXhosa and on both NER and topic classification.	2
We show that in combination with transfer learning or distant supervision, these models can achieve with as little as 10 or 100 labeled sentences the same performance as baselines with much more supervised training data.	4
However, we also find settings where this does not hold.	4
Our discussions and additional experiments on assumptions such as time and hardware restrictions highlight challenges and opportunities in low-resource learning.	5
----------
The translation quality estimation (QE) task, particularly the QE as a Metric task, aims to evaluate the general quality of a translation based on the translation and the source sentence without using reference translations.	1
Supervised learning of this QE task requires human evaluation of translation quality as training data.	1
Human evaluation of translation quality can be performed in different ways, including assigning an absolute score to a translation or ranking different translations.	1
In order to make use of different types of human evaluation data for supervised learning, we present a multi-task learning QE model that jointly learns two tasks: score a translation and rank two translations.	2+3
Our QE model exploits cross-lingual sentence embeddings from pre-trained multilingual language models.	3
We obtain new state-of-the-art results on the WMT 2019 QE as a Metric task and outperform sentBLEU on the WMT 2019 Metrics task.	4
----------
The cascade approach to Speech Translation (ST) is based on a pipeline that concatenates an Automatic Speech Recognition (ASR) system followed by a Machine Translation (MT) system.	1
These systems are usually connected by a segmenter that splits the ASR output into hopefully, semantically self-contained chunks to be fed into the MT system.	1
This is specially challenging in the case of streaming ST, where latency requirements must also be taken into account.	1
This work proposes novel segmentation models for streaming ST that incorporate not only textual, but also acoustic information to decide when the ASR output is split into a chunk.	2
An extensive and throughly experimental setup is carried out on the Europarl-ST dataset to prove the contribution of acoustic information to the performance of the segmentation model in terms of BLEU score in a streaming ST scenario.	4
Finally, comparative results with previous work also show the superiority of the segmentation models proposed in this work.	5
----------
Despite being the seventh most widely spoken language in the world, Bengali has received much less attention in machine translation literature due to being low in resources.	1
Most publicly available parallel corpora for Bengali are not large enough; and have rather poor quality, mostly because of incorrect sentence alignments resulting from erroneous sentence segmentation, and also because of a high volume of noise present in them.	1
In this work, we build a customized sentence segmenter for Bengali and propose two novel methods for parallel corpus creation on low-resource setups: aligner ensembling and batch filtering.	1+2
With the segmenter and the two methods combined, we compile a high-quality Bengali-English parallel corpus comprising of 2.75 million sentence pairs, more than 2 million of which were not available before.	3
Training on neural models, we achieve an improvement of more than 9 BLEU score over previous approaches to Bengali-English machine translation.	4
We also evaluate on a new test set of 1000 pairs made with extensive quality control.	4
We release the segmenter, parallel corpus, and the evaluation set, thus elevating Bengali from its low-resource status.	4
To the best of our knowledge, this is the first ever large scale study on Bengali-English machine translation.	5
We believe our study will pave the way for future research on Bengali-English machine translation as well as other low-resource languages.	5
Our data and code are available at https://github.com/csebuetnlp/banglanmt.	6
----------
This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT).	1+2
Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language.	3
Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons.	3
CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragment of the input sentence.	3
In this way, CSP is able to pre-train the NMT model by explicitly making the most of the alignment information extracted from the source and target monolingual corpus.	3
Additionally, we relieve the pretrain-finetune discrepancy caused by the artificial symbols like [mask].	3
To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT.	3
Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.	4
----------
The one-sided focus on English in previous studies of gender bias in NLP misses out on opportunities in other languages: English challenge datasets such as GAP and WinoGender highlight model preferences that are “hallucinatory”, e.g., disambiguating gender-ambiguous occurrences of ‘doctor’ as male doctors.	1
We show that for languages with type B reflexivization, e.g., Swedish and Russian, we can construct multi-task challenge datasets for detecting gender bias that lead to unambiguously wrong model predictions: In these languages, the direct translation of ‘the doctor removed his mask’ is not ambiguous between a coreferential reading and a disjoint reading.	1
Instead, the coreferential reading requires a non-gendered pronoun, and the gendered, possessive pronouns are anti-reflexive.	1
We present a multilingual, multi-task challenge dataset, which spans four languages and four NLP tasks and focuses only on this phenomenon.	2
We find evidence for gender bias across all task-language combinations and correlate model bias with national labor market statistics.	3+4
----------
We investigate the following question for machine translation (MT): can we develop a single universal MT model to serve as the common seed and obtain derivative and improved models on arbitrary language pairs?	1
We propose mRASP, an approach to pre-train a universal multilingual neural machine translation model.	2
Our key idea in mRASP is its novel technique of random aligned substitution, which brings words and phrases with similar meanings across multiple languages closer in the representation space.	3
We pre-train a mRASP model on 32 language pairs jointly with only public datasets.	3
The model is then fine-tuned on downstream language pairs to obtain specialized MT models.	3
We carry out extensive experiments on 42 translation directions across a diverse settings, including low, medium, rich resource, and as well as transferring to exotic language pairs.	3
Experimental results demonstrate that mRASP achieves significant performance improvement compared to directly training on those target pairs.	4
It is the first time to verify that multiple lowresource language pairs can be utilized to improve rich resource MT.	4
Surprisingly, mRASP is even able to improve the translation quality on exotic languages that never occur in the pretraining corpus.	5
Code, data, and pre-trained models are available at https://github.com/linzehui/mRASP.	6
----------
The attention mechanism is the crucial component of the transformer architecture.	1
Recent research shows that most attention heads are not confident in their decisions and can be pruned.	1
However, removing them before training a model results in lower quality.	1
In this paper, we apply the lottery ticket hypothesis to prune heads in the early stages of training.	1+2
Our experiments on machine translation show that it is possible to remove up to three-quarters of attention heads from transformer-big during early training with an average -0.1 change in BLEU for Turkish→English.	4
The pruned model is 1.5 times as fast at inference, albeit at the cost of longer training.	4
Our method is complementary to other approaches, such as teacher-student, with English→German student model gaining an additional 10% speed-up with 75% encoder attention removed and 0.2 BLEU loss.	4
----------
Neural machine translation (NMT) has achieved great success due to the ability to generate high-quality sentences.	1
Compared with human translations, one of the drawbacks of current NMT is that translations are not usually faithful to the input, e.g., omitting information or generating unrelated fragments, which inevitably decreases the overall quality, especially for human readers.	1
In this paper, we propose a novel training strategy with a multi-task learning paradigm to build a faithfulness enhanced NMT model (named FEnmt).	1+2
During the NMT training process, we sample a subset from the training set and translate them to get fragments that have been mistranslated.	3
Afterward, the proposed multi-task learning paradigm is employed on both encoder and decoder to guide NMT to correctly translate these fragments.	3
Both automatic and human evaluations verify that our FEnmt could improve translation quality by effectively reducing unfaithful translations.	3
----------
We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements.	1+2
Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality.	3
To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric.	3
Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems.	5
----------
Using a language model (LM) pretrained on two languages with large monolingual data in order to initialize an unsupervised neural machine translation (UNMT) system yields state-of-the-art results.	1
When limited data is available for one language, however, this method leads to poor translations.	1
We present an effective approach that reuses an LM that is pretrained only on the high-resource language.	2
The monolingual LM is fine-tuned on both languages and is then used to initialize a UNMT model.	3
To reuse the pretrained LM, we have to modify its predefined vocabulary, to account for the new language.	3
We therefore propose a novel vocabulary extension method.	4
Our approach, RE-LM, outperforms a competitive cross-lingual pretraining model (XLM) in English-Macedonian (En-Mk) and English-Albanian (En-Sq), yielding more than +8.3 BLEU points for all four translation directions.	4
----------
Most of the successful and predominant methods for Bilingual Lexicon Induction (BLI) are mapping-based, where a linear mapping function is learned with the assumption that the word embedding spaces of different languages exhibit similar geometric structures (i.e. approximately isomorphic).	1
However, several recent studies have criticized this simplified assumption showing that it does not hold in general even for closely related languages.	1
In this work, we propose a novel semi-supervised method to learn cross-lingual word embeddings for BLI.	2
Our model is independent of the isomorphic assumption and uses non-linear mapping in the latent space of two independently pre-trained autoencoders.	3
Through extensive experiments on fifteen (15) different language pairs (in both directions) comprising resource-rich and low-resource languages from two different datasets, we demonstrate that our method outperforms existing models by a good margin.	4
Ablation studies show the importance of different model components and the necessity of non-linear mapping.	5
----------
As a sequence-to-sequence generation task, neural machine translation (NMT) naturally contains intrinsic uncertainty, where a single sentence in one language has multiple valid counterparts in the other.	1
However, the dominant methods for NMT only observe one of them from the parallel corpora for the model training but have to deal with adequate variations under the same meaning at inference.	1
This leads to a discrepancy of the data distribution between the training and the inference phases.	1
To address this problem, we propose uncertainty-aware semantic augmentation, which explicitly captures the universal semantic information among multiple semantically-equivalent source sentences and enhances the hidden representations with this information for better translations.	2+3
Extensive experiments on various translation tasks reveal that our approach significantly outperforms the strong baselines and the existing methods.	4
----------
A gapping construction consists of a coordinated structure where redundant elements are elided from all but one conjuncts.	1
This paper proposes a method of parsing sentences with gapping to recover elided elements.	1+2
The proposed method is based on constituent trees annotated with grammatical and semantic roles that are useful for identifying elided elements.	3
Our method outperforms the previous method in terms of F-measure and recall.	5
----------
We introduce a novel chart-based algorithm for span-based parsing of discontinuous constituency trees of block degree two, including ill-nested structures.	1+2
In particular, we show that we can build variants of our parser with smaller search spaces and time complexities ranging from O(nˆ6) down to O(nˆ3).	2
The cubic time variant covers 98% of constituents observed in linguistic treebanks while having the same complexity as continuous constituency parsers.	4
We evaluate our approach on German and English treebanks (Negra, Tiger, and DPTB) and report state-of-the-art results in the fully supervised setting.	4
We also experiment with pre-trained word embeddings and Bert-based neural networks.	4
----------
Cross-language differences in (universal) dependency parsing performance are mostly attributed to treebank size, average sentence length, average dependency length, morphological complexity, and domain differences.	1
We point at a factor not previously discussed: If we abstract away from words and dependency labels, how many graphs in the test data were seen in the training data?	1
We compute graph isomorphisms, and show that, treebank size aside, overlap between training and test graphs explain more of the observed variation than standard explanations such as the above.	2+3
----------
This paper reduces discontinuous parsing to sequence labeling.	1+2
It first shows that existing reductions for constituent parsing as labeling do not support discontinuities.	3
Second, it fills this gap and proposes to encode tree discontinuities as nearly ordered permutations of the input sequence.	3
Third, it studies whether such discontinuous representations are learnable.	3
The experiments show that despite the architectural simplicity, under the right representation, the models are fast and accurate.	5
----------
This paper focuses on tree-based modeling for the sentence classification task.	2
In existing works, aggregating on a syntax tree usually considers local information of sub-trees.	3
In contrast, in addition to the local information, our proposed Modularized Syntactic Neural Network (MSNN) utilizes the syntax category labels and takes advantage of the global context while modeling sub-trees.	3
In MSNN, each node of a syntax tree is modeled by a label-related syntax module.	3
Each syntax module aggregates the outputs of lower-level modules, and finally, the root module provides the sentence representation.	3
We design a tree-parallel mini-batch strategy for efficient training and predicting.	3
Experimental results on four benchmark datasets show that our MSNN significantly outperforms previous state-of-the-art tree-based methods on the sentence classification task.	4
----------
This paper proposes to adapt self-attention to discourse level for modeling discourse elements in argumentative student essays.	1+2
Specifically, we focus on two issues.	2
First, we propose structural sentence positional encodings to explicitly represent sentence positions.	2
Second, we propose to use inter-sentence attentions to capture sentence interactions and enhance sentence representation.	2+3
We conduct experiments on two datasets: a Chinese dataset and an English dataset.	3
We find that (i) sentence positional encoding can lead to a large improvement for identifying discourse elements; (ii) a structural relative positional encoding of sentences shows to be most effective; (iii) inter-sentence attention vectors are useful as a kind of sentence representations for identifying discourse elements.	5
----------
Existing pre-trained large language models have shown unparalleled generative capabilities.	1
However, they are not controllable.	1
In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base.	2+3
Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator.	3
As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding.	3
The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset.	4
We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process.	4
Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords.	4
Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).	4
----------
Recent years the task of incomplete utterance rewriting has raised a large attention.	1
Previous works usually shape it as a machine translation task and employ sequence to sequence based architecture with copy mechanism.	1
In this paper, we present a novel and extensive approach, which formulates it as a semantic segmentation task.	2
Instead of generating from scratch, such a formulation introduces edit operations and shapes the problem as prediction of a word-level edit matrix.	3
Benefiting from being able to capture both local and global information, our approach achieves state-of-the-art performance on several public datasets.	4
Furthermore, our approach is four times faster than the standard approach in inference.	4
----------
A sequence-to-sequence (seq2seq) learning with neural networks empirically shows to be an effective framework for grammatical error correction (GEC), which takes a sentence with errors as input and outputs the corrected one.	1
However, the performance of GEC models with the seq2seq framework heavily relies on the size and quality of the corpus on hand.	1
We propose a method inspired by adversarial training to generate more meaningful and valuable training examples by continually identifying the weak spots of a model, and to enhance the model by gradually adding the generated adversarial examples to the training set.	2+3
Extensive experimental results show that such adversarial training can improve both the generalization and robustness of GEC models.	4+5
----------
Punning is a creative way to make conversation enjoyable and literary writing elegant.	1
In this paper, we focus on the task of generating a pun sentence given a pair of homophones.	2
We first find the constraint words supporting the semantic incongruity for a sentence.	3
Then we rewrite the sentence with explicit positive and negative constraints.	3
Our model achieves the state-of-the-art results in both automatic and human evaluations.	4
We further make an error analysis and discuss the challenges for the computational pun models.	5
----------
Neural Natural Language Generation (NLG) systems are well known for their unreliability.	1
To overcome this issue, we propose a data augmentation approach which allows us to restrict the output of a network and guarantee reliability.	2
While this restriction means generation will be less diverse than if randomly sampled, we include experiments that demonstrate the tendency of existing neural generation approaches to produce dull and repetitive text, and we argue that reliability is more important than diversity for this task.	4
The system trained using this approach scored 100% in semantic accuracy on the E2E NLG Challenge dataset, the same as a template system.	4
----------
Generating text from structured data is challenging because it requires bridging the gap between (i) structure and natural language (NL) and (ii) semantically underspecified input and fully specified NL output.	1
Multilingual generation brings in an additional challenge: that of generating into languages with varied word order and morphological properties.	1
In this work, we focus on Abstract Meaning Representations (AMRs) as structured input, where previous research has overwhelmingly focused on generating only into English.	2
We leverage advances in cross-lingual embeddings, pretraining, and multilingual models to create multilingual AMR-to-text models that generate in twenty one different languages.	3
Our multilingual models surpass baselines that generate into one language in eighteen languages, based on automatic metrics.	4
We analyze the ability of our multilingual models to accurately capture morphology and word order using human evaluation, and find that native speakers judge our generations to be fluent.	5
----------
Bolukbasi et al.(2016) presents one of the first gender bias mitigation techniques for word embeddings.	1
Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings.	1
As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings.	1
However, an implicit and untested assumption of their method is that the bias subspace is actually linear.	1
In this work, we generalize their method to a kernelized, non-linear version.	2
We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique.	4
We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear.	4
Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al.(2016).	5
----------
It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts.	1
To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation.	2
Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation.	3
Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge.	4
Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.	4+5
----------
To scale non-parametric extensions of probabilistic topic models such as Latent Dirichlet allocation to larger data sets, practitioners rely increasingly on parallel and distributed systems.	1
In this work, we study data-parallel training for the hierarchical Dirichlet process (HDP) topic model.	2
Based upon a representation of certain conditional distributions within an HDP, we propose a doubly sparse data-parallel sampler for the HDP topic model.	3
This sampler utilizes all available sources of sparsity found in natural language - an important way to make computation efficient.	3
We benchmark our method on a well-known corpus (PubMed) with 8m documents and 768m tokens, using a single multi-core machine in under four days.	3
----------
Few/Zero-shot learning is a big challenge of many classifications tasks, where a classifier is required to recognise instances of classes that have very few or even no training samples.	1
It becomes more difficult in multi-label classification, where each instance is labelled with more than one class.	1
In this paper, we present a simple multi-graph aggregation model that fuses knowledge from multiple label graphs encoding different semantic label relationships in order to study how the aggregated knowledge can benefit multi-label zero/few-shot document classification.	1+2
The model utilises three kinds of semantic information, i.e., the pre-trained word embeddings, label description, and pre-defined label relations.	3
Experimental results derived on two large clinical datasets (i.e., MIMIC-II and MIMIC-III ) and the EU legislation dataset show that methods equipped with the multi-graph knowledge aggregation achieve significant performance improvement across almost all the measures on few/zero-shot labels.	4
----------
One key principle for assessing textual similarity is measuring the degree of semantic overlap between texts by considering the word alignment.	1
Such alignment-based approaches are both intuitive and interpretable; however, they are empirically inferior to the simple cosine similarity between general-purpose sentence vectors.	1
We focus on the fact that the norm of word vectors is a good proxy for word importance, and the angle of them is a good proxy for word similarity.	2
However, alignment-based approaches do not distinguish the norm and direction, whereas sentence-vector approaches automatically use the norm as the word importance.	3
Accordingly, we propose decoupling word vectors into their norm and direction then computing the alignment-based similarity with the help of earth mover’s distance (optimal transport), which we refer to as word rotator’s distance.	4
Furthermore, we demonstrate how to grow the norm and direction of word vectors (vector converter); this is a new systematic approach derived from the sentence-vector estimation methods, which can significantly improve the performance of the proposed method.	5
On several STS benchmarks, the proposed methods outperform not only alignment-based approaches but also strong baselines.	5
The source code is avaliable at https://github.com/eumesy/wrd	6
----------
Graph embedding (GE) methods embed nodes (and/or edges) in graph into a low-dimensional semantic space, and have shown its effectiveness in modeling multi-relational data.	1
However, existing GE models are not practical in real-world applications since it overlooked the streaming nature of incoming data.	1
To address this issue, we study the problem of continual graph representation learning which aims to continually train a GE model on new data to learn incessantly emerging multi-relational data while avoiding catastrophically forgetting old learned knowledge.	2
Moreover, we propose a disentangle-based continual graph representation learning (DiCGRL) framework inspired by the human’s ability to learn procedural knowledge.	2+3
The experimental results show that DiCGRL could effectively alleviate the catastrophic forgetting problem and outperform state-of-the-art continual learning models.	4
The code and datasets are released on https://github.com/KXY-PUBLIC/DiCGRL.	6
----------
Semi-supervision is a promising paradigm for Bilingual Lexicon Induction (BLI) with limited annotations.	1
However, previous semisupervised methods do not fully utilize the knowledge hidden in annotated and nonannotated data, which hinders further improvement of their performance.	1
In this paper, we propose a new semi-supervised BLI framework to encourage the interaction between the supervised signal and unsupervised alignment.	2
We design two message-passing mechanisms to transfer knowledge between annotated and non-annotated data, named prior optimal transport and bi-directional lexicon update respectively.	3
Then, we perform semi-supervised learning based on a cyclic or a parallel parameter feeding routine to update our models.	3
Our framework is a general framework that can incorporate any supervised and unsupervised BLI methods based on optimal transport.	3
Experimental results on MUSE and VecMap datasets show significant improvement of our models.	4
Ablation study also proves that the two-way interaction between the supervised signal and unsupervised alignment accounts for the gain of the overall performance.	5
Results on distant language pairs further illustrate the advantage and robustness of our proposed method.	5
----------
One approach to matching texts from asymmetrical domains is projecting the input sequences into a common semantic space as feature vectors upon which the matching function can be readily defined and learned.	1
In real-world matching practices, it is often observed that with the training goes on, the feature vectors projected from different domains tend to be indistinguishable.	1
The phenomenon, however, is often overlooked in existing matching models.	1
As a result, the feature vectors are constructed without any regularization, which inevitably increases the difficulty of learning the downstream matching functions.	1
In this paper, we propose a novel match method tailored for text matching in asymmetrical domains, called WD-Match.	2+3
In WD-Match, a Wasserstein distance-based regularizer is defined to regularize the features vectors projected from different domains.	3
As a result, the method enforces the feature projection function to generate vectors such that those correspond to different domains cannot be easily discriminated.	3
The training process of WD-Match amounts to a game that minimizes the matching loss regularized by the Wasserstein distance.	4
WD-Match can be used to improve different text matching methods, by using the method as its underlying matching model.	3
Four popular text matching methods have been exploited in the paper.	3
Experimental results based on four publicly available benchmarks showed that WD-Match consistently outperformed the underlying methods and the baselines.	1
----------
Recent progress on unsupervised cross-lingual embeddings in the bilingual setting has given the impetus to learning a shared embedding space for several languages.	1
A popular framework to solve the latter problem is to solve the following two sub-problems jointly: 1) learning unsupervised word alignment between several language pairs, and 2) learning how to map the monolingual embeddings of every language to shared multilingual space.	1
In contrast, we propose a simple approach by decoupling the above two sub-problems and solving them separately, one after another, using existing techniques.	2+3
We show that this proposed approach obtains surprisingly good performance in tasks such as bilingual lexicon induction, cross-lingual word similarity, multilingual document classification, and multilingual dependency parsing.	4
When distant languages are involved, the proposed approach shows robust behavior and outperforms existing unsupervised multilingual word embedding approaches.	4
----------
We show that Reinforcement Learning (RL) methods for solving Text-Based Games (TBGs) often fail to generalize on unseen games, especially in small data regimes.	1
To address this issue, we propose Context Relevant Episodic State Truncation (CREST) for irrelevant token removal in observation text for improved generalization.	2+3
Our method first trains a base model using Q-learning, which typically overfits the training games.	3
The base model’s action token distribution is used to perform observation pruning that removes irrelevant tokens.	3
A second bootstrapped model is then retrained on the pruned observation text.	3
Our bootstrapped agent shows improved generalization in solving unseen TextWorld games, using 10x-20x fewer training games compared to previous state-of-the-art (SOTA) methods despite requiring fewer number of training episodes.	3+4
----------
Pre-trained language models (e.g., BERT) have achieved significant success in various natural language processing (NLP) tasks.	1
However, high storage and computational costs obstruct pre-trained language models to be effectively deployed on resource-constrained devices.	1
In this paper, we propose a novel BERT distillation method based on many-to-many layer mapping, which allows each intermediate student layer to learn from any intermediate teacher layers.	2
In this way, our model can learn from different teacher layers adaptively for different NLP tasks.	3
In addition, we leverage Earth Mover’s Distance (EMD) to compute the minimum cumulative cost that must be paid to transform knowledge from teacher network to student network.	3
EMD enables effective matching for the many-to-many layer mapping.	4
Furthermore, we propose a cost attention mechanism to learn the layer weights used in EMD automatically, which is supposed to further improve the model’s performance and accelerate convergence time.	2+3
Extensive experiments on GLUE benchmark demonstrate that our model achieves competitive performance compared to strong competitors in terms of both accuracy and model compression	5
----------
Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking (DST).	1
Existing approaches generally fall into two extremes: choosing models without ontology or embedding ontology in models leading to over-dependence.	1
In this paper, we propose a new architecture to cleverly exploit ontology, which consists of Slot Attention (SA) and Value Normalization (VN), referred to as SAVN.	2
Moreover, we supplement the annotation of supporting span for MultiWOZ 2.1, which is the shortest span in utterances to support the labeled value.	2+3
SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span.	3
VN is designed specifically for the use of ontology, which can convert supporting spans to the values.	3
Empirical results demonstrate that SAVN achieves the state-of-the-art joint accuracy of 54.52% on MultiWOZ 2.0 and 54.86% on MultiWOZ 2.1.	4
Besides, we evaluate VN with incomplete ontology.	4
The results show that even if only 30% ontology is used, VN can also contribute to our model.	4
----------
Most approaches to Open-Domain Question Answering consist of a light-weight retriever that selects a set of candidate passages, and a computationally expensive reader that examines the passages to identify the correct answer.	1
Previous works have shown that as the number of retrieved passages increases, so does the performance of the reader.	1
However, they assume all retrieved passages are of equal importance and allocate the same amount of computation to them, leading to a substantial increase in computational cost.	1
To reduce this cost, we propose the use of adaptive computation to control the computational budget allocated for the passages to be read.	2+3
We first introduce a technique operating on individual passages in isolation which relies on anytime prediction and a per-layer estimation of early exit probability.	3
We then introduce SKYLINEBUILDER, an approach for dynamically deciding on which passage to allocate computation at each step, based on a resource allocation policy trained via reinforcement learning.	3
Our results on SQuAD-Open show that adaptive computation with global prioritisation improves over several strong static and adaptive methods, leading to a 4.3x reduction in computation while retaining 95% performance of the full model.	4
----------
Complex reasoning over text requires understanding and chaining together free-form predicates and logical connectives.	1
Prior work has largely tried to do this either symbolically or with black-box transformers.	1
We present a middle ground between these two extremes: a compositional model reminiscent of neural module networks that can perform chained logical reasoning.	2
This model first finds relevant sentences in the context and then chains them together using neural modules.	3
Our model gives significant performance improvements (up to 29% relative error reduction when combined with a reranker) on ROPES, a recently-introduced complex reasoning dataset.	5
----------
State-of-the-art question answering (QA) relies upon large amounts of training data for which labeling is time consuming and thus expensive.	1
For this reason, customizing QA systems is challenging.	1
As a remedy, we propose a novel framework for annotating QA datasets that entails learning a cost-effective annotation policy and a semi-supervised annotation scheme.	2+3
The latter reduces the human effort: it leverages the underlying QA system to suggest potential candidate annotations.	2
Human annotators then simply provide binary feedback on these candidates.	3
Our system is designed such that past annotations continuously improve the future performance and thus overall annotation cost.	3
To the best of our knowledge, this is the first paper to address the problem of annotating questions with minimal annotation cost.	5
We compare our framework against traditional manual annotations in an extensive set of experiments.	3
We find that our approach can reduce up to 21.1% of the annotation cost.	5
----------
This paper focuses on machine reading comprehension for narrative passages.	1
Narrative passages usually describe a chain of events.	1
When reading this kind of passage, humans tend to restore a scene according to the text with their prior knowledge, which helps them understand the passage comprehensively.	1
Inspired by this behavior of humans, we propose a method to let the machine imagine a scene during reading narrative for better comprehension.	2
Specifically, we build a scene graph by utilizing Atomic as the external knowledge and propose a novel Graph Dimensional-Iteration Network (GDIN) to encode the graph.	2+3
We conduct experiments on the ROCStories, a dataset of Story Cloze Test (SCT), and CosmosQA, a dataset of multiple choice.	3
Our method achieves state-of-the-art.	3
----------
Models for reading comprehension (RC) commonly restrict their output space to the set of all single contiguous spans from the input, in order to alleviate the learning problem and avoid the need for a model that generates text explicitly.	1
However, forcing an answer to be a single span can be restrictive, and some recent datasets also include multi-span questions, i.e., questions whose answer is a set of non-contiguous spans in the text.	1
Naturally, models that return single spans cannot answer these questions.	1
In this work, we propose a simple architecture for answering multi-span questions by casting the task as a sequence tagging problem, namely, predicting for each input token whether it should be part of the output or not.	2+3
Our model substantially improves performance on span extraction questions from DROP and Quoref by 9.9 and 5.5 EM points respectively.	4
----------
The growth of domain-specific applications of semantic models, boosted by the recent achievements of unsupervised embedding learning algorithms, demands domain-specific evaluation datasets.	1
In many cases, content-based recommenders being a prime example, these models are required to rank words or texts according to their semantic relatedness to a given concept, with particular focus on top ranks.	1
In this work, we give a threefold contribution to address these requirements: (i) we define a protocol for the construction, based on adaptive pairwise comparisons, of a relatedness-based evaluation dataset tailored on the available resources and optimized to be particularly accurate in top-rank evaluation; (ii) we define appropriate metrics, extensions of well-known ranking correlation coefficients, to evaluate a semantic model via the aforementioned dataset by taking into account the greater significance of top ranks.	2+3
Finally, (iii) we define a stochastic transitivity model to simulate semantic-driven pairwise comparisons, which confirms the effectiveness of the proposed dataset construction protocol.	2+3
----------
Pre-trained neural language models bring significant improvement for various NLP tasks, by fine-tuning the models on task-specific training sets.	1
During fine-tuning, the parameters are initialized from pre-trained models directly, which ignores how the learning process of similar NLP tasks in different domains is correlated and mutually reinforced.	1
In this paper, we propose an effective learning procedure named Meta Fine-Tuning (MFT), serving as a meta-learner to solve a group of similar NLP tasks for neural language models.	2
Instead of simply multi-task training over all the datasets, MFT only learns from typical instances of various domains to acquire highly transferable knowledge.	3
It further encourages the language model to encode domain-invariant representations by optimizing a series of novel domain corruption loss functions.	3
After MFT, the model can be fine-tuned for each domain with better parameter initializations and higher generalization ability.	3
We implement MFT upon BERT to solve several multi-domain text mining tasks.	3
Experimental results confirm the effectiveness of MFT and its usefulness for few-shot learning.	4
----------
Generative neural networks have been shown effective on query suggestion.	1
Commonly posed as a conditional generation problem, the task aims to leverage earlier inputs from users in a search session to predict queries that they will likely issue at a later time.	2
User inputs come in various forms such as querying and clicking, each of which can imply different semantic signals channeled through the corresponding behavioral patterns.	3
This paper induces these behavioral biases as hypotheses for query generation, where a generic encoder-decoder Transformer framework is presented to aggregate arbitrary hypotheses of choice.	2+3
Our experimental results show that the proposed approach leads to significant improvements on top-k word error rate and Bert F1 Score compared to a recent BART model.	4
----------
The causal relationships between emotions and causes in text have recently received a lot of attention.	1
Most of the existing works focus on the extraction of the causally related clauses from documents.	1
However, none of these works has considered the possibility that the causal relationships among the extracted emotion and cause clauses may only be valid under a specific context, without which the extracted clauses may not be causally related.	1
To address such an issue, we propose a new task of determining whether or not an input pair of emotion and cause has a valid causal relationship under different contexts, and construct a corresponding dataset via manual annotation and negative sampling based on an existing benchmark dataset.	2+3
Furthermore, we propose a prediction aggregation module with low computational overhead to fine-tune the prediction results based on the characteristics of the input clauses.	3
Experiments demonstrate the effectiveness and generality of our aggregation module.	5
----------
The question of how to probe contextual word representations in a way that is principled and useful has seen significant recent attention.	1
In our contribution to this discussion, we argue, first, for a probe metric that reflects the trade-off between probe complexity and performance: the Pareto hypervolume.	1
To measure complexity, we present a number of parametric and non-parametric metrics.	2
Our experiments with such metrics show that probe’s performance curves often fail to align with widely accepted rankings between language representations (with, e.g., non-contextual representations outperforming contextual ones).	4
These results lead us to argue, second, that common simplistic probe tasks such as POS labeling and dependency arc labeling, are inadequate to evaluate the properties encoded in contextual word representations.	4
We propose full dependency parsing as an example probe task, and demonstrate it with the Pareto hypervolume.	3
In support of our arguments, the results of this illustrative experiment conform closer to accepted rankings among contextual word representations.	4
----------
To demystify the “black box” property of deep neural networks for natural language processing (NLP), several methods have been proposed to interpret their predictions by measuring the change in prediction probability after erasing each token of an input.	2
Since existing methods replace each token with a predefined value (i.e., zero), the resulting sentence lies out of the training data distribution, yielding misleading interpretations.	1
In this study, we raise the out-of-distribution problem induced by the existing interpretation methods and present a remedy; we propose to marginalize each token out.	2+3
We interpret various NLP models trained for sentiment analysis and natural language inference using the proposed method.	3
----------
Existing algorithms for aligning cross-lingual word vector spaces assume that vector spaces are approximately isomorphic.	1
As a result, they perform poorly or fail completely on non-isomorphic spaces.	1
Such non-isomorphism has been hypothesised to result from typological differences between languages.	1
In this work, we ask whether non-isomorphism is also crucially a sign of degenerate word vector spaces.	2
We present a series of experiments across diverse languages which show that variance in performance across language pairs is not only due to typological differences, but can mostly be attributed to the size of the monolingual resources available, and to the properties and duration of monolingual training (e.g.“under-training”).	4+5
----------
Neural networks can achieve impressive performance on many natural language processing applications, but they typically need large labeled data for training and are not easily interpretable.	1
On the other hand, symbolic rules such as regular expressions are interpretable, require no training, and often achieve decent accuracy; but rules cannot benefit from labeled data when available and hence underperform neural networks in rich-resource scenarios.	1
In this paper, we propose a type of recurrent neural networks called FA-RNNs that combine the advantages of neural networks and regular expression rules.	2+3
An FA-RNN can be converted from regular expressions and deployed in zero-shot and cold-start scenarios.	5
It can also utilize labeled data for training to achieve improved prediction accuracy.	5
After training, an FA-RNN often remains interpretable and can be converted back into regular expressions.	5
We apply FA-RNNs to text classification and observe that FA-RNNs significantly outperform previous neural approaches in both zero-shot and low-resource settings and remain very competitive in rich-resource settings.	5
----------
Large Transformer-based models were shown to be reducible to a smaller number of self-attention heads and layers.	1
We consider this phenomenon from the perspective of the lottery ticket hypothesis, using both structured and magnitude pruning.	2+3
For fine-tuned BERT, we show that (a) it is possible to find subnetworks achieving performance that is comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse.	3+4
Strikingly, with structured pruning even the worst possible subnetworks remain highly trainable, indicating that most pre-trained BERT weights are potentially useful.	5
We also study the “good” subnetworks to see if their success can be attributed to superior linguistic knowledge, but find them unstable, and not explained by meaningful self-attention patterns.	5
----------
Given the success of Transformer-based models, two directions of study have emerged: interpreting role of individual attention heads and down-sizing the models for efficiency.	1
Our work straddles these two streams: We analyse the importance of basing pruning strategies on the interpreted role of the attention heads.	2
We evaluate this on Transformer and BERT models on multiple NLP tasks.	3
Firstly, we find that a large fraction of the attention heads can be randomly pruned with limited effect on accuracy.	4
Secondly, for Transformers, we find no advantage in pruning attention heads identified to be important based on existing studies that relate importance to the location of a head.	4
On the BERT model too we find no preference for top or bottom layers, though the latter are reported to have higher importance.	4
However, strategies that avoid pruning middle layers and consecutive layers perform better.	4
Finally, during fine-tuning the compensation for pruned attention heads is roughly equally distributed across the un-pruned heads.	4
Our results thus suggest that interpretation of attention heads does not strongly inform pruning.	4
----------
BERT and its variants have achieved state-of-the-art performance in various NLP tasks.	1
Since then, various works have been proposed to analyze the linguistic information being captured in BERT.	1
However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering.	1
In this work, we attempt to interpret BERT for RCQA.	2
Since BERT layers do not have predefined roles, we define a layer’s role or functionality using Integrated Gradients.	3
Based on the defined roles, we perform a preliminary analysis across all layers.	3
We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction.	4
Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly.	4
The fine-tuning and analysis scripts will be publicly available at https://github.com/iitmnlp/BERT-Analysis-RCQA.	6
----------
Attribution methods assess the contribution of inputs to the model prediction.	1
One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction.	1
Though conceptually simple, erasure’s objective is intractable and approximate search remains expensive with modern deep NLP models.	1
Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model ‘knows’ it can be dropped.	1
The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction.	1
To deal with these challenges, we introduce Differentiable Masking.	2
DiffMask learns to mask-out subsets of the input while maintaining differentiability.	3
The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model.	3
First, this makes the approach efficient because we predict rather than search.	4
Second, as with probing classifiers, this reveals what the network ‘knows’ at the corresponding layers.	4
This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers.	1
We use DiffMask to study BERT models on sentiment classification and question answering.	2+3
----------
Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity.	1
Efforts to make the rationales behind the models’ predictions transparent have inspired an abundance of new explainability techniques.	1
Provided with an already trained model, they compute saliency scores for the words of an input instance.	1
However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique.	1
In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques.	2
We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures.	3
We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model’s performance and the agreement of its rationales with human ones.	3
Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques.	2+3
----------
Chart Question Answering (CQA) is the task of answering natural language questions about visualisations in the chart image.	1
Recent solutions, inspired by VQA approaches, rely on image-based attention for question/answering while ignoring the inherent chart structure.	1
We propose STL-CQA which improves the question/answering through sequential elements localization, question encoding and then, a structural transformer-based learning approach.	2+3
We conduct extensive experiments while proposing pre-training tasks, methodology and also an improved dataset with more complex and balanced questions of different types.	3
The proposed methodology shows a significant accuracy improvement compared to the state-of-the-art approaches on various chart Q/A datasets, while outperforming even human baseline on the DVQA Dataset.	4
We also demonstrate interpretability while examining different components in the inference pipeline.	4
----------
In the task of Visual Question Answering (VQA), most state-of-the-art models tend to learn spurious correlations in the training set and achieve poor performance in out-of-distribution test data.	1
Some methods of generating counterfactual samples have been proposed to alleviate this problem.	1
However, the counterfactual samples generated by most previous methods are simply added to the training data for augmentation and are not fully utilized.	1
Therefore, we introduce a novel self-supervised contrastive learning mechanism to learn the relationship between original samples, factual samples and counterfactual samples.	2+3
With the better cross-modal joint embeddings learned from the auxiliary training objective, the reasoning capability and robustness of the VQA model are boosted significantly.	3+4
We evaluate the effectiveness of our method by surpassing current state-of-the-art models on the VQA-CP dataset, a diagnostic benchmark for assessing the VQA model’s robustness.	4+5
----------
Physical common sense plays an essential role in the cognition abilities of robots for human-robot interaction.	1
Machine learning methods have shown promising results on physical commonsense learning in natural language processing but still suffer from model generalization.	1
In this paper, we formulate physical commonsense learning as a knowledge graph completion problem to better use the latent relationships among training samples.	2+3
Compared with completing general knowledge graphs, completing a physical commonsense knowledge graph has three unique characteristics: training data are scarce, not all facts can be mined from existing texts, and the number of relationships is small.	3
To deal with these problems, we first use a pre-training language model BERT to augment training data, and then employ constrained tucker factorization to model complex relationships by constraining types and adding negative relationships.	3
We compare our method with existing state-of-the-art knowledge graph embedding methods and show its superior performance.	4
----------
Social media produces large amounts of contents every day.	1
To help users quickly capture what they need, keyphrase prediction is receiving a growing attention.	1
Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images.	1
In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post.	2
To better align social media style texts and images, we propose: (1) a novel Multi-Modality MultiHead Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities.	2+3
Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages.	3
Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention mechanisms.	4
Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios.	4
----------
Visual dialog is a challenging vision-language task, where a dialog agent needs to answer a series of questions through reasoning on the image content and dialog history.	1
Prior work has mostly focused on various attention mechanisms to model such intricate interactions.	1
By contrast, in this work, we propose VD-BERT, a simple yet effective framework of unified vision-dialog Transformer that leverages the pretrained BERT language models for Visual Dialog tasks.	2+3
The model is unified in that (1) it captures all the interactions between the image and the multi-turn dialog using a single-stream Transformer encoder, and (2) it supports both answer ranking and answer generation seamlessly through the same architecture.	3
More crucially, we adapt BERT for the effective fusion of vision and dialog contents via visually grounded training.	3
Without the need of pretraining on external vision-language data, our model yields new state of the art, achieving the top position in both single-model and ensemble settings (74.54 and 75.35 NDCG scores) on the visual dialog leaderboard.	3+4
Our code and pretrained models are released at https://github.com/salesforce/VD-BERT.	6
----------
In this paper, we consider the syntactic properties of languages emerged in referential games, using unsupervised grammar induction (UGI) techniques originally designed to analyse natural language.	2
We show that the considered UGI techniques are appropriate to analyse emergent languages and we then study if the languages that emerge in a typical referential game setup exhibit syntactic structure, and to what extent this depends on the maximum message length and number of symbols that the agents are allowed to use.	2
Our experiments demonstrate that a certain message length and vocabulary size are required for structure to emerge, but they also illustrate that more sophisticated game scenarios are required to obtain syntactic properties more akin to those observed in human language.	4
We argue that UGI techniques should be part of the standard toolkit for analysing emergent languages and release a comprehensive library to facilitate such analysis for future researchers.	5
----------
Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions.	1
Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences.	1
Meanwhile, due to the lack of intermediate supervision, the agent’s performance at following each part of the instruction cannot be assessed during navigation.	1
In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction.	2
We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time.	4
We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths.	3
To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step.	3
We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents.	3+4
We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.	3
----------
We study knowledge-grounded dialogue generation with pre-trained language models.	2+3
To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pre-trained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues.	3
Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.	4
----------
In this paper, we propose Minimalist Transfer Learning (MinTL) to simplify the system design process of task-oriented dialogue systems and alleviate the over-dependency on annotated data.	1+2
MinTL is a simple yet effective transfer learning framework, which allows us to plug-and-play pre-trained seq2seq models, and jointly learn dialogue state tracking and dialogue response generation.	1
Unlike previous approaches, which use a copy mechanism to “carryover” the old dialogue states to the new one, we introduce Levenshtein belief spans (Lev), that allows efficient dialogue state tracking with a minimal generation length.	2+3
We instantiate our learning framework with two pre-trained backbones: T5 and BART, and evaluate them on MultiWOZ.	3
Extensive experiments demonstrate that: 1) our systems establish new state-of-the-art results on end-to-end response generation, 2) MinTL-based systems are more robust than baseline methods in the low resource setting, and they achieve competitive results with only 20% training data, and 3) Lev greatly improves the inference efficiency.	5
----------
Recent works have shown that generative data augmentation, where synthetic samples generated from deep generative models complement the training dataset, benefit NLP tasks.	1
In this work, we extend this approach to the task of dialog state tracking for goaloriented dialogs.	1
Due to the inherent hierarchical structure of goal-oriented dialogs over utterances and related annotations, the deep generative model must be capable of capturing the coherence among different hierarchies and types of dialog features.	1
We propose the Variational Hierarchical Dialog Autoencoder (VHDA) for modeling the complete aspects of goal-oriented dialogs, including linguistic features and underlying structured annotations, namely speaker information, dialog acts, and goals.	1
The proposed architecture is designed to model each aspect of goal-oriented dialogs using inter-connected latent variables and learns to generate coherent goal-oriented dialogs from the latent spaces.	2+3
To overcome training issues that arise from training complex variational models, we propose appropriate training strategies.	3
Experiments on various dialog datasets show that our model improves the downstream dialog trackers’ robustness via generative data augmentation.	5
We also discover additional benefits of our unified approach to modeling goal-oriented dialogs – dialog response generation and user simulation, where our model outperforms previous strong baselines.	4+5
----------
Knowledge selection plays an important role in knowledge-grounded dialogue, which is a challenging task to generate more informative responses by leveraging external knowledge.	1
Recently, latent variable models have been proposed to deal with the diversity of knowledge selection by using both prior and posterior distributions over knowledge and achieve promising performance.	1
However, these models suffer from a huge gap between prior and posterior knowledge selection.	1
Firstly, the prior selection module may not learn to select knowledge properly because of lacking the necessary posterior information.	1
Secondly, latent variable models suffer from the exposure bias that dialogue generation is based on the knowledge selected from the posterior distribution at training but from the prior distribution at inference.	1
Here, we deal with these issues on two aspects: 	2
(1) We enhance the prior selection module with the necessary posterior information obtained from the specially designed Posterior Information Prediction Module (PIPM); 	2+3
(2) We propose a Knowledge Distillation Based Training Strategy (KDBTS) to train the decoder with the knowledge selected from the prior distribution, removing the exposure bias of knowledge selection.	2+3
Experimental results on two knowledge-grounded dialogue datasets show that both PIPM and KDBTS achieve performance improvement over the state-of-the-art latent variable model and their combination shows further improvement.	4
----------
Open-domain dialogue generation suffers from the data insufficiency problem due to the vast size of potential responses.	1
In this paper, we propose to explore potential responses by counterfactual reasoning.	2
Given an observed response, the counterfactual reasoning model automatically infers the outcome of an alternative policy that could have been taken.	1
The resulting counterfactual response synthesized in hindsight is of higher quality than the response synthesized from scratch.	4
Training on the counterfactual responses under the adversarial learning framework helps to explore the high-reward area of the potential response space.	5
An empirical study on the DailyDialog dataset shows that our approach significantly outperforms the HRED model as well as the conventional adversarial learning approaches.	4
----------
Recent advances in open-domain dialogue systems rely on the success of neural models that are trained on large-scale data.	1
However, collecting large-scale dialogue data is usually time-consuming and labor-intensive.	1
To address this data dilemma, we propose a novel data augmentation method for training open-domain dialogue models by utilizing unpaired data.	2+3
Specifically, a data-level distillation process is first proposed to construct augmented dialogues where both post and response are retrieved from the unpaired data.	3
A ranking module is employed to filter out low-quality dialogues.	3
Further, a model-level distillation process is employed to distill a teacher model trained on high-quality paired data to augmented dialogue pairs, thereby preventing dialogue models from being affected by the noise in the augmented data.	3
Automatic and manual evaluation indicates that our method can produce high-quality dialogue pairs with diverse contents, and the proposed data-level and model-level dialogue distillation can improve the performance of competitive baselines.	5
----------
We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning.	1+2
Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors.	1
Such idea arises naturally in human behaviors, e.g. predicting others’ responses and then deciding our own actions.	1
In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly.	4
We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.	3
----------
We study multi-turn response generation for open-domain dialogues.	2
The existing state-of-the-art addresses the problem with deep neural architectures.	1
While these models improved response quality, their complexity also hinders the application of the models in real systems.	1
In this work, we pursue a model that has a simple structure yet can effectively leverage conversation contexts for response generation.	3
To this end, we propose four auxiliary tasks including word order recovery, utterance order recovery, masked word recovery, and masked utterance recovery, and optimize the objectives of these tasks together with maximizing the likelihood of generation.	2+3
By this means, the auxiliary tasks that relate to context understanding can guide the learning of the generation model to achieve a better local optimum.	3+4
Empirical studies with three benchmarks indicate that our model can significantly outperform state-of-the-art generation models in terms of response quality on both automatic evaluation and human judgment, and at the same time enjoys a much faster decoding process.	5
----------
Retrieving the proper knowledge relevant to conversational context is an important challenge in dialogue systems, to engage users with more informative response.	1
Several recent works propose to formulate this knowledge selection problem as a path traversal over an external knowledge graph (KG), but show only a limited utilization of KG structure, leaving rooms of improvement in performance.	1
To this effect, we present AttnIO, a new dialog-conditioned path traversal model that makes a full use of rich structural information in KG based on two directions of attention flows.	2
Through the attention flows, AttnIO is not only capable of exploring a broad range of multi-hop knowledge paths, but also learns to flexibly adjust the varying range of plausible nodes and edges to attend depending on the dialog context.	3
Empirical evaluations present a marked performance improvement of AttnIO compared to all baselines in OpenDialKG dataset.	4
Also, we find that our model can be trained to generate an adequate knowledge path even when the paths are not available and only the destination nodes are given as label, making it more applicable to real-world dialogue systems.	5
----------
The challenge of both achieving task completion by querying the knowledge base and generating human-like responses for task-oriented dialogue systems is attracting increasing research attention.	1
In this paper, we propose a “Two-Teacher One-Student” learning framework (TTOS) for task-oriented dialogue, with the goal of retrieving accurate KB entities and generating human-like responses simultaneously.	2
TTOS amalgamates knowledge from two teacher networks that together provide comprehensive guidance to build a high-quality task-oriented dialogue system (student network).	3
Each teacher network is trained via reinforcement learning with a goal-specific reward, which can be viewed as an expert towards the goal and transfers the professional characteristic to the student network.	3
Instead of adopting the classic student-teacher learning of forcing the output of a student network to exactly mimic the soft targets produced by the teacher networks, we introduce two discriminators as in generative adversarial network (GAN) to transfer knowledge from two teachers to the student.	3
The usage of discriminators relaxes the rigid coupling between the student and teachers.	4
Extensive experiments on two benchmark datasets (i.e., CamRest and In-Car Assistant) demonstrate that TTOS significantly outperforms baseline methods.	4
----------
Meta-embedding learning, which combines complementary information in different word embeddings, have shown superior performances across different Natural Language Processing tasks.	1
However, domain-specific knowledge is still ignored by existing meta-embedding methods, which results in unstable performances across specific domains.	1
Moreover, the importance of general and domain word embeddings is related to downstream tasks, how to regularize meta-embedding to adapt downstream tasks is an unsolved problem.	1
In this paper, we propose a method to incorporate both domain-specific and task-oriented information into meta-embeddings.	2
We conducted extensive experiments on four text classification datasets and the results show the effectiveness of our proposed method.	4
----------
State-of-the-art methods for Word Sense Disambiguation (WSD) combine two different features: the power of pre-trained language models and a propagation method to extend the coverage of such models.	1
This propagation is needed as current sense-annotated corpora lack coverage of many instances in the underlying sense inventory (usually WordNet).	1
At the same time, unambiguous words make for a large portion of all words in WordNet, while being poorly covered in existing sense-annotated corpora.	1
In this paper, we propose a simple method to provide annotations for most unambiguous words in a large corpus.	2
We introduce the UWA (Unambiguous Word Annotations) dataset and show how a state-of-the-art propagation-based model can use it to extend the coverage and quality of its word sense embeddings by a significant margin, improving on its original results on WSD.	3
----------
We propose the novel Within-Between Relation model for recognizing lexical-semantic relations between words.	2
Our model integrates relational and distributional signals, forming an effective sub-space representation for each relation.	3
We show that the proposed model is competitive and outperforms other baselines, across various benchmarks.	4
----------
Contextualized word embeddings have been employed effectively across several tasks in Natural Language Processing, as they have proved to carry useful semantic information.	1
However, it is still hard to link them to structured sources of knowledge.	1
In this paper we present ARES (context-AwaRe Embeddings of Senses), a semi-supervised approach to producing sense embeddings for the lexical meanings within a lexical knowledge base that lie in a space that is comparable to that of contextualized word vectors.	2+3
ARES representations enable a simple 1 Nearest-Neighbour algorithm to outperform state-of-the-art models, not only in the English Word Sense Disambiguation task, but also in the multilingual one, whilst training on sense-annotated data in English only.	3
We further assess the quality of our embeddings in the Word-in-Context task, where, when used as an external source of knowledge, they consistently improve the performance of a neural model, leading it to compete with other more complex architectures.	3
ARES embeddings for all WordNet concepts and the automatically-extracted contexts used for creating the sense representations are freely available at http://sensembert.org/ares.	6
----------
The state-of-the-art methods in aspect-level sentiment classification have leveraged the graph based models to incorporate the syntactic structure of a sentence.	1
While being effective, these methods ignore the corpus level word co-occurrence information, which reflect the collocations in linguistics like “nothing special”.	1
Moreover, they do not distinguish the different types of syntactic dependency, e.g., a nominal subject relation “food-was” is treated equally as an adjectival complement relation “was-okay” in “food was okay”.	1
To tackle the above two limitations, we propose a novel architecture which convolutes over hierarchical syntactic and lexical graphs.	2
Specifically, we employ a global lexical graph to encode the corpus level word co-occurrence information.	3
Moreover, we build a concept hierarchy on both the syntactic and lexical graphs for differentiating various types of dependency relations or lexical word pairs.	3
Finally, we design a bi-level interactive graph convolution network to fully exploit these two graphs.	3
Extensive experiments on five bench- mark datasets show that our method outperforms the state-of-the-art baselines.	4
----------
Aspect-category sentiment analysis (ACSA) aims to predict sentiment polarities of sentences with respect to given aspect categories.	2
To detect the sentiment toward a particular aspect category in a sentence, most previous methods first generate an aspect category-specific sentence representation for the aspect category, then predict the sentiment polarity based on the representation.	3
These methods ignore the fact that the sentiment of an aspect category mentioned in a sentence is an aggregation of the sentiments of the words indicating the aspect category in the sentence, which leads to suboptimal performance.	3
In this paper, we propose a Multi-Instance Multi-Label Learning Network for Aspect-Category sentiment analysis (AC-MIMLLN), which treats sentences as bags, words as instances, and the words indicating an aspect category as the key instances of the aspect category.	2+3
Given a sentence and the aspect categories mentioned in the sentence, AC-MIMLLN first predicts the sentiments of the instances, then finds the key instances for the aspect categories, finally obtains the sentiments of the sentence toward the aspect categories by aggregating the key instance sentiments.	3
Experimental results on three public datasets demonstrate the effectiveness of AC-MIMLLN.	4
----------
Aspect based sentiment analysis, predicting sentiment polarity of given aspects, has drawn extensive attention.	1
Previous attention-based models emphasize using aspect semantics to help extract opinion features for classification.	1
However, these works are either not able to capture opinion spans as a whole, or not able to capture variable-length opinion spans.	1
In this paper, we present a neat and effective structured attention model by aggregating multiple linear-chain CRFs.	2
Such a design allows the model to extract aspect-specific opinion spans and then evaluate sentiment polarity by exploiting the extracted opinion features.	5
The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our analysis demonstrates that our model can capture aspect-specific opinion spans.	4+5
----------
Emotion-cause pair extraction (ECPE) is a new task that aims to extract the potential pairs of emotions and their corresponding causes in a document.	1
The existing methods first perform emotion extraction and cause extraction independently, and then perform emotion-cause pairing and filtering.	1
However, the above methods ignore the fact that the cause and the emotion it triggers are inseparable, and the extraction of the cause without specifying the emotion is pathological, which greatly limits the performance of the above methods in the first step.	1
To tackle these shortcomings, we propose two joint frameworks for ECPE: 1) multi-label learning for the extraction of the cause clauses corresponding to the specified emotion clause (CMLL) and 2) multi-label learning for the extraction of the emotion clauses corresponding to the specified cause clause (EMLL).	2+3
The window of multi-label learning is centered on the specified emotion clause or cause clause and slides as their positions move.	3
Finally, CMLL and EMLL are integrated to obtain the final result.	3
We evaluate our model on a benchmark emotion cause corpus, the results show that our approach achieves the best performance among all compared systems on the ECPE task.	4
----------
As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years.	1
However, almost all existing studies focus on one modality (e.g., textual modality).	1
In this paper, we focus on multi-label emotion detection in a multi-modal scenario.	2
In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence).	3
Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection.	3
The detailed evaluation demonstrates the effectiveness of our approach.	4
----------
Modeling content importance is an essential yet challenging task for summarization.	1
Previous work is mostly based on statistical methods that estimate word-level salience, which does not consider semantics and larger context when quantifying importance.	1
It is thus hard for these methods to generalize to semantic units of longer text spans.	1
In this work, we apply information theory on top of pre-trained language models and define the concept of importance from the perspective of information amount.	2+3
It considers both the semantics and context when evaluating the importance of each semantic unit.	3
With the help of pre-trained language models, it can easily generalize to different kinds of semantic units n-grams or sentences.	3
Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores.	4
----------
Evaluation of a document summarization system has been a critical factor to impact the success of the summarization task.	1
Previous approaches, such as ROUGE, mainly consider the informativeness of the assessed summary and require human-generated references for each test summary.	1
In this work, we propose to evaluate the summary qualities without reference summaries by unsupervised contrastive learning.	2+3
Specifically, we design a new metric which covers both linguistic qualities and semantic informativeness based on BERT.	3
To learn the metric, for each summary, we construct different types of negative samples with respect to different aspects of the summary qualities, and train our model with a ranking loss.	3
Experiments on Newsroom and CNN/Daily Mail demonstrate that our new evaluation method outperforms other metrics even without reference summaries.	4
Furthermore, we show that our method is general and transferable across datasets.	4
----------
Sentence-level extractive text summarization is substantially a node classification task of network mining, adhering to the informative components and concise representations.	1
There are lots of redundant phrases between extracted sentences, but it is difficult to model them exactly by the general supervised methods.	1
Previous sentence encoders, especially BERT, specialize in modeling the relationship between source sentences.	1
While, they have no ability to consider the overlaps of the target selected summary, and there are inherent dependencies among target labels of sentences.	1
In this paper, we propose HAHSum (as shorthand for Hierarchical Attentive Heterogeneous Graph for Text Summarization), which well models different levels of information, including words and sentences, and spotlights redundancy dependencies between sentences.	2+3
Our approach iteratively refines the sentence representations with redundancy-aware graph and delivers the label dependencies by message passing.	3
Experiments on large scale benchmark corpus (CNN/DM, NYT, and NEWSROOM) demonstrate that HAHSum yields ground-breaking performance and outperforms previous extractive summarizers.	4
----------
We consider the problem of better modeling query-cluster interactions to facilitate query focused multi-document summarization.	1
Due to the lack of training data, existing work relies heavily on retrieval-style methods for assembling query relevant summaries.	1
We propose a coarse-to-fine modeling framework which employs progressively more accurate modules for estimating whether text segments are relevant, likely to contain an answer, and central.	2+3
The modules can be independently developed and leverage training data if available.	3
We present an instantiation of this framework with a trained evidence estimator which relies on distant supervision from question answering (where various resources exist) to identify segments which are likely to answer the query and should be included in the summary.	3
Our framework is robust across domains and query types (i.e., long vs short) and outperforms strong comparison systems on benchmark datasets.	5
----------
Abstractive document summarization is usually modeled as a sequence-to-sequence (SEQ2SEQ) learning problem.	1
Unfortunately, training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging.	1
This paper presents three sequence-to-sequence pre-training (in shorthand, STEP) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text.	2
The main idea is that, given an input text artificially constructed from a document, a model is pre-trained to reinstate the original document.	3
These objectives include sentence reordering, next sentence generation and masked document generation, which have close relations with the abstractive document summarization task.	3
Experiments on two benchmark summarization datasets (i.e., CNN/DailyMail and New York Times) show that all three objectives can improve performance upon baselines.	4
Compared to models pre-trained on large-scale data (larger than 160GB), our method, with only 19GB text for pre-training, achieves comparable results, which demonstrates its effectiveness.	4
----------
Neural models have achieved remarkable success on relation extraction (RE) benchmarks.	1
However, there is no clear understanding what information in text affects existing RE models to make decisions and how to further improve the performance of these models.	1
To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names).	1
We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks.	1
Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions.	2
We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios.	4
All the code and datasets are released at https://github.com/thunlp/RE-Context-or-Names.	6
----------
Open relation extraction is the task of extracting open-domain relation facts from natural language sentences.	1
Existing works either utilize heuristics or distant-supervised annotations to train a supervised classifier over pre-defined relations, or adopt unsupervised methods with additional assumptions that have less discriminative power.	1
In this work, we propose a self-supervised framework named SelfORE, which exploits weak, self-supervised signals by leveraging large pretrained language model for adaptive clustering on contextualized relational features, and bootstraps the self-supervised signals by improving contextualized features in relation classification.	2+3
Experimental results on three datasets show the effectiveness and robustness of SelfORE on open-domain Relation Extraction when comparing with competitive baselines.	4+5
----------
Distant supervision (DS) has been widely adopted to generate auto-labeled data for sentence-level relation extraction (RE) and achieved great results.	1
However, the existing success of DS cannot be directly transferred to more challenging document-level relation extraction (DocRE), as the inevitable noise caused by DS may be even multiplied in documents and significantly harm the performance of RE.	1
To alleviate this issue, we propose a novel pre-trained model for DocRE, which de-emphasize noisy DS data via multiple pre-training tasks.	2
The experimental results on the large-scale DocRE benchmark show that our model can capture useful information from noisy data and achieve promising results.	4
----------
Despite efforts to distinguish three different evaluation setups (Bekoulis et al., 2018), numerous end-to-end Relation Extraction (RE) articles present unreliable performance comparison to previous work.	1
In this paper, we first identify several patterns of invalid comparisons in published papers and describe them to avoid their propagation.	2
We then propose a small empirical study to quantify the most common mistake’s impact and evaluate it leads to overestimating the final RE performance by around 5% on ACE05.	2+3
We also seize this opportunity to study the unexplored ablations of two recent developments: the use of language model pretraining (specifically BERT) and span-level NER.	3
This meta-analysis emphasizes the need for rigor in the report of both the evaluation setting and the dataset statistics.	3
We finally call for unifying the evaluation setting in end-to-end RE.	3
----------
The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior.	1
We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process.	2
We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior.	3
Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data.	4
Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance.	4
By adding some of the challenge data as training examples, the performance of the model improves.	4
Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.	5
----------
Relation extraction (RE) aims to identify the semantic relations between named entities in text.	2
Recent years have witnessed it raised to the document level, which requires complex reasoning with entities and mentions throughout an entire document.	2
In this paper, we propose a novel model to document-level RE, by encoding the document information in terms of entity global and local representations as well as context relation representations.	2
Entity global representations model the semantic information of all entities in the document, entity local representations aggregate the contextual information of multiple mentions of specific entities, and context relation representations encode the topic information of other relations.	3
Experimental results demonstrate that our model achieves superior performance on two public datasets for document-level RE.	5
It is particularly effective in extracting relations between entities of long distance and having multiple mentions.	5
----------
The idea of using multi-task learning approaches to address the joint extraction of entity and relation is motivated by the relatedness between the entity recognition task and the relation classification task.	1
Existing methods using multi-task learning techniques to address the problem learn interactions among the two tasks through a shared network, where the shared information is passed into the task-specific networks for prediction.	1
However, such an approach hinders the model from learning explicit interactions between the two tasks to improve the performance on the individual tasks.	1
As a solution, we design a multi-task learning model which we refer to as recurrent interaction network which allows the learning of interactions dynamically, to effectively model task-specific features for classification.	2+3
Empirical studies on two real-world datasets confirm the superiority of the proposed model.	4
----------
Research on temporal knowledge bases, which associate a relational fact (s,r,o) with a validity time period (or time instant), is in its early days.	1
Our work considers predicting missing entities (link prediction) and missing time intervals (time prediction) as joint Temporal Knowledge Base Completion (TKBC) tasks, and presents TIMEPLEX, a novel TKBC method, in which entities, relations and, time are all embedded in a uniform, compatible space.	2
TIMEPLEX exploits the recurrent nature of some facts/events and temporal interactions between pairs of relations, yielding state-of-the-art results on both prediction tasks.	3+4
We also find that existing TKBC models heavily overestimate link prediction performance due to imperfect evaluation mechanisms.	4
In response, we propose improved TKBC evaluation protocols for both link and time prediction tasks, dealing with subtle issues that arise from the partial overlap of time intervals in gold instances and system predictions.	2+3
----------
A recent state-of-the-art neural open information extraction (OpenIE) system generates extractions iteratively, requiring repeated encoding of partial outputs.	1
This comes at a significant computational cost.	1
On the other hand,sequence labeling approaches for OpenIE are much faster, but worse in extraction quality.	1
In this paper, we bridge this trade-off by presenting an iterative labeling-based system that establishes a new state of the art for OpenIE, while extracting 10x faster.	2+3
This is achieved through a novel Iterative Grid Labeling (IGL) architecture, which treats OpenIE as a 2-D grid labeling task.	3
We improve its performance further by applying coverage (soft) constraints on the grid at training time.	3
Moreover, on observing that the best OpenIE systems falter at handling coordination structures, our OpenIE system also incorporates a new coordination analyzer built with the same IGL architecture.	5
This IGL based coordination analyzer helps our OpenIE system handle complicated coordination structures, while also establishing a new state of the art on the task of coordination analysis, with a 12.3 pts improvement in F1 over previous analyzers.	3
Our OpenIE system - OpenIE6 - beats the previous systems by as much as 4 pts in F1, while being much faster.	5
----------
Detecting public sentiment drift is a challenging task due to sentiment change over time.	1
Existing methods first build a classification model using historical data and subsequently detect drift if the model performs much worse on new data.	1
In this paper, we focus on distribution learning by proposing a novel Hierarchical Variational Auto-Encoder (HVAE) model to learn better distribution representation, and design a new drift measure to directly evaluate distribution changes between historical data and new data.Our experimental results demonstrate that our proposed model achieves better results than three existing state-of-the-art methods.	2+3
----------
Solving algebraic word problems has recently emerged as an important natural language processing task.	1
To solve algebraic word problems, recent studies suggested neural models that generate solution equations by using ‘Op (operator/operand)’ tokens as a unit of input/output.	1
However, such a neural model suffered two issues: expression fragmentation and operand-context separation.	1
To address each of these two issues, we propose a pure neural model, Expression-Pointer Transformer (EPT), which uses (1) ‘Expression’ token and (2) operand-context pointers when generating solution equations.	2+3
The performance of the EPT model is tested on three datasets: ALG514, DRAW-1K, and MAWPS.	3
Compared to the state-of-the-art (SoTA) models, the EPT model achieved a comparable performance accuracy in each of the three datasets; 81.3% on ALG514, 59.5% on DRAW-1K, and 84.5% on MAWPS.	4
The contribution of this paper is two-fold; (1) We propose a pure neural model, EPT, which can address the expression fragmentation and the operand-context separation.	4
(2) The fully automatic EPT model, which does not use hand-crafted features, yields comparable performance to existing models using hand-crafted features, and achieves better performance than existing pure neural models by at most 40%.	4
----------
A practical automatic textual math word problems (MWPs) solver should be able to solve various textual MWPs while most existing works only focused on one-unknown linear MWPs.	1
Herein, we propose a simple but efficient method called Universal Expression Tree (UET) to make the first attempt to represent the equations of various MWPs uniformly.	1+2
Then a semantically-aligned universal tree-structured solver (SAU-Solver) based on an encoder-decoder framework is proposed to resolve multiple types of MWPs in a unified model, benefiting from our UET representation.	2+3
Our SAU-Solver generates a universal expression tree explicitly by deciding which symbol to generate according to the generated symbols’ semantic meanings like human solving MWPs.	3
Besides, our SAU-Solver also includes a novel subtree-level semanticallyaligned regularization to further enforce the semantic constraints and rationality of the generated expression tree by aligning with the contextual information.	3
Finally, to validate the universality of our solver and extend the research boundary of MWPs, we introduce a new challenging Hybrid Math Word Problems dataset (HMWP), consisting of three types of MWPs.	4
Experimental results on several MWPs datasets show that our model can solve universal types of MWPs and outperforms several state-of-the-art models.	4
----------
Graph Neural Networks (GNNs) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community.	1
In this paper, we propose Graph Topic Model (GTM), a GNN based neural topic model that represents a corpus as a document relationship graph.	2
Documents and words in the corpus become nodes in the graph and are connected based on document-word co-occurrences.	3
By introducing the graph structure, the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution.	3
Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach.	4
----------
One of the most challenging part of recipe generation is to deal with the complex restrictions among the input ingredients.	1
Previous researches simplify the problem by treating the inputs independently and generating recipes containing as much information as possible.	1
In this work, we propose a routing method to dive into the content selection under the internal restrictions.	2
The routing enforced generative model (RGM) can generate appropriate recipes according to the given ingredients and user preferences.	3
Our model yields new state-of-the-art results on the recipe generation task with significant improvements on BLEU, F1 and human evaluation.	4
----------
Many English-as-a-second language learners have trouble using near-synonym words (e.g., small vs.little; briefly vs.shortly) correctly, and often look for example sentences to learn how two nearly synonymous terms differ.	1
Prior work uses hand-crafted scores to recommend sentences but has difficulty in adopting such scores to all the near-synonyms as near-synonyms differ in various ways.	1
We notice that the helpfulness of the learning material would reflect on the learners’ performance.	1
Thus, we propose the inference-based learner-like agent to mimic learner behavior and identify good learning materials by examining the agent’s performance.	2
To enable the agent to behave like a learner, we leverage entailment modeling’s capability of inferring answers from the provided materials.	3
Experimental results show that the proposed agent is equipped with good learner-like behavior to achieve the best performance in both fill-in-the-blank (FITB) and good example sentence selection tasks.	4
We further conduct a classroom user study with college ESL learners.	4
The results of the user study show that the proposed agent can find out example sentences that help students learn more easily and efficiently.	5
Compared to other models, the proposed agent improves the score of more than 17% of students after learning.	5
----------
As the E-commerce thrives, high-quality online advertising copywriting has attracted more and more attention.	1
Different from the advertising copywriting for a single product, an advertisement (AD) post includes an attractive topic that meets the customer needs and description copywriting about several products under its topic.	1
A good AD post can highlight the characteristics of each product, thus helps customers make a good choice among candidate products.	1
Hence, multi-product AD post generation is meaningful and important.	1
We propose a novel end-to-end model named S-MG Net to generate the AD post.	2
Targeted at such a challenging real-world problem, we split the AD post generation task into two subprocesses: (1) select a set of products via the SelectNet (Selection Network).	3
(2) generate a post including selected products via the MGenNet (Multi-Generator Network).	3
Concretely, SelectNet first captures the post topic and the relationship among the products to output the representative products.	3
Then, MGenNet generates the description copywriting of each product.	3
Experiments conducted on a large-scale real-world AD post dataset demonstrate that our proposed model achieves impressive performance in terms of both automatic metrics as well as human evaluations.	4
----------
Document structure extraction has been a widely researched area for decades with recent works performing it as a semantic segmentation task over document images using fully-convolution networks.	1
Such methods are limited by image resolution due to which they fail to disambiguate structures in dense regions which appear commonly in forms.	1
To mitigate this, we propose Form2Seq, a novel sequence-to-sequence (Seq2Seq) inspired framework for structure extraction using text, with a specific focus on forms, which leverages relative spatial arrangement of structures.	2+3
We discuss two tasks; 1) Classification of low-level constituent elements (TextBlock and empty fillable Widget) into ten types such as field captions, list items, and others; 2) Grouping lower-level elements into higher-order constructs, such as Text Fields, ChoiceFields and ChoiceGroups, used as information collection mechanism in forms.	3
To achieve this, we arrange the constituent elements linearly in natural reading order, feed their spatial and textual representations to Seq2Seq framework, which sequentially outputs prediction of each element depending on the final task.	3
We modify Seq2Seq for grouping task and discuss improvements obtained through cascaded end-to-end training of two tasks versus training in isolation.	3
Experimental results show the effectiveness of our text-based approach achieving an accuracy of 90% on classification task and an F1 of 75.82, 86.01, 61.63 on groups discussed above respectively, outperforming segmentation baselines.	4
Further we show our framework achieves state of the results for table structure recognition on ICDAR 2013 dataset.	4
----------
Like many Natural Language Processing tasks, Thai word segmentation is domain-dependent.	1
Researchers have been relying on transfer learning to adapt an existing model to a new domain.	1
However, this approach is inapplicable to cases where we can interact with only input and output layers of the models, also known as “black boxes”.	1
We propose a filter-and-refine solution based on the stacked-ensemble learning paradigm to address this black-box limitation.	1+2
We conducted extensive experimental studies comparing our method against state-of-the-art models and transfer learning.	3
Experimental results show that our proposed solution is an effective domain adaptation method and has a similar performance as the transfer learning method.	4
----------
Can pretrained language models (PLMs) generate derivationally complex words?	1
We present the first study investigating this question, taking BERT as the example PLM.	1+2
We examine BERT’s derivational capabilities in different settings, ranging from using the unmodified pretrained model to full finetuning.	3
Our best model, DagoBERT (Derivationally and generatively optimized BERT), clearly outperforms the previous state of the art in derivation generation (DG).	4
Furthermore, our experiments show that the input segmentation crucially impacts BERT’s derivational knowledge, suggesting that the performance of PLMs could be further improved if a morphologically informed vocabulary of units were used.	4+5
----------
Taking greedy decoding algorithm as it should be, this work focuses on further strengthening the model itself for Chinese word segmentation (CWS), which results in an even more fast and more accurate CWS model.	2+3
Our model consists of an attention only stacked encoder and a light enough decoder for the greedy segmentation plus two highway connections for smoother training, in which the encoder is composed of a newly proposed Transformer variant, Gaussian-masked Directional (GD) Transformer, and a biaffine attention scorer.	3
With the effective encoder design, our model only needs to take unigram features for scoring.	3
Our model is evaluated on SIGHAN Bakeoff benchmark datasets.	4
The experimental results show that with the highest segmentation speed, the proposed model achieves new state-of-the-art or comparable performance against strong baselines in terms of strict closed test setting.	4
----------
Word-level information is important in natural language processing (NLP), especially for the Chinese language due to its high linguistic complexity.	1
Chinese word segmentation (CWS) is an essential task for Chinese downstream NLP tasks.	1
Existing methods have already achieved a competitive performance for CWS on large-scale annotated corpora.	1
However, the accuracy of the method will drop dramatically when it handles an unsegmented text with lots of out-of-vocabulary (OOV) words.	1
In addition, there are many different segmentation criteria for addressing different requirements of downstream NLP tasks.	1
Excessive amounts of models with saving different criteria will generate the explosive growth of the total parameters.	1
To this end, we propose a joint multiple criteria model that shares all parameters to integrate different segmentation criteria into one model.	2+3
Besides, we utilize a transfer learning method to improve the performance of OOV words.	3
Our proposed method is evaluated by designing comprehensive experiments on multiple benchmark datasets (e.g., Bakeoff 2005, Bakeoff 2008 and SIGHAN 2010).	3
Our method achieves the state-of-the-art performances on all datasets.	4
Importantly, our method also shows a competitive practicability and generalization ability for the CWS task.	5
----------
Cross-lingual semantic role labeling (SRL) aims at leveraging resources in a source language to minimize the effort required to construct annotations or models for a new target language.	1
Recent approaches rely on word alignments, machine translation engines, or preprocessing tools such as parsers or taggers.	1
We propose a cross-lingual SRL model which only requires annotations in a source language and access to raw text in the form of a parallel corpus.	2
The backbone of our model is an LSTM-based semantic role labeler jointly trained with a semantic role compressor and multilingual word embeddings.	3
The compressor collects useful information from the output of the semantic role labeler, filtering noisy and conflicting evidence.	3
It lives in a multilingual embedding space and provides direct supervision for predicting semantic roles in the target language.	3
Results on the Universal Proposition Bank and manually annotated datasets show that our method is highly effective, even against systems utilizing supervised features.	4
----------
We study the detection of propagandistic text fragments in news articles.	2
Instead of merely learning from input-output datapoints in training data, we introduce an approach to inject declarative knowledge of fine-grained propaganda techniques.	3
Specifically, we leverage the declarative knowledge expressed in both first-order logic and natural language.	3
The former refers to the logical consistency between coarse- and fine-grained predictions, which is used to regularize the training process with propositional Boolean expressions.	3
The latter refers to the literal definition of each propaganda technique, which is utilized to get class representations for regularizing the model parameters.	3
We conduct experiments on Propaganda Techniques Corpus, a large manually annotated dataset for fine-grained propaganda detection.	3
Experiments show that our method achieves superior performance, demonstrating that leveraging declarative knowledge can help the model to make more accurate predictions.	5
----------
Even though SRL is researched for many languages, major improvements have mostly been obtained for English, for which more resources are available.	1
In fact, existing multilingual SRL datasets contain disparate annotation styles or come from different domains, hampering generalization in multilingual learning.	1
In this work we propose a method to automatically construct an SRL corpus that is parallel in four languages: English, French, German, Spanish, with unified predicate and role annotations that are fully comparable across languages.	2
We apply high-quality machine translation to the English CoNLL-09 dataset and use multilingual BERT to project its high-quality annotations to the target languages.	3
We include human-validated test sets that we use to measure the projection quality, and show that projection is denser and more precise than a strong baseline.	3+4
Finally, we train different SOTA models on our novel corpus for mono- and multilingual SRL, showing that the multilingual annotations improve performance especially for the weaker languages.	4
----------
Semantic role labeling (SRL) is the task of identifying predicates and labeling argument spans with semantic roles.	1
Even though most semantic-role formalisms are built upon constituent syntax, and only syntactic constituents can be labeled as arguments (e.g., FrameNet and PropBank), all the recent work on syntax-aware SRL relies on dependency representations of syntax.	1
In contrast, we show how graph convolutional networks (GCNs) can be used to encode constituent structures and inform an SRL system.	2+3
Nodes in our SpanGCN correspond to constituents.	3
The computation is done in 3 stages.	3
First, initial node representations are produced by ‘composing’ word representations of the first and last words in the constituent.	3
Second, graph convolutions relying on the constituent tree are performed, yielding syntactically-informed constituent representations.	3
Finally, the constituent representations are ‘decomposed’ back into word representations, which are used as input to the SRL classifier.	3
We evaluate SpanGCN against alternatives, including a model using GCNs over dependency trees, and show its effectiveness on standard English SRL benchmarks CoNLL-2005, CoNLL-2012, and FrameNet.	4
----------
AM dependency parsing is a linguistically principled method for neural semantic parsing with high accuracy across multiple graphbanks.	1
It relies on a type system that models semantic valency but makes existing parsers slow.	1
We describe an A* parser and a transition-based parser for AM dependency parsing which guarantee well-typedness and improve parsing speed by up to 3 orders of magnitude, while maintaining or improving accuracy.	1+2
----------
This paper explores how intent classification can be improved by representing the class labels not as a discrete set of symbols but as a space where the word graphs associated to each class are mapped using typical graph embedding techniques.	2
The approach, inspired by a previous algorithm used for an inverse dictionary task, allows the classification algorithm to take in account inter-class similarities provided by the repeated occurrence of some words in the training examples of the different classes.	3
The classification is carried out by mapping text embeddings to the word graph embeddings of the classes.	3
Focusing solely on improving the representation of the class label set, we show in experiments conducted in both private and public intent classification datasets, that better detection of out-of-scope examples (OOS) is achieved and, as a consequence, that the overall accuracy of intent classification is also improved.	4
In particular, using the recently-released Larson dataset, an error of about 9.9% has been achieved for OOS detection, beating the previous state-of-the-art result by more than 31 percentage points.	4
----------
Language drift has been one of the major obstacles to train language models through interaction.	1
When word-based conversational agents are trained towards completing a task, they tend to invent their language rather than leveraging natural language.	1
In recent literature, two general methods partially counter this phenomenon: Supervised Selfplay (S2P) and Seeded Iterated Learning (SIL).	1
While S2P jointly trains interactive and supervised losses to counter the drift, SIL changes the training dynamics to prevent language drift from occurring.	1
In this paper, we first highlight their respective weaknesses, i.e., late-stage training collapses and higher negative likelihood when evaluated on human corpus.	2
Given these observations, we introduce Supervised Seeded Iterated Learning (SSIL) to combine both methods to minimize their respective weaknesses.	3
We then show the effectiveness of in the language-drift translation game.	5
----------
The lack of time efficient and reliable evalu-ation methods is hampering the development of conversational dialogue systems (chat bots).	2
Evaluations that require humans to converse with chat bots are time and cost intensive, put high cognitive demands on the human judges, and tend to yield low quality results.	2
In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots.	2
Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations).	2+3
These annotations then allow us to rank chat bots regarding their ability to mimic conversational behaviour of humans.	3
Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chat bot is able to uphold human-like be-havior the longest, i.e.Survival Analysis.	3
This metric has the ability to correlate a bot’s performance to certain of its characteristics (e.g.fluency or sensibleness), yielding interpretable results.	4
The comparably low cost of our frame-work allows for frequent evaluations of chatbots during their evaluation cycle.	4
We empirically validate our claims by applying Spot The Bot to three domains, evaluating several state-of-the-art chat bots, and drawing comparisonsto related work.	4
The framework is released asa ready-to-use tool.	4
----------
How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors?	1
We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using offline reinforcement learning (RL).	2+3
We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions.	3
A well-known challenge is that learning an RL policy in an offline setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward.	3+4
These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions.	3+4
We solve the challenge by developing a novel class of offline RL algorithms.	3
These algorithms use KL-control to penalize divergence from a pre-trained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty.	4
We test the resulting dialog model with ratings from 80 users in an open-domain setting and find it achieves significant improvements over existing deep offline RL approaches.	4
The novel offline RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.	5
----------
Across languages, multiple consecutive adjectives modifying a noun (e.g. “the big red dog”) follow certain unmarked ordering rules.	1
While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data.	1
We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different.	2+3
We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.	3
----------
Why do bilinguals switch languages within a sentence?	1
The present observational study asks whether word surprisal and word entropy predict code-switching in bilingual written conversation.	1
We describe and model a new dataset of Chinese-English text with 1476 clean code-switched sentences, translated back into Chinese.	2+3
The model includes known control variables together with word surprisal and word entropy.	4
We found that word surprisal, but not entropy, is a significant predictor that explains code-switching above and beyond other well-known predictors.	4
We also found sentence length to be a significant predictor, which has been related to sentence complexity.	5
We propose high cognitive effort as a reason for code-switching, as it leaves fewer resources for inhibition of the alternative language.	5
We also corroborate previous findings, but this time using a computational model of surprisal, a new language pair, and doing so for written language.	5
----------
Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages.	1
In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy.	2
Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models.	3
Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks.	4
Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data.	4
The results present a paradox: there should be less variation in grammatical performance than is actually observed.	4
----------
It has been conjectured that multilingual information can help monolingual word sense disambiguation (WSD).	1
However, existing WSD systems rarely consider multilingual information, and no effective method has been proposed for improving WSD by generating translations.	1
In this paper, we present a novel approach that improves the performance of a base WSD system using machine translation.	2
Since our approach is language independent, we perform WSD experiments on several languages.	3
The results demonstrate that our methods can consistently improve the performance of WSD systems, and obtain state-ofthe-art results in both English and multilingual WSD.	4
To facilitate the use of lexical translation information, we also propose BABALIGN, an precise bitext alignment algorithm which is guided by multilingual lexical correspondences from BabelNet.	3
----------
One of the most powerful features of contextualized models is their dynamic embeddings for words in context, leading to state-of-the-art representations for context-aware lexical semantics.	1
In this paper, we present a post-processing technique that enhances these representations by learning a transformation through static anchors.	2
Our method requires only another pre-trained model and no labeled data is needed.	3
We show consistent improvement in a range of benchmark tasks that test contextual variations of meaning both across different usages of a word and across different words as they are used in context.	4
We demonstrate that while the original contextual representations can be improved by another embedding space from both contextualized and static models, the static embeddings, which have lower computational requirements, provide the most gains.	4
----------
Word embeddings are usually derived from corpora containing text from many individuals, thus leading to general purpose representations rather than individually personalized representations.	1
While personalized embeddings can be useful to improve language model performance and other language processing tasks, they can only be computed for people with a large amount of longitudinal data, which is not the case for new users.	1
We propose a new form of personalized word embeddings that use demographic-specific word representations derived compositionally from full or partial demographic information for a user (i.e., gender, age, location, religion).	2+3
We show that the resulting demographic-aware word representations outperform generic word representations on two tasks for English: language modeling and word associations.	4
We further explore the trade-off between the number of available attributes and their relative effectiveness and discuss the ethical implications of using them.	5
----------
In politics, neologisms are frequently invented for partisan objectives.	1
For example, “undocumented workers” and “illegal aliens” refer to the same group of people (i.e., they have the same denotation), but they carry clearly different connotations.	1
Examples like these have traditionally posed a challenge to reference-based semantic theories and led to increasing acceptance of alternative theories (e.g., Two-Factor Semantics) among philosophers and cognitive scientists.	1
In NLP, however, popular pretrained models encode both denotation and connotation as one entangled representation.	1
In this study, we propose an adversarial nerual netowrk that decomposes a pretrained representation as independent denotation and connotation representations.	2
For intrinsic interpretability, we show that words with the same denotation but different connotations (e.g., “immigrants” vs. “aliens”, “estate tax” vs. “death tax”) move closer to each other in denotation space while moving further apart in connotation space.	3
For extrinsic application, we train an information retrieval system with our disentangled representations and show that the denotation vectors improve the viewpoint diversity of document rankings.	4
----------
Text summarization is one of the most challenging and interesting problems in NLP.	1
Although much attention has been paid to summarizing structured text like news reports or encyclopedia articles, summarizing conversations—an essential part of human-human/machine interaction where most important pieces of information are scattered across various utterances of different speakers—remains relatively under-investigated.	1
This work proposes a multi-view sequence-to-sequence model by first extracting conversational structures of unstructured daily chats from different views to represent conversations and then utilizing a multi-view decoder to incorporate different views to generate dialogue summaries.	2+3
Experiments on a large-scale dialogue summarization corpus demonstrated that our methods significantly outperformed previous state-of-the-art models via both automatic evaluations and human judgment.	4
We also discussed specific challenges that current approaches faced with this task.	4
We have publicly released our code at https://github.com/GT-SALT/Multi-View-Seq2Seq.	6
----------
Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product.	1
The task is practically important and has attracted a lot of attention.	1
However, due to the high cost of summary production, datasets large enough for training supervised models are lacking.	1
Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way.	1
Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion.	1
However, these models, not being exposed to actual summaries, fail to capture their essential properties.	1
In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation.	1+2
We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product.	3
The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort.	3
In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries.	3
This lets us switch the generator to the summarization mode.	3
We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.	4
----------
The ability to fuse sentences is highly attractive for summarization systems because it is an essential step to produce succinct abstracts.	1
However, to date, summarizers can fail on fusing sentences.	1
They tend to produce few summary sentences by fusion or generate incorrect fusions that lead the summary to fail to retain the original meaning.	1
In this paper, we explore the ability of Transformers to fuse sentences and propose novel algorithms to enhance their ability to perform sentence fusion by leveraging the knowledge of points of correspondence between sentences.	2
Through extensive experiments, we investigate the effects of different design choices on Transformer’s performance.	4
Our findings highlight the importance of modeling points of correspondence between sentences for effective sentence fusion.	4
----------
We propose encoder-centric stepwise models for extractive summarization using structured transformers – HiBERT and Extended Transformers.	2+3
We enable stepwise summarization by injecting the previously generated summary into the structured transformer as an auxiliary sub-structure.	3
Our models are not only efficient in modeling the structure of long inputs, but they also do not rely on task-specific redundancy-aware modeling, making them a general purpose extractive content planner for different tasks.	3
When evaluated on CNN/DailyMail extractive summarization, stepwise models achieve state-of-the-art performance in terms of Rouge without any redundancy aware modeling or sentence filtering.	4
This also holds true for Rotowire table-to-text generation, where our models surpass previously reported metrics for content selection, planning and ordering, highlighting the strength of stepwise modeling.	3
Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.	4
----------
We present CLIRMatrix, a massively large collection of bilingual and multilingual datasets for Cross-Lingual Information Retrieval extracted automatically from Wikipedia.	2+3
CLIRMatrix comprises (1) BI-139, a bilingual dataset of queries in one language matched with relevant documents in another language for 139x138=19,182 language pairs, and (2) MULTI-8, a multilingual dataset of queries and documents jointly aligned in 8 different languages.	3
In total, we mined 49 million unique queries and 34 billion (query, document, label) triplets, making it the largest and most comprehensive CLIR dataset to date.	3
This collection is intended to support research in end-to-end neural information retrieval and is publicly available at [url].	6
We provide baseline neural model results on BI-139, and evaluate MULTI-8 in both single-language retrieval and mix-language retrieval settings.	3
----------
With worldwide concerns surrounding the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), there is a rapidly growing body of scientific literature on the virus.	1
Clinicians, researchers, and policy-makers need to be able to search these articles effectively.	1
In this work, we present a zero-shot ranking algorithm that adapts to COVID-related scientific literature.	2
Our approach filters training data from another collection down to medical-related queries, uses a neural re-ranking model pre-trained on scientific text (SciBERT), and filters the target document collection.	3
This approach ranks top among zero-shot methods on the TREC COVID Round 1 leaderboard, and exhibits a P@5 of 0.80 and an nDCG@10 of 0.68 when evaluated on both Round 1 and 2 judgments.	3
Despite not relying on TREC-COVID data, our method outperforms models that do.	4
As one of the first search methods to thoroughly evaluate COVID-19 search, we hope that this serves as a strong baseline and helps in the global crisis.	5
----------
Recent innovations in Transformer-based ranking models have advanced the state-of-the-art in information retrieval.	1
However, these Transformers are computationally expensive, and their opaque hidden states make it hard to understand the ranking process.	1
In this work, we modularize the Transformer ranker into separate modules for text representation and interaction.	2
We show how this design enables substantially faster ranking using offline pre-computed representations and light-weight online interactions.	3
The modular design is also easier to interpret and sheds light on the ranking process in Transformer rankers.	3+4
----------
We describe a weakly-supervised method for training deep learning models for the task of ad-hoc document retrieval.	2
Our method is based on generative and discriminative models that are trained using weak-supervision just from the documents in the corpus.	3
We present an end-to-end retrieval system that starts with traditional information retrieval methods, followed by two deep learning re-rankers.	2
We evaluate our method on three different datasets: a COVID-19 related scientific literature dataset and two news datasets.	3
We show that our method outperforms state-of-the-art methods; this without the need for the expensive process of manually labeling data.	4
----------
We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models.	2+3
We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts—including paraphrase identification, document retrieval, response suggestion, and extractive summarization—are vulnerable to semantic collisions.	2+3
For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3.	3
We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations.	5
Our code is available at https://github.com/csong27/collision-bert.	6
----------
Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world.	1
We present RuleNN, a neural network architecture for learning transparent models for sentence classification.	1+2
The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics.	3
More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding.	3
Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks.	4
Our user studies confirm that the learned LEs are explainable and capture domain semantics.	5
Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.	5
----------
The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining.	1
Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts.	1
To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search.	2+3
Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models.	3+4
We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models.	4
These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.	5
----------
To build an interpretable neural text classifier, most of the prior work has focused on designing inherently interpretable models or finding faithful explanations.	1
A new line of work on improving model interpretability has just started, and many existing methods require either prior information or human annotations as additional inputs in training.	1
To address this limitation, we propose the variational word mask (VMASK) method to automatically learn task-specific important words and reduce irrelevant information on classification, which ultimately improves the interpretability of model predictions.	2+3
The proposed method is evaluated with three neural text classifiers (CNN, LSTM, and BERT) on seven benchmark text classification datasets.	3
Experiments show the effectiveness of VMASK in improving both model prediction accuracy and interpretability.	4
----------
Current state-of-the-art text generators build on powerful language models such as GPT-2, achieving impressive performance.	1
However, to avoid degenerate text, they require sampling from a modified softmax, via temperature parameters or ad-hoc truncation techniques, as in top-k or nucleus sampling.	1
This creates a mismatch between training and testing conditions.	1
In this paper, we use the recently introduced entmax transformation to train and sample from a natively sparse language model, avoiding this mismatch.	2
The result is a text generator with favorable performance in terms of fluency and consistency, fewer repetitions, and n-gram diversity closer to human text.	4
In order to evaluate our model, we propose three new metrics for comparing sparse or truncated distributions: 𝜖-perplexity, sparsemax score, and Jensen-Shannon divergence.	2+3
Human-evaluated experiments in story completion and dialogue generation show that entmax sampling leads to more engaging and coherent stories and conversations.	4
----------
We propose the task of outline-conditioned story generation: given an outline as a set of phrases that describe key characters and events to appear in a story, the task is to generate a coherent narrative that is consistent with the provided outline.	2+3
This task is challenging as the input only provides a rough sketch of the plot, and thus, models need to generate a story by interweaving the key points provided in the outline.	3
This requires the model to keep track of the dynamic states of the latent plot, conditioning on the input outline while generating the full story.	3
We present PlotMachines, a neural narrative model that learns to transform an outline into a coherent story by tracking the dynamic plot states.	3
In addition, we enrich PlotMachines with high-level discourse structure so that the model can learn different writing styles corresponding to different parts of the narrative.	3
Comprehensive experiments over three fiction and non-fiction datasets demonstrate that large-scale language models, such as GPT-2 and Grover, despite their impressive generation performance, are not sufficient in generating coherent narratives for the given outline, and dynamic plot state tracking is important for composing narratives with tighter, more consistent plots.	5
----------
Autoregressive language models are powerful and relatively easy to train.	1
However, these models are usually trained without explicit conditioning labels and do not offer easy ways to control global aspects such as sentiment or topic during generation.	1
Bowman & al. 2016 adapted the Variational Autoencoder (VAE) for natural language with the sequence-to-sequence architecture and claimed that the latent vector was able to capture such global features in an unsupervised manner.	1
We question this claim.	2
We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence.	3
Using this method, we find that VAEs are prone to memorizing the first words and the sentence length, producing local features of limited usefulness.	4
To alleviate this, we investigate alternative architectures based on bag-of-words assumptions and language model pretraining.	3
These variants learn latent variables that are more global, i.e., more predictive of topic or sentiment labels.	3
Moreover, using reconstructions, we observe that they decrease memorization: the first word and the sentence length are not recovered as accurately than with the baselines, consequently yielding more diverse reconstructions.	4
----------
Long-form narrative text generated from large language models manages a fluent impersonation of human writing, but only at the local sentence level, and lacks structure or global cohesion.	1
We posit that many of the problems of story generation can be addressed via high-quality content planning, and present a system that focuses on how to learn good plot structures to guide story generation.	2
We utilize a plot-generation language model along with an ensemble of rescoring models that each implement an aspect of good story-writing as detailed in Aristotle’s Poetics.	3
We find that stories written with our more principled plot-structure are both more relevant to a given prompt and higher quality than baselines that do not content plan, or that plan in an unprincipled way.	4
----------
Existing open-domain dialogue generation models are usually trained to mimic the gold response in the training set using cross-entropy loss on the vocabulary.	1
However, a good response does not need to resemble the gold response, since there are multiple possible responses to a given prompt.	1
In this work, we hypothesize that the current models are unable to integrate information from multiple semantically similar valid responses of a prompt, resulting in the generation of generic and uninformative responses.	2
To address this issue, we propose an alternative to the end-to-end classification on vocabulary.	2
We learn the pair relationship between the prompts and responses as a regression task on a latent space instead.	3
In our novel dialog generation model, the representations of semantically related sentences are close to each other on the latent space.	4
Human evaluation showed that learning the task on a continuous space can generate responses that are both relevant and informative.	4
----------
Dialogue participants often refer to entities or situations repeatedly within a conversation, which contributes to its cohesiveness.	1
Subsequent references exploit the common ground accumulated by the interlocutors and hence have several interesting properties, namely, they tend to be shorter and reuse expressions that were effective in previous mentions.	1
In this paper, we tackle the generation of first and subsequent references in visually grounded dialogue.	2
We propose a generation model that produces referring utterances grounded in both the visual and the conversational context.	2
To assess the referring effectiveness of its output, we also implement a reference resolution system.	2
Our experiments and analyses show that the model produces better, more effective referring utterances than a model not grounded in the dialogue context, and generates subsequent references that exhibit linguistic patterns akin to humans.	4
----------
Exploiting visual groundings for language understanding has recently been drawing much attention.	1
In this work, we study visually grounded grammar induction and learn a constituency parser from both unlabeled text and its visual groundings.	2
Existing work on this task (Shi et al., 2019) optimizes a parser via Reinforce and derives the learning signal only from the alignment of images and sentences.	3
While their model is relatively accurate overall, its error distribution is very uneven, with low performance on certain constituents types (e.g., 26.2% recall on verb phrases, VPs) and high on others (e.g., 79.6% recall on noun phrases, NPs).	3
This is not surprising as the learning signal is likely insufficient for deriving all aspects of phrase-structure syntax and gradient estimates are noisy.	4
We show that using an extension of probabilistic context-free grammar model we can do fully-differentiable end-to-end visually grounded learning.	4
Additionally, this enables us to complement the image-text alignment loss with a language modeling objective.	4
On the MSCOCO test captions, our model establishes a new state of the art, outperforming its non-grounded version and, thus, confirming the effectiveness of visual groundings in constituency grammar induction.	5
It also substantially outperforms the previous grounded model, with largest improvements on more ‘abstract’ categories (e.g., +55.1% recall on VPs).	5
----------
Training a supervised neural network classifier typically requires many annotated training samples.	1
Collecting and annotating a large number of data points are costly and sometimes even infeasible.	1
Traditional annotation process uses a low-bandwidth human-machine communication interface: classification labels, each of which only provides a few bits of information.	1
We propose Active Learning with Contrastive Explanations (ALICE), an expert-in-the-loop training framework that utilizes contrastive natural language explanations to improve data efficiency in learning.	2
AL-ICE learns to first use active learning to select the most informative pairs of label classes to elicit contrastive natural language explanations from experts.	3
Then it extracts knowledge from these explanations using a semantic parser.	3
Finally, it incorporates the extracted knowledge through dynamically changing the learning model’s structure.	3
We applied ALICEin two visual recognition tasks, bird species classification and social relationship classification.	3
We found by incorporating contrastive explanations, our models outperform baseline models that are trained with 40-100% more training data.	3+4
We found that adding1expla-nation leads to similar performance gain as adding 13-30 labeled training data points.	4
----------
Iterative Language-Based Image Editing (ILBIE) tasks follow iterative instructions to edit images step by step.	1
Data scarcity is a significant issue for ILBIE as it is challenging to collect large-scale examples of images before and after instruction-based changes.	1
Yet, humans still accomplish these editing tasks even when presented with an unfamiliar image-instruction pair.	1
Such ability results from counterfactual thinking, the ability to think about possible alternatives to events that have happened already.	1
In this paper, we introduce a Self-Supervised Counterfactual Reasoning (SSCR) framework that incorporates counterfactual thinking to overcome data scarcity.	2
SSCR allows the model to consider out-of-distribution instructions paired with previous images.	2
With the help of cross-task consistency (CTC), we train these counterfactual instructions in a self-supervised scenario.	3
Extensive results show that SSCR improves the correctness of ILBIE in terms of both object identity and position, establishing a new state of the art (SOTA) on two IBLIE datasets (i-CLEVR and CoDraw).	4
Even with only 50% of the training data, SSCR achieves a comparable result to using complete data.	4
----------
It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer.	1
This is surprising given that mBERT does not use any crosslingual signal during training.	1
While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure.	1
We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual.	2
To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data.	2
Overall, we identify four architectural and two linguistic elements that influence multilinguality.	3
Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment.	3
Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings.	4
----------
Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages.	1
However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference.	1
In this paper, we present the first systematic study of negative interference.	2
We show that, contrary to previous belief, negative interference also impacts low-resource languages.	4
While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference.	3
Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers’ generalization on all languages.	2+3
Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.	4
----------
Cross-lingual word embedding (CWE) algorithms represent words in multiple languages in a unified vector space.	1
Multi-Word Expressions (MWE) are common in every language.	1
When training word embeddings, each component word of an MWE gets its own separate embedding, and thus, MWEs are not translated by CWEs.	1
We propose a simple method for word translation of MWEs to and from English in ten languages: we first compile lists of MWEs in each language and then tokenize the MWEs as single tokens before training word embeddings.	2+3
CWEs are trained on a word-translation task using the dictionaries that only contain single words.	3
In order to evaluate MWE translation, we created bilingual word lists from multilingual WordNet that include single-token words and MWEs, and most importantly, include MWEs that correspond to single words in another language.	3
We release these dictionaries to the research community.	3
We show that the pre-tokenization of MWEs as single tokens performs better than averaging the embeddings of the individual tokens of the MWE.	4
We can translate MWEs at a top-10 precision of 30-60%.	4
The tokenization of MWEs makes the occurrences of single words in a training corpus more sparse, but we show that it does not pose negative impacts on single-word translations.	5
----------
We propose a novel adapter layer formalism for adapting multilingual models.	2
They are more parameter-efficient than existing adapter layers while obtaining as good or better performance.	4
The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs.	3+4
In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.	4
----------
Multilingual BERT (mBERT), XLM-RoBERTa (XLMR) and other unsupervised multilingual encoders can effectively learn cross-lingual representation.	1
Explicit alignment objectives based on bitexts like Europarl or MultiUN have been shown to further improve these representations.	1
However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages.	1
In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection.	2
Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks.	2
Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework.	4
Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training.	4
These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.	4
----------
Massively multilingual transformers (MMTs) pretrained via language modeling (e.g., mBERT, XLM-R) have become a default paradigm for zero-shot language transfer in NLP, offering unmatched transfer performance.	1
Current evaluations, however, verify their efficacy in transfers (a) to languages with sufficiently large pretraining corpora, and (b) between close languages.	1
In this work, we analyze the limitations of downstream language transfer with MMTs, showing that, much like cross-lingual word embeddings, they are substantially less effective in resource-lean scenarios and for distant languages.	2
Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER) and two high-level tasks (NLI, QA), empirically correlate transfer performance with linguistic proximity between source and target languages, but also with the size of target language corpora used in MMT pretraining.	3
Most importantly, we demonstrate that the inexpensive few-shot transfer (i.e., additional fine-tuning on a few target-language instances) is surprisingly effective across the board, warranting more research efforts reaching beyond the limiting zero-shot conditions.	4
----------
Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource.	1
The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity).	1
In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage.	2
We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines.	4
In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.	4
----------
We present an easy and efficient method to extend existing sentence embedding models to new languages.	2
This allows to create multilingual versions from previously monolingual models.	2
The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence.	3
We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model.	3
Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower.	4
We demonstrate the effectiveness of our approach for 50+ languages from various language families.	4
Code to extend sentence embeddings models to more than 400 languages is publicly available.	4
----------
We propose an efficient batching strategy for variable-length decoding on GPU architectures.	2
During decoding, when candidates terminate or are pruned according to heuristics, our streaming approach periodically “refills” the batch before proceeding with a selected subset of candidates.	3
We apply our method to variable-width beam search on a state-of-the-art machine translation model.	3
Our method decreases runtime by up to 71% compared to a fixed-width beam search baseline and 17% compared to a variable-width baseline, while matching baselines’ BLEU.	4
Finally, experiments show that our method can speed up decoding in other domains, such as semantic and syntactic parsing.	4
----------
State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications.	1
In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies.	2+3
Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.	4
----------
Learning what to share between tasks has become a topic of great importance, as strategic sharing of knowledge has been shown to improve downstream task performance.	1
This is particularly important for multilingual applications, as most languages in the world are under-resourced.	1
Here, we consider the setting of training models on multiple different languages at the same time, when little or no data is available for languages other than English.	2
We show that this challenging setup can be approached using meta-learning: in addition to training a source language model, another model learns to select which training instances are the most beneficial to the first.	2
We experiment using standard supervised, zero-shot cross-lingual, as well as few-shot cross-lingual settings for different natural language understanding tasks (natural language inference, question answering).	3
Our extensive experimental setup demonstrates the consistent effectiveness of meta-learning for a total of 15 languages.	4
We improve upon the state-of-the-art for zero-shot and few-shot NLI (on MultiNLI and XNLI) and QA (on the MLQA dataset).	4
A comprehensive error analysis indicates that the correlation of typological features between languages can partly explain when parameter sharing learned via meta-learning is beneficial.	4
----------
We present the Multilingual Amazon Reviews Corpus (MARC), a large-scale collection of Amazon reviews for multilingual text classification.	2
The corpus contains reviews in English, Japanese, German, French, Spanish, and Chinese, which were collected between 2015 and 2019.	3
Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID, and the coarse-grained product category (e.g., ‘books’, ‘appliances’, etc.)	3
The corpus is balanced across the 5 possible star ratings, so each rating constitutes 20% of the reviews in each language.	3
For each language, there are 200,000, 5,000, and 5,000 reviews in the training, development, and test sets, respectively.	3
We report baseline results for supervised text classification and zero-shot cross-lingual transfer learning by fine-tuning a multilingual BERT model on reviews data.	3
We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings.	4
----------
We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing.	2+3
Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets.	3
For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings.	3
A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena.	4
----------
Knowledge of a disease includes information of various aspects of the disease, such as signs and symptoms, diagnosis and treatment.	1
This disease knowledge is critical for many health-related and biomedical tasks, including consumer health question answering, medical language inference and disease name recognition.	1
While pre-trained language models like BERT have shown success in capturing syntactic, semantic, and world knowledge from text, we find they can be further complemented by specific information like knowledge of symptoms, diagnoses, treatments, and other disease aspects.	2
Hence, we integrate BERT with disease knowledge for improving these important tasks.	2
Specifically, we propose a new disease knowledge infusion training procedure and evaluate it on a suite of BERT models including BERT, BioBERT, SciBERT, ClinicalBERT, BlueBERT, and ALBERT.	2
Experiments over the three tasks show that these models can be enhanced in nearly all cases, demonstrating the viability of disease knowledge infusion.	4
For example, accuracy of BioBERT on consumer health question answering is improved from 68.29% to 72.09%, while new SOTA results are observed in two datasets.	4
We make our data and code freely available.	4
----------
Natural language understanding involves reading between the lines with implicit background knowledge.	1
Current systems either rely on pre-trained language models as the sole implicit source of world knowledge, or resort to external knowledge bases (KBs) to incorporate additional relevant knowledge.	1
We propose an unsupervised framework based on self-talk as a novel alternative to multiple-choice commonsense tasks.	2
Inspired by inquiry-based discovery learning (Bruner, 1961), our approach inquires language models with a number of information seeking questions such as “what is the definition of...” to discover additional background knowledge.	3
Empirical results demonstrate that the self-talk procedure substantially improves the performance of zero-shot language model baselines on four out of six commonsense benchmarks, and competes with models that obtain knowledge from external KBs.	4
While our approach improves performance on several benchmarks, the self-talk induced knowledge even when leading to correct answers is not always seen as helpful by human judges, raising interesting questions about the inner-workings of pre-trained language models for commonsense reasoning.	5
----------
We propose a suite of reasoning tasks on two types of relations between procedural events: goal-step relations (“learn poses” is a step in the larger goal of “doing yoga”) and step-step temporal relations (“buy a yoga mat” typically precedes “learn poses”).	2
We introduce a dataset targeting these two relations based on wikiHow, a website of instructional how-to articles.	2
Our human-validated test set serves as a reliable benchmark for common-sense inference, with a gap of about 10% to 20% between the performance of state-of-the-art transformer models and human performance.	3
Our automatically-generated training set allows models to effectively transfer to out-of-domain tasks requiring knowledge of procedural events, with greatly improved performances on SWAG, Snips, and Story Cloze Test in zero- and few-shot settings.	4
----------
Humans can learn structural properties about a word from minimal experience, and deploy their learned syntactic representations uniformly in different grammatical contexts.	1
We assess the ability of modern neural language models to reproduce this behavior in English and evaluate the effect of structural supervision on learning outcomes.	2
First, we assess few-shot learning capabilities by developing controlled experiments that probe models’ syntactic nominal number and verbal argument structure generalizations for tokens seen as few as two times during training.	3
Second, we assess invariance properties of learned representation: the ability of a model to transfer syntactic generalizations from a base context (e.g., a simple declarative active-voice sentence) to a transformed context (e.g., an interrogative sentence).	3
We test four models trained on the same dataset: an n-gram baseline, an LSTM, and two LSTM-variants trained with explicit structural supervision.	3
We find that in most cases, the neural models are able to induce the proper syntactic generalizations after minimal exposure, often from just two examples during training, and that the two structurally supervised models generalize more accurately than the LSTM model.	4
All neural models are able to leverage information learned in base contexts to drive expectations in transformed contexts, indicating that they have learned some invariance properties of syntax.	4
----------
When speakers describe an image, they tend to look at objects before mentioning them.	1
In this paper, we investigate such sequential cross-modal alignment by modelling the image description generation process computationally.	2
We take as our starting point a state-of-the-art image captioning system and develop several model variants that exploit information from human gaze patterns recorded during language production.	2
In particular, we propose the first approach to image description generation where visual processing is modelled sequentially.	2
Our experiments and analyses confirm that better descriptions can be obtained by exploiting gaze-driven attention and shed light on human cognitive processes by comparing different ways of aligning the gaze modality with language production.	4
We find that processing gaze data sequentially leads to descriptions that are better aligned to those produced by speakers, more diverse, and more natural—particularly when gaze is encoded with a dedicated recurrent component.	5
----------
When trained effectively, the Variational Autoencoder (VAE) can be both a powerful generative model and an effective representation learning framework for natural language.	1
In this paper, we propose the first large-scale language VAE model Optimus (Organizing sentences via Pre-Trained Modeling of a Universal Space).	2
A universal latent embedding space for sentences is first pre-trained on large text corpus, and then fine-tuned for various language generation and understanding tasks.	3
Compared with GPT-2, Optimus enables guided language generation from an abstract level using the latent vectors.	4
Compared with BERT, Optimus can generalize better on low-resource language understanding tasks due to the smooth latent space structure.	4
Extensive experimental results on a wide range of language tasks demonstrate the effectiveness of Optimus.	4
It achieves new state-of-the-art on VAE language modeling benchmarks.	4
----------
There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books.	1
Yet, most works do not study the factors affecting each domain language application deeply.	1
Additionally, the study of model size on domain-specific models has been mostly missing.	1
We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer.	2
We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications.	4
We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction.	4
Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].	6
----------
Document and discourse segmentation are two fundamental NLP tasks pertaining to breaking up text into constituents, which are commonly used to help downstream tasks such as information retrieval or text summarization.	1
In this work, we propose three transformer-based architectures and provide comprehensive comparisons with previously proposed approaches on three standard datasets.	2
We establish a new state-of-the-art, reducing in particular the error rates by a large margin in all cases.	2
We further analyze model sizes and find that we can build models with many fewer parameters while keeping good performance, thus facilitating real-world applications.	4
----------
Multilingual pre-trained Transformers, such as mBERT (Devlin et al., 2019) and XLM-RoBERTa (Conneau et al., 2020a), have been shown to enable effective cross-lingual zero-shot transfer.	1
In this paper, we pre-train a customized bilingual BERT, dubbed GigaBERT, that is designed specifically for Arabic NLP and English-to-Arabic zero-shot transfer learning.	2
We study GigaBERT’s effectiveness on zero-short transfer across four IE tasks: named entity recognition, part-of-speech tagging, argument role labeling, and relation extraction.	2
Our best model significantly outperforms mBERT, XLM-RoBERTa, and AraBERT (Antoun et al., 2020) in both the supervised and zero-shot transfer settings.	4
We have made our pre-trained models publicly available at: https://github.com/lanwuwei/GigaBERT.	6
----------
In this work, we present a new language pre-training model TNT (Text Normalization based pre-training of Transformers) for content moderation.	2
Inspired by the masking strategy and text normalization, TNT is developed to learn language representation by training transformers to reconstruct text from four operation types typically seen in text manipulation: substitution, transposition, deletion, and insertion.	3
Furthermore, the normalization involves the prediction of both operation types and token labels, enabling TNT to learn from more challenging tasks than the standard task of masked word recovery.	3
As a result, the experiments demonstrate that TNT outperforms strong baselines on the hate speech classification task.	4
Additional text normalization experiments and case studies show that TNT is a new potential approach to misspelling correction.	5
----------
Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text.	1
This leads to problems with numerical reasoning involving tasks such as question answering.	1
We propose a new methodology to assign and learn embeddings for numbers.	2
Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line.	3
DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition.	4
We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction.	3
We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.	3
----------
We conduct a large scale empirical investigation of contextualized number prediction in running text.	2
Specifically, we consider two tasks: (1)masked number prediction– predict-ing a missing numerical value within a sentence, and (2)numerical anomaly detection–detecting an errorful numeric value within a sentence.	3
We experiment with novel combinations of contextual encoders and output distributions over the real number line.	3
Specifically, we introduce a suite of output distribution parameterizations that incorporate latent variables to add expressivity and better fit the natural distribution of numeric values in running text, and combine them with both recur-rent and transformer-based encoder architectures.	2+3
We evaluate these models on two numeric datasets in the financial and scientific domain.	3
Our findings show that output distributions that incorporate discrete latent variables and allow for multiple modes outperform simple flow-based counterparts on all datasets, yielding more accurate numerical pre-diction and anomaly detection.	4
We also show that our models effectively utilize textual con-text and benefit from general-purpose unsupervised pretraining.	4
----------
Exposure to violent, sexual, or substance-abuse content in media increases the willingness of children and adolescents to imitate similar behaviors.	1
Computational methods that identify portrayals of risk behaviors from audio-visual cues are limited in their applicability to films in post-production, where modifications might be prohibitively expensive.	1
To address this limitation, we propose a model that estimates content ratings based on the language use in movie scripts, making our solution available at the earlier stages of creative production.	2
Our model significantly improves the state-of-the-art by adapting novel techniques to learn better movie representations from the semantic and sentiment aspects of a character’s language use, and by leveraging the co-occurrence of risk behaviors, following a multi-task approach.	3
Additionally, we show how this approach can be useful to learn novel insights on the joint portrayal of these behaviors, and on the subtleties that filmmakers may otherwise not pick up on.	4
----------
Morphologically rich languages seem to benefit from joint processing of morphology and syntax, as compared to pipeline architectures.	1
We propose a graph-based model for joint morphological parsing and dependency parsing in Sanskrit.	2
Here, we extend the Energy based model framework (Krishna et al., 2020), proposed for several structured prediction tasks in Sanskrit, in 2 simple yet significant ways.	3
First, the framework’s default input graph generation method is modified to generate a multigraph, which enables the use of an exact search inference.	3
Second, we prune the input search space using a linguistically motivated approach, rooted in the traditional grammatical analysis of Sanskrit.	3
Our experiments show that the morphological parsing from our joint model outperforms standalone morphological parsers.	4
We report state of the art results in morphological parsing, and in dependency parsing, both in standalone (with gold morphological tags) and joint morphosyntactic parsing setting.	4
----------
We propose a method for unsupervised parsing based on the linguistic notion of a constituency test.	2
One type of constituency test involves modifying the sentence via some transformation (e.g. replacing the span with a pronoun) and then judging the result (e.g. checking if it is grammatical).	3
Motivated by this idea, we design an unsupervised parser by specifying a set of transformations and using an unsupervised neural acceptability model to make grammaticality decisions.	3
To produce a tree given a sentence, we score each span by aggregating its constituency test judgments, and we choose the binary tree with the highest total score.	3
While this approach already achieves performance in the range of current methods, we further improve accuracy by fine-tuning the grammaticality model through a refinement procedure, where we alternate between improving the estimated trees and improving the grammaticality model.	3
The refined model achieves 62.8 F1 on the Penn Treebank test set, an absolute improvement of 7.6 points over the previously best published result.	4
----------
The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers.	1
However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree.	1
We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.	2
In fact, the worst constraint-violation rate we observe is 24%.	4
Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime.	1
We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.	3+4
----------
We describe a fully unsupervised cross-lingual transfer approach for part-of-speech (POS) tagging under a truly low resource scenario.	2
We assume access to parallel translations between the target language and one or more source languages for which POS taggers are available.	3
We use the Bible as parallel data in our experiments: small size, out-of-domain and covering many diverse languages.	3
Our approach innovates in three ways: 1) a robust approach of selecting training instances via cross-lingual annotation projection that exploits best practices of unsupervised type and token constraints, word-alignment confidence and density of projected POS, 2) a Bi-LSTM architecture that uses contextualized word embeddings, affix embeddings and hierarchical Brown clusters, and 3) an evaluation on 12 diverse languages in terms of language family and morphological typology.	3
In spite of the use of limited and out-of-domain parallel data, our experiments demonstrate significant improvements in accuracy over previous work.	4
In addition, we show that using multi-source information, either via projection or output combination, improves the performance for most target languages.	4
----------
The deep inside-outside recursive autoencoder (DIORA; Drozdov et al. 2019) is a self-supervised neural model that learns to induce syntactic tree structures for input sentences *without access to labeled training data*.	1
In this paper, we discover that while DIORA exhaustively encodes all possible binary trees of a sentence with a soft dynamic program, its vector averaging approach is locally greedy and cannot recover from errors when computing the highest scoring parse tree in bottom-up chart parsing.	2+3
To fix this issue, we introduce S-DIORA, an improved variant of DIORA that encodes a single tree rather than a softly-weighted mixture of trees by employing a hard argmax operation and a beam at each cell in the chart.	3
Our experiments show that through *fine-tuning* a pre-trained DIORA with our new algorithm, we improve the state of the art in *unsupervised* constituency parsing on the English WSJ Penn Treebank by 2.2-6% F1, depending on the data used for fine-tuning.	3+4
----------
Text classification is a critical research topic with broad applications in natural language processing.	1
Recently, graph neural networks (GNNs) have received increasing attention in the research community and demonstrated their promising results on this canonical task.	1
Despite the success, their performance could be largely jeopardized in practice since they are: (1) unable to capture high-order interaction between words; (2) inefficient to handle large datasets and new documents.	1
To address those issues, in this paper, we propose a principled model – hypergraph attention networks (HyperGAT), which can obtain more expressive power with less computational consumption for text representation learning.	1+2
Extensive experiments on various benchmark datasets demonstrate the efficacy of the proposed approach on the text classification task.	3+4
----------
We focus on the problem of capturing declarative knowledge about entities in the learned parameters of a language model.	1
We introduce a new model—Entities as Experts (EaE)—that can access distinct memories of the entities mentioned in a piece of text.	2
Unlike previous efforts to integrate entity knowledge into sequence models, EaE’s entity representations are learned directly from text.	3
We show that EaE’s learned representations capture sufficient knowledge to answer TriviaQA questions such as “Which Dr. Who villain has been played by Roger Delgado, Anthony Ainley, Eric Roberts?”, outperforming an encoder-generator Transformer model with 10x the parameters on this task.	4
According to the Lama knowledge probes, EaE contains more factual knowledge than a similar sized Bert, as well as previous approaches that integrate external sources of entity knowledge.Because EaE associates parameters with specific entities, it only needs to access a fraction of its parameters at inference time, and we show that the correct identification and representation of entities is essential to EaE’s performance.	5
----------
Label smoothing has been shown to be an effective regularization strategy in classification, that prevents overfitting and helps in label de-noising.	1
However, extending such methods directly to seq2seq settings, such as Machine Translation, is challenging: the large target output space of such problems makes it intractable to apply label smoothing over all possible outputs.	1
Most existing approaches for seq2seq settings either do token level smoothing, or smooth over sequences generated by randomly substituting tokens in the target sequence.	2
Unlike these works, in this paper, we propose a technique that smooths over well formed relevant sequences that not only have sufficient n-gram overlap with the target sequence, but are also semantically similar.	2
Our method shows a consistent and significant improvement over the state-of-the-art techniques on different datasets.	4
----------
Most recent improvements in NLP come from changes to the neural network architectures modeling the text input.	1
Yet, state-of-the-art models often rely on simple approaches to model the label space, e.g. bigram Conditional Random Fields (CRFs) in sequence tagging.	1
More expressive graphical models are rarely used due to their prohibitive computational cost.	1
In this work, we present an approach for efficiently training and decoding hybrids of graphical models and neural networks based on Gibbs sampling.	2
Our approach is the natural adaptation of SampleRank (Wick et al., 2011) to neural models, and is widely applicable to tasks beyond sequence tagging.	2
We apply our approach to named entity recognition and present a neural skip-chain CRF model, for which exact inference is impractical.	2
The skip-chain model improves over a strong baseline on three languages from CoNLL-02/03.	2
We obtain new state-of-the-art results on Dutch.	4
----------
Text alignment finds application in tasks such as citation recommendation and plagiarism detection.	1
Existing alignment methods operate at a single, predefined level and cannot learn to align texts at, for example, sentence and document levels.	1
We propose a new learning approach that equips previously established hierarchical attention encoders for representing documents with a cross-document attention component, enabling structural comparisons across different levels (document-to-document and sentence-to-document).	2
Our component is weakly supervised from document pairs and can align at multiple levels.	4
Our evaluation on predicting document-to-document relationships and sentence-to-document relationships on the tasks of citation recommendation and plagiarism detection shows that our approach outperforms previously established hierarchical, attention encoders based on recurrent and transformer contextualization that are unaware of structural correspondence between documents.	5
----------
This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks.	2
We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe.	3
We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way.	3
Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering.	3
The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.	5
----------
Natural language understanding (NLU) in the context of goal-oriented dialog systems typically includes intent classification and slot labeling tasks.	1
Existing methods to expand an NLU system to new languages use machine translation with slot label projection from source to the translated utterances, and thus are sensitive to projection errors.	1
In this work, we propose a novel end-to-end model that learns to align and predict target slot labels jointly for cross-lingual transfer.	2
We introduce MultiATIS++, a new multilingual NLU corpus that extends the Multilingual ATIS corpus to nine languages across four language families, and evaluate our method using the corpus.	2
Results show that our method outperforms a simple label projection method using fast-align on most languages, and achieves competitive performance to the more complex, state-of-the-art projection method with only half of the training time.	4
We release our MultiATIS++ corpus to the community to continue future research on cross-lingual NLU.	6
----------
Intent detection is one of the core components of goal-oriented dialog systems, and detecting out-of-scope (OOS) intents is also a practically important skill.	1
Few-shot learning is attracting much attention to mitigate data scarcity, but OOS detection becomes even more challenging.	1
In this paper, we present a simple yet effective approach, discriminative nearest neighbor classification with deep self-attention.	2
Unlike softmax classifiers, we leverage BERT-style pairwise encoding to train a binary classifier that estimates the best matched training example for a user input.	3
We propose to boost the discriminative ability by transferring a natural language inference (NLI) model.	2
Our extensive experiments on a large-scale multi-domain intent detection task show that our method achieves more stable and accurate in-domain and OOS detection accuracy than RoBERTa-based classifiers and embedding-based nearest neighbor approaches.	4
More notably, the NLI transfer enables our 10-shot model to perform competitively with 50-shot or even full-shot classifiers, while we can keep the inference time constant by leveraging a faster embedding retrieval model.	5
----------
The concept of Dialogue Act (DA) is universal across different task-oriented dialogue domains - the act of “request” carries the same speaker intention whether it is for restaurant reservation or flight booking.	1
However, DA taggers trained on one domain do not generalize well to other domains, which leaves us with the expensive need for a large amount of annotated data in the target domain.	1
In this work, we investigate how to better adapt DA taggers to desired target domains with only unlabeled data.	2
We propose MaskAugment, a controllable mechanism that augments text input by leveraging the pre-trained Mask token from BERT model.	2
Inspired by consistency regularization, we use MaskAugment to introduce an unsupervised teacher-student learning scheme to examine the domain adaptation of DA taggers.	3
Our extensive experiments on the Simulated Dialogue (GSim) and Schema-Guided Dialogue (SGD) datasets show that MaskAugment is useful in improving the cross-domain generalization for DA tagging.	4
----------
Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user’s intents (set reminder, play music, etc.).	1
Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music).	1
In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction.	2
In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques.	3
Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work.	3
Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains.	3
This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.	4
----------
We introduce a new task of rephrasing for a more natural virtual assistant.	1
Currently, virtual assistants work in the paradigm of intent-slot tagging and the slot values are directly passed as-is to the execution engine.	1
However, this setup fails in some scenarios such as messaging when the query given by the user needs to be changed before repeating it or sending it to another user.	1
For example, for queries like ‘ask my wife if she can pick up the kids’ or ‘remind me to take my pills’, we need to rephrase the content to ‘can you pick up the kids’ and ‘take your pills’.	1
In this paper, we study the problem of rephrasing with messaging as a use case and release a dataset of 3000 pairs of original query and rephrased query.	2
We show that BART, a pre-trained transformers-based masked language model, is a strong baseline for the task, and show improvements by adding a copy-pointer and copy loss to it.	4
We analyze different trade-offs of BART-based and LSTM-based seq2seq models, and propose a distilled LSTM-based seq2seq as the best practical model	2
----------
Sentence simplification aims to make sentences easier to read and understand.	1
Recent approaches have shown promising results with encoder-decoder models trained on large amounts of parallel data which often only exists in English.	1
We propose a zero-shot modeling framework which transfers simplification knowledge from English to another language (for which no parallel simplification corpus exists) while generalizing across languages and tasks.	2
A shared transformer encoder constructs language-agnostic representations, with a combination of task-specific encoder layers added on top (e.g., for translation and simplification).	3
Empirical results using both human and automatic metrics show that our approach produces better simplifications than unsupervised and pivot-based methods.	4
----------
Aided by technology, people are increasingly able to communicate across geographical, cultural, and language barriers.	1
This ability also results in new challenges, as interlocutors need to adapt their communication approaches to increasingly diverse circumstances.	1
In this work, we take the first steps towards automatically assisting people in adjusting their language to a specific communication circumstance.	2
As a case study, we focus on facilitating the accurate transmission of pragmatic intentions and introduce a methodology for suggesting paraphrases that achieve the intended level of politeness under a given communication circumstance.	2+3
We demonstrate the feasibility of this approach by evaluating our method in two realistic communication scenarios and show that it can reduce the potential for misalignment between the speaker’s intentions and the listener’s perceptions in both cases.	3+4
----------
NLP models are shown to suffer from robustness issues, i.e., a model’s prediction can be easily changed under small perturbations to the input.	1
In this work, we present a Controlled Adversarial Text Generation (CAT-Gen) model that, given an input text, generates adversarial texts through controllable attributes that are known to be invariant to task labels.	2
For example, in order to attack a model for sentiment classification over product reviews, we can use the product categories as the controllable attribute which would not change the sentiment of the reviews.	3
Experiments on real-world NLP datasets demonstrate that our method can generate more diverse and fluent adversarial texts, compared to many existing adversarial text generation approaches.	4
We further use our generated adversarial examples to improve models through adversarial training, and we demonstrate that our generated attacks are more robust against model re-training and different model architectures.	3+4
----------
We propose Seq2Edits, an open-vocabulary approach to sequence editing for natural language processing (NLP) tasks with a high degree of overlap between input and output texts.	2
In this approach, each sequence-to-sequence transduction is represented as a sequence of edit operations, where each operation either replaces an entire source span with target tokens or keeps it unchanged.	3
We evaluate our method on five NLP tasks (text normalization, sentence fusion, sentence splitting & rephrasing, text simplification, and grammatical error correction) and report competitive results across the board.	4
For grammatical error correction, our method speeds up inference by up to 5.2x compared to full sequence models because inference time depends on the number of edits rather than the number of target tokens.	5
For text normalization, sentence fusion, and grammatical error correction, our approach improves explainability by associating each edit operation with a human-readable tag.	5
----------
We study the degree to which neural sequence-to-sequence models exhibit fine-grained controllability when performing natural language generation from a meaning representation.	2
Using two task-oriented dialogue generation benchmarks, we systematically compare the effect of four input linearization strategies on controllability and faithfulness.	3
Additionally, we evaluate how a phrase-based data augmentation method can improve performance.	3
We find that properly aligning input sequences during training leads to highly controllable generation, both when training from scratch or when fine-tuning a larger pre-trained model.	4
Data augmentation further improves control on difficult, randomly generated utterance plans.	5
----------
We propose Blank Language Model (BLM), a model that generates sequences by dynamically creating and filling in blanks.	2
The blanks control which part of the sequence to expand, making BLM ideal for a variety of text editing and rewriting tasks.	3
The model can start from a single blank or partially completed text with blanks at specified locations.	3
It iteratively determines which word to place in a blank and whether to insert new blanks, and stops generating when no blanks are left to fill.	3
BLM can be efficiently trained using a lower bound of the marginal data likelihood.	3
On the task of filling missing text snippets, BLM significantly outperforms all other baselines in terms of both accuracy and fluency.	4
Experiments on style transfer and damaged ancient text restoration demonstrate the potential of this framework for a wide range of applications.	5
----------
We present COD3S, a novel method for generating semantically diverse sentences using neural sequence-to-sequence (seq2seq) models.	2
Conditioned on an input, seq2seqs typically produce semantically and syntactically homogeneous sets of sentences and thus perform poorly on one-to-many sequence generation tasks.	3
Our two-stage approach improves output diversity by conditioning generation on locality-sensitive hash (LSH)-based semantic sentence codes whose Hamming distances highly correlate with human judgments of semantic textual similarity.	3
Though it is generally applicable, we apply to causal generation, the task of predicting a proposition’s plausible causes or effects.	3
We demonstrate through automatic and human evaluation that responses produced using our method exhibit improved diversity without degrading task performance.	4
----------
Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation.	1
However, at the same time it is a tedious, time-consuming task.	1
In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format.	2
We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world’s languages.	3
We apply our framework to all languages included in the Universal Dependencies project, with promising results.	3
Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data.	3
We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%.	4
We release an interface demonstrating the extracted rules at https://neulab.github.io/lase/	6
----------
An intermediate step in the linguistic analysis of an under-documented language is to find and organize inflected forms that are attested in natural speech.	1
From this data, linguists generate unseen inflected word forms in order to test hypotheses about the language’s inflectional patterns and to complete inflectional paradigm tables.	1
To get the data linguists spend many hours manually creating interlinear glossed texts (IGTs).	1
We introduce a new task that speeds this process and automatically generates new morphological resources for natural language processing systems: IGT-to-paradigms (IGT2P).	2
IGT2P generates entire morphological paradigms from IGT input.	3
We show that existing morphological reinflection models can solve the task with 21% to 64% accuracy, depending on the language.	4
We further find that (i) having a language expert spend only a few hours cleaning the noisy IGT data improves performance by as much as 21 percentage points, and (ii) POS tags, which are generally considered a necessary part of NLP morphological reinflection input, have no effect on the accuracy of the models considered here.	4+5
----------
Empathy is critical to successful mental health support.	1
Empathy measurement has predominantly occurred in synchronous, face-to-face settings, and may not translate to asynchronous, text-based contexts.	1
Because millions of people use text-based platforms for mental health support, understanding empathy in these contexts is crucial.	1
In this work, we present a computational approach to understanding how empathy is expressed in online mental health platforms.	2
We develop a novel unifying theoretically-grounded framework for characterizing the communication of empathy in text-based conversations.	2+3
We collect and share a corpus of 10k (post, response) pairs annotated using this empathy framework with supporting evidence for annotations (rationales).	3
We develop a multi-task RoBERTa-based bi-encoder model for identifying empathy in conversations and extracting rationales underlying its predictions.	3
Experiments demonstrate that our approach can effectively identify empathic conversations.	4
We further apply this model to analyze 235k mental health interactions and show that users do not self-learn empathy over time, revealing opportunities for empathy training and feedback.	4
----------
Emotions and their evolution play a central role in creating a captivating story.	1
In this paper, we present the first study on modeling the emotional trajectory of the protagonist in neural storytelling.	2
We design methods that generate stories that adhere to given story titles and desired emotion arcs for the protagonist.	3
Our models include Emotion Supervision (EmoSup) and two Emotion-Reinforced (EmoRL) models.	3
The EmoRL models use special rewards designed to regularize the story generation process through reinforcement learning.	3
Our automatic and manual evaluations demonstrate that these models are significantly better at generating stories that follow the desired emotion arcs compared to baseline methods, without sacrificing story quality.	4
----------
Intimacy is a fundamental aspect of how we relate to others in social settings.	1
Language encodes the social information of intimacy through both topics and other more subtle cues (such as linguistic hedging and swearing).	1
Here, we introduce a new computational framework for studying expressions of the intimacy in language with an accompanying dataset and deep learning model for accurately predicting the intimacy level of questions (Pearson r = 0.87).	2
Through analyzing a dataset of 80.5M questions across social media, books, and films, we show that individuals employ interpersonal pragmatic moves in their language to align their intimacy with social settings.	3+4
Then, in three studies, we further demonstrate how individuals modulate their intimacy to match social norms around gender, social distance, and audience, each validating key findings from studies in social psychology.	4
Our work demonstrates that intimacy is a pervasive and impactful social dimension of language.	4
----------
Subevents elaborate an event and widely exist in event descriptions.	1
Subevent knowledge is useful for discourse analysis and event-centric applications.	1
Acknowledging the scarcity of subevent knowledge, we propose a weakly supervised approach to extract subevent relation tuples from text and build the first large scale subevent knowledge base.	2
We first obtain the initial set of event pairs that are likely to have the subevent relation, by exploiting two observations that 1) subevents are temporally contained by the parent event, and 2) the definitions of the parent event can be used to further guide the identification of subevents.	3
Then, we collect rich weak supervision using the initial seed subevent pairs to train a contextual classifier using BERT and apply the classifier to identify new subevent pairs.	3
The evaluation showed that the acquired subevent tuples (239K) are of high quality (90.1% accuracy) and cover a wide range of event types.	4
The acquired subevent knowledge has been shown useful for discourse analysis and identifying a range of event-event relations.	4
----------
We introduce Biomedical Event Extraction as Sequence Labeling (BeeSL), a joint end-to-end neural information extraction model.	2
BeeSL recasts the task as sequence labeling, taking advantage of a multi-label aware encoding strategy and jointly modeling the intermediate tasks via multi-task learning.	3
BeeSL is fast, accurate, end-to-end, and unlike current methods does not require any external knowledge base or preprocessing tools.	3
BeeSL outperforms the current best system (Li et al., 2019) on the Genia 2011 benchmark by 1.57% absolute F1 score reaching 60.22% F1, establishing a new state of the art for the task.	3
Importantly, we also provide first results on biomedical event extraction without gold entity information.	4
Empirical results show that BeeSL’s speed and accuracy makes it a viable approach for large-scale real-world scenarios.	4
----------
Personal knowledge about users’ professions, hobbies, favorite food, and travel preferences, among others, is a valuable asset for individualized AI, such as recommenders or chatbots.	1
Conversations in social media, such as Reddit, are a rich source of data for inferring personal facts.	1
Prior work developed supervised methods to extract this knowledge, but these approaches can not generalize beyond attribute values with ample labeled training samples.	1
This paper overcomes this limitation by devising CHARM: a zero-shot learning method that creatively leverages keyword extraction and document retrieval in order to predict attribute values that were never seen during training.	2+3
Experiments with large datasets from Reddit show the viability of CHARM for open-ended attributes, such as professions and hobbies.	5
----------
Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance.	1
However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction.	1
In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance.	1
In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate.	2
We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.	2
The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.	4
----------
In this paper, we propose a neural architecture and a set of training methods for ordering events by predicting temporal relations.	2
Our proposed models receive a pair of events within a span of text as input and they identify temporal relations (Before, After, Equal, Vague) between them.	3
Given that a key challenge with this task is the scarcity of annotated data, our models rely on either pretrained representations (i.e. RoBERTa, BERT or ELMo), transfer and multi-task learning (by leveraging complementary datasets), and self-training techniques.	3
Experiments on the MATRES dataset of English documents establish a new state-of-the-art on this task.	4
----------
We propose an end-to-end approach for synthetic QA data generation.	2
Our model comprises a single transformer-based encoder-decoder network that is trained end-to-end to generate both answers and questions.	3
In a nutshell, we feed a passage to the encoder and ask the decoder to generate a question and an answer token-by-token.	3
The likelihood produced in the generation process is used as a filtering score, which avoids the need for a separate filtering model.	3
Our generator is trained by fine-tuning a pretrained LM using maximum likelihood estimation.	3
The experimental results indicate significant improvements in the domain adaptation of QA models outperforming current state-of-the-art methods.	4
----------
Transfer learning techniques are particularly useful for NLP tasks where a sizable amount of high-quality annotated data is difficult to obtain.	1
Current approaches directly adapt a pretrained language model (LM) on in-domain text before fine-tuning to downstream tasks.	1
We show that extending the vocabulary of the LM with domain-specific terms leads to further gains.	2
To a bigger effect, we utilize structure in the unlabeled data to create auxiliary synthetic tasks, which helps the LM transfer to downstream tasks.	2+3
We apply these approaches incrementally on a pretrained Roberta-large LM and show considerable performance gain on three tasks in the IT domain: Extractive Reading Comprehension, Document Ranking and Duplicate Question Detection.	4
----------
Textbook Question Answering is a complex task in the intersection of Machine Comprehension and Visual Question Answering that requires reasoning with multimodal information from text and diagrams.	1
For the first time, this paper taps on the potential of transformer language models and bottom-up and top-down attention to tackle the language and visual understanding challenges this task entails.	2
Rather than training a language-visual transformer from scratch we rely on pre-trained transformers, fine-tuning and ensembling.	3
We add bottom-up and top-down attention to identify regions of interest corresponding to diagram constituents and their relationships, improving the selection of relevant visual information for each question and answer options.	3
Our system ISAAQ reports unprecedented success in all TQA question types, with accuracies of 81.36%, 71.11% and 55.12% on true/false, text-only and diagram multiple choice questions.	4
ISAAQ also demonstrates its broad applicability, obtaining state-of-the-art results in other demanding datasets.	5
----------
We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining.	2
Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled.	1
Thus they cannot incorporate visual information when encoding plain text alone.	1
In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network.	2
MACD forces the decoupled text encoder to represent the visual information via contrastive learning.	3
Therefore, it embeds visual knowledge even for plain text inference.	3
We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B).	3
The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.	4
----------
In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses.	2
While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech.	1
We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals.	2
Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another.	3+4
To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.	4
----------
Adversaries may look to steal or attack black-box NLP systems, either for financial gain or to exploit model errors.	1
One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly.	1
We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats.	2
We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs.	3
Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models.	3
Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs.	3
We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems.	3
We use gradient-based attacks that expose inputs which lead to semantically-incorrect translations, dropped content, and vulgar model outputs.	3
To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models.	2+3
This defense degrades the adversary’s BLEU score and attack success rate at some cost in the defender’s BLEU and inference speed.	4
----------
Despite their empirical success, neural networks still have difficulty capturing compositional aspects of natural language.	1
This work proposes a simple data augmentation approach to encourage compositional behavior in neural models for sequence-to-sequence problems.	2
Our approach, SeqMix, creates new synthetic examples by softly combining input/output sequences from the training set.	3
We connect this approach to existing techniques such as SwitchOut and word dropout, and show that these techniques are all essentially approximating variants of a single objective.	3+4
SeqMix consistently yields approximately 1.0 BLEU improvement on five different translation datasets over strong Transformer baselines.	4
On tasks that require strong compositional generalization such as SCAN and semantic parsing, SeqMix also offers further improvements.	4
----------
Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition.	1
We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms.	2
To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model.	2+3
We prove that commonly used incomplete decoding algorithms – greedy search, beam search, top-k sampling, and nucleus sampling – are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length.	4
Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model.	2
Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.	4
----------
Many tasks in natural language processing involve predicting structured outputs, e.g., sequence labeling, semantic role labeling, parsing, and machine translation.	1
Researchers are increasingly applying deep representation learning to these problems, but the structured component of these approaches is usually quite simplistic.	1
In this work, we propose several high-order energy terms to capture complex dependencies among labels in sequence labeling, including several that consider the entire label sequence.	2
We use neural parameterizations for these energy terms, drawing from convolutional, recurrent, and self-attention networks.	3
We use the framework of learning energy-based inference networks (Tu and Gimpel, 2018) for dealing with the difficulties of training and inference with such models.	3
We empirically demonstrate that this approach achieves substantial improvement using a variety of high-order energy terms on four sequence labeling tasks, while having the same decoding speed as simple, local classifiers.	3+4
We also find high-order energies to help in noisy data conditions.	4
----------
Modern neural networks do not always produce well-calibrated predictions, even when trained with a proper scoring function such as cross-entropy.	1
In classification settings, simple methods such as isotonic regression or temperature scaling may be used in conjunction with a held-out dataset to calibrate model outputs.	1
However, extending these methods to structured prediction is not always straightforward or effective; furthermore, a held-out calibration set may not always be available.	1
In this paper, we study ensemble distillation as a general framework for producing well-calibrated structured prediction models while avoiding the prohibitive inference-time cost of ensembles.	2
We validate this framework on two tasks: named-entity recognition and machine translation.	2
We find that, across both tasks, ensemble distillation produces models which retain much of, and occasionally improve upon, the performance and calibration benefits of ensembles, while only requiring a single model during test-time.	4
----------
Aspect-level sentiment analysis aims to recognize the sentiment polarity of an aspect or a target in a comment.	1
Recently, graph convolutional networks based on linguistic dependency trees have been studied for this task.	1
However, the dependency parsing accuracy of commercial product comments or tweets might be unsatisfactory.	1
To tackle this problem, we associate linguistic dependency trees with automatically induced aspectspecific graphs.	2
We propose gating mechanisms to dynamically combine information from word dependency graphs and latent graphs which are learned by self-attention networks.	2
Our model can complement supervised syntactic features with latent semantic dependencies.	3+4
Experimental results on five benchmarks show the effectiveness of our proposed latent models, giving significantly better results than models without using latent graphs.	4
----------
Prior research has recognized the need to associate affective polarities with events and has produced several techniques and lexical resources for identifying affective events.	1
Our research introduces new classification models to assign affective polarity to event phrases.	2
First, we present a BERT-based model for affective event classification and show that the classifier achieves substantially better performance than a large affective event knowledge base.	2
Second, we present a discourse-enhanced self-training method that iteratively improves the classifier with unlabeled data.	2
The key idea is to exploit event phrases that occur with a coreferent sentiment expression.	3
The discourse-enhanced self-training algorithm iteratively labels new event phrases based on both the classifier’s predictions and the polarities of the event’s coreferent sentiment expressions.	3
Our results show that discourse-enhanced self-training further improves both recall and precision for affective event classification.	3+4
----------
Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables.	1
On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process.	1
Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT).	1
We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework.	2
The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent.	3
Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.	4
----------
This paper considers the problem of characterizing stories by inferring properties such as theme and style using written synopses and reviews of movies.	2
We experiment with a multi-label dataset of movie synopses and a tagset representing various attributes of stories (e.g., genre, type of events).	2
Our proposed multi-view model encodes the synopses and reviews using hierarchical attention and shows improvement over methods that only use synopses.	3+4
Finally, we demonstrate how we can take advantage of such a model to extract a complementary set of story-attributes from reviews without direct supervision.	4
We have made our dataset and source code publicly available at https://ritual.uh.edu/multiview-tag-2020.	6
----------
Inflectional variation is a common feature of World Englishes such as Colloquial Singapore English and African American Vernacular English.	1
Although comprehension by human readers is usually unimpaired by non-standard inflections, current NLP systems are not yet robust.	1
We propose Base-Inflection Encoding (BITE), a method to tokenize English text by reducing inflected words to their base forms before reinjecting the grammatical information as special symbols.	2
Fine-tuning pretrained NLP models for downstream tasks using our encoding defends against inflectional adversaries while maintaining performance on clean data.	3
Models using BITE generalize better to dialects with non-standard inflections without explicit training and translation models converge faster when trained with BITE.	3
Finally, we show that our encoding improves the vocabulary efficiency of popular data-driven subword tokenizers.	4
Since there has been no prior work on quantitatively evaluating vocabulary efficiency, we propose metrics to do so.	4
----------
A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories.	1
How similar are these gender systems across languages?	1
To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation.	2+3
We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information theoretic metrics for measuring similarity between gender systems.	3
We first validate our metrics, then use them to measure gender system similarity in 20 languages.	3
We then ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages.	3
Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages.	3
Of particular interest, languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.	3
----------
The performance of the Chinese Word Segmentation (CWS) systems has gradually reached a plateau with the rapid development of deep neural networks, especially the successful use of large pre-trained models.	1
In this paper, we take stock of what we have achieved and rethink what’s left in the CWS task.	2
Methodologically, we propose a fine-grained evaluation for existing CWS systems, which not only allows us to diagnose the strengths and weaknesses of existing models (under the in-dataset setting), but enables us to quantify the discrepancy between different criterion and alleviate the negative transfer problem when doing multi-criteria learning.	3+4
Strategically, despite not aiming to propose a novel model in this paper, our comprehensive experiments on eight models and seven datasets, as well as thorough analysis, could search for some promising direction for future research.	5
We make all codes publicly available and release an interface that can quickly evaluate and diagnose user’s models: https://github.com/neulab/InterpretEval	6
----------
We demonstrate a program that learns to pronounce Chinese text in Mandarin, without a pronunciation dictionary.	2
From non-parallel streams of Chinese characters and Chinese pinyin syllables, it establishes a many-to-many mapping between characters and pronunciations.	3
Using unsupervised methods, the program effectively deciphers writing into speech.	3+4
Its token-level character-to-syllable accuracy is 89%, which significantly exceeds the 22% accuracy of prior work.	4
----------
Multi-hop reasoning has been widely studied in recent years to seek an effective and interpretable method for knowledge graph (KG) completion.	1
Most previous reasoning methods are designed for dense KGs with enough paths between entities, but cannot work well on those sparse KGs that only contain sparse paths for reasoning.	1
On the one hand, sparse KGs contain less information, which makes it difficult for the model to choose correct paths.	1
On the other hand, the lack of evidential paths to target entities also makes the reasoning process difficult.	1
To solve these problems, we propose a multi-hop reasoning model over sparse KGs, by applying novel dynamic anticipation and completion strategies: (1) The anticipation strategy utilizes the latent prediction of embedding-based models to make our model perform more potential path search over sparse KGs.	2+3
(2) Based on the anticipation information, the completion strategy dynamically adds edges as additional actions during the path search, which further alleviates the sparseness problem of KGs.	3
The experimental results on five datasets sampled from Freebase, NELL and Wikidata show that our method outperforms state-of-the-art baselines.	4
Our codes and datasets can be obtained from https://github.com/THU-KEG/DacKGR.	6
----------
Capturing associations for knowledge graphs (KGs) through entity alignment, entity type inference and other related tasks benefits NLP applications with comprehensive knowledge representations.	1
Recent related methods built on Euclidean embeddings are challenged by the hierarchical structures and different scales of KGs.	1
They also depend on high embedding dimensions to realize enough expressiveness.	1
Differently, we explore with low-dimensional hyperbolic embeddings for knowledge association.	2+3
We propose a hyperbolic relational graph neural network for KG embedding and capture knowledge associations with a hyperbolic transformation.	2+3
Extensive experiments on entity alignment and type inference demonstrate the effectiveness and efficiency of our method.	3+4
----------
Extracting event temporal relations is a critical task for information extraction and plays an important role in natural language understanding.	1
Prior systems leverage deep learning and pre-trained language models to improve the performance of the task.	1
However, these systems often suffer from two shortcomings: 1) when performing maximum a posteriori (MAP) inference based on neural models, previous systems only used structured knowledge that is assumed to be absolutely correct, i.e., hard constraints; 2) biased predictions on dominant temporal relations when training with a limited amount of data.	1
To address these issues, we propose a framework that enhances deep neural network with distributional constraints constructed by probabilistic domain knowledge.	2
We solve the constrained inference problem via Lagrangian Relaxation and apply it to end-to-end event temporal relation extraction tasks.	2+3
Experimental results show our framework is able to improve the baseline neural network models with strong statistical significance on two widely used datasets in news and clinical domains.	4
----------
Inferring missing facts in temporal knowledge graphs (TKGs) is a fundamental and challenging task.	1
Previous works have approached this problem by augmenting methods for static knowledge graphs to leverage time-dependent representations.	1
However, these methods do not explicitly leverage multi-hop structural information and temporal facts from recent time steps to enhance their predictions.	1
Additionally, prior work does not explicitly address the temporal sparsity and variability of entity distributions in TKGs.	1
We propose the Temporal Message Passing (TeMP) framework to address these challenges by combining graph neural networks, temporal dynamics models, data imputation and frequency-based gating techniques.	2
Experiments on standard TKG tasks show that our approach provides substantial gains compared to the previous state of the art, achieving a 10.7% average relative improvement in Hits@10 across three standard benchmarks.	4
Our analysis also reveals important sources of variability both within and across TKG datasets, and we introduce several simple but strong baselines that outperform the prior state of the art in certain settings.	4
----------
Transformers have proved effective in many NLP tasks.	1
However, their training requires non-trivial efforts regarding carefully designing cutting-edge optimizers and learning rate schedulers (e.g., conventional SGD fails to train Transformers effectively).	1
Our objective here is to understand __what complicates Transformer training__ from both empirical and theoretical perspectives.	2
Our analysis reveals that unbalanced gradients are not the root cause of the instability of training.	3
Instead, we identify an amplification effect that influences training substantially—for each layer in a multi-layer Transformer model, heavy dependency on its residual branch makes training unstable, since it amplifies small parameter perturbations (e.g., parameter updates) and results in significant disturbances in the model output.	3
Yet we observe that a light dependency limits the model potential and leads to inferior trained models.	4
Inspired by our analysis, we propose Admin (Adaptive model initialization) to stabilize the early stage’s training and unleash its full potential in the late stage.	4
Extensive experiments show that Admin is more stable, converges faster, and leads to better performance	5
----------
In this work, we present an empirical study of generation order for machine translation.	2
Building on recent advances in insertion-based modeling, we first introduce a soft order-reward framework that enables us to train models to follow arbitrary oracle generation policies.	3
We then make use of this framework to explore a large variety of generation orders, including uninformed orders, location-based orders, frequency-based orders, content-based orders, and model-based orders.	3
Curiously, we find that for the WMT’14 English → German and WMT’18 English → Chinese translation tasks, order does not have a substantial impact on output quality.	4
Moreover, for English → German, we even discover that unintuitive orderings such as alphabetical and shortest-first can match the performance of a standard Transformer, suggesting that traditional left-to-right generation may not be necessary to achieve high performance.	4+5
----------
Conditional masked language model (CMLM) training has proven successful for non-autoregressive and semi-autoregressive sequence generation tasks, such as machine translation.	1
Given a trained CMLM, however, it is not clear what the best inference strategy is.	1
We formulate masked inference as a factorization of conditional probabilities of partial sequences, show that this does not harm performance, and investigate a number of simple heuristics motivated by this perspective.	2+3
We identify a thresholding strategy that has advantages over the standard “mask-predict” algorithm, and provide analyses of its behavior on machine translation tasks.	3
----------
Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer.	1
In this paper, we introduce AmbigQA, a new open-domain question answering task which involves finding every plausible answer, and then rewriting the question for each one to resolve the ambiguity.	2
To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark.	2+3
We find that over half of the questions in NQ-open are ambiguous, with diverse sources of ambiguity such as event and entity references.	4
We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort.	4+5
Our data and baselines are available at https://nlp.cs.washington.edu/ambigqa.	6
----------
In this paper, we propose a novel data augmentation method, referred to as Controllable Rewriting based Question Data Augmentation (CRQDA), for machine reading comprehension (MRC), question generation, and question-answering natural language inference tasks.	2
We treat the question data augmentation task as a constrained question rewriting problem to generate context-relevant, high-quality, and diverse question data samples.	2+3
CRQDA utilizes a Transformer Autoencoder to map the original discrete question into a continuous embedding space.	3
It then uses a pre-trained MRC model to revise the question representation iteratively with gradient-based optimization.	3
Finally, the revised question representations are mapped back into the discrete space, which serve as additional question data.	3
Comprehensive experiments on SQuAD 2.0, SQuAD 1.1 question generation, and QNLI tasks demonstrate the effectiveness of CRQDA.	4
----------
Complex question-answering (CQA) involves answering complex natural-language questions on a knowledge base (KB).	1
However, the conventional neural program induction (NPI) approach exhibits uneven performance when the questions have different types, harboring inherently different characteristics, e.g., difficulty level.	1
This paper proposes a meta-reinforcement learning approach to program induction in CQA to tackle the potential distributional bias in questions.	2
Our method quickly and effectively adapts the meta-learned programmer to new questions based on the most similar questions retrieved from the training data.	4
The meta-learned policy is then used to learn a good programming policy, utilizing the trial trajectories and their rewards for similar questions in the support set.	4
Our method achieves state-of-the-art performance on the CQA dataset (Saha et al., 2018) while using only five trial trajectories for the top-5 retrieved questions in each support set, and meta-training on tasks constructed from only 1% of the training set.	5
We have released our code at https://github.com/DevinJake/MRL-CQA.	6
----------
Offensive content is pervasive in social media and a reason for concern to companies and government organizations.	1
Several studies have been recently published investigating methods to detect the various forms of such content (e.g. hate speech, cyberbulling, and cyberaggression).	1
The clear majority of these studies deal with English partially because most annotated datasets available contain English data.	1
In this paper, we take advantage of English data available by applying cross-lingual contextual word embeddings and transfer learning to make predictions in languages with less resources.	2
We project predictions on comparable data in Bengali, Hindi, and Spanish and we report results of 0.8415 F1 macro for Bengali, 0.8568 F1 macro for Hindi, and 0.7513 F1 macro for Spanish.	2
Finally, we show that our approach compares favorably to the best systems submitted to recent shared tasks on these three languages, confirming the robustness of cross-lingual contextual embeddings and transfer learning for this task.	3+4
----------
We solve difficult word-based substitution codes by constructing a decoding lattice and searching that lattice with a neural language model.	2
We apply our method to a set of enciphered letters exchanged between US Army General James Wilkinson and agents of the Spanish Crown in the late 1700s and early 1800s, obtained from the US Library of Congress.	3
We are able to decipher 75.1% of the cipher-word tokens correctly.	4
----------
Although prediction of dialects is an important language processing task, with a wide range of applications, existing work is largely limited to coarse-grained varieties.	1
Inspired by geolocation research, we propose the novel task of Micro-Dialect Identification (MDI) and introduce MARBERT, a new language model with striking abilities to predict a fine-grained variety (as small as that of a city) given a single, short message.	2
For modeling, we offer a range of novel spatially and linguistically-motivated multi-task learning models.	2
To showcase the utility of our models, we introduce a new, large-scale dataset of Arabic micro-varieties (low-resource) suited to our tasks.	2
MARBERT predicts micro-dialects with 9.9% F1, 76 better than a majority class baseline.	4
Our new language model also establishes new state-of-the-art on several external tasks.	5
----------
In this paper, we focus on the domain-specific translation with low resources, where in-domain parallel corpora are scarce or nonexistent.	1
One common and effective strategy for this case is exploiting in-domain monolingual data with the back-translation method.	1
However, the synthetic parallel data is very noisy because they are generated by imperfect out-of-domain systems, resulting in the poor performance of domain adaptation.	1
To address this issue, we propose a novel iterative domain-repaired back-translation framework, which introduces the Domain-Repair (DR) model to refine translations in synthetic bilingual data.	2
To this end, we construct corresponding data for the DR model training by round-trip translating the monolingual sentences, and then design the unified training framework to optimize paired DR and NMT models jointly.	2+3
Experiments on adapting NMT models between specific domains and from the general domain to specific domains demonstrate the effectiveness of our proposed approach, achieving 15.79 and 4.47 BLEU improvements on average over unadapted models and back-translation.	3+4
----------
Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance.	1
Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality and reflect the target domain.	1
To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text.	2
In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models.	2
In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration.	2
We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs.	3
Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.	4
----------
We propose Semantic Parser Localizer (SPL), a toolkit that leverages Neural Machine Translation (NMT) systems to localize a semantic parser for a new language.	2
Our methodology is to (1) generate training data automatically in the target language by augmenting machine-translated datasets with local entities scraped from public websites, (2) add a few-shot boost of human-translated sentences and train a novel XLMR-LSTM semantic parser, and (3) test the model on natural utterances curated using human translators.	3
We assess the effectiveness of our approach by extending the current capabilities of Schema2QA, a system for English Question Answering (QA) on the open web, to 10 new languages for the restaurants and hotels domains.	3
Our model achieves an overall test accuracy ranging between 61% and 69% for the hotels domain and between 64% and 78% for restaurants domain, which compares favorably to 69% and 80% obtained for English parser trained on gold English data and a few examples from validation set.	4
We show our approach outperforms the previous state-of-the-art methodology by more than 30% for hotels and 40% for restaurants with localized ontologies for the subset of languages tested.	4
Our methodology enables any software developer to add a new language capability to a QA system for a new domain, leveraging machine translation, in less than 24 hours.	5
Our code is released open-source.	6
----------
Cross-lingual word embeddings transfer knowledge between languages: models trained on high-resource languages can predict in low-resource languages.	1
We introduce CLIME, an interactive system to quickly refine cross-lingual word embeddings for a given classification problem.	2
First, CLIME ranks words by their salience to the downstream task.	3
Then, users mark similarity between keywords and their nearest neighbors in the embedding space.	3
Finally, CLIME updates the embeddings using the annotations.	3
We evaluate CLIME on identifying health-related text in four low-resource languages: Ilocano, Sinhalese, Tigrinya, and Uyghur.	3
Embeddings refined by CLIME capture more nuanced word semantics and have higher test accuracy than the original embeddings.	4
CLIME often improves accuracy faster than an active learning baseline and can be easily combined with active learning to improve results.	5
----------
We present a simple document alignment method that incorporates sentence order information in both candidate generation and candidate re-scoring.	2
Our method results in 61% relative reduction in error compared to the best previously published result on the WMT16 document alignment shared task.	3+4
Our method improves downstream MT performance on web-scraped Sinhala–English documents from ParaCrawl, outperforming the document alignment method used in the most recent ParaCrawl release.	3+4
It also outperforms a comparable corpora method which uses the same multilingual embeddings, demonstrating that exploiting sentence order is beneficial even if the end goal is sentence-level bitext.	4
----------
In this paper, we introduce XGLUE, a new benchmark dataset to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, and evaluate their performance across a diverse set of cross-lingual tasks.	2+3
Comparing to GLUE (Wang et al.,2019), which is labeled in English and includes natural language understanding tasks only, XGLUE has three main advantages: (1) it provides two corpora with different sizes for cross-lingual pre-training; (2) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (3) for each task, it provides labeled data in multiple languages.	3
We extend a recent cross-lingual pre-trained model Unicoder (Huang et al., 2019) to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline.	3
We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison.	3
----------
The linear-chain Conditional Random Field (CRF) model is one of the most widely-used neural sequence labeling approaches.	1
Exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are typically applied in training and prediction stages of the CRF model.	1
However, these algorithms require sequential computation that makes parallelization impossible.	1
In this paper, we propose to employ a parallelizable approximate variational inference algorithm for the CRF model.	2
Based on this algorithm, we design an approximate inference network that can be connected with the encoder of the neural CRF model to form an end-to-end network, which is amenable to parallelization for faster training and prediction.	2+3
The empirical results show that our proposed approaches achieve a 12.7-fold improvement in decoding speed with long sentences and a competitive accuracy compared with the traditional CRF approach.	4
----------
Named Entity Recognition (NER) is a fundamental task in natural language processing.	1
In order to identify entities with nested structure, many sophisticated methods have been recently developed based on either the traditional sequence labeling approaches or directed hypergraph structures.	1
Despite being successful, these methods often fall short in striking a good balance between the expression power for nested structure and the model complexity.	1
To address this issue, we present a novel nested NER model named HIT.	2
Our proposed HIT model leverages two key properties pertaining to the (nested) named entity, including (1) explicit boundary tokens and (2) tight internal connection between tokens within the boundary.	3
Specifically, we design (1) Head-Tail Detector based on the multi-head self-attention mechanism and bi-affine classifier to detect boundary tokens, and (2) Token Interaction Tagger based on traditional sequence labeling approaches to characterize the internal token connection within the boundary.	3
Experiments on three public NER datasets demonstrate that the proposed HIT achieves state-of-the-art performance.	4
----------
Supertagging is conventionally regarded as an important task for combinatory categorial grammar (CCG) parsing, where effective modeling of contextual information is highly important to this task.	1
However, existing studies have made limited efforts to leverage contextual features except for applying powerful encoders (e.g., bi-LSTM).	1
In this paper, we propose attentive graph convolutional networks to enhance neural CCG supertagging through a novel solution of leveraging contextual information.	2
Specifically, we build the graph from chunks (n-grams) extracted from a lexicon and apply attention over the graph, so that different word pairs from the contexts within and across chunks are weighted in the model and facilitate the supertagging accordingly.	3
The experiments performed on the CCGbank demonstrate that our approach outperforms all previous studies in terms of both supertagging and parsing.	4
Further analyses illustrate the effectiveness of each component in our approach to discriminatively learn from word pairs to enhance CCG supertagging.	5
----------
Data augmentation techniques have been widely used to improve machine learning performance as they facilitate generalization.	1
In this work, we propose a novel augmentation method to generate high quality synthetic data for low-resource tagging tasks with language models trained on the linearized labeled sentences.	2
Our method is applicable to both supervised and semi-supervised settings.	3
For the supervised settings, we conduct extensive experiments on named entity recognition (NER), part of speech (POS) tagging and end-to-end target based sentiment analysis (E2E-TBSA) tasks.	3
For the semi-supervised settings, we evaluate our method on the NER task under the conditions of given unlabeled data only and unlabeled data plus a knowledge base.	3
The results show that our method can consistently outperform the baselines, particularly when the given gold training data are less.	4
----------
With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits.	1
Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices.	1
In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task.	1+2
The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems.	3+4
By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: https://github.com/neulab/InterpretEval	6
----------
Open-vocabulary slots, such as file name, album name, or schedule title, significantly degrade the performance of neural-based slot filling models since these slots can take on values from a virtually unlimited set and have no semantic restriction nor a length limit.	1
In this paper, we propose a robust adversarial model-agnostic slot filling method that explicitly decouples local semantics inherent in open-vocabulary slot words from the global context.	2
We aim to depart entangled contextual semantics and focus more on the holistic context at the level of the whole sentence.	2
Experiments on two public datasets show that our method consistently outperforms other methods with a statistically significant margin on all the open-vocabulary slots without deteriorating the performance of normal slots.	4
----------
Text autoencoders are commonly used for conditional generation tasks such as style transfer.	1
We propose methods which are plug and play, where any pretrained autoencoder can be used, and only require learning a mapping within the autoencoder’s embedding space, training embedding-to-embedding (Emb2Emb).	2+3
This reduces the need for labeled training data for the task and makes the training procedure more efficient.	2
Crucial to the success of this method is a loss term for keeping the mapped embedding on the manifold of the autoencoder and a mapping which is trained to navigate the manifold by learning offset vectors.	3
Evaluations on style transfer tasks both with and without sequence-to-sequence supervision show that our method performs better than or comparable to strong baselines while being up to four times faster.	4
----------
Learning low-dimensional representations for entities and relations in knowledge graphs using contrastive estimation represents a scalable and effective method for inferring connectivity patterns.	1
A crucial aspect of contrastive learning approaches is the choice of corruption distribution that generates hard negative samples, which force the embedding model to learn discriminative representations and find critical characteristics of observed data.	1+2
While earlier methods either employ too simple corruption distributions, i.e. uniform, yielding easy uninformative negatives or sophisticated adversarial distributions with challenging optimization schemes, they do not explicitly incorporate known graph structure resulting in suboptimal negatives.	1
In this paper, we propose Structure Aware Negative Sampling (SANS), an inexpensive negative sampling strategy that utilizes the rich graph structure by selecting negative samples from a node’s k-hop neighborhood.	2+3
Empirically, we demonstrate that SANS finds semantically meaningful negatives and is competitive with SOTA approaches while requires no additional parameters nor difficult adversarial optimization.	4
----------
We propose a method to automatically generate a domain- and task-adaptive maskings of the given text for self-supervised pre-training, such that we can effectively adapt the language model to a particular target task (e.g. question answering).	1+2
Specifically, we present a novel reinforcement learning-based framework which learns the masking policy, such that using the generated masks for further pre-training of the target language model helps improve task performance on unseen texts.	3
We use off-policy actor-critic with entropy regularization and experience replay for reinforcement learning, and propose a Transformer-based policy network that can consider the relative importance of words in a given text.	3
We validate our Neural Mask Generator (NMG) on several question answering and text classification datasets using BERT and DistilBERT as the language models, on which it outperforms rule-based masking strategies, by automatically learning optimal adaptive maskings.	3+4
----------
The performance of autoregressive models on natural language generation tasks has dramatically improved due to the adoption of deep, self-attentive architectures.	1
However, these gains have come at the cost of hindering inference speed, making state-of-the-art models cumbersome to deploy in real-world, time-sensitive settings.	1
We develop a compression technique for autoregressive models that is driven by an imitation learning perspective on knowledge distillation.	2+3
The algorithm is designed to address the exposure bias problem.	1+2
On prototypical language generation tasks such as translation and summarization, our method consistently outperforms other distillation algorithms, such as sequence-level knowledge distillation.	3+4
Student models trained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those trained from scratch, while increasing inference speed by up to 14 times in comparison to the teacher model.	3+4
----------
Adversarial attacks against natural language processing systems, which perform seemingly innocuous modifications to inputs, can induce arbitrary mistakes to the target models.	1
Though raised great concerns, such adversarial attacks can be leveraged to estimate the robustness of NLP models.	1
Compared with the adversarial example generation in continuous data domain (e.g., image), generating adversarial text that preserves the original meaning is challenging since the text space is discrete and non-differentiable.	1
To handle these challenges, we propose a target-controllable adversarial attack framework T3, which is applicable to a range of NLP tasks.	2
In particular, we propose a tree-based autoencoder to embed the discrete text data into a continuous representation space, upon which we optimize the adversarial perturbation.	3
A novel tree-based decoder is then applied to regularize the syntactic correctness of the generated text and manipulate it on either sentence (T3(Sent)) or word (T3(Word)) level.	3
We consider two most representative NLP tasks: sentiment analysis and question answering (QA).	3
Extensive experimental results and human studies show that T3 generated adversarial texts can successfully manipulate the NLP models to output the targeted incorrect answer without misleading the human.	4
Moreover, we show that the generated adversarial texts have high transferability which enables the black-box attacks in practice.	4
Our work sheds light on an effective and general way to examine the robustness of NLP models.	3
Our code is publicly available at https://github.com/AI-secure/T3/.	1
----------
Large language models have recently achieved state of the art performance across a wide variety of natural language tasks.	1
Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large?	1
We study this question through the lens of model compression.	2
We present a generic, structured pruning approach by parameterizing each weight matrix using its low-rank factorization, and adaptively removing rank-1 components during training.	3
On language modeling tasks, our structured approach outperforms other unstructured and block-structured pruning baselines at various compression levels, while achieving significant speedups during both training and inference.	3+4
We also demonstrate that our method can be applied to pruning adaptive word embeddings in large language models, and to pruning the BERT model on several downstream fine-tuning classification benchmarks.	3+4
----------
Recent work has shown the importance of adaptation of broad-coverage contextualised embedding models on the domain of the target task of interest.	1
Current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens.	1
In this paper, we show that careful masking strategies can bridge the knowledge gap of masked language models (MLMs) about the domains more effectively by allocating self-supervision where it is needed.	1
Furthermore, we propose an effective training strategy by adversarially masking out those tokens which are harder to reconstruct by the underlying MLM.	2+3
The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lowerbound and dynamic programming.	3
On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements.	3+4
----------
Modern text classification models are susceptible to adversarial examples, perturbed versions of the original text indiscernible by humans which get misclassified by the model.	1
Recent works in NLP use rule-based synonym replacement strategies to generate adversarial examples.	1
These strategies can lead to out-of-context and unnaturally complex token replacements, which are easily identifiable by humans.	1
We present BAE, a black box attack for generating adversarial examples using contextual perturbations from a BERT masked language model.	1+2
BAE replaces and inserts tokens in the original text by masking a portion of the text and leveraging the BERT-MLM to generate alternatives for the masked tokens.	3
Through automatic and human evaluations, we show that BAE performs a stronger attack, in addition to generating adversarial examples with improved grammaticality and semantic coherence as compared to prior work.	3+4
----------
Large pre-trained transformer-based language models have achieved impressive results on a wide range of NLP tasks.	1
In the past few years, Knowledge Distillation(KD) has become a popular paradigm to compress a computationally expensive model to a resource-efficient lightweight model.	1
However, most KD algorithms, especially in NLP, rely on the accessibility of the original training dataset, which may be unavailable due to privacy issues.	1
To tackle this problem, we propose a novel two-stage data-free distillation method, named Adversarial self-Supervised Data-Free Distillation (AS-DFD), which is designed for compressing large-scale transformer-based models (e.g., BERT).	2+3
To avoid text generation in discrete space, we introduce a Plug & Play Embedding Guessing method to craft pseudo embeddings from the teacher’s hidden knowledge.	3
Meanwhile, with a self-supervised module to quantify the student’s ability, we adapt the difficulty of pseudo embeddings in an adversarial training manner.	2+3
To the best of our knowledge, our framework is the first data-free distillation framework designed for NLP tasks.	1
We verify the effectiveness of our method on several text classification datasets.	3
----------
Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods.	1
Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency.	1
In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT.	2
We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly.	2
Our method outperforms state-of-the-art attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved.	3+4
Also, the cost of calculation is low, thus possible for large-scale generations.	2
The code is available at https://github.com/LinyangLee/BERT-Attack.	6
----------
Pre-training in natural language processing makes it easier for an adversary with only query access to a victim model to reconstruct a local copy of the victim by training with gibberish input data paired with the victim’s labels for that data.	1
We discover that this extraction process extends to local copies initialized from a pre-trained, multilingual model while the victim remains monolingual.	2
The extracted model learns the task from the monolingual victim, but it generalizes far better than the victim to several other languages.	3+4
This is done without ever showing the multilingual, extracted model a well-formed input in any of the languages for the target task.	3
We also demonstrate that a few real examples can greatly improve performance, and we analyze how these results shed light on how such extraction methods succeed.	5
----------
We address hypernymy detection, i.e., whether an is-a relationship exists between words (x ,y), with the help of large textual corpora.	1
Most conventional approaches to this task have been categorized to be either pattern-based or distributional.	1
Recent studies suggest that pattern-based ones are superior, if large-scale Hearst pairs are extracted and fed, with the sparsity of unseen (x ,y) pairs relieved.	1
However, they become invalid in some specific sparsity cases, where x or y is not involved in any pattern.	1
For the first time, this paper quantifies the non-negligible existence of those specific cases.	1+2
We also demonstrate that distributional methods are ideal to make up for pattern-based ones in such cases.	4
We devise a complementary framework, under which a pattern-based and a distributional model collaborate seamlessly in cases which they each prefer.	2+3
On several benchmark datasets, our framework demonstrates improvements that are both competitive and explainable.	3+4
----------
This paper proposes an open-domain method for automatically annotating modifier constituents (20th-century’) within Wikipedia categories (20th-century male writers) with properties (date of birth).	2
The annotations offer a semantically-anchored understanding of the role of the constituents in defining the underlying meaning of the categories.	1
In experiments over an evaluation set of Wikipedia categories, the proposed method annotates constituent modifiers as semantically-anchored properties, rather than as mere strings in a previous method.	3
It does so at a better trade-off between precision and recall.	3+4
----------
Contextual embeddings are proved to be overwhelmingly effective to the task of Word Sense Disambiguation (WSD) compared with other sense representation techniques.	1
However, these embeddings fail to embed sense knowledge in semantic networks.	1
In this paper, we propose a Synset Relation-Enhanced Framework (SREF) that leverages sense relations for both sense embedding enhancement and a try-again mechanism that implements WSD again, after obtaining basic sense embeddings from augmented WordNet glosses.	2+3
Experiments on all-words and lexical sample datasets show that the proposed system achieves new state-of-the-art results, defeating previous knowledge-based systems by at least 5.5 F1 measure.	4
When the system utilizes sense embeddings learned from SemCor, it outperforms all previous supervised systems with only 20% SemCor data.	4
----------
News headline generation aims to produce a short sentence to attract readers to read the news.	2
One news article often contains multiple keyphrases that are of interest to different users, which can naturally have multiple reasonable headlines.	1
However, most existing methods focus on the single headline generation.	1
In this paper, we propose generating multiple headlines with keyphrases of user interests, whose main idea is to generate multiple keyphrases of interest to users for the news first, and then generate multiple keyphrase-relevant headlines.	2
We propose a multi-source Transformer decoder, which takes three sources as inputs: (a) keyphrase, (b) keyphrase-filtered article, and (c) original article to generate keyphrase-relevant, high-quality, and diverse headlines.	2+3
Furthermore, we propose a simple and effective method to mine the keyphrases of interest in the news article and build a first large-scale keyphrase-aware news headline corpus, which contains over 180K aligned triples of <news article, headline, keyphrase>.	2+3
Extensive experimental comparisons on the real-world dataset show that the proposed method achieves state-of-the-art results in terms of quality and diversity.	3+4
----------
Neural abstractive summarization systems have achieved promising progress, thanks to the availability of large-scale datasets and models pre-trained with self-supervised methods.	1
However, ensuring the factual consistency of the generated summaries for abstractive summarization systems is a challenge.	1
We propose a post-editing corrector module to address this issue by identifying and correcting factual errors in generated summaries.	2
The neural corrector model is pre-trained on artificial examples that are created by applying a series of heuristic transformations on reference summaries.	3
These transformations are inspired by the error analysis of state-of-the-art summarization model outputs.	3
Experimental results show that our model is able to correct factual errors in summaries generated by other neural summarization models and outperforms previous models on factual consistency evaluation on the CNN/DailyMail dataset.	4
We also find that transferring from artificial error correction to downstream settings is still very challenging.	4
----------
Compressive summarization systems typically rely on a seed set of syntactic rules to determine under what circumstances deleting a span is permissible, then learn which compressions to actually apply by optimizing for ROUGE.	1
In this work, we propose to relax these explicit syntactic constraints on candidate spans, and instead leave the decision about what to delete to two data-driven criteria: plausibility and salience.	1+2
Deleting a span is plausible if removing it maintains the grammaticality and factuality of a sentence, and it is salient if it removes important information from the summary.	3
Each of these is judged by a pre-trained Transformer model, and only deletions that are both plausible and not salient can be applied.	3
When integrated into a simple extraction-compression pipeline, our method achieves strong in-domain results on benchmark datasets, and human evaluation shows that the plausibility model generally selects for grammatical and factual deletions.	3+4
Furthermore, the flexibility of our approach allows it to generalize cross-domain, and we show that our system fine-tuned on only 500 samples from a new domain can match or exceed a strong in-domain extractive model.	4
----------
Amongst the best means to summarize is highlighting.	1
In this paper, we aim to generate summary highlights to be overlaid on the original documents to make it easier for readers to sift through a large amount of text.	2
The method allows summaries to be understood in context to prevent a summarizer from distorting the original meaning, of which abstractive summarizers usually fall short.	2+3
In particular, we present a new method to produce self-contained highlights that are understandable on their own to avoid confusion.	2+3
Our method combines determinantal point processes and deep contextualized representations to identify an optimal set of sub-sentence segments that are both important and non-redundant to form summary highlights.	3
To demonstrate the flexibility and modeling power of our method, we conduct extensive experiments on summarization datasets.	3
Our analysis provides evidence that highlighting is a promising avenue of research towards future summarization.	4+5
----------
Given a document and a target aspect (e.g., a topic of interest), aspect-based abstractive summarization attempts to generate a summary with respect to the aspect.	1
Previous studies usually assume a small pre-defined set of aspects and fall short of summarizing on other diverse topics.	1
In this work, we study summarizing on arbitrary aspects relevant to the document, which significantly expands the application of the task in practice.	1+2
Due to the lack of supervision data, we develop a new weak supervision construction method and an aspect modeling scheme, both of which integrate rich external knowledge sources such as ConceptNet and Wikipedia.	2+3
Experiments show our approach achieves performance boosts on summarizing both real and synthetic documents given pre-defined or arbitrary aspects.	3+4
----------
In this paper, we introduce a novel BERT-enhanced Relational Sentence Ordering Network (referred to as BRSON) by leveraging BERT for capturing better dependency relationship among sentences to enhance the coherence modeling for the entire paragraph.	1+2
In particular, we develop a new Relational Pointer Decoder (referred as RPD) by incorporating the relative ordering information into the pointer network with a Deep Relational Module (referred as DRM), which utilizes BERT to exploit the deep semantic connection and relative ordering between sentences.This enables us to strengthen both local and global dependencies among sentences.	2+3
Extensive evaluations are conducted on six public datasets.	3
The experimental results demonstrate the effectiveness and promise of our BRSON, showing a significant improvement over the state-of-the-art by a wide margin.	4
----------
Huge amounts of textual conversations occur online every day, where multiple conversations take place concurrently.	1
Interleaved conversations lead to difficulties in not only following the ongoing discussions but also extracting relevant information from simultaneous messages.	1
Conversation disentanglement aims to separate intermingled messages into detached conversations.	1
However, existing disentanglement methods rely mostly on handcrafted features that are dataset specific, which hinders generalization and adaptability.	1
In this work, we propose an end-to-end online framework for conversation disentanglement that avoids time-consuming domain-specific feature engineering.	2
We design a novel way to embed the whole utterance that comprises timestamp, speaker, and message text, and propose a custom attention mechanism that models disentanglement as a pointing problem while effectively capturing inter-utterance interactions in an end-to-end fashion.	2+3
We also introduce a joint-learning objective to better capture contextual information.	2
Our experiments on the Ubuntu IRC dataset show that our method achieves state-of-the-art performance in both link and conversation prediction tasks.	4
----------
In this paper, we tackle the task of definition modeling, where the goal is to learn to generate definitions of words and phrases.	1+2
Existing approaches for this task are discriminative, combining distributional and lexical semantics in an implicit rather than direct way.	3
To tackle this issue we propose a generative model for the task, introducing a continuous latent variable to explicitly model the underlying relationship between a phrase used within a context and its definition.	2
We rely on variational inference for estimation and leverage contextualized word embeddings for improved performance.	2+3
Our approach is evaluated on four existing challenging benchmarks with the addition of two new datasets, “Cambridge” and the first non-English corpus “Robert”, which we release to complement our empirical study.	3
Our Variational Contextual Definition Modeler (VCDM) achieves state-of-the-art performance in terms of automatic and human evaluation metrics, demonstrating the effectiveness of our approach.	3+4
----------
More recently, Named Entity Recognition hasachieved great advances aided by pre-trainingapproaches such as BERT.	1
However, currentpre-training techniques focus on building lan-guage modeling objectives to learn a gen-eral representation, ignoring the named entity-related knowledge.	1
To this end, we proposea NER-specific pre-training framework to in-ject coarse-to-fine automatically mined entityknowledge into pre-trained models.	1+2
Specifi-cally, we first warm-up the model via an en-tity span identification task by training it withWikipedia anchors, which can be deemed asgeneral-typed entities.	3
Then we leverage thegazetteer-based distant supervision strategy totrain the model extract coarse-grained typedentities.	3
Finally, we devise a self-supervisedauxiliary task to mine the fine-grained namedentity knowledge via clustering.Empiricalstudies on three public NER datasets demon-strate that our framework achieves significantimprovements against several pre-trained base-lines, establishing the new state-of-the-art per-formance on three benchmarks.	2+3
Besides, weshow that our framework gains promising re-sults without using human-labeled trainingdata, demonstrating its effectiveness in label-few and low-resource scenarios.	3+4
----------
Entity alignment (EA) aims at building a unified Knowledge Graph (KG) of rich content by linking the equivalent entities from various KGs.	2
GNN-based EA methods present promising performance by modeling the KG structure defined by relation triples.	3
However, attribute triples can also provide crucial alignment signal but have not been well explored yet.	1
In this paper, we propose to utilize an attributed value encoder and partition the KG into subgraphs to model the various types of attribute triples efficiently.	2+3
Besides, the performances of current EA methods are overestimated because of the name-bias of existing EA datasets.	3
To make an objective evaluation, we propose a hard experimental setting where we select equivalent entity pairs with very different names as the test set.	3
Under both the regular and hard settings, our method achieves significant improvements (5.10% on average Hits@1 in DBP15k) over 12 baselines in cross-lingual and monolingual datasets.	4
Ablation studies on different subgraphs and a case study about attribute types further demonstrate the effectiveness of our method.	4
Source code and data can be found at https://github.com/thunlp/explore-and-evaluate.	6
----------
We present a simple few-shot named entity recognition (NER) system based on nearest neighbor learning and structured inference.	2+3
Our system uses a supervised NER model trained on the source domain, as a feature extractor.	3
Across several test domains, we show that a nearest neighbor classifier in this feature-space is far more effective than the standard meta-learning approaches.	4
We further propose a cheap but effective method to capture the label dependencies between entity tags without expensive CRF training.	3
We show that our method of combining structured decoding with nearest neighbor learning achieves state-of-the-art performance on standard few-shot NER evaluation tasks, improving F1 scores by 6% to 16% absolute points over prior meta-learning based systems.	4
----------
Structured representations of entity names are useful for many entity-related tasks such as entity normalization and variant generation.	1
Learning the implicit structured representations of entity names without context and external knowledge is particularly challenging.	1
In this paper, we present a novel learning framework that combines active learning and weak supervision to solve this problem.	2+3
Our experimental evaluation show that this framework enables the learning of high-quality models from merely a dozen or so labeled examples.	4
----------
Character-level BERT pre-trained in Chinese suffers a limitation of lacking lexicon information, which shows effectiveness for Chinese NER.	1
To integrate the lexicon into pre-trained LMs for Chinese NER, we investigate a semi-supervised entity enhanced BERT pre-training method.	2+3
In particular, we first extract an entity lexicon from the relevant raw text using a new-word discovery method.	3
We then integrate the entity information into BERT using Char-Entity-Transformer, which augments the self-attention using a combination of character and entity representations.	3
In addition, an entity classification task helps inject the entity information into model parameters in pre-training.	3
The pre-trained models are used for NER fine-tuning.	3
Experiments on a news dataset and two datasets annotated by ourselves for NER in long-text show that our method is highly effective and achieves the best results.	4
----------
This paper introduces a conceptually simple, scalable, and highly effective BERT-based entity linking model, along with an extensive evaluation of its accuracy-speed trade-off.	1
We present a two-stage zero-shot linking algorithm, where each entity is defined only by a short textual description.	2+3
The first stage does retrieval in a dense space defined by a bi-encoder that independently embeds the mention context and the entity descriptions.	3
Each candidate is then re-ranked with a cross-encoder, that concatenates the mention and entity text.	3
Experiments demonstrate that this approach is state of the art on recent zero-shot benchmarks (6 point absolute gains) and also on more established non-zero-shot evaluations (e.g. TACKBP-2010), despite its relative simplicity (e.g. no explicit entity embeddings or manually engineered mention tables).	4
We also show that bi-encoder linking is very fast with nearest neighbor search (e.g. linking with 5.9 million candidates in 2 milliseconds), and that much of the accuracy gain from the more expensive cross-encoder can be transferred to the bi-encoder via knowledge distillation.	4
Our code and models are available at https://github.com/facebookresearch/BLINK.	6
----------
We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass.	2+3
Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively.	3+4
With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems.	3
In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever.	4
----------
Entity representations are useful in natural language tasks involving entities.	1
In this paper, we propose new pretrained contextualized representations of words and entities based on the bidirectional transformer.	2
The proposed model treats words and entities in a given text as independent tokens, and outputs contextualized representations of them.	3
Our model is trained using a new pretraining task based on the masked language model of BERT.	3
The task involves predicting randomly masked words and entities in a large entity-annotated corpus retrieved from Wikipedia.	3
We also propose an entity-aware self-attention mechanism that is an extension of the self-attention mechanism of the transformer, and considers the types of tokens (words or entities) when computing attention scores.	3
The proposed model achieves impressive empirical performance on a wide range of entity-related tasks.	4
In particular, it obtains state-of-the-art results on five well-known datasets: Open Entity (entity typing), TACRED (relation classification), CoNLL-2003 (named entity recognition), ReCoRD (cloze-style question answering), and SQuAD 1.1 (extractive question answering).	4
Our source code and pretrained representations are available at https://github.com/studio-ousia/luke.	6
----------
Literary tropes, from poetry to stories, are at the crux of human imagination and communication.	1
Figurative language such as a simile go beyond plain expressions to give readers new insights and inspirations.	1
In this paper, we tackle the problem of simile generation.	2
Generating a simile requires proper understanding for effective mapping of properties between two concepts.	1
To this end, we first propose a method to automatically construct a parallel corpus by transforming a large number of similes collected from Reddit to their literal counterpart using structured common sense knowledge.	3
We then propose to fine-tune a pre-trained sequence to sequence model, BART (Lewis et al 2019), on the literal-simile pairs to gain generalizability, so that we can generate novel similes given a literal sentence.	3
Experiments show that our approach generates 88% novel similes that do not share properties with the training data.	4
Human evaluation on an independent set of literal statements shows that our model generates similes better than two literary experts 37% of the time when compared pairwise.	4
We also show how replacing literal sentences with similes from our best model in machine-generated stories improves evocativeness and leads to better acceptance by human judges.	4
----------
Existing language models excel at writing from scratch, but many real-world scenarios require rewriting an existing document to fit a set of constraints.	1
Although sentence-level rewriting has been fairly well-studied, little work has addressed the challenge of rewriting an entire document coherently.	1
In this work, we introduce the task of document-level targeted content transfer and address it in the recipe domain, with a recipe as the document and a dietary restriction (such as vegan or dairy-free) as the targeted constraint.	2+3
We propose a novel model for this task based on the generative pre-trained language model (GPT-2) and train on a large number of roughly-aligned recipe pairs.	2+3
Both automatic and human evaluations show that our model out-performs existing methods by generating coherent and diverse rewrites that obey the constraint while remaining close to the original document.	3+4
Finally, we analyze our model’s rewrites to assess progress toward the goal of making language generation more attuned to constraints that are substantive rather than stylistic.	3
----------
Virtual assistants such as Google Assistant, Amazon Alexa, and Apple Siri enable users to interact with a large number of services and APIs on the web using natural language.	1
In this work, we investigate two methods for Natural Language Generation (NLG) using a single domain-independent model across a large number of APIs.	2+3
First, we propose a schema-guided approach which conditions the generation on a schema describing the API in natural language.	2+3
Our second method investigates the use of a small number of templates, growing linearly in number of slots, to convey the semantics of the API.	3
To generate utterances for an arbitrary slot combination, a few simple templates are first concatenated to give a semantically correct, but possibly incoherent and ungrammatical utterance.	3
A pre-trained language model is subsequently employed to rewrite it into coherent, natural sounding text.	3
Through automatic metrics and human evaluation, we show that our method improves over strong baselines, is robust to out-of-domain inputs and shows improved sample efficiency.	4
----------
Despite the recent success of contextualized language models on various NLP tasks, language model itself cannot capture textual coherence of a long, multi-sentence document (e.g., a paragraph).	1
Humans often make structural decisions on what and how to say about before making utterances.	1
Guiding surface realization with such high-level decisions and structuring text in a coherent way is essentially called a planning process.	1
Where can the model learn such high-level coherence?	1
A paragraph itself contains various forms of inductive coherence signals called self-supervision in this work, such as sentence orders, topical keywords, rhetorical structures, and so on.	1+2
Motivated by that, this work proposes a new paragraph completion task PARCOM; predicting masked sentences in a paragraph.	2
However, the task suffers from predicting and selecting appropriate topical content with respect to the given context.	1
To address that, we propose a self-supervised text planner SSPlanner that predicts what to say first (content prediction), then guides the pretrained language model (surface realization) using the predicted content.	2+3
SSPlanner outperforms the baseline generation models on the paragraph completion task in both automatic and human evaluation.	4
We also find that a combination of noun and verb types of keywords is the most effective for content selection.	4
As more number of content keywords are provided, overall generation quality also increases.	1
----------
Empathetic conversational models have been shown to improve user satisfaction and task outcomes in numerous domains.	1
In Psychology, persona has been shown to be highly correlated to personality, which in turn influences empathy.	1
In addition, our empirical analysis also suggests that persona plays an important role in empathetic conversations.	1+2
To this end, we propose a new task towards persona-based empathetic conversations and present the first empirical study on the impact of persona on empathetic responding.	2+3
Specifically, we first present a novel large-scale multi-domain dataset for persona-based empathetic conversations.	2+3
We then propose CoBERT, an efficient BERT-based response selection model that obtains the state-of-the-art performance on our dataset.	2+3
Finally, we conduct extensive experiments to investigate the impact of persona on empathetic responding.	2+3
Notably, our results show that persona improves empathetic responding more when CoBERT is trained on empathetic conversations than non-empathetic ones, establishing an empirical link between persona and empathy in human conversations.	4
----------
The global market size of conversational assistants (chatbots) is expected to grow to USD 9.4 billion by 2024, according to MarketsandMarkets.	1
Despite the wide use of chatbots, leakage of personal information through chatbots poses serious privacy concerns for their users.	1
In this work, we propose to protect personal information by warning users of detected suspicious sentences generated by conversational assistants.	2
The detection task is formulated as an alignment optimization problem and a new dataset PERSONA-LEAKAGE is collected for evaluation.	3
In this paper, we propose two novel constrained alignment models, which consistently outperform baseline methods on.	2
Moreover, we conduct analysis on the behavior of recently proposed personalized chit-chat dialogue systems.	3
The empirical results show that those systems suffer more from personal information disclosure than the widely used Seq2Seq model and the language model.	4
In those cases, a significant number of information leaking utterances can be detected by our models with high precision.	5
----------
While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario.	1
Hence, the prolongation and transition of conversation topics are ignored by current methods.	1
In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context.	1+2
With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection.	2+3
We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning.	2+3
Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.	4
----------
Human dialogues are scenario-based and appropriate responses generally relate to the latent context knowledge entailed by the specific scenario.	1
To enable responses that are more meaningful and context-specific, we propose to improve generative dialogue systems from the scenario perspective, where both dialogue history and future conversation are taken into account to implicitly reconstruct the scenario knowledge.	2+3
More importantly, the conversation scenarios are further internalized using imitation learning framework, where the conventional dialogue model that has no access to future conversations is effectively regularized by transferring the scenario knowledge contained in hierarchical supervising signals from the scenario-based dialogue model, so that the future conversation is not required in actual inference.	4
Extensive evaluations show that our approach significantly outperforms state-of-the-art baselines on diversity and relevance, and expresses scenario-specific knowledge.	3+4
----------
Being able to perform in-depth chat with humans in a closed domain is a precondition before an open-domain chatbot can be ever claimed.	1
In this work, we take a close look at the movie domain and present a large-scale high-quality corpus with fine-grained annotations in hope of pushing the limit of movie-domain chatbots.	1+2
We propose a unified, readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval.	2+3
The model is first pretrained on the huge general-domain data, then finetuned on our corpus.	3
We show this simple neural approach trained on high-quality data is able to outperform commercial systems replying on complex rules.	3+4
On both the static and interactive tests, we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human-written ones.	4
We further analyze the limits of our work and point out potential directions for future work	5
----------
Despite the significant progress on entity coreference resolution observed in recent years, there is a general lack of understanding of what has been improved.	1
We present an empirical analysis of state-of-the-art resolvers with the goal of providing the general NLP audience with a better understanding of the state of the art and coreference researchers with directions for future research.	1+2
----------
For multi-turn dialogue rewriting, the capacity of effectively modeling the linguistic knowledge in dialog context and getting ride of the noises is essential to improve its performance.	1
Existing attentive models attend to all words without prior focus, which results in inaccurate concentration on some dispensable words.	1
In this paper, we propose to use semantic role labeling (SRL), which highlights the core semantic information of who did what to whom, to provide additional guidance for the rewriter model.	2+3
Experiments show that this information significantly improves a RoBERTa-based model that already outperforms previous state-of-the-art systems.	4
----------
Quotations are crucial for successful explanations and persuasions in interpersonal communications.	1
However, finding what to quote in a conversation is challenging for both humans and machines.	1
This work studies automatic quotation generation in an online conversation and explores how language consistency affects whether a quotation fits the given context.	2
Here, we capture the contextual consistency of a quotation in terms of latent topics, interactions with the dialogue history, and coherence to the query turn’s existing contents.	2
Further, an encoder-decoder neural framework is employed to continue the context with a quotation via language generation.	2+3
Experiment results on two large-scale datasets in English and Chinese demonstrate that our quotation generation model outperforms the state-of-the-art models.	4
Further analysis shows that topic, interaction, and query consistency are all helpful to learn how to quote in online conversations.	5
----------
Existing works have proved that using law articles as external knowledge can improve the performance of the Legal Judgment Prediction.	1
However, they do not fully use law article information and most of the current work is only for single label samples.	1
In this paper, we propose a Law Article Element-aware Multi-representation Model (LEMM), which can make full use of law article information and can be used for multi-label samples.	2
The model uses the labeled elements of law articles to extract fact description features from multiple angles.	3
It generates multiple representations of a fact for classification.	3
Every label has a law-aware fact representation to encode more information.	4
To capture the dependencies between law articles, the model also introduces a self-attention mechanism between multiple representations.	4
Compared with baseline models like TopJudge, this model improves the accuracy of 5.84%, the macro F1 of 6.42%, and the micro F1 of 4.28%.	4
----------
Knowledge graph reasoning is a critical task in natural language processing.	1
The task becomes more challenging on temporal knowledge graphs, where each fact is associated with a timestamp.	1
Most existing methods focus on reasoning at past timestamps and they are not able to predict facts happening in the future.	1
This paper proposes Recurrent Event Network (RE-Net), a novel autoregressive architecture for predicting future interactions.	1+2
The occurrence of a fact (event) is modeled as a probability distribution conditioned on temporal sequences of past knowledge graphs.	3
Specifically, our RE-Net employs a recurrent event encoder to encode past facts, and uses a neighborhood aggregator to model the connection of facts at the same timestamp.	3
Future facts can then be inferred in a sequential manner based on the two modules.	3
We evaluate our proposed method via link prediction at future times on five public datasets.	3
Through extensive experiments, we demonstrate the strength of RE-Net, especially on multi-step inference over future timestamps, and achieve state-of-the-art performance on all five datasets.	2
----------
The central problem of sentence classification is to extract multi-scale n-gram features for understanding the semantic meaning of sentences.	1
Most existing models tackle this problem by stacking CNN and RNN models, which easily leads to feature redundancy and overfitting because of relatively limited datasets.	1
In this paper, we propose a simple yet effective model called Multi-scale Orthogonal inDependEnt LSTM (MODE-LSTM), which not only has effective parameters and good generalization ability, but also considers multiscale n-gram features.	2
We disentangle the hidden state of the LSTM into several independently updated small hidden states and apply an orthogonal constraint on their recurrent matrices.	3
We then equip this structure with sliding windows of different sizes for extracting multi-scale n-gram features.	3
Extensive experiments demonstrate that our model achieves better or competitive performance against state-of-the-art baselines on eight benchmark datasets.	4
We also combine our model with BERT to further boost the generalization performance.	4
----------
The data imbalance problem is a crucial issue for the multi-label text classification.	1
Some existing works tackle it by proposing imbalanced loss objectives instead of the vanilla cross-entropy loss, but their performances remain limited in the cases of extremely imbalanced data.	1
We propose a hybrid solution which adapts general networks for the head categories, and few-shot techniques for the tail categories.	2+3
We propose a Hybrid-Siamese Convolutional Neural Network (HSCNN) with additional technical attributes, i.e., a multi-task architecture based on Single and Siamese networks; a category-specific similarity in the Siamese structure; a specific sampling method for training HSCNN.	2+3
The results using two benchmark datasets and three loss objectives show that our method can improve the performance of Single networks with diverse loss objectives on the tail or entire categories.	3+4
----------
This paper proposes a pre-training based automated Chinese essay scoring method.	2
The method involves three components: weakly supervised pre-training, supervised cross- prompt fine-tuning and supervised target- prompt fine-tuning.	3
An essay scorer is first pre- trained on a large essay dataset covering diverse topics and with coarse ratings, i.e., good and poor, which are used as a kind of weak supervision.	3
The pre-trained essay scorer would be further fine-tuned on previously rated es- says from existing prompts, which have the same score range with the target prompt and provide extra supervision.	3+4
At last, the scorer is fine-tuned on the target-prompt training data.	3
The evaluation on four prompts shows that this method can improve a state-of-the-art neural essay scorer in terms of effectiveness and domain adaptation ability, while in-depth analysis also reveals its limitations..	4
----------
Question-driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for non-factoid questions.	1
In this work, we propose a novel question-driven abstractive summarization method, Multi-hop Selective Generator (MSG), to incorporate multi-hop reasoning into question-driven summarization and, meanwhile, provide justifications for the generated summaries.	1+2
Specifically, we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module, which captures important sentences for justifying the summarized answer.	3
A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives.	3
Experimental results show that the proposed method consistently outperforms state-of-the-art methods on two non-factoid QA datasets, namely WikiHow and PubMedQA.	4
----------
We focus on the task of reasoning over paragraph effects in situation, which requires a model to understand the cause and effect described in a background paragraph, and apply the knowledge to a novel situation.	1
Existing works ignore the complicated reasoning process and solve it with a one-step “black box” model.	1
Inspired by human cognitive processes, in this paper we propose a sequential approach for this task which explicitly models each step of the reasoning process with neural network modules.	2+3
In particular, five reasoning modules are designed and learned in an end-to-end manner, which leads to a more interpretable model.	3
Experimental results on the ROPES dataset demonstrate the effectiveness and explainability of our proposed approach.	4
----------
Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation.	1
To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph.	2
Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.	3+4
----------
Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method.	1
In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework.	2+3
When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.	4
----------
There is an increasing interest in developing text-based relational reasoning systems, which are capable of systematically reasoning about the relationships between entities mentioned in a text.	1
However, there remains a substantial performance gap between NLP models for relational reasoning and models based on graph neural networks (GNNs), which have access to an underlying symbolic representation of the text.	1
In this work, we investigate how the structured knowledge of a GNN can be distilled into various NLP models in order to improve their performance.	1+2
We first pre-train a GNN on a reasoning task using structured inputs and then incorporate its knowledge into an NLP model (e.g., an LSTM) via knowledge distillation.	3
To overcome the difficulty of cross-modal knowledge transfer, we also employ a contrastive learning based module to align the latent representations of NLP models and the GNN.	2+3
We test our approach with two state-of-the-art NLP models on 13 different inductive reasoning datasets from the CLUTRR benchmark and obtain significant improvements.	4
----------
The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure?	1
If so, how is this structure encoded?	1
To investigate this, we introduce latent subclass learning (LSL): a modification to classifier-based probing that induces a latent categorization (or ontology) of the probe’s inputs.	2+3
Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form.	3+4
In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments.	3+4
Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.	4
----------
While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied.	1
We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model.	1+2
Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining.	4
We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks’ performance.	4
These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge.	4+5
We provide source codes and pretrained models to reproduce our results at https://github.com/d223302/albert-embryology.	6
----------
We propose transfer learning as a method for analyzing the encoding of grammatical structure in neural language models.	2+3
We train LSTMs on non-linguistic data and evaluate their performance on natural language to assess which kinds of data induce generalizable structural features that LSTMs can use for natural language.	2+3
We find that training on non-linguistic data with latent structure (MIDI music or Java code) improves test performance on natural language, despite no overlap in surface form or vocabulary.	4
To pinpoint the kinds of abstract structure that models may be encoding to lead to this improvement, we run similar experiments with two artificial parentheses languages: one which has a hierarchical recursive structure, and a control which has paired tokens but no recursion.	3
Surprisingly, training a model on either of these artificial languages leads the same substantial gains when testing on natural language.	4
Further experiments on transfer between natural languages controlling for vocabulary overlap show that zero-shot performance on a test language is highly correlated with typological syntactic similarity to the training language, suggesting that representations induced by pre-training correspond to the cross-linguistic syntactic properties.	4+5
Our results provide insights into the ways that neural models represent abstract syntactic structure, and also about the kind of structural inductive biases which allow for natural language acquisition.	4
----------
Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models.	1
We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump).	2
While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts.	4
For example, endings generated for ‘Donald is a’ substantially differ from those of other names, and often have more-than-average negative sentiment.	4
We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers.	3+4
As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.	5
----------
We propose Grounded Adaptation for Zeroshot Executable Semantic Parsing (GAZP) to adapt an existing semantic parser to new environments (e.g. new database schemas).	2+3
GAZP combines a forward semantic parser with a backward utterance generator to synthesize data (e.g.	3
utterances and SQL queries) in the new environment, then selects cycle-consistent examples to adapt the parser.	3
Unlike data-augmentation, which typically synthesizes unverified examples in the training environment, GAZP synthesizes examples in the new environment whose input-output consistency are verified through execution.	3
On the Spider, Sparc, and CoSQL zero-shot semantic parsing tasks, GAZP improves logical form and execution accuracy of the baseline parser.	3+4
Our analyses show that GAZP outperforms data-augmentation in the training environment, performance increases with the amount of GAZP-synthesized data, and cycle-consistency is central to successful adaptation.	4
----------
Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks.	1
In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users.	1+2
A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain.	1
In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions.	3
To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011).	3
We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem.	4+5
Code will be available at https://github.com/sunlab-osu/MISP.	6
----------
Context-dependent text-to-SQL task has drawn much attention in recent years.	1
Previous models on context-dependent text-to-SQL task only concentrate on utilizing historic user inputs.	1
In this work, in addition to using encoders to capture historic information of user inputs, we propose a database schema interaction graph encoder to utilize historic information of database schema items.	2+3
In decoding phase, we introduce a gate mechanism to weigh the importance of different vocabularies and then make the prediction of SQL tokens.	3
We evaluate our model on the benchmark SParC and CoSQL datasets, which are two large complex context-dependent cross-domain text-to-SQL datasets.	2
Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets.	4
The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder.	2
----------
In Natural Language Interfaces to Databases systems, the text-to-SQL technique allows users to query databases by using natural language questions.	1
Though significant progress in this area has been made recently, most parsers may fall short when they are deployed in real systems.	1
One main reason stems from the difficulty of fully understanding the users’ natural language questions.	1
In this paper, we include human in the loop and present a novel parser-independent interactive approach (PIIA) that interacts with users using multi-choice questions and can easily work with arbitrary parsers.	2+3
Experiments were conducted on two cross-domain datasets, the WikiSQL and the more complex Spider, with five state-of-the-art parsers.	3
These demonstrated that PIIA is capable of enhancing the text-to-SQL performance with limited interaction turns by using both simulation and human evaluation.	3+4
----------
On the WikiSQL benchmark, state-of-the-art text-to-SQL systems typically take a slot- filling approach by building several dedicated models for each type of slots.	1
Such modularized systems are not only complex but also of limited capacity for capturing inter-dependencies among SQL clauses.	1
To solve these problems, this paper proposes a novel extraction-linking approach, where a unified extractor recognizes all types of slot mentions appearing in the question sentence before a linker maps the recognized columns to the table schema to generate executable SQL queries.	2+3
Trained with automatically generated annotations, the proposed method achieves the first place on the WikiSQL benchmark.	4
----------
(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories.	1
Incremental learning on new categories is necessary for (T)ACSA real applications.	1
Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks.	1
In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net).	2
We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem.	3
Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination.	3
Our model achieved state-of-the-art on two (T)ACSA benchmark datasets.	4
Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines.	4
----------
Recently, pre-trained language models mostly follow the pre-train-then-fine-tuning paradigm and have achieved great performance on various downstream tasks.	1
However, since the pre-training stage is typically task-agnostic and the fine-tuning stage usually suffers from insufficient supervised data, the models cannot always well capture the domain-specific and task-specific patterns.	1
In this paper, we propose a three-stage framework by adding a task-guided pre-training stage with selective masking between general pre-training and fine-tuning.	2+3
In this stage, the model is trained by masked language modeling on in-domain unsupervised data to learn domain-specific patterns and we propose a novel selective masking strategy to learn task-specific patterns.	2+3
Specifically, we design a method to measure the importance of each token in sequences and selectively mask the important tokens.	2
Experimental results on two sentiment analysis tasks show that our method can achieve comparable or even better performance with less than 50% of computation cost, which indicates our method is both effective and efficient.	4+5
The source code of this paper can be obtained from https://github.com/thunlp/SelectiveMasking.	6
----------
Most of the existing pre-trained language representation models neglect to consider the linguistic knowledge of texts, which can promote language understanding in NLP tasks.	1
To benefit the downstream tasks in sentiment analysis, we propose a novel language representation model called SentiLARE, which introduces word-level linguistic knowledge including part-of-speech tag and sentiment polarity (inferred from SentiWordNet) into pre-trained models.	2+3
We first propose a context-aware sentiment attention mechanism to acquire the sentiment polarity of each word with its part-of-speech tag by querying SentiWordNet.	2+3
Then, we devise a new pre-training task called label-aware masked language model to construct knowledge-aware language representation.	2+3
Experiments show that SentiLARE obtains new state-of-the-art performance on a variety of sentiment analysis tasks.	4
----------
Aspect-based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner.	1
It has in general two sub-tasks: (i) extracting aspects from each review, and (ii) classifying aspect-based reviews by sentiment polarity.	1
In this paper, we propose a weakly-supervised approach for aspect-based sentiment analysis, which uses only a few keywords describing each aspect/sentiment without using any labeled examples.	2+3
Existing methods are either designed only for one of the sub-tasks, or are based on topic models that may contain overlapping concepts.	3
We propose to first learn <sentiment, aspect> joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness, and then use neural models to generalize the word-level discriminative information by pre-training the classifiers with embedding-based predictions and self-training them on unlabeled data.	3
Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly (7.4% and 5.1% F1-score gain on average for aspect and sentiment classification respectively) on benchmark datasets.	4
----------
Peer review and rebuttal, with rich interactions and argumentative discussions in between, are naturally a good resource to mine arguments.	1
However, few works study both of them simultaneously.	1
In this paper, we introduce a new argument pair extraction (APE) task on peer review and rebuttal in order to study the contents, the structure and the connections between them.	1+2
We prepare a challenging dataset that contains 4,764 fully annotated review-rebuttal passage pairs from an open review platform to facilitate the study of this task.	3
To automatically detect argumentative propositions and extract argument pairs from this corpus, we cast it as the combination of a sequence labeling task and a text relation classification task.	3
Thus, we propose a multitask learning framework based on hierarchical LSTM networks.	4
Extensive experiments and analysis demonstrate the effectiveness of our multi-task framework, and also show the challenges of the new task as well as motivate future research directions.	5
----------
Neural Document-level Multi-aspect Sentiment Classification (DMSC) usually requires a lot of manual aspect-level sentiment annotations, which is time-consuming and laborious.	1
As document-level sentiment labeled data are widely available from online service, it is valuable to perform DMSC with such free document-level annotations.	1
To this end, we propose a novel Diversified Multiple Instance Learning Network (D-MILN), which is able to achieve aspect-level sentiment classification with only document-level weak supervision.	2+3
Specifically, we connect aspect-level and document-level sentiment by formulating this problem as multiple instance learning, providing a way to learn aspect-level classifier from the back propagation of document-level supervision.	3
Two diversified regularizations are further introduced in order to avoid the overfitting on document-level signals during training.	3
Diversified textual regularization encourages the classifier to select aspect-relevant snippets, and diversified sentimental regularization prevents the aspect-level sentiments from being overly consistent with document-level sentiment.	3
Experimental results on TripAdvisor and BeerAdvocate datasets show that D-MILN remarkably outperforms recent weakly-supervised baselines, and is also comparable to the supervised method.	4+5
----------
While hyperbole is one of the most prevalent rhetorical devices, it is arguably one of the least studied devices in the figurative language processing community.	1
We contribute to the study of hyperbole by (1) creating a corpus focusing on sentence-level hyperbole detection, (2) performing a statistical and manual analysis of our corpus, and (3) addressing the automatic hyperbole detection task.	2+3
----------
The supervised models for aspect-based sentiment analysis (ABSA) rely heavily on labeled data.	1
However, fine-grained labeled data are scarce for the ABSA task.	1
To alleviate the dependence on labeled data, prior works mainly focused on feature-based adaptation, which used the domain-shared knowledge to construct auxiliary tasks or domain adversarial learning to bridge the gap between domains, while ignored the attribute of instance-based adaptation.	1
To resolve this limitation, we propose an end-to-end framework to jointly perform feature and instance based adaptation for the ABSA task in this paper.	1+2
Based on BERT, we learn domain-invariant feature representations by using part-of-speech features and syntactic dependency relations to construct auxiliary tasks, and jointly perform word-level instance weighting in the framework of sequence labeling.	3
Experiment results on four benchmarks show that the proposed method can achieve significant improvements in comparison with the state-of-the-arts in both tasks of cross-domain End2End ABSA and cross-domain aspect extraction.	4
----------
Explainable question answering systems predict an answer together with an explanation showing why the answer has been selected.	1
The goal is to enable users to assess the correctness of the system and understand its reasoning process.	1
However, we show that current models and evaluation settings have shortcomings regarding the coupling of answer and explanation which might cause serious issues in user experience.	1
As a remedy, we propose a hierarchical model and a new regularization term to strengthen the answer-explanation coupling as well as two evaluation scores to quantify the coupling.	2
We conduct experiments on the HOTPOTQA benchmark data set and perform a user study.	2+3
The user study shows that our models increase the ability of the users to judge the correctness of the system and that scores like F1 are not enough to estimate the usefulness of a model in a practical setting with human users.	4
Our scores are better aligned with user experience, making them promising candidates for model selection.	5
----------
Knowledge graphs (KGs) can vary greatly from one domain to another.	1
Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graph-text data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations.	1
This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well on different domains.	1
To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing.	2+3
We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome.	3
Our system outperforms strong baselines for both text<->graph conversion tasks without any manual adaptation from one dataset to the other.	4
In additional experiments, we investigate the impact of using different unsupervised objectives.	3
----------
We propose DGST, a novel and simple Dual-Generator network architecture for text Style Transfer.	2+3
Our model employs two generators only, and does not rely on any discriminators or parallel corpus for training.	3
Both quantitative and qualitative experiments on the Yelp and IMDb datasets show that our model gives competitive performance compared to several strong baselines with more complicated architecture designs.	4
----------
With the advancements in natural language processing tasks, math word problem solving has received increasing attention.	1
Previous methods have achieved promising results but ignore background common-sense knowledge not directly provided by the problem.	1
In addition, during generation, they focus on local features while neglecting global information.	1
To incorporate external knowledge and global expression information, we propose a novel knowledge-aware sequence-to-tree (KA-S2T) network in which the entities in the problem sequences and their categories are modeled as an entity graph.	2+3
Based on this entity graph, a graph attention network is used to capture knowledge-aware problem representations.	2
Further, we use a tree-structured decoder with a state aggregation mechanism to capture the long-distance dependency and global expression information.	2
Experimental results on the Math23K dataset revealed that the KA-S2T model can achieve better performance than previously reported best results.	4+5
----------
We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC).	2+3
ESD identifies grammatically incorrect text spans with an efficient sequence tagging model.	3
Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans.	3
Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50% time cost for inference.	4
----------
Language representation models such as BERT could effectively capture contextual semantic information from plain text, and have been proved to achieve promising results in lots of downstream NLP tasks with appropriate fine-tuning.	1
However, most existing language representation models cannot explicitly handle coreference, which is essential to the coherent understanding of the whole discourse.	1
To address this issue, we present CorefBERT, a novel language representation model that can capture the coreferential relations in context.	2
The experimental results show that, compared with existing baseline models, CorefBERT can achieve significant improvements consistently on various downstream NLP tasks that require coreferential reasoning, while maintaining comparable performance to previous models on other common NLP tasks.	4
The source code and experiment details of this paper can be obtained from https://github.com/thunlp/CorefBERT.	6
----------
Mainstream computational lexical semantics embraces the assumption that word senses can be represented as discrete items of a predefined inventory.	1
In this paper we show this needs not be the case, and propose a unified model that is able to produce contextually appropriate definitions.	2
In our model, Generationary, we employ a novel span-based encoding scheme which we use to fine-tune an English pre-trained Encoder-Decoder system to generate glosses.	2+3
We show that, even though we drop the need of choosing from a predefined sense inventory, our model can be employed effectively: not only does Generationary outperform previous approaches in the generative task of Definition Modeling in many settings, but it also matches or surpasses the state of the art in discriminative tasks such as Word Sense Disambiguation and Word-in-Context.	4
Finally, we show that Generationary benefits from training on data from multiple inventories, with strong gains on various zero-shot benchmarks, including a novel dataset of definitions for free adjective-noun phrases.	4+5
The software and reproduction materials are available at http://generationary.org.	6
----------
The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture.	1
While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context.	1
In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance?	2+3
How consistent are the observed effects across tasks and languages?	2
2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network?	3
3) How do these representations fare against traditional static word vectors in lexical tasks 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities?	3
Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks.	4
Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.	4
----------
Despite the promising results of current cross-lingual models for spoken language understanding systems, they still suffer from imperfect cross-lingual representation alignments between the source and target languages, which makes the performance sub-optimal.	1
To cope with this issue, we propose a regularization approach to further align word-level and sentence-level representations across languages without any external resource.	2+3
First, we regularize the representation of user utterances based on their corresponding labels.	3
Second, we regularize the latent variable model (Liu et al., 2019) by leveraging adversarial training to disentangle the latent variables.	3
Experiments on the cross-lingual spoken language understanding task show that our model outperforms current state-of-the-art methods in both few-shot and zero-shot scenarios, and our model, trained on a few-shot setting with only 3% of the target language training data, achieves comparable performance to the supervised training with all the training data.	4
----------
Past progress on neural models has proven that named entity recognition is no longer a problem if we have enough labeled data.	1
However, collecting enough data and annotating them are labor-intensive, time-consuming, and expensive.	1
In this paper, we decompose the sentence into two parts: entity and context, and rethink the relationship between them and model performance from a causal perspective.	2+3
Based on this, we propose the Counterfactual Generator, which generates counterfactual examples by the interventions on the existing observational examples to enhance the original dataset.	2+3
Experiments across three datasets show that our method improves the generalization ability of models under limited observational examples.	3+4
Besides, we provide a theoretical foundation by using a structural causal model to explore the spurious correlations between input features and output labels.	4
We investigate the causal effects of entity or context on model performance under both conditions: the non-augmented and the augmented.	2
Interestingly, we find that the non-spurious correlations are more located in entity representation rather than context representation.	4
As a result, our method eliminates part of the spurious correlations between context representation and output labels.	4
The code is available at https://github.com/xijiz/cfgen.	6
----------
The task of procedural text comprehension aims to understand the dynamic nature of entities/objects in a process.	1
Here, the key is to track how the entities interact with each other and how their states are changing along the procedure.	1
Recent efforts have made great progress to track multiple entities in a procedural text, but usually treat each entity separately and ignore the fact that there are often multiple entities interacting with each other during one process, some of which are even explicitly mentioned.	1
In this paper, we propose a novel Interactive Entity Network (IEN), which is a recurrent network with memory equipped cells for state tracking.	2
In each IEN cell, we maintain different attention matrices through specific memories to model different types of entity interactions.	2+3
Importantly, we can update these memories in a sequential manner so as to explore the causal relationship between entity actions and subsequent state changes.	2
We evaluate our model on a benchmark dataset, and the results show that IEN outperforms state-of-the-art models by precisely capturing the interactions of multiple entities and explicitly leverage the relationship between entity interactions and subsequent state changes.	4
----------
There has recently been increasing interest in learning representations of temporal knowledge graphs (KGs), which record the dynamic relationships between entities over time.	1
Temporal KGs often exhibit multiple simultaneous non-Euclidean structures, such as hierarchical and cyclic structures.	1
However, existing embedding approaches for temporal KGs typically learn entity representations and their dynamic evolution in the Euclidean space, which might not capture such intrinsic structures very well.	1
To this end, we propose DyERNIE, a non-Euclidean embedding approach that learns evolving entity representations in a product of Riemannian manifolds, where the composed spaces are estimated from the sectional curvatures of underlying data.	2+3
Product manifolds enable our approach to better reflect a wide variety of geometric structures on temporal KGs.	3
Besides, to capture the evolutionary dynamics of temporal KGs, we let the entity representations evolve according to a velocity vector defined in the tangent space at each timestamp.	3
We analyze in detail the contribution of geometric spaces to representation learning of temporal KGs and evaluate our model on temporal knowledge graph completion tasks.	2+3
Extensive experiments on three real-world datasets demonstrate significantly improved performance, indicating that the dynamics of multi-relational graph data can be more properly modeled by the evolution of embeddings on Riemannian manifolds.	4
----------
It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe).	1
While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings.	1
We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end.	1
In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes.	1
We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm.	2+3
We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks.	4
Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.	4
----------
It has been shown that knowledge graph embeddings encode potentially harmful social biases, such as the information that women are more likely to be nurses, and men more likely to be bankers.	1
As graph embeddings begin to be used more widely in NLP pipelines, there is a need to develop training methods which remove such biases.	1
Previous approaches to this problem both significantly increase the training time, by a factor of eight or more, and decrease the accuracy of the model substantially.	1
We present a novel approach, in which all embeddings are trained to be neutral to sensitive attributes such as gender by default using an adversarial loss.	2
We then add sensitive attributes back on in whitelisted cases.	2+3
Training time only marginally increases over a baseline model, and the debiased embeddings perform almost as accurately in the triple prediction task as their non-debiased counterparts.	4
----------
Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact.	1
In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs.	2
Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact.	2
We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K.	3+4
Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks.	3+4
We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.	2+3
----------
Interest in emotion recognition in conversations (ERC) has been increasing in various fields, because it can be used to analyze user behaviors and detect fake news.	1
Many recent ERC methods use graph-based neural networks to take the relationships between the utterances of the speakers into account.	1
In particular, the state-of-the-art method considers self- and inter-speaker dependencies in conversations by using relational graph attention networks (RGAT).	1
However, graph-based neural networks do not take sequential information into account.	1
In this paper, we propose relational position encodings that provide RGAT with sequential information reflecting the relational graph structure.	2
Accordingly, our RGAT model can capture both the speaker dependency and the sequential information.	2
Experiments on four ERC datasets show that our model is beneficial to recognizing emotions expressed in conversations.	4
In addition, our approach empirically outperforms the state-of-the-art on all of the benchmark datasets.	4+5
----------
Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity.	1
These differences are important for natural language understanding and reasoning.	1
We propose a novel BERT-based approach to intensity detection for scalar adjectives.	2
We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives.	4
We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task.	4
Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.	4+5
----------
Adapting pre-trained language models (PrLMs) (e.g., BERT) to new domains has gained much attention recently.	1
Instead of fine-tuning PrLMs as done in most previous work, we investigate how to adapt the features of PrLMs to new domains without fine-tuning.	1
We explore unsupervised domain adaptation (UDA) in this paper.	2
With the features from PrLMs, we adapt the models trained with labeled data from the source domain to the unlabeled target domain.	3
Self-training is widely used for UDA, and it predicts pseudo labels on the target domain data for training.	1
However, the predicted pseudo labels inevitably include noise, which will negatively affect training a robust model.	1
To improve the robustness of self-training, in this paper we present class-aware feature self-distillation (CFd) to learn discriminative features from PrLMs, in which PrLM features are self-distilled into a feature adaptation module and the features from the same class are more tightly clustered.	2+3
We further extend CFd to a cross-language setting, in which language discrepancy is studied.	2
Experiments on two monolingual and multilingual Amazon review datasets show that CFd can consistently improve the performance of self-training in cross-domain and cross-language settings.	4
----------
In this paper we propose a novel data augmentation approach where guided outputs of a language generation model, e.g. GPT-2, when labeled, can improve the performance of text classifiers through an active learning process.	2
We transform the data generation task into an optimization problem which maximizes the usefulness of the generated output, using Monte Carlo Tree Search (MCTS) as the optimization strategy and incorporating entropy as one of the optimization criteria.	3
We test our approach against a Non-Guided Data Generation (NGDG) process that does not optimize for a reward function.	3
Starting with a small set of data, our results show an increased performance with MCTS of 26% on the TREC-6 Questions dataset, and 10% on the Stanford Sentiment Treebank SST-2 dataset.	4
Compared with NGDG, we are able to achieve increases of 3% and 5% on TREC-6 and SST-2.	5
----------
Unconscious biases continue to be prevalent in modern text and media, calling for algorithms that can assist writers with bias correction.	1
For example, a female character in a story is often portrayed as passive and powerless (“_She daydreams about being a doctor_”) while a man is portrayed as more proactive and powerful (“_He pursues his dream of being a doctor_”).	1
We formulate **Controllable Debiasing**, a new revision task that aims to rewrite a given text to correct the implicit and potentially undesirable bias in character portrayals.	1+2
We then introduce PowerTransformer as an approach that debiases text through the lens of connotation frames (Sap et al., 2017), which encode pragmatic knowledge of implied power dynamics with respect to verb predicates.	2+3
One key challenge of our task is the lack of parallel corpora.	1
To address this challenge, we adopt an unsupervised approach using auxiliary supervision with related tasks such as paraphrasing and self-supervision based on a reconstruction loss, building on pretrained language models.	2+3
Through comprehensive experiments based on automatic and human evaluations, we demonstrate that our approach outperforms ablations and existing methods from related tasks.	4
Furthermore, we demonstrate the use of PowerTransformer as a step toward mitigating the well-documented gender bias in character portrayal in movie scripts.	4
----------
Previous neural coherence models have focused on identifying semantic relations between adjacent sentences.	1
However, they do not have the means to exploit structural information.	1
In this work, we propose a coherence model which takes discourse structural information into account without relying on human annotations.	2
We approximate a linguistic theory of coherence, Centering theory, which we use to track the changes of focus between discourse segments.	3
Our model first identifies the focus of each sentence, recognized with regards to the context, and constructs the structural relationship for discourse segments by tracking the changes of the focus.	3
The model then incorporates this structural information into a structure-aware transformer.	3
We evaluate our model on two tasks, automated essay scoring and assessing writing quality.	3
Our results demonstrate that our model, built on top of a pretrained language model, achieves state-of-the-art performance on both tasks.	4
We next statistically examine the identified trees of texts assigned to different quality scores.	3
Finally, we investigate what our model learns in terms of theoretical claims.	3
----------
The notion of face refers to the public self-image of an individual that emerges both from the individual’s own actions as well as from the interaction with others.	1
Modeling face and understanding its state changes throughout a conversation is critical to the study of maintenance of basic human needs in and through interaction.	1
Grounded in the politeness theory of Brown and Levinson (1978), we propose a generalized framework for modeling face acts in persuasion conversations, resulting in a reliable coding manual, an annotated corpus, and computational models.	2+3
The framework reveals insights about differences in face act utilization between asymmetric roles in persuasion conversations.	1
Using computational models, we are able to successfully identify face acts as well as predict a key conversational outcome (e.g. donation success).	2+3
Finally, we model a latent representation of the conversational state to analyze the impact of predicted face acts on the probability of a positive conversational outcome and observe several correlations that corroborate previous findings.	3
----------
We present our HABERTOR model for detecting hatespeech in large scale user-generated content.	2
Inspired by the recent success of the BERT model, we propose several modifications to BERT to enhance the performance on the downstream hatespeech classification task.	2
HABERTOR inherits BERT’s architecture, but is different in four aspects: 	1
(i) it generates its own vocabularies and is pre-trained from the scratch using the largest scale hatespeech dataset; 	3
(ii) it consists of Quaternion-based factorized components, resulting in a much smaller number of parameters, faster training and inferencing, as well as less memory usage; 	3
(iii) it uses our proposed multi-source ensemble heads with a pooling layer for separate input sources, to further enhance its effectiveness;	3
and (iv) it uses a regularized adversarial training with our proposed fine-grained and adaptive noise magnitude to enhance its robustness.	3
Through experiments on the large-scale real-world hatespeech dataset with 1.4M annotated comments, we show that HABERTOR works better than 15 state-of-the-art hatespeech detection methods, including fine-tuning Language Models.	4
In particular, comparing with BERT, our HABERTOR is 4 5 times faster in the training/inferencing phase, uses less than 1/3 of the memory, and has better performance, even though we pre-train it by using less than 1% of the number of words.	4
Our generalizability analysis shows that HABERTOR transfers well to other unseen hatespeech datasets and is a more efficient and effective alternative to BERT for the hatespeech classification.	5
----------
Large-scale Multi-label Text Classification (LMTC) has a wide range of Natural Language Processing (NLP) applications and presents interesting challenges.	1
First, not all labels are well represented in the training set, due to the very large label set and the skewed label distributions of datasets.	1
Also, label hierarchies and differences in human labelling guidelines may affect graph-aware annotation proximity.	1
Finally, the label hierarchies are periodically updated, requiring LMTC models capable of zero-shot generalization.	1
Current state-of-the-art LMTC models employ Label-Wise Attention Networks (LWANs), which (1) typically treat LMTC as flat multi-label classification; (2) may use the label hierarchy to improve zero-shot learning, although this practice is vastly understudied; and (3) have not been combined with pre-trained Transformers (e.g. BERT), which have led to state-of-the-art results in several NLP benchmarks.	1
Here, for the first time, we empirically evaluate a battery of LMTC methods from vanilla LWANs to hierarchical classification approaches and transfer learning, on frequent, few, and zero-shot learning on three datasets from different domains.	2+3
We show that hierarchical methods based on Probabilistic Label Trees (PLTs) outperform LWANs.	2
Furthermore, we show that Transformer-based approaches outperform the state-of-the-art in two of the datasets, and we propose a new state-of-the-art method which combines BERT with LWAN.	3
Finally, we propose new models that leverage the label hierarchy to improve few and zero-shot learning, considering on each dataset a graph-aware annotation proximity measure that we introduce.	3+4
----------
We introduce scientific claim verification, a new task to select abstracts from the research literature containing evidence that SUPPORTS or REFUTES a given scientific claim, and to identify rationales justifying each decision.	2
To study this task, we construct SciFact, a dataset of 1.4K expert-written scientific claims paired with evidence-containing abstracts annotated with labels and rationales.	2+3
We develop baseline models for SciFact, and demonstrate that simple domain adaptation techniques substantially improve performance compared to models trained on Wikipedia or political news.	3+4
We show that our system is able to verify claims related to COVID-19 by identifying evidence from the CORD-19 corpus.	4
Our experiments indicate that SciFact will provide a challenging testbed for the development of new systems designed to retrieve and reason over corpora containing specialized domain knowledge.	3+4
Data and code for this new task are publicly available at https://github.com/allenai/scifact.	6
A leaderboard and COVID-19 fact-checking demo are available at https://scifact.apps.allenai.org.	6
----------
We reduce the task of (span-based) PropBank-style semantic role labeling (SRL) to syntactic dependency parsing.	2
Our approach is motivated by our empirical analysis that shows three common syntactic patterns account for over 98% of the SRL annotations for both English and Chinese data.	1
Based on this observation, we present a conversion scheme that packs SRL annotations into dependency tree representations through joint labels that permit highly accurate recovery back to the original format.	2+3
This representation allows us to train statistical dependency parsers to tackle SRL and achieve competitive performance with the current state of the art.	4
Our findings show the promise of syntactic dependency trees in encoding semantic role relations within their syntactic domain of locality, and point to potential further integration of syntactic methods into semantic role labeling in the future.	4+5
----------
When does a sequence of events define an everyday scenario and how can this knowledge be induced from text?	1
Prior works in inducing such scripts have relied on, in one form or another, measures of correlation between instances of events in a corpus.	1
We argue from both a conceptual and practical sense that a purely correlation-based approach is insufficient, and instead propose an approach to script induction based on the causal effect between events, formally defined via interventions.	2+3
Through both human and automatic evaluations, we show that the output of our method based on causal effects better matches the intuition of what a script represents.	3+4
----------
NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task.	1
Recently proposed debiasing methods are shown to be effective in mitigating this tendency.	1
However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.	1
In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance.	1+2
The proposed framework is general and complementary to the existing debiasing methods.	3
We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models’ reliance on biases) without specifically targeting certain biases.	3+4
Furthermore, the evaluation suggests that applying the framework results in improved overall robustness.	5
----------
The scarcity of large parallel corpora is an important obstacle for neural machine translation.	1
A common solution is to exploit the knowledge of language models (LM) trained on abundant monolingual data.	1
In this work, we propose a novel approach to incorporate a LM as prior in a neural translation model (TM).	2
Specifically, we add a regularization term, which pushes the output distributions of the TM to be probable under the LM prior, while avoiding wrong predictions when the TM “disagrees” with the LM.	3
This objective relates to knowledge distillation, where the LM can be viewed as teaching the TM about the target language.	2
The proposed approach does not compromise decoding speed, because the LM is used only at training time, unlike previous work that requires it during inference.	3
We present an analysis of the effects that different methods have on the distributions of the TM.	2+3
Results on two low-resource machine translation datasets show clear improvements even with limited monolingual data.	4
----------
Word sense disambiguation is a well-known source of translation errors in NMT.	1
We posit that some of the incorrect disambiguation choices are due to models’ over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text.	1
We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types.	2+3
Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models.	2+3
Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.	4
----------
The main goal behind state-of-the-art pre-trained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer.	1
However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pre-training.	1
We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations.	2+3
In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pre-trained multilingual model to a new language.	2
MAD-X outperforms the state of the art in cross lingual transfer across a representative set of typologically diverse languages on named entity recognition and causal commonsense reasoning, and achieves competitive results on question answering.	3+4
Our code and adapters are available at AdapterHub.ml.	6
----------
Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique.	1
In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models.	2
For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to.	1
We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon.	4
Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.	4
----------
Social media’s ubiquity fosters a space for users to exhibit suicidal thoughts outside of traditional clinical settings.	1
Understanding the build-up of such ideation is critical for the identification of at-risk users and suicide prevention.	1
Suicide ideation is often linked to a history of mental depression.	1
The emotional spectrum of a user’s historical activity on social media can be indicative of their mental state over time.	1
In this work, we focus on identifying suicidal intent in English tweets by augmenting linguistic models with historical context.	1
We propose STATENet, a time-aware transformer based model for preliminary screening of suicidal risk on social media.	2+3
STATENet outperforms competitive methods, demonstrating the utility of emotional and temporal contextual cues for suicide risk assessment.	2
We discuss the empirical, qualitative, practical, and ethical aspects of STATENet for suicide ideation detection.	2
----------
In this paper, we suggest a minimally supervised approach for identifying nuanced frames in news article coverage of politically divisive topics.	2+3
We suggest to break the broad policy frames suggested by Boydstun et al., 2014 into fine-grained subframes which can capture differences in political ideology in a better way.	1+2
We evaluate the suggested subframes and their embedding, learned using minimal supervision, over three topics, namely, immigration, gun-control, and abortion.	2+3
We demonstrate the ability of the subframes to capture ideological differences and analyze political discourse in news media.	2
----------
Although many fact-checking systems have been developed in academia and industry, fake news is still proliferating on social media.	1
These systems mostly focus on fact-checking but usually neglect online users who are the main drivers of the spread of misinformation.	1
How can we use fact-checked information to improve users’ consciousness of fake news to which they are exposed?	1
How can we stop users from spreading fake news?	1
To tackle these questions, we propose a novel framework to search for fact-checking articles, which address the content of an original tweet (that may contain misinformation) posted by online users.	1+2
The search can directly warn fake news posters and online users (e.g. the posters’ followers) about misinformation, discourage them from spreading fake news, and scale up verified content on social media.	5
Our framework uses both text and images to search for fact-checking articles, and achieves promising results on real-world datasets.	2
Our code and datasets are released at https://github.com/nguyenvo09/EMNLP2020.	6
----------
Modern toxic speech detectors are incompetent in recognizing disguised offensive language, such as adversarial attacks that deliberately avoid known toxic lexicons, or manifestations of implicit bias.	1
Building a large annotated dataset for such veiled toxicity can be very expensive.	1
In this work, we propose a framework aimed at fortifying existing toxic speech detectors without a large labeled corpus of veiled toxicity.	2+3
Just a handful of probing examples are used to surface orders of magnitude more disguised offenses.	3
We augment the toxic speech detector’s training data with these discovered offensive examples, thereby making it more robust to veiled toxicity while preserving its utility in detecting overt toxicity.	4+5
----------
Policy gradients-based reinforcement learning has proven to be a promising approach for directly optimizing non-differentiable evaluation metrics for language generation tasks.	1
However, optimizing for a specific metric reward leads to improvements in mostly that metric only, suggesting that the model is gaming the formulation of that metric in a particular way without often achieving real qualitative improvements.	1
Hence, it is more beneficial to make the model optimize multiple diverse metric rewards jointly.	1
While appealing, this is challenging because one needs to manually decide the importance and scaling weights of these metric rewards.	1
Further, it is important to consider using a dynamic combination and curriculum of metric rewards that flexibly changes over time.	1
Considering the above aspects, in our work, we automate the optimization of multiple metric rewards simultaneously via a multi-armed bandit approach (DORB), where at each round, the bandit chooses which metric reward to optimize next, based on expected arm gains.	2+3
We use the Exp3 algorithm for bandits and formulate two approaches for bandit rewards: (1) Single Multi-reward Bandit (SM-Bandit); (2) Hierarchical Multi-reward Bandit (HM-Bandit).	3
We empirically show the effectiveness of our approaches via various automatic metrics and human evaluation on two important NLG tasks: question generation and data-to-text generation.	4
Finally, we present interpretable analyses of the learned bandit curriculum over the optimized rewards.	4
----------
Information extraction from conversational data is particularly challenging because the task-centric nature of conversation allows for effective communication of implicit information by humans, but is challenging for machines.	1
The challenges may differ between utterances depending on the role of the speaker within the conversation, especially when relevant expertise is distributed asymmetrically across roles.	1
Further, the challenges may also increase over the conversation as more shared context is built up through information communicated implicitly earlier in the dialogue.	1
In this paper, we propose the novel modeling approach MedFilter, which addresses these insights in order to increase performance at identifying and categorizing task-relevant utterances, and in so doing, positively impacts performance at a downstream information extraction task.	1+2
We evaluate this approach on a corpus of nearly 7,000 doctor-patient conversations where MedFilter is used to identify medically relevant contributions to the discussion (achieving a 10% improvement over SOTA baselines in terms of area under the PR curve).	4+5
Identifying task-relevant utterances benefits downstream medical processing, achieving improvements of 15%, 105%, and 23% respectively for the extraction of symptoms, medications, and complaints.	4+5
----------
Automated fact extraction and verification is a challenging task that involves finding relevant evidence sentences from a reliable corpus to verify the truthfulness of a claim.	1
Existing models either (i) concatenate all the evidence sentences, leading to the inclusion of redundant and noisy information; or (ii) process each claim-evidence sentence pair separately and aggregate all of them later, missing the early combination of related sentences for more accurate claim verification.	1
Unlike the prior works, in this paper, we propose Hierarchical Evidence Set Modeling (HESM), a framework to extract evidence sets (each of which may contain multiple evidence sentences), and verify a claim to be supported, refuted or not enough info, by encoding and attending the claim and evidence sets at different levels of hierarchy.	2+3
Our experimental results show that HESM outperforms 7 state-of-the-art methods for fact extraction and claim verification.	4
Our source code is available at https://github.com/ShyamSubramanian/HESM.	6
----------
Performing fact verification based on structured data is important for many real-life applications and is a challenging research problem, particularly when it involves both symbolic operations and informal inference based on language understanding.	1
In this paper, we present a Program-enhanced Verbalization and Graph Attention Network (ProgVGAT) to integrate programs and execution into textual inference models.	2
Specifically, a verbalization with program execution model is proposed to accumulate evidences that are embedded in operations over the tables.	3
Built on that, we construct the graph attention verification networks, which are designed to fuse different sources of evidences from verbalized program execution, program structures, and the original statements and tables, to make the final verification decision.	3
To support the above framework, we propose a program selection module optimized with a new training strategy based on margin loss, to produce more accurate programs, which is shown to be effective in enhancing the final verification results.	3+4
Experimental results show that the proposed framework achieves the new state-of-the-art performance, a 74.4% accuracy, on the benchmark dataset TABFACT.	4
----------
Fact-verification systems are well explored in the NLP literature with growing attention owing to shared tasks like FEVER.	1
Though the task requires reasoning on extracted evidence to verify a claim’s factuality, there is little work on understanding the reasoning process.	1
In this work, we propose a new methodology for fact-verification, specifically FEVER, that enforces a closed-world reliance on extracted evidence.	2
We present an extensive evaluation of state-of-the-art verification models under these constraints.	3
----------
We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base.	2+3
We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities.	2+3
The model outperforms state-of-the-art results from a far more limited cross-lingual linking task.	4
Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation.	4
To this end, we provide Mewsli-9, a large new multilingual dataset matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements.	4
----------
In this paper, we propose a novel model compression approach to effectively compress BERT by progressive module replacing.	2
Our approach first divides the original BERT into several modules and builds their compact substitutes.	3
Then, we randomly replace the original modules with their substitutes to train the compact modules to mimic the behavior of the original modules.	3
We progressively increase the probability of replacement through the training.	3
In this way, our approach brings a deeper level of interaction between the original and compact models.	4
Compared to the previous knowledge distillation approaches for BERT compression, our approach does not introduce any additional loss function.	5
Our approach outperforms existing knowledge distillation approaches on GLUE benchmark, showing a new perspective of model compression.	4+5
----------
Deep pretrained language models have achieved great success in the way of pretraining first and then fine-tuning.	1
But such a sequential transfer learning paradigm often confronts the catastrophic forgetting problem and leads to sub-optimal performance.	1
To fine-tune with less forgetting, we propose a recall and learn mechanism, which adopts the idea of multi-task learning and jointly learns pretraining tasks and downstream tasks.	2+3
Specifically, we introduce a Pretraining Simulation mechanism to recall the knowledge from pretraining tasks without data, and an Objective Shifting mechanism to focus the learning on downstream tasks gradually.	3
Experiments show that our method achieves state-of-the-art performance on the GLUE benchmark.	4
Our method also enables BERT-base to achieve better average performance than directly fine-tuning of BERT-large.	4
Further, we provide the open-source RecAdam optimizer, which integrates the proposed mechanisms into Adam optimizer, to facility the NLP community.	4
----------
Active learning strives to reduce annotation costs by choosing the most critical examples to label.	1
Typically, the active learning strategy is contingent on the classification model.	1
For instance, uncertainty sampling depends on poorly calibrated model confidence scores.	1
In the cold-start setting, active learning is impractical because of model instability and data scarcity.	1
Fortunately, modern NLP provides an additional source of information: pre-trained language models.	1
The pre-training loss can find examples that surprise the model and should be labeled for efficient fine-tuning.	1
Therefore, we treat the language modeling loss as a proxy for classification uncertainty.	2
With BERT, we develop a simple strategy based on the masked language modeling loss that minimizes labeling costs for text classification.	3
Compared to other baselines, our approach reaches higher accuracy within less sampling iterations and computation time.	4
----------
Deep neural networks have become the standard approach to building reliable Natural Language Processing (NLP) applications, ranging from Neural Machine Translation (NMT) to dialogue systems.	1
However, improving accuracy by increasing the model size requires a large number of hardware computations, which can slow down NLP applications significantly at inference time.	1
To address this issue, we propose a novel vector-vector-matrix architecture (VVMA), which greatly reduces the latency at inference time for NMT.	2+3
This architecture takes advantage of specialized hardware that has low-latency vector-vector operations and higher-latency vector-matrix operations.	3
It also reduces the number of parameters and FLOPs for virtually all models that rely on efficient matrix multipliers without significantly impacting accuracy.	4
We present empirical results suggesting that our framework can reduce the latency of sequence-to-sequence and Transformer models used for NMT by a factor of four.	4+5
Finally, we show evidence suggesting that our VVMA extends to other domains, and we discuss novel hardware for its efficient use.	5
----------
Prosody is a rich information source in natural language, serving as a marker for phenomena such as contrast.	1
In order to make this information available to downstream tasks, we need a way to detect prosodic events in speech.	1
We propose a new model for pitch accent detection, inspired by the work of Stehwien et al. (2018), who presented a CNN-based model for this task.	2
Our model makes greater use of context by using full utterances as input and adding an LSTM layer.	3
We find that these innovations lead to an improvement from 87.5% to 88.7% accuracy on pitch accent detection on American English speech in the Boston University Radio News Corpus, a state-of-the-art result.	4+5
We also find that a simple baseline that just predicts a pitch accent on every content word yields 82.2% accuracy, and we suggest that this is the appropriate baseline for this task.	4+5
Finally, we conduct ablation tests that show pitch is the most important acoustic feature for this task and this corpus.	4
----------
Natural language processing has recently made stock movement forecasting and volatility forecasting advances, leading to improved financial forecasting.	1
Transcripts of companies’ earnings calls are well studied for risk modeling, offering unique investment insight into stock performance.	1
However, vocal cues in the speech of company executives present an underexplored rich source of natural language data for estimating financial risk.	1
Additionally, most existing approaches ignore the correlations between stocks.	1
Building on existing work, we introduce a neural model for stock volatility prediction that accounts for stock interdependence via graph convolutions while fusing verbal, vocal, and financial features in a semi-supervised multi-task risk forecasting formulation.	2+3
Our proposed model, VolTAGE, outperforms existing methods demonstrating the effectiveness of multimodal learning for volatility prediction.	4
----------
Directly translating from speech to text using an end-to-end approach is still challenging for many language pairs due to insufficient data.	1
Although pretraining the encoder parameters using the Automatic Speech Recognition (ASR) task improves the results in low resource settings, attempting to use pretrained parameters from the Neural Machine Translation (NMT) task has been largely unsuccessful in previous works.	1
In this paper, we will show that by using an adversarial regularizer, we can bring the encoder representations of the ASR and NMT tasks closer even though they are in different modalities, and how this helps us effectively use a pretrained NMT decoder for speech translation.	2+3
----------
We introduce a new keyphrase generation approach using Generative Adversarial Networks (GANs).	2+3
For a given document, the generator produces a sequence of keyphrases, and the discriminator distinguishes between human-curated and machine-generated keyphrases.	3
We evaluated this approach on standard benchmark datasets.	3
We observed that our model achieves state-of-the-art performance in the generation of abstractive keyphrases and is comparable to the best performing extractive techniques.	4
Although we achieve promising results using GANs, they are not significantly better than the state-of-the-art generative models.	5
To our knowledge, this is one of the first works that use GANs for keyphrase generation.	5
We present a detailed analysis of our observations and expect that these findings would help other researchers to further study the use of GANs for the task of keyphrase generation.	5
----------
Text classification is a fundamental problem in natural language processing.	1
Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus.	1
However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph.	1
To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer).	2
Our model learns effective node representations by capturing structure and heterogeneity from the text graph.	3
We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus.	3
Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.	3+4
----------
Books are typically segmented into chapters and sections, representing coherent sub-narratives and topics.	1
We investigate the task of predicting chapter boundaries, as a proxy for the general task of segmenting long texts.	2
We build a Project Gutenberg chapter segmentation data set of 9,126 English novels, using a hybrid approach combining neural inference and rule matching to recognize chapter title headers in books, achieving an F1-score of 0.77 on this task.	3+4
Using this annotated data as ground truth after removing structural cues, we present cut-based and neural methods for chapter segmentation, achieving a F1-score of 0.453 on the challenging task of exact break prediction over book-length documents.	3+4
Finally, we reveal interesting historical trends in the chapter structure of novels.	5
----------
Millions of people irrespective of socioeconomic and demographic backgrounds, depend on Wikipedia articles everyday for keeping themselves informed regarding popular as well as obscure topics.	1
Articles have been categorized by editors into several quality classes, which indicate their reliability as encyclopedic content.	1
This manual designation is an onerous task because it necessitates profound knowledge about encyclopedic language, as well navigating circuitous set of wiki guidelines.	1
In this paper we propose Neural wikipedia Quality Monitor (NwQM), a novel deep learning model which accumulates signals from several key information sources such as article text, meta data and images to obtain improved Wikipedia article representation.	2+3
We present comparison of our approach against a plethora of available solutions and show 8% improvement over state-of-the-art approaches with detailed ablation studies.	3+4
----------
In the financial domain, risk modeling and profit generation heavily rely on the sophisticated and intricate stock movement prediction task.	1
Stock forecasting is complex, given the stochastic dynamics and non-stationary behavior of the market.	1
Stock movements are influenced by varied factors beyond the conventionally studied historical prices, such as social media and correlations among stocks.	1
The rising ubiquity of online content and knowledge mandates an exploration of models that factor in such multimodal signals for accurate stock forecasting.	1
We introduce an architecture that achieves a potent blend of chaotic temporal signals from financial data, social media, and inter-stock relationships via a graph neural network in a hierarchical temporal fashion.	2+3
Through experiments on real-world S&P 500 index data and English tweets, we show the practical applicability of our model as a tool for investment decision making and trading.	3+4
----------
State of the art research for date-time entity extraction from text is task agnostic.	1
Consequently, while the methods proposed in literature perform well for generic date-time extraction from texts, they don’t fare as well on task specific date-time entity extraction where only a subset of the date-time entities present in the text are pertinent to solving the task.	1
Furthermore, some tasks require identifying negation constraints associated with the date-time entities to correctly reason over time.	1
We showcase a novel model for extracting task-specific date-time entities along with their negation constraints.	2
We show the efficacy of our method on the task of date-time understanding in the context of scheduling meetings for an email-based digital AI scheduling assistant.	4
Our method achieves an absolute gain of 19% f-score points compared to baseline methods in detecting the date-time entities relevant to scheduling meetings and a 4% improvement over baseline methods for detecting negation constraints over date-time entities.	4+5
----------
This paper presents a comprehensive study on resume classification to reduce the time and labor needed to screen an overwhelming number of applications significantly, while improving the selection of suitable candidates.	1+2
A total of 6,492 resumes are extracted from 24,933 job applications for 252 positions designated into four levels of experience for Clinical Research Coordinators (CRC).	3
Each resume is manually annotated to its most appropriate CRC position by experts through several rounds of triple annotation to establish guidelines.	3
As a result, a high Kappa score of 61% is achieved for inter-annotator agreement.	4
Given this dataset, novel transformer-based classification models are developed for two tasks: the first task takes a resume and classifies it to a CRC level (T1), and the second task takes both a resume and a job description to apply and predicts if the application is suited to the job (T2).	3
Our best models using section encoding and a multi-head attention decoding give results of 73.3% to T1 and 79.2% to T2.	4
Our analysis shows that the prediction errors are mostly made among adjacent CRC levels, which are hard for even experts to distinguish, implying the practical value of our models in real HR platforms.	5
----------
Semantic change detection concerns the task of identifying words whose meaning has changed over time.	1
Current state-of-the-art approaches operating on neural embeddings detect the level of semantic change in a word by comparing its vector representation in two distinct time periods, without considering its evolution through time.	1
In this work, we propose three variants of sequential models for detecting semantically shifted words, effectively accounting for the changes in the word representations over time.	2+3
Through extensive experimentation under various settings with synthetic and real data we showcase the importance of sequential modelling of word vectors through time for semantic change detection.	4
Finally, we compare different approaches in a quantitative manner, demonstrating that temporal modelling of word representations yields a clear-cut advantage in performance.	4
----------
In this paper, we demonstrate that by utilizing sparse word representations, it becomes possible to surpass the results of more complex task-specific models on the task of fine-grained all-words word sense disambiguation.	2+3
Our proposed algorithm relies on an overcomplete set of semantic basis vectors that allows us to obtain sparse contextualized word representations.	2
We introduce such an information theory-inspired synset representation based on the co-occurrence of word senses and non-zero coordinates for word forms which allows us to achieve an aggregated F-score of 78.8 over a combination of five standard word sense disambiguating benchmark datasets.	3+4
We also demonstrate the general applicability of our proposed framework by evaluating it towards part-of-speech tagging on four different treebanks.	3
Our results indicate a significant improvement over the application of the dense word representations.	4
----------
Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models.	1
Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents.	1
We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document.	2
We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy	4
----------
Extracting temporal relations between events and time expressions has many applications such as constructing event timelines and time-related question answering.	1
It is a challenging problem which requires syntactic and semantic information at sentence or discourse levels, which may be captured by deep contextualized language models (LMs) such as BERT (Devlin et al., 2019).	1
In this paper, we develop several variants of BERT-based temporal dependency parser, and show that BERT significantly improves temporal dependency parsing (Zhang and Xue, 2018a).	2+3
We also present a detailed analysis on why deep contextualized neural LMs help and where they may fall short.	5
Source code and resources are made available at https://github.com/bnmin/tdp_ranking.	6
----------
The goal of open information extraction (OIE) is to extract facts from natural language text, and to represent them as structured triples of the form <subject,predicate, object>.	2
For example, given the sentence “Beethoven composed the Ode to Joy.”, we are expected to extract the triple <Beethoven, composed, Ode to Joy>.	3
In this work, we systematically compare different neural network architectures and training approaches, and improve the performance of the currently best models on the OIE16 benchmark (Stanovsky and Dagan, 2016) by 0.421 F1 score and 0.420 AUC-PR, respectively, in our experiments (i.e., by more than 200% in both cases).	3+4
Furthermore, we show that appropriate problem and loss formulations often affect the performance more than the network architecture.	4
----------
Active learning is an important technique for low-resource sequence labeling tasks.	1
However, current active sequence labeling methods use the queried samples alone in each iteration, which is an inefficient way of leveraging human annotations.	1
We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling.	2+3
Our method, SeqMix, simply augments the queried samples by generating extra labeled sequences in each iteration.	3
The key difficulty is to generate plausible sequences along with token-level labels.	3
In SeqMix, we address this challenge by performing mixup for both sequences and token-level labels of the queried samples.	3
Furthermore, we design a discriminator during sequence mixup, which judges whether the generated sequences are plausible or not.	3
Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27%–3.75% in terms of F1 scores.	4
The code and data for SeqMix can be found at https://github.com/rz-zhang/SeqMix.	4
----------
In this work, we present a dual learning approach for unsupervised text to path and path to text transfers in Commonsense Knowledge Bases (KBs).	2
We investigate the impact of weak supervision by creating a weakly supervised dataset and show that even a slight amount of supervision can significantly improve the model performance and enable better-quality transfers.	3+4
We examine different model architectures, and evaluation metrics, proposing a novel Commonsense KB completion metric tailored for generative models.	2+3
Extensive experimental results show that the proposed method compares very favorably to the existing baselines.	4
This approach is a viable step towards a more advanced system for automatic KB construction/expansion and the reverse operation of KB conversion to coherent textual descriptions.	4
----------
Data-to-text generation has recently attracted substantial interests due to its wide applications.	1
Existing methods have shown impressive performance on an array of tasks.	1
However, they rely on a significant amount of labeled data for each task, which is costly to acquire and thus limits their application to new tasks and domains.	1
In this paper, we propose to leverage pre-training and transfer learning to address this issue.	2+3
We propose a knowledge-grounded pre-training (KGPT), which consists of two parts, 1) a general knowledge-grounded generation model to generate knowledge-enriched text.	2+3
2) a pre-training paradigm on a massive knowledge-grounded text corpus crawled from the web.	3
The pre-trained model can be fine-tuned on various data-to-text generation tasks to generate task-specific text.	3
We adopt three settings, namely fully-supervised, zero-shot, few-shot to evaluate its effectiveness.	3
Under the fully-supervised setting, our model can achieve remarkable gains over the known baselines.	4
Under zero-shot setting, our model without seeing any examples achieves over 30 ROUGE-L on WebNLG while all other baselines fail.	4
Under the few-shot setting, our model only needs about one-fifteenth as many labeled examples to achieve the same level of performance as baseline models.	1
These experiments consistently prove the strong generalization ability of our proposed framework.	1
----------
Large-scale pre-trained language models, such as BERT and GPT-2, have achieved excellent performance in language representation learning and free-form text generation.	1
However, these models cannot be directly employed to generate text under specified lexical constraints.	1
To address this challenge, we present POINTER (PrOgressive INsertion-based TransformER), a simple yet novel insertion-based approach for hard-constrained text generation.	1+2
The proposed method operates by progressively inserting new tokens between existing tokens in a parallel manner.	3
This procedure is recursively applied until a sequence is completed.	3
The resulting coarse-to-fine hierarchy makes the generation process intuitive and interpretable.	4
We pre-train our model with the proposed progressive insertion-based objective on a 12GB Wikipedia dataset, and fine-tune it on downstream hard-constrained generation tasks.	3
Non-autoregressive decoding yields a logarithmic time complexity during inference time.	3
Experimental results on both News and Yelp datasets demonstrate that Pointer achieves state-of-the-art performance on constrained text generation.	4
We released the pre-trained models and the source code to facilitate future research.	5
----------
Self-supervised pre-training, such as BERT, MASS and BART, has emerged as a powerful technique for natural language understanding and generation.	1
Existing pre-training techniques employ autoencoding and/or autoregressive objectives to train Transformer-based models by recovering original word tokens from corrupted text with some masked tokens.	1
The training goals of existing techniques are often inconsistent with the goals of many language generation tasks, such as generative question answering and conversational response generation, for producing new text given context.	1
This work presents PALM with a novel scheme that jointly pre-trains an autoencoding and autoregressive language model on a large unlabeled corpus, specifically designed for generating new text conditioned on context.	1+2
The new scheme alleviates the mismatch introduced by the existing denoising scheme between pre-training and fine-tuning where generation is more than reconstructing original text.	4
An extensive set of experiments show that PALM achieves new state-of-the-art results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues.	4
----------
Lexically constrained generation requires the target sentence to satisfy some lexical constraints, such as containing some specific words or being the paraphrase to a given sentence, which is very important in many real-world natural language generation applications.	1
Previous works usually apply beam-search-based methods or stochastic searching methods to lexically-constrained generation.	1
However, when the search space is too large, beam-search-based methods always fail to find the constrained optimal solution.	1
At the same time, stochastic search methods always cost too many steps to find the correct optimization direction.	1
In this paper, we propose a novel method G2LC to solve the lexically-constrained generation as an unsupervised gradient-guided optimization problem.	1+2
We propose a differentiable objective function and use the gradient to help determine which position in the sequence should be changed (deleted or inserted/replaced by another word).	2+3
The word updating process of the inserted/replaced word also benefits from the guidance of gradient.	3
Besides, our method is free of parallel data training, which is flexible to be used in the inference stage of any pre-trained generation model.	3
We apply G2LC to two generation tasks: keyword-to-sentence generation and unsupervised paraphrase generation.	3
The experiment results show that our method achieves state-of-the-art compared to previous lexically-constrained methods.	4
----------
Sequence generation models trained with teacher-forcing suffer from issues related to exposure bias and lack of differentiability across timesteps.	1
Our proposed method, Teacher-Forcing with N-grams (TeaForN), addresses both these problems directly, through the use of a stack of N decoders trained to decode along a secondary time axis that allows model-parameter updates based on N prediction steps.	2+3
TeaForN can be used with a wide class of decoder architectures and requires minimal modifications from a standard teacher-forcing setup.	3
Empirically, we show that TeaForN boosts generation quality on one Machine Translation benchmark, WMT 2014 English-French, and two News Summarization benchmarks, CNN/Dailymail and Gigaword.	4
----------
Text-based games present a unique challenge for autonomous agents to operate in natural language and handle enormous action spaces.	1
In this paper, we propose the Contextual Action Language Model (CALM) to generate a compact set of action candidates at each game state.	2
Our key insight is to train language models on human gameplay, where people demonstrate linguistic priors and a general game sense for promising actions conditioned on game history.	2
We combine CALM with a reinforcement learning agent which re-ranks the generated action candidates to maximize in-game rewards.	3
We evaluate our approach using the Jericho benchmark, on games unseen by CALM during training.	3
Our method obtains a 69% relative improvement in average game score over the previous state-of-the-art model.	4
Surprisingly, on half of these games, CALM is competitive with or better than other models that have access to ground truth admissible actions.	4+5
Code and data are available at https://github.com/princeton-nlp/calm-textgame.	6
----------
Mirroring the success of masked language models, vision-and-language counterparts like VILBERT, LXMERT and UNITER have achieved state of the art performance on a variety of multimodal discriminative tasks like visual question answering and visual grounding.	1
Recent work has also successfully adapted such models towards the generative task of image captioning.	1
This begs the question: Can these models go the other way and generate images from pieces of text?	1
Our analysis of a popular representative from this model family – LXMERT – finds that it is unable to generate rich and semantically meaningful imagery with its current training setup.	1
We introduce X-LXMERT, an extension to LXMERT with training refinements including: discretizing visual representations, using uniform masking with a large range of masking ratios and aligning the right pre-training datasets to the right objectives which enables it to paint.	2
X-LXMERT’s image generation capabilities rival state of the art generative models while its question answering and captioning abilities remains comparable to LXMERT.	4
Finally, we demonstrate the generality of these training refinements by adding image generation capabilities into UNITER to produce X-UNITER.	4
----------
In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering.	2
To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders.	3
Given this hierarchical graph, the initial node representations are updated through graph propagation, and multi-hop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction).	3
By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously.	4
Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches.	4
----------
Has there been real progress in multi-hop question-answering?	1
Models often exploit dataset artifacts to produce correct answers, without connecting information across multiple supporting facts.	1
This limits our ability to measure true progress and defeats the purpose of building multi-hop QA datasets.	1
We make three contributions towards addressing this.	1
First, we formalize such undesirable behavior as disconnected reasoning across subsets of supporting facts.	2
This allows developing a model-agnostic probe for measuring how much any model can cheat via disconnected reasoning.	4
Second, using a notion of contrastive support sufficiency, we introduce an automatic transformation of existing datasets that reduces the amount of disconnected reasoning.	3
Third, our experiments suggest that there hasn’t been much progress in multi-hop QA in the reading comprehension setting.	3
For a recent large-scale model (XLNet), we show that only 18 points out of its answer F1 score of 72 on HotpotQA are obtained through multifact reasoning, roughly the same as that of a simpler RNN baseline.	3
Our transformation substantially reduces disconnected reasoning (19 points in answer F1).	4
It is complementary to adversarial approaches, yielding further reductions in conjunction.	4+5
----------
We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering.	1
Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet.	1+2
Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions.	2+3
We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer.	3
We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets.	4
ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency.	4
Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction.	4
----------
This work deals with the challenge of learning and reasoning over multi-hop question answering (QA).	1
We propose a graph reasoning network based on the semantic structure of the sentences to learn cross paragraph reasoning paths and find the supporting facts and the answer jointly.	2+3
The proposed graph is a heterogeneous document-level graph that contains nodes of type sentence (question, title, and other sentences), and semantic role labeling sub-graphs per sentence that contain arguments as nodes and predicates as edges.	3
Incorporating the argument types, the argument phrases, and the semantics of the edges originated from SRL predicates into the graph encoder helps in finding and also the explainability of the reasoning paths.	3
Our proposed approach shows competitive performance on the HotpotQA distractor setting benchmark compared to the recent state-of-the-art models.	4
----------
Sentiment classification on tweets often needs to deal with the problems of under-specificity, noise, and multilingual content.	1
This study proposes a heterogeneous multi-layer network-based representation of tweets to generate multiple representations of a tweet and address the above issues.	2
The generated representations are further ensembled and classified using a neural-based early fusion approach.	3
Further, we propose a centrality aware random-walk for node embedding and tweet representations suitable for the multi-layer network.	2
From various experimental analysis, it is evident that the proposed method can address the problem of under-specificity, noisy text, and multilingual content present in a tweet and provides better classification performance than the text-based counterparts.	4
Further, the proposed centrality aware based random walk provides better representations than unbiased and other biased counterparts.	4
----------
Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence.	1
Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research.	1
In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words.	2
We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE.	2
The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.	4
----------
Current approaches to empathetic response generation view the set of emotions expressed in the input text as a flat structure, where all the emotions are treated uniformly.	1
We argue that empathetic responses often mimic the emotion of the user to a varying degree, depending on its positivity or negativity and content.	1
We show that the consideration of these polarity-based emotion clusters and emotional mimicry results in improved empathy and contextual relevance of the response as compared to the state-of-the-art.	1
Also, we introduce stochasticity into the emotion mixture that yields emotionally more varied empathetic responses than the previous work.	2
We demonstrate the importance of these factors to empathetic response generation using both automatic- and human-based evaluations.	3
The implementation of MIME is publicly available at https://github.com/declare-lab/MIME.	6
----------
In this work, we aim at equipping pre-trained language models with structured knowledge.	2
We present two self-supervised tasks learning over raw text with the guidance from knowledge graphs.	3
Building upon entity-level masked language models, our first contribution is an entity masking scheme that exploits relational knowledge underlying the text.	3
This is fulfilled by using a linked knowledge graph to select informative entities and then masking their mentions.	3
In addition, we use knowledge graphs to obtain distractors for the masked entities, and propose a novel distractor-suppressed ranking objective that is optimized jointly with masked language model.	3
In contrast to existing paradigms, our approach uses knowledge graphs implicitly, only during pre-training, to inject language models with structured knowledge via learning from raw text.	3
It is more efficient than retrieval-based methods that perform entity linking and integration during finetuning and inference, and generalizes more effectively than the methods that directly learn from concatenated graph triples.	4
Experiments show that our proposed model achieves improved performance on five benchmarks, including question answering and knowledge base completion.	4
----------
Deep neural network models have helped named entity recognition achieve amazing performance without handcrafting features.	1
However, existing systems require large amounts of human annotated training data.	1
Efforts have been made to replace human annotations with external knowledge (e.g., NE dictionary, part-of-speech tags), while it is another challenge to obtain such effective resources.	1
In this work, we propose a fully unsupervised NE recognition model which only needs to take informative clues from pre-trained word embeddings.	2
We first apply Gaussian Hidden Markov Model and Deep Autoencoding Gaussian Mixture Model on word embeddings for entity span detection and type prediction, and then further design an instance selector based on reinforcement learning to distinguish positive sentences from noisy sentences and then refine these coarse-grained annotations through neural networks.	3
Extensive experiments on two CoNLL benchmark NER datasets (CoNLL-2003 English dataset and CoNLL-2002 Spanish dataset) demonstrate that our proposed light NE recognition model achieves remarkable performance without using any annotated lexicon or corpus.	4
----------
Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications.	1
Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified.	1
In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents.	1+2
We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification.	3
Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training.	3
We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name.	4
----------
Advances on deep generative models have attracted significant research interest in neural topic modeling.	1
The recently proposed Adversarial-neural Topic Model models topics with an adversarially trained generator network and employs Dirichlet prior to capture the semantic patterns in latent topics.	1
It is effective in discovering coherent topics but unable to infer topic distributions for given documents or utilize available document labels.	1
To overcome such limitations, we propose Topic Modeling with Cycle-consistent Adversarial Training (ToMCAT) and its supervised version sToMCAT.	2+3
ToMCAT employs a generator network to interpret topics and an encoder network to infer document topics.	3
Adversarial training and cycle-consistent constraints are used to encourage the generator and the encoder to produce realistic samples that coordinate with each other.	3
sToMCAT extends ToMCAT by incorporating document labels into the topic modeling process to help discover more coherent topics.	4
The effectiveness of the proposed models is evaluated on unsupervised/supervised topic modeling and text classification.	4
The experimental results show that our models can produce both coherent and informative topics, outperforming a number of competitive baselines.	4
----------
In this paper, we study automatic rumor detection for in social media at the event level where an event consists of a sequence of posts organized according to the posting time.	2
It is common that the state of an event is dynamically evolving.	1
However, most of the existing methods to this task ignored this problem, and established a global representation based on all the posts in the event’s life cycle.	1
Such coarse-grained methods failed to capture the event’s unique features in different states.	1
To address this limitation, we propose a state-independent and time-evolving Network (STN) for rumor detection based on fine-grained event state detection and segmentation.	2
Given an event composed of a sequence of posts, STN first predicts the corresponding sequence of states and segments the event into several state-independent sub-events.	3
For each sub-event, STN independently trains an encoder to learn the feature representation for that sub-event and incrementally fuses the representation of the current sub-event with previous ones for rumor prediction.	3
This framework can more accurately learn the representation of an event in the initial stage and enable early rumor detection.	4
Experiments on two benchmark datasets show that STN can significantly improve the rumor detection accuracy in comparison with some strong baseline systems.	4
We also design a new evaluation metric to measure the performance of early rumor detection, under which STN shows a higher advantage in comparison.	4
----------
Simultaneously modeling source code and natural language has many exciting applications in automated software development and understanding.	1
Pursuant to achieving such technology, we introduce PyMT5, the Python method text-to-text transfer transformer, which is trained to translate between all pairs of Python method feature combinations: a single model that can both predict whole methods from natural language documentation strings (docstrings) and summarize code into docstrings of any common style.	1+2
We present an analysis and modeling effort of a large-scale parallel corpus of 26 million Python methods and 7.7 million method-docstring pairs, demonstrating that for docstring and method generation, PyMT5 outperforms similarly-sized auto-regressive language models (GPT2) which were English pre-trained or randomly initialized.	3+4
On the CodeSearchNet test set, our best model predicts 92.1% syntactically correct method bodies, achieved a BLEU score of 8.59 for method generation and 16.3 for docstring generation (summarization), and achieved a ROUGE-L F-score of 24.8 for method generation and 36.7 for docstring generation.	4
----------
Existing research for question generation encodes the input text as a sequence of tokens without explicitly modeling fact information.	1
These models tend to generate irrelevant and uninformative questions.	1
In this paper, we explore to incorporate facts in the text for question generation in a comprehensive way.	2
We present a novel task of question generation given a query path in the knowledge graph constructed from the input text.	3
We divide the task into two steps, namely, query representation learning and query-based question generation.	3
We formulate query representation learning as a sequence labeling problem for identifying the involved facts to form a query and employ an RNN-based generator for question generation.	3
We first train the two modules jointly in an end-to-end fashion, and further enforce the interaction between these two modules in a variational framework.	3
We construct the experimental datasets on top of SQuAD and results show that our model outperforms other state-of-the-art approaches, and the performance margin is larger when target questions are complex.	4
Human evaluation also proves that our model is able to generate relevant and informative questions.	5
----------
Recognizing the flow of time in a story is a crucial aspect of understanding it.	1
Prior work related to time has primarily focused on identifying temporal expressions or relative sequencing of events, but here we propose computationally annotating each line of a book with wall clock times, even in the absence of explicit time-descriptive phrases.	2+3
To do so, we construct a data set of hourly time phrases from 52,183 fictional books.	3
We then construct a time-of-day classification model that achieves an average error of 2.27 hours.	3
Furthermore, we show that by analyzing a book in whole using dynamic programming of breakpoints, we can roughly partition a book into segments that each correspond to a particular time-of-day.	3+4
This approach improves upon baselines by over two hour.	4
Finally, we apply our model to a corpus of literature categorized by different periods in history, to show interesting trends of hourly activity throughout the past.	4
Among several observations we find that the fraction of events taking place past 10 P.M jumps past 1880 - coincident with the advent of the electric light bulb and city lights.	5
----------
Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts.	1
To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English.	1+2
The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures.	3
In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96–99%), but generalization accuracy was substantially lower (16–35%) and showed high sensitivity to random seed (+-6–8%).	4
These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.	4+5
----------
Pre-trained contextual representations like BERT have achieved great success in natural language processing.	1
However, the sentence embeddings from the pre-trained language models without fine-tuning have been found to poorly capture semantic meaning of sentences.	1
In this paper, we argue that the semantic information in the BERT embeddings is not fully exploited.	2
We first reveal the theoretical connection between the masked language model pre-training objective and the semantic similarity task theoretically, and then analyze the BERT sentence embeddings empirically.	3
We find that BERT always induces a non-smooth anisotropic semantic space of sentences, which harms its performance of semantic similarity.	4
To address this issue, we propose to transform the anisotropic sentence embedding distribution to a smooth and isotropic Gaussian distribution through normalizing flows that are learned with an unsupervised objective.	4
Experimental results show that our proposed BERT-flow method obtains significant performance gains over the state-of-the-art sentence embeddings on a variety of semantic textual similarity tasks.	4
The code is available at https://github.com/bohanli/BERT-flow.	6
----------
Neural language models are often trained with maximum likelihood estimation (MLE), where the next word is generated conditioned on the ground-truth word tokens.	1
During testing, however, the model is instead conditioned on previously generated tokens, resulting in what is termed exposure bias.	1
To reduce this gap between training and testing, we propose using optimal transport (OT) to match the sequences generated in these two modes.	1+2
We examine the necessity of adding Student-Forcing scheme during training with an imitation learning interpretation.	3
An extension is further proposed to improve the OT learning for long sequences, based on the structural and contextual information of the text sequences.	2+3
The effectiveness of the proposed method is validated on machine translation, text summarization, and text generation tasks.	4
----------
Despite the success of existing referenced metrics (e.g., BLEU and MoverScore), they correlate poorly with human judgments for open-ended text generation including story or dialog generation because of the notorious one-to-many issue: there are many plausible outputs for the same input, which may differ substantially in literal or semantics from the limited number of given references.	1
To alleviate this issue, we propose UNION, a learnable UNreferenced metrIc for evaluating Open-eNded story generation, which measures the quality of a generated story without any reference.	2+3
Built on top of BERT, UNION is trained to distinguish human-written stories from negative samples and recover the perturbation in negative stories.	3
We propose an approach of constructing negative samples by mimicking the errors commonly observed in existing NLG models, including repeated plots, conflicting logic, and long-range incoherence.	2+3
Experiments on two story datasets demonstrate that UNION is a reliable measure for evaluating the quality of generated stories, which correlates better with human judgments and is more generalizable than existing state-of-the-art metrics.	4
----------
Despite recent advances in neural text generation, encoding the rich diversity in human language remains elusive.	1
We argue that the sub-optimal text generation is mainly attributable to the imbalanced token distribution, which particularly misdirects the learning model when trained with the maximum-likelihood objective.	1
As a simple yet effective remedy, we propose two novel methods, Fˆ2-Softmax and MefMax, for a balanced training even with the skewed frequency distribution.	2+3
MefMax assigns tokens uniquely to frequency classes, trying to group tokens with similar frequencies and equalize frequency mass between the classes.	3
Fˆ2-Softmax then decomposes a probability distribution of the target token into a product of two conditional probabilities of (1) frequency class, and (2) token from the target frequency class.	3
Models learn more uniform probability distributions because they are confined to subsets of vocabularies.	4
Significant performance gains on seven relevant metrics suggest the supremacy of our approach in improving not only the diversity but also the quality of generated texts.	5
----------
The Data-to-Text task aims to generate human-readable text for describing some given structured data enabling more interpretability.	1
However, the typical generation task is confined to a few particular domains since it requires well-aligned data which is difficult and expensive to obtain.	1
Using partially-aligned data is an alternative way of solving the dataset scarcity problem.	1+2
This kind of data is much easier to obtain since it can be produced automatically.	1+2
However, using this kind of data induces the over-generation problem posing difficulties for existing models, which tends to add unrelated excerpts during the generation procedure.	3
In order to effectively utilize automatically annotated partially-aligned datasets, we extend the traditional generation task to a refined task called Partially-Aligned Data-to-Text Generation (PADTG) which is more practical since it utilizes automatically annotated data for training and thus considerably expands the application domains.	3
To tackle this new task, we propose a novel distant supervision generation framework.	2
It firstly estimates the input data’s supportiveness for each target word with an estimator and then applies a supportiveness adaptor and a rebalanced beam search to harness the over-generation problem in the training and generation phases respectively.	3
We also contribute a partially-aligned dataset (The data and source code of this paper can be obtained from https://github.com/fuzihaofzh/distant_supervision_nlg) by sampling sentences from Wikipedia and automatically extracting corresponding KB triples for each sentence from Wikidata.	3+4
The experimental results show that our framework outperforms all baseline models as well as verify the feasibility of utilizing partially-aligned data.	4+5
----------
Existing persona-grounded dialog models often fail to capture simple implications of given persona descriptions, something which humans are able to do seamlessly.	1
For example, state-of-the-art models cannot infer that interest in hiking might imply love for nature or longing for a break.	1
In this paper, we propose to expand available persona sentences using existing commonsense knowledge bases and paraphrasing resources to imbue dialog models with access to an expanded and richer set of persona descriptions.	2
Additionally, we introduce fine-grained grounding on personas by encouraging the model to make a discrete choice among persona sentences while synthesizing a dialog response.	3
Since such a choice is not observed in the data, we model it using a discrete latent random variable and use variational learning to sample from hundreds of persona expansions.	3
Our model outperforms competitive baselines on the Persona-Chat dataset in terms of dialog quality and diversity while achieving persona-consistent and controllable dialog generation.	4+5
----------
Structured belief states are crucial for user goal tracking and database query in task-oriented dialog systems.	1
However, training belief trackers often requires expensive turn-level annotations of every user utterance.	1
In this paper we aim at alleviating the reliance on belief state labels in building end-to-end dialog systems, by leveraging unlabeled dialog data towards semi-supervised learning.	2
We propose a probabilistic dialog model, called the LAtent BElief State (LABES) model, where belief states are represented as discrete latent variables and jointly modeled with system responses given user inputs.	2+3
Such latent variable modeling enables us to develop semi-supervised learning under the principled variational learning framework.	4
Furthermore, we introduce LABES-S2S, which is a copy-augmented Seq2Seq model instantiation of LABES.	4
In supervised experiments, LABES-S2S obtains strong results on three benchmark datasets of different scales.	4
In utilizing unlabeled dialog data, semi-supervised LABES-S2S significantly outperforms both supervised-only and semi-supervised baselines.	4
Remarkably, we can reduce the annotation demands to 50% without performance loss on MultiWOZ.	4
----------
Automatically evaluating dialogue coherence is a challenging but high-demand ability for developing high-quality open-domain dialogue systems.	1
However, current evaluation metrics consider only surface features or utterance-level semantics, without explicitly considering the fine-grained topic transition dynamics of dialogue flows.	1
Here, we first consider that the graph structure constituted with topics in a dialogue can accurately depict the underlying communication logic, which is a more natural way to produce persuasive metrics.	1+2
Capitalized on the topic-level dialogue graph, we propose a new evaluation metric GRADE, which stands for Graph-enhanced Representations for Automatic Dialogue Evaluation.	2+3
Specifically, GRADE incorporates both coarse-grained utterance-level contextualized representations and fine-grained topic-level graph representations to evaluate dialogue coherence.	3
The graph representations are obtained by reasoning over topic-level dialogue graphs enhanced with the evidence from a commonsense graph, including k-hop neighboring representations and hop-attention weights.	3
Experimental results show that our GRADE significantly outperforms other state-of-the-art metrics on measuring diverse dialogue models in terms of the Pearson and Spearman correlations with human judgments.	4
Besides, we release a new large-scale human evaluation benchmark to facilitate future research on automatic metrics.	5
----------
Two main approaches for evaluating the quality of machine-generated rationales are: 1) using human rationales as a gold standard; and 2) automated metrics based on how rationales affect model behavior.	1
An open question, however, is how human rationales fare with these automatic metrics.	1
Analyzing a variety of datasets and models, we find that human rationales do not necessarily perform well on these metrics.	1
To unpack this finding, we propose improved metrics to account for model-dependent baseline performance.	1+2
We then propose two methods to further characterize rationale quality, one based on model retraining and one on using “fidelity curves” to reveal properties such as irrelevance and redundancy.	3
Our work leads to actionable suggestions for evaluating and characterizing rationales.	3
----------
We present a method to produce abstractive summaries of long documents that exceed several thousand words via neural abstractive summarization.	2+3
We perform a simple extractive step before generating a summary, which is then used to condition the transformer language model on relevant information before being tasked with generating a summary.	3
We also show that this approach produces more abstractive summaries compared to prior work that employs a copy mechanism while still achieving higher ROUGE scores.	3+4
We provide extensive comparisons with strong baseline methods, prior state of the art work as well as multiple variants of our approach including those using only transformers, only extractive techniques and combinations of the two.	4
We examine these models using four different summarization tasks and datasets: arXiv papers, PubMed papers, the Newsroom and BigPatent datasets.	4
We find that transformer based methods produce summaries with fewer n-gram copies, leading to n-gram copying statistics that are more similar to human generated abstracts.	4
We include a human evaluation, finding that transformers are ranked highly for coherence and fluency, but purely extractive methods score higher for informativeness and relevance.	4
We hope that these architectures and experiments may serve as strong points of comparison for future work.	5
Note: The abstract above was collaboratively written by the authors and one of the models presented in this paper based on an earlier draft of this paper.	6
----------
Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE.	1
However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text.	1
To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection.	2+3
Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t.	3
the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models.	3
Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation.	4
----------
The most common metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents.	1
We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and generated summaries.	2
Training data is generated by applying a series of rule-based transformations to the sentences of source documents.The factual consistency model is then trained jointly for three tasks: 1) predict whether each summary sentence is factually consistent or not, 2) in either case, extract a span in the source document to support this consistency prediction, 3) for each summary sentence that is deemed inconsistent, extract the inconsistent span from it.	3
Transferring this model to summaries generated by several neural models reveals that this highly scalable approach outperforms previous models, including those trained with strong supervision using datasets from related domains, such as natural language inference and fact checking.	4
Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.	5
We also release a manually annotated dataset for factual consistency verification, code for training data generation, and trained model weights at https://github.com/salesforce/factCC.	5
----------
Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization.	1
However, while the field has progressed, our standard metrics have not – for nearly 20 years ROUGE has been the standard evaluation in most summarization papers.	1
In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings.	2+3
We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.	4
We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive).	5
----------
A popular multimedia news format nowadays is providing users with a lively video and a corresponding news article, which is employed by influential news media including CNN, BBC, and social media including Twitter and Weibo.	1
In such a case, automatically choosing a proper cover frame of the video and generating an appropriate textual summary of the article can help editors save time, and readers make the decision more effectively.	1
Hence, in this paper, we propose the task of Video-based Multimodal Summarization with Multimodal Output (VMSMO) to tackle such a problem.	1+2
The main challenge in this task is to jointly model the temporal dependency of video with semantic meaning of article.	2
To this end, we propose a Dual-Interaction-based Multimodal Summarizer (DIMS), consisting of a dual interaction module and multimodal generator.	3
In the dual interaction module, we propose a conditional self-attention mechanism that captures local semantic information within video and a global-attention mechanism that handles the semantic relationship between news text and video from a high level.	3
Extensive experiments conducted on a large-scale real-world VMSMO dataset show that DIMS achieves the state-of-the-art performance in terms of both automatic metrics and human evaluations.	4
----------
We introduce Span-ConveRT, a light-weight model for dialog slot-filling which frames the task as a turn-based span extraction task.	2
This formulation allows for a simple integration of conversational knowledge coded in large pretrained conversational models such as ConveRT (Henderson et al., 2019).	1
In order to inspire more work on span extraction for the slot-filling task, we also release RESTAURANTS-8K, a new challenging data set of 8,198 utterances, compiled from actual conversations in the restaurant booking domain.	3
We show that leveraging such knowledge in Span-ConveRT is especially useful for few-shot learning scenarios: we report consistent gains over 1) a span extractor that trains representations from scratch in the target domain, and 2) a BERT-based span extractor.	4
----------
Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition.	1
This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain.	1+2
We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset.	4
We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset.	4
We improve the zero-shot learning state of the art on average across domains by 21%.	5
----------
This work proposes a standalone, complete Chinese discourse parser for practical applications.	1+2
We approach Chinese discourse parsing from a variety of aspects and improve the shift-reduce parser not only by integrating the pre-trained text encoder, but also by employing novel training strategies.	1+2
We revise the dynamic-oracle procedure for training the shift-reduce parser, and apply unsupervised data augmentation to enhance rhetorical relation recognition.	3
Experimental results show that our Chinese discourse parser achieves the state-of-the-art performance.	4
----------
Implicit discourse relation recognition is a challenging task due to the lack of connectives as strong linguistic clues.	1
Previous methods primarily encode two arguments separately or extract the specific interaction patterns for the task, which have not fully exploited the annotated relation signal.	1
Therefore, we propose a novel TransS-driven joint learning architecture to address the issues.	2
Specifically, based on the multi-level encoder, we 1) translate discourse relations in low-dimensional embedding space (called TransS), which could mine the latent geometric structure information of argument-relation instances; 2) further exploit the semantic features of arguments to assist discourse understanding; 3) jointly learn 1) and 2) to mutually reinforce each other to obtain the better argument representations, so as to improve the performance of the task.	3
Extensive experimental results on the Penn Discourse TreeBank (PDTB) show that our model achieves competitive results against several state-of-the-art systems.	4
----------
Non-autoregressive (NAR) models generate all the tokens of a sequence in parallel, resulting in faster generation speed compared to their autoregressive (AR) counterparts but at the cost of lower accuracy.	1
Different techniques including knowledge distillation and source-target alignment have been proposed to bridge the gap between AR and NAR models in various tasks such as neural machine translation (NMT), automatic speech recognition (ASR), and text to speech (TTS).	1
With the help of those techniques, NAR models can catch up with the accuracy of AR models in some tasks but not in some others.	1
In this work, we conduct a study to understand the difficulty of NAR sequence generation and try to answer:	2
(1) Why NAR models can catch up with AR models in some tasks but not all?	2
(2) Why techniques like knowledge distillation and source-target alignment can help NAR models.	2
Since the main difference between AR and NAR models is that NAR models do not use dependency among target tokens while AR models do, intuitively the difficulty of NAR sequence generation heavily depends on the strongness of dependency among target tokens.	1
To quantify such dependency, we propose an analysis model called CoMMA to characterize the difficulty of different NAR sequence generation tasks.	2
We have several interesting findings:	4
1) Among the NMT, ASR and TTS tasks, ASR has the most target-token dependency while TTS has the least.	4
2) Knowledge distillation reduces the target-token dependency in target sequence and thus improves the accuracy of NAR models.	4
3) Source-target alignment constraint encourages dependency of a target token on source tokens and thus eases the training of NAR models.	4
----------
Cross-modal language generation tasks such as image captioning are directly hurt in their ability to support non-English languages by the trend of data-hungry models combined with the lack of non-English annotations.	1
We investigate potential solutions for combining existing language-generation annotations in English with translation capabilities in order to create solutions at web-scale in both domain and language coverage.	2+3
We describe an approach called Pivot-Language Generation Stabilization (PLuGS), which leverages directly at training time both existing English annotations (gold data) as well as their machine-translated versions (silver data); at run-time, it generates first an English caption and then a corresponding target-language caption.	3
We show that PLuGS models outperform other candidate solutions in evaluations performed over 5 different target languages, under a large-domain testset using images from the Open Images dataset.	4
Furthermore, we find an interesting effect where the English captions generated by the PLuGS models are better than the captions generated by the original, monolingual English model.	4
----------
Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data.	1
In this work, we propose the new task of few-shot natural language generation.	2
Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains.	2
The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge	3
With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement.	4+5
Our code and data can be found at https://github.com/czyssrs/Few-Shot-NLG	6
----------
Question answering (QA) is an important aspect of open-domain conversational agents, garnering specific research focus in the conversational QA (ConvQA) subtask.	1
One notable limitation of recent ConvQA efforts is the response being answer span extraction from the target corpus, thus ignoring the natural language generation (NLG) aspect of high-quality conversational agents.	1
In this work, we propose a method for situating QA responses within a SEQ2SEQ NLG approach to generate fluent grammatical answer responses while maintaining correctness.	2
From a technical perspective, we use data augmentation to generate training data for an end-to-end system.	3
Specifically, we develop Syntactic Transformations (STs) to produce question-specific candidate answer responses and rank them using a BERT-based classifier (Devlin et al., 2019).	3
Human evaluation on SQuAD 2.0 data (Rajpurkar et al., 2018) demonstrate that the proposed model outperforms baseline CoQA and QuAC models in generating conversational responses.	5
We further show our model’s scalability by conducting tests on the CoQA dataset.	6
The code and data are available at https://github.com/abaheti95/QADialogSystem.	6
----------
One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation.	1
An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia).	1
In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency.	2
We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semi-supervised learning) for training, against state-of-the-art baseline models.	3
The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training.	4
----------
Paraphrasing natural language sentences is a multifaceted process:	1
it might involve replacing individual words or short phrases, local rearrangement of content, or high-level restructuring like topicalization or passivization.	1
Past approaches struggle to cover this space of paraphrase possibilities in an interpretable manner.	1
Our work, inspired by pre-ordering literature in machine translation, uses syntactic transformations to softly “reorder” the source sentence and guide our neural paraphrasing model.	2
First, given an input sentence, we derive a set of feasible syntactic rearrangements using an encoder-decoder model.	3
This model operates over a partially lexical, partially syntactic view of the sentence and can reorder big chunks.	3
Next, we use each proposed rearrangement to produce a sequence of position embeddings, which encourages our final encoder-decoder paraphrase model to attend to the source words in a particular order.	3
Our evaluation, both automatic and human, shows that the proposed system retains the quality of the baseline approaches while giving a substantial increase in the diversity of the generated paraphrases.	4
----------
Conditional Text Generation has drawn much attention as a topic of Natural Language Generation (NLG) which provides the possibility for humans to control the properties of generated contents.	1
Current conditional generation models cannot handle emerging conditions due to their joint end-to-end learning fashion.	1
When a new condition added, these techniques require full retraining.	1
In this paper, we present a new framework named Pre-train and Plug-in Variational Auto-Encoder (PPVAE) towards flexible conditional text generation.	2
PPVAE decouples the text generation module from the condition representation module to allow “one-to-many” conditional generation.	3
When a fresh condition emerges, only a lightweight network needs to be trained and works as a plug-in for PPVAE, which is efficient and desirable for real-world applications.	4
Extensive experiments demonstrate the superiority of PPVAE against the existing alternatives with better conditionality and diversity but less training effort.	4
----------
Masked language model and autoregressive language model are two types of language models.	1
While pretrained masked language models such as BERT overwhelm the line of natural language understanding (NLU) tasks, autoregressive language models such as GPT are especially capable in natural language generation (NLG).	1
In this paper, we propose a probabilistic masking scheme for the masked language model, which we call probabilistically masked language model (PMLM).	2
We implement a specific PMLM with a uniform prior distribution on the masking ratio named u-PMLM.	3
We prove that u-PMLM is equivalent to an autoregressive permutated language model.	4
One main advantage of the model is that it supports text generation in arbitrary order with surprisingly good quality, which could potentially enable new applications over traditional unidirectional generation.	4
Besides, the pretrained u-PMLM also outperforms BERT on a bunch of downstream NLU tasks.	4
----------
Recent advances in neural text generation modeling have resulted in a number of societal concerns related to how such approaches might be used in malicious ways.	1
It is therefore desirable to develop a deeper understanding of the fundamental properties of such models.	1
The study of artifacts that emerge in machine generated text as a result of modeling choices is a nascent research area.	1
To this end, the extent and degree to which these artifacts surface in generated text is still unclear.	1
In the spirit of better understanding generative text models and their artifacts, we propose the new task of distinguishing which of several variants of a given model generated some piece of text.	2
Specifically, we conduct an extensive suite of diagnostic tests to observe whether modeling choices (e.g., sampling methods, top-k probabilities, model architectures, etc.) leave detectable artifacts in the text they generate.	3
Our key finding, which is backed by a rigorous set of experiments, is that such artifacts are present and that different modeling choices can be inferred by looking at generated text alone.	4
This suggests that neural text generators may actually be more sensitive to various modeling choices than previously thought.	4
----------
While online reviews of products and services become an important information source, it remains inefficient for potential consumers to exploit verbose reviews for fulfilling their information need.	1
We propose to explore question generation as a new way of review information exploitation, namely generating questions that can be answered by the corresponding review sentences.	2+3
One major challenge of this generation task is the lack of training data, i.e. explicit mapping relation between the user-posed questions and review sentences.	3
To obtain proper training instances for the generation model, we propose an iterative learning framework with adaptive instance transfer and augmentation.	3
To generate to the point questions about the major aspects in reviews, related features extracted in an unsupervised manner are incorporated without the burden of aspect annotation.	3
Experiments on data from various categories of a popular E-commerce site demonstrate the effectiveness of the framework, as well as the potentials of the proposed review-based question generation task.	4
----------
Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc.	1
However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information.	1
In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node.	2+3
Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code.	3
We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework.	3
Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.	4
----------
We propose UPSA, a novel approach that accomplishes Unsupervised Paraphrasing by Simulated Annealing.	1+2
We model paraphrase generation as an optimization problem and propose a sophisticated objective function, involving semantic similarity, expression diversity, and language fluency of paraphrases.	2+3
UPSA searches the sentence space towards this objective by performing a sequence of local editing.	3
We evaluate our approach on various datasets, namely, Quora, Wikianswers, MSCOCO, and Twitter.	3
Extensive results show that UPSA achieves the state-of-the-art performance compared with previous unsupervised methods in terms of both automatic and human evaluations.	4
Further, our approach outperforms most existing domain-adapted supervised models, showing the generalizability of UPSA.	5
----------
Text segmentation aims to uncover latent structure by dividing text from a document into coherent sections.	1
Where previous work on text segmentation considers the tasks of document segmentation and segment labeling separately, we show that the tasks contain complementary information and are best addressed jointly.	1
We introduce Segment Pooling LSTM (S-LSTM), which is capable of jointly segmenting a document and labeling segments.	2
In support of joint training, we develop a method for teaching the model to recover from errors by aligning the predicted and ground truth segments.	3
We show that S-LSTM reduces segmentation error by 30% on average, while also improving segment labeling.	4+5
----------
Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers.	1
Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked.	1
In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification.	2
Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus.	3
This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner.	3
This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized.	4
Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained.	5
----------
Text classification is fundamental in natural language processing (NLP) and Graph Neural Networks (GNN) are recently applied in this task.	1
However, the existing graph-based works can neither capture the contextual word relationships within each document nor fulfil the inductive learning of new words.	1
Therefore in this work, to overcome such problems, we propose TextING for inductive text classification via GNN.	2
We first build individual graphs for each document and then use GNN to learn the fine-grained word representations based on their local structure, which can also effectively produce embeddings for unseen words in the new document.	2+3
Finally, the word nodes are aggregated as the document embedding.	3
Extensive experiments on four benchmark datasets show that our method outperforms state-of-the-art text classification methods.	4
----------
Recent years have witnessed a surge of interests of using neural topic models for automatic topic extraction from text, since they avoid the complicated mathematical derivations for model inference as in traditional topic models such as Latent Dirichlet Allocation (LDA).	1
However, these models either typically assume improper prior (e.g. Gaussian or Logistic Normal) over latent topic space or could not infer topic distribution for a given document.	1
To address these limitations, we propose a neural topic modeling approach, called Bidirectional Adversarial Topic (BAT) model, which represents the first attempt of applying bidirectional adversarial training for neural topic modeling.	2+3
The proposed BAT builds a two-way projection between the document-topic distribution and the document-word distribution.	2+3
It uses a generator to capture the semantic patterns from texts and an encoder for topic inference.	3
Furthermore, to incorporate word relatedness information, the Bidirectional Adversarial Topic model with Gaussian (Gaussian-BAT) is extended from BAT.	3
To verify the effectiveness of BAT and Gaussian-BAT, three benchmark corpora are used in our experiments.	3
The experimental results show that BAT and Gaussian-BAT obtain more coherent topics, outperforming several competitive baselines.	4
Moreover, when performing text clustering based on the extracted topics, our models outperform all the baselines, with more significant improvements achieved by Gaussian-BAT where an increase of near 6% is observed in accuracy.	4+5
----------
Advanced pre-trained models for text representation have achieved state-of-the-art performance on various text classification tasks.	1
However, the discrepancy between the semantic similarity of texts and labelling standards affects classifiers, i.e. leading to lower performance in cases where classifiers should assign different labels to semantically similar texts.	1
To address this problem, we propose a simple multitask learning model that uses negative supervision.	2
Specifically, our model encourages texts with different labels to have distinct representations.	3
Comprehensive experiments show that our model outperforms the state-of-the-art pre-trained model on both single- and multi-label classifications, sentence and document classifications, and classifications in three different languages.	4
----------
Neural machine translation (NMT) encodes the source sentence in a universal way to generate the target sentence word-by-word.	1
However, NMT does not consider the importance of word in the sentence meaning, for example, some words (i.e., content words) express more important meaning than others (i.e., function words).	1
To address this limitation, we first utilize word frequency information to distinguish between content and function words in a sentence, and then design a content word-aware NMT to improve translation performance.	2+3
Empirical results on the WMT14 English-to-German, WMT14 English-to-French, and WMT17 Chinese-to-English translation tasks show that the proposed methods can significantly improve the performance of Transformer-based NMT.	4
----------
The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks.	1
However, few works have adopted this technique in the sequence-to-sequence models.	1
In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation~(NAT).	2
Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality.	1
Therefore, we propose to train the encoder more rigorously by masking the encoder input while training.	2+3
As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an n-gram loss function to alleviate the problem of translating duplicate words.	3
The two types of masks are applied to the model jointly at the training stage.	3
We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.	4
----------
The Transformer translation model (Vaswani et al., 2017) based on a multi-head attention mechanism can be computed effectively in parallel and has significantly pushed forward the performance of Neural Machine Translation (NMT).	1
Though intuitively the attentional network can connect distant words via shorter network paths than RNNs, empirical analysis demonstrates that it still has difficulty in fully capturing long-distance dependencies (Tang et al., 2018).	1
Considering that modeling phrases instead of words has significantly improved the Statistical Machine Translation (SMT) approach through the use of larger translation blocks (“phrases”) and its reordering ability, modeling NMT at phrase level is an intuitive proposal to help the model capture long-distance relationships.	1+2
In this paper, we first propose an attentive phrase representation generation mechanism which is able to generate phrase representations from corresponding token representations.	2
In addition, we incorporate the generated phrase representations into the Transformer translation model to enhance its ability to capture long-distance relationships.	4
In our experiments, we obtain significant improvements on the WMT 14 English-German and English-French tasks on top of the strong Transformer baseline, which shows the effectiveness of our approach.	4
Our approach helps Transformer Base models perform at the level of Transformer Big models, and even significantly better for long sentences, but with substantially fewer parameters and training steps.	4+5
The fact that phrase representations help even in the big setting further supports our conjecture that they make a valuable contribution to long-distance relations.	4
----------
The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure.	1
Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge.	1
In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers.	2+3
We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.	2+3
In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers.	4
In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.	4
----------
A neural machine translation (NMT) system is expensive to train, especially with high-resource settings.	1
As the NMT architectures become deeper and wider, this issue gets worse and worse.	1
In this paper, we aim to improve the efficiency of training an NMT by introducing a novel norm-based curriculum learning method.	2
We use the norm (aka length or module) of a word embedding as a measure of 1) the difficulty of the sentence, 2) the competence of the model, and 3) the weight of the sentence.	3
The norm-based sentence difficulty takes the advantages of both linguistically motivated and model-based sentence difficulties.	4
It is easy to determine and contains learning-dependent features.	4
The norm-based model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT.	4+5
Experimental results for the WMT’14 English-German and WMT’17 Chinese-English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x).	4+5
----------
Simultaneous translation has many important application scenarios and attracts much attention from both academia and industry recently.	1
Most existing frameworks, however, have difficulties in balancing between the translation quality and latency, i.e., the decoding policy is usually either too aggressive or too conservative.	1
We propose an opportunistic decoding technique with timely correction ability, which always (over-)generates a certain mount of extra words at each step to keep the audience on track with the latest information.	2+3
At the same time, it also corrects, in a timely fashion, the mistakes in the former overgenerated words when observing more source context to ensure high translation quality.	3
Experiments show our technique achieves substantial reduction in latency and up to +3.1 increase in BLEU, with revision rate under 8% in Chinese-to-English and English-to-Chinese translation.	4+5
----------
We develop a formal hierarchy of the expressive capacity of RNN architectures.	2+3
The hierarchy is based on two formal properties: space complexity, which measures the RNN’s memory, and rational recurrence, defined as whether the recurrent update can be described by a weighted finite-state machine.	1
We place several RNN variants within this hierarchy.	3
For example, we prove the LSTM is not rational, which formally separates it from the related QRNN (Bradbury et al., 2016).	3
We also show how these models’ expressive capacity is expanded by stacking multiple layers or composing them with different pooling functions.	3
Our results build on the theory of “saturated” RNNs (Merrill, 2019). While formally extending these findings to unsaturated RNNs is left to future work, we hypothesize that the practical learnable capacity of unsaturated RNNs obeys a similar hierarchy.	4+5
We provide empirical results to support this conjecture.	4
Experimental findings from training unsaturated networks on formal languages support this conjecture.	4
----------
We present that, the rank-frequency relation in textual data follows f ∝ r-𝛼(r+𝛾)-𝛽, where f is the token frequency and r is the rank by frequency, with (𝛼, 𝛽, 𝛾) as parameters.	1
The formulation is derived based on the empirical observation that d2 (x+y)/dx2 is a typical impulse function, where (x,y)=(log r, log f).	1
The formulation is the power law when 𝛽=0 and the Zipf–Mandelbrot law when 𝛼=0.	1
We illustrate that 𝛼 is related to the analytic features of syntax and 𝛽+𝛾 to those of morphology in natural languages from an investigation of multilingual corpora.	2+3
----------
Many NLP tasks such as tagging and machine reading comprehension are faced with the severe data imbalance issue: negative examples significantly outnumber positive examples, and the huge number of easy-negative examples overwhelms the training.	1
The most commonly used cross entropy (CE) criteria is actually an accuracy-oriented objective, and thus creates a discrepancy between training and test: at training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.	1
In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks.	2+3
Dice loss is based on the Sørensen--Dice coefficient or Tversky index , which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue.	3
To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples.	3
Theoretical analysis shows that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.	4
With the proposed training objective, we observe significant performance boost on a wide range of data imbalanced NLP tasks.	4
Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task; SOTA results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task; along with competitive results on the tasks of machine reading comprehension and paraphrase identification.	5
----------
This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance.	1+2
Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information.	3
We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.	4
----------
We examine a methodology using neural language models (LMs) for analyzing the word order of language.	1+2
This LM-based method has the potential to overcome the difficulties existing methods face, such as the propagation of preprocessor errors in count-based methods.	1
In this study, we explore whether the LM-based method is valid for analyzing the word order.	2
As a case study, this study focuses on Japanese due to its complex and flexible word order.	1+2
To validate the LM-based method, we test (i) parallels between LMs and human word order preference, and (ii) consistency of the results obtained using the LM-based method with previous linguistic studies.	3
Through our experiments, we tentatively conclude that LMs display sufficient word order knowledge for usage as an analysis tool.	4
Finally, using the LM-based method, we demonstrate the relationship between the canonical word order and topicalization, which had yet to be analyzed by large-scale experiments.	4
----------
This paper solves the fake news detection problem under a more realistic scenario on social media.	1
Given the source short-text tweet and the corresponding sequence of retweet users without text comments, we aim at predicting whether the source tweet is fake or not, and generating explanation by highlighting the evidences on suspicious retweeters and the words they concern.	2
We develop a novel neural network-based model, Graph-aware Co-Attention Networks (GCAN), to achieve the goal.	2+3
Extensive experiments conducted on real tweet datasets exhibit that GCAN can significantly outperform state-of-the-art methods by 16% in accuracy on average.	4+5
In addition, the case studies also show that GCAN can produce reasonable explanations.	5
----------
Identifying controversial posts on social media is a fundamental task for mining public sentiment, assessing the influence of events, and alleviating the polarized views.	1
However, existing methods fail to 1) effectively incorporate the semantic information from content-related posts; 2) preserve the structural information for reply relationship modeling; 3) properly handle posts from topics dissimilar to those in the training set.	1
To overcome the first two limitations, we propose Topic-Post-Comment Graph Convolutional Network (TPC-GCN), which integrates the information from the graph structure and content of topics, posts, and comments for post-level controversy detection.	2+3
As to the third limitation, we extend our model to Disentangled TPC-GCN (DTPC-GCN), to disentangle topic-related and topic-unrelated features and then fuse dynamically.	2+3
Extensive experiments on two real-world datasets demonstrate that our models outperform existing methods.	4
Analysis of the results and cases proves that our models can integrate both semantic and structural information with significant generalizability.	4
----------
Discovering the stances of media outlets and influential people on current, debatable topics is important for social statisticians and policy makers.	1
Many supervised solutions exist for determining viewpoints, but manually annotating training data is costly.	1
In this paper, we propose a cascaded method that uses unsupervised learning to ascertain the stance of Twitter users with respect to a polarizing topic by leveraging their retweet behavior; then, it uses supervised learning based on user labels to characterize both the general political leaning of online media and of popular Twitter users, as well as their stance with respect to the target polarizing topic.	2+3
We evaluate the model by comparing its predictions to gold labels from the Media Bias/Fact Check website, achieving 82.6% accuracy.	3+4
----------
The problem of comparing two bodies of text and searching for words that differ in their usage between them arises often in digital humanities and computational social science.	1
This is commonly approached by training word embeddings on each corpus, aligning the vector spaces, and looking for words whose cosine distance in the aligned space is large.	1
However, these methods often require extensive filtering of the vocabulary to perform well, and - as we show in this work - result in unstable, and hence less reliable, results.	1
We propose an alternative approach that does not use vector space alignment, and instead considers the neighbors of each word.	2+3
The method is simple, interpretable and stable.	6
We demonstrate its effectiveness in 9 different setups, considering different corpus splitting criteria (age, gender and profession of tweet authors, time of tweet) and different languages (English, French and Hebrew).	4
----------
Emotion-controllable response generation is an attractive and valuable task that aims to make open-domain conversations more empathetic and engaging.	1
Existing methods mainly enhance the emotion expression by adding regularization terms to standard cross-entropy loss and thus influence the training process.	1
However, due to the lack of further consideration of content consistency, the common problem of response generation tasks, safe response, is intensified.	1+2
Besides, query emotions that can help model the relationship between query and response are simply ignored in previous models, which would further hurt the coherence.	1
To alleviate these problems, we propose a novel framework named Curriculum Dual Learning (CDL) which extends the emotion-controllable response generation to a dual task to generate emotional responses and emotional queries alternatively.	2+3
CDL utilizes two rewards focusing on emotion and content to improve the duality.	3
Additionally, it applies curriculum learning to gradually generate high-quality responses based on the difficulties of expressing various emotions.	3
Experimental results show that CDL significantly outperforms the baselines in terms of coherence, diversity, and relation to emotion factors.	4
----------
Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches.	1
However, they are inefficient in that they predict the dialogue state at every turn from scratch.	1
Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST.	2+3
This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations.	3
Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder.	3
This enhances the effectiveness of training and DST performance.	4
Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting.	5
In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.	5
----------
The goal-oriented dialogue system needs to be optimized for tracking the dialogue flow and carrying out an effective conversation under various situations to meet the user goal.	1
The traditional approach to build such a dialogue system is to take a pipelined modular architecture, where its modules are optimized individually.	1
However, such an optimization scheme does not necessarily yield the overall performance improvement of the whole system.	1
On the other hand, end-to-end dialogue systems with monolithic neural architecture are often trained only with input-output utterances, without taking into account the entire annotations available in the corpus.	1
This scheme makes it difficult for goal-oriented dialogues where the system needs to integrate with external systems or to provide interpretable information about why the system generated a particular response.	1
In this paper, we present an end-to-end neural architecture for dialogue systems that addresses both challenges above.	1+2
In the human evaluation, our dialogue system achieved the success rate of 68.32%, the language understanding score of 4.149, and the response appropriateness score of 4.287, which ranked the system at the top position in the end-to-end multi-domain dialogue system task in the 8th dialogue systems technology challenge (DSTC8).	4
----------
Off-topic spoken response detection, the task aiming at predicting whether a response is off-topic for the corresponding prompt, is important for an automated speaking assessment system.	1
In many real-world educational applications, off-topic spoken response detectors are required to achieve high recall for off-topic responses not only on seen prompts but also on prompts that are unseen during training.	1
In this paper, we propose a novel approach for off-topic spoken response detection with high off-topic recall on both seen and unseen prompts.	2
We introduce a new model, Gated Convolutional Bidirectional Attention-based Model (GCBiA), which applies bi-attention mechanism and convolutions to extract topic words of prompts and key-phrases of responses, and introduces gated unit and residual connections between major layers to better represent the relevance of responses and prompts.	3+4
Moreover, a new negative sampling method is proposed to augment training data.	4
Experiment results demonstrate that our novel approach can achieve significant improvements in detecting off-topic responses with extremely high on-topic recall, for both seen and unseen prompts.	4
----------
Existing end-to-end dialog systems perform less effectively when data is scarce.	1
To obtain an acceptable success in real-life online services with only a handful of training examples, both fast adaptability and reliable performance are highly desirable for dialog systems.	1
In this paper, we propose the Meta-Dialog System (MDS), which combines the advantages of both meta-learning approaches and human-machine collaboration.	2
We evaluate our methods on a new extended-bAbI dataset and a transformed MultiWOZ dataset for low-resource goal-oriented dialog learning.	3
Experimental results show that MDS significantly outperforms non-meta-learning baselines and can achieve more than 90% per-turn accuracies with only 10 dialogs on the extended-bAbI dataset.	4
----------
Neural-based context-aware models for slot tagging have achieved state-of-the-art performance.	1
However, the presence of OOV(out-of-vocab) words significantly degrades the performance of neural-based models, especially in a few-shot scenario.	1
In this paper, we propose a novel knowledge-enhanced slot tagging model to integrate contextual representation of input text and the large-scale lexical background knowledge.	2
Besides, we use multi-level graph attention to explicitly model lexical relations.	3
The experiments show that our proposed knowledge integration mechanism achieves consistent improvements across settings with different sizes of training data on two public benchmark datasets.	4
----------
Many studies have applied reinforcement learning to train a dialog policy and show great promise these years.	1
One common approach is to employ a user simulator to obtain a large number of simulated user experiences for reinforcement learning algorithms.	1
However, modeling a realistic user simulator is challenging.	1
A rule-based simulator requires heavy domain expertise for complex tasks, and a data-driven simulator requires considerable data and it is even unclear how to evaluate a simulator.	1
To avoid explicitly building a user simulator beforehand, we propose Multi-Agent Dialog Policy Learning, which regards both the system and the user as the dialog agents.	1+2
Two agents interact with each other and are jointly learned simultaneously.	3
The method uses the actor-critic framework to facilitate pretraining and improve scalability.	3
We also propose Hybrid Value Network for the role-aware reward decomposition to integrate role-specific domain knowledge of each agent in the task-oriented dialog.	3
Results show that our method can successfully build a system policy and a user policy simultaneously, and two agents can achieve a high task success rate through conversational interaction.	4
----------
Neural generative models have achieved promising performance on dialog generation tasks if given a huge data set.	1
However, the lack of high-quality dialog data and the expensive data annotation process greatly limit their application in real world settings.	1
We propose a paraphrase augmented response generation (PARG) framework that jointly trains a paraphrase model and a response generation model to improve the dialog generation performance.	2+3
We also design a method to automatically construct paraphrase training data set based on dialog state and dialog act labels.	3
PARG is applicable to various dialog generation models, such as TSCP (Lei et al., 2018) and DAMD (Zhang et al., 2019).	4
Experimental results show that the proposed framework improves these state-of-the-art dialog models further on CamRest676 and MultiWOZ.	4
PARG also outperforms other data augmentation methods significantly in dialog generation tasks, especially under low resource settings.	4
----------
Neural conversation models are known to generate appropriate but non-informative responses in general.	1
A scenario where informativeness can be significantly enhanced is Conversing by Reading (CbR), where conversations take place with respect to a given external document.	1
In previous work, the external document is utilized by (1) creating a context-aware document memory that integrates information from the document and the conversational context, and then (2) generating responses referring to the memory.	1
In this paper, we propose to create the document memory with some anticipated responses in mind.	2
This is achieved using a teacher-student framework.	3
The teacher is given the external document, the context, and the ground-truth response, and learns how to build a response-aware document memory from three sources of information.	3
The student learns to construct a response-anticipated document memory from the first two sources, and teacher’s insight on memory creation.	3
Empirical results show that our model outperforms the previous state-of-the-art for the CbR task.	4
----------
Dialogue policy optimization often obtains feedback until task completion in task-oriented dialogue systems.	1
This is insufficient for training intermediate dialogue turns since supervision signals (or rewards) are only provided at the end of dialogues.	1
To address this issue, reward learning has been introduced to learn from state-action pairs of an optimal policy to provide turn-by-turn rewards.	1
This approach requires complete state-action annotations of human-to-human dialogues (i.e., expert demonstrations), which is labor intensive.	1
To overcome this limitation, we propose a novel reward learning approach for semi-supervised policy learning.	2
The proposed approach learns a dynamics model as the reward function which models dialogue progress (i.e., state-action sequences) based on expert demonstrations, either with or without annotations.	3
The dynamics model computes rewards by predicting whether the dialogue progress is consistent with expert demonstrations.	3
We further propose to learn action embeddings for a better generalization of the reward function.	3
The proposed approach outperforms competitive policy learning baselines on MultiWOZ, a benchmark multi-domain dataset.	4
----------
In modular dialogue systems, natural language understanding (NLU) and natural language generation (NLG) are two critical components, where NLU extracts the semantics from the given texts and NLG is to construct corresponding natural language sentences based on the input semantic representations.	1
However, the dual property between understanding and generation has been rarely explored.	1
The prior work is the first attempt that utilized the duality between NLU and NLG to improve the performance via a dual supervised learning framework.	1
However, the prior work still learned both components in a supervised manner; instead, this paper introduces a general learning framework to effectively exploit such duality, providing flexibility of incorporating both supervised and unsupervised learning algorithms to train language understanding and generation models in a joint fashion.	1+2
The benchmark experiments demonstrate that the proposed approach is capable of boosting the performance of both NLU and NLG.	4
The source code is available at: https://github.com/MiuLab/DuaLUG.	6
----------
The lack of meaningful automatic evaluation metrics for dialog has impeded open-domain dialog research.	1
Standard language generation metrics have been shown to be ineffective for evaluating dialog models.	1
To this end, this paper presents USR, an UnSupervised and Reference-free evaluation metric for dialog.	2
USR is a reference-free metric that trains unsupervised models to measure several desirable qualities of dialog.	3
USR is shown to strongly correlate with human judgment on both Topical-Chat (turn-level: 0.42, system-level: 1.0) and PersonaChat (turn-level: 0.48 and system-level: 1.0).	4
USR additionally produces interpretable measures for several desirable properties of dialog.	3
----------
Definition generation, which aims to automatically generate dictionary definitions for words, has recently been proposed to assist the construction of dictionaries and help people understand unfamiliar texts.	1
However, previous works hardly consider explicitly modeling the “components” of definitions, leading to under-specific generation results.	1
In this paper, we propose ESD, namely Explicit Semantic Decomposition for definition Generation, which explicitly decomposes the meaning of words into semantic components, and models them with discrete latent variables for definition generation.	2+3
Experimental results show that achieves top results on WordNet and Oxford benchmarks, outperforming strong previous baselines.	4
----------
Neural language models are usually trained to match the distributional properties of large-scale corpora by minimizing the log loss.	1
While straightforward to optimize, this approach forces the model to reproduce all variations in the dataset, including noisy and invalid references (e.g., misannotations and hallucinated facts).	1
Even a small fraction of noisy data can degrade the performance of log loss.	1
As an alternative, prior work has shown that minimizing the distinguishability of generated samples is a principled and robust loss that can handle invalid references.	1
However, distinguishability has not been used in practice due to challenges in optimization and estimation.	1
We propose loss truncation: a simple and scalable procedure which adaptively removes high log loss examples as a way to optimize for distinguishability.	2+3
Empirically, we demonstrate that loss truncation outperforms existing baselines on distinguishability on a summarization task.	4
Furthermore, we show that samples generated by the loss truncation model have factual accuracy ratings that exceed those of baselines and match human references.	4
----------
Efficient structure encoding for graphs with labeled edges is an important yet challenging point in many graph-based models.	1
This work focuses on AMR-to-text generation – A graph-to-sequence task aiming to recover natural language from Abstract Meaning Representations (AMR).	2+3
Existing graph-to-sequence approaches generally utilize graph neural networks as their encoders, which have two limitations:	1
1) The message propagation process in AMR graphs is only guided by the first-order adjacency information.	1
2) The relationships between labeled edges are not fully considered.	1
In this work, we propose a novel graph encoding framework which can effectively explore the edge relations.	2
We also adopt graph attention networks with higher-order neighborhood information to encode the rich structure in AMR graphs.	3
Experiment results show that our approach obtains new state-of-the-art performance on English AMR benchmark datasets.	4
The ablation analyses also demonstrate that both edge relations and higher-order information are beneficial to graph-to-sequence modeling.	4
----------
Neural text generation has made tremendous progress in various tasks.	1
One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating.	1
However, we may confront some special text paradigms such as Lyrics (assume the music score is given), Sonnet, SongCi (classical Chinese poetry of the Song dynasty), etc.	1
The typical characteristics of these texts are in three folds:	1
(1) They must comply fully with the rigid predefined formats.	1
(2) They must obey some rhyming schemes.	1
(3) Although they are restricted to some formats, the sentence integrity must be guaranteed.	1
To the best of our knowledge, text generation based on the predefined rigid formats has not been well investigated.	1
Therefore, we propose a simple and elegant framework named SongNet to tackle this problem.	2
The backbone of the framework is a Transformer-based auto-regressive language model.	3
Sets of symbols are tailor-designed to improve the modeling performance especially on format, rhyme, and sentence integrity.	3
We improve the attention mechanism to impel the model to capture some future information on the format.	3
A pre-training and fine-tuning framework is designed to further improve the generation quality.	3
Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation.	4
----------
Question Generation (QG) is fundamentally a simple syntactic transformation; however, many aspects of semantics influence what questions are good to form.	1
We implement this observation by developing Syn-QG, a set of transparent syntactic rules leveraging universal dependencies, shallow semantic parsing, lexical resources, and custom rules which transform declarative sentences into question-answer pairs.	2+3
We utilize PropBank argument descriptions and VerbNet state predicates to incorporate shallow semantic content, which helps generate questions of a descriptive nature and produce inferential and semantically richer questions than existing systems.	3+4
In order to improve syntactic fluency and eliminate grammatically incorrect questions, we employ back-translation over the output of these syntactic rules.	3
A set of crowd-sourced evaluations shows that our system can generate a larger number of highly grammatical and relevant questions than previous QG systems and that back-translation drastically improves grammaticality at a slight cost of generating irrelevant questions.	4
----------
Clustering short text streams is a challenging task due to its unique properties: infinite length, sparse data representation and cluster evolution.	1
Existing approaches often exploit short text streams in a batch way.	1
However, determine the optimal batch size is usually a difficult task since we have no priori knowledge when the topics evolve.	1
In addition, traditional independent word representation in graphical model tends to cause “term ambiguity” problem in short text clustering.	1
Therefore, in this paper, we propose an Online Semantic-enhanced Dirichlet Model for short sext stream clustering, called OSDM, which integrates the word-occurance semantic information (i.e., context) into a new graphical model and clusters each arriving short text automatically in an online way.	2+3
Extensive results have demonstrated that OSDM has better performance compared to many state-of-the-art algorithms on both synthetic and real-world data sets.	4
----------
Generative semantic hashing is a promising technique for large-scale information retrieval thanks to its fast retrieval speed and small memory footprint.	1
For the tractability of training, existing generative-hashing methods mostly assume a factorized form for the posterior distribution, enforcing independence among the bits of hash codes.	1
From the perspectives of both model representation and code space size, independence is always not the best assumption.	1
In this paper, to introduce correlations among the bits of hash codes, we propose to employ the distribution of Boltzmann machine as the variational posterior.	2+3
To address the intractability issue of training, we first develop an approximate method to reparameterize the distribution of a Boltzmann machine by augmenting it as a hierarchical concatenation of a Gaussian-like distribution and a Bernoulli distribution.	3
Based on that, an asymptotically-exact lower bound is further derived for the evidence lower bound (ELBO).	4
With these novel techniques, the entire model can be optimized efficiently.	3+4
Extensive experimental results demonstrate that by effectively modeling correlations among different bits within a hash code, our model can achieve significant performance gains.	3+4
----------
We propose a methodology to construct a term dictionary for text analytics through an interactive process between a human and a machine, which helps the creation of flexible dictionaries with precise granularity required in typical text analysis.	1
This paper introduces the first formulation of interactive dictionary construction to address this issue.	1
To optimize the interaction, we propose a new algorithm that effectively captures an analyst’s intention starting from only a small number of sample terms.	2+3
Along with the algorithm, we also design an automatic evaluation framework that provides a systematic assessment of any interactive method for the dictionary creation task.	3
Experiments using real scenario based corpora and dictionaries show that our algorithm outperforms baseline methods, and works even with a small number of interactions.	4+5
----------
This paper presents a tree-structured neural topic model, which has a topic distribution over a tree with an infinite number of branches.	2
Our model parameterizes an unbounded ancestral and fraternal topic distribution by applying doubly-recurrent neural networks.	3
With the help of autoencoding variational Bayes, our model improves data scalability and achieves competitive performance when inducing latent topics and tree structures, as compared to a prior tree-structured topic model (Blei et al., 2010).	3
This work extends the tree-structured topic model such that it can be incorporated with neural models for downstream tasks.	3+4
----------
We focus on the task of Frequently Asked Questions (FAQ) retrieval.	1
A given user query can be matched against the questions and/or the answers in the FAQ.	1
We present a fully unsupervised method that exploits the FAQ pairs to train two BERT models.	2+3
The two models match user queries to FAQ answers and questions, respectively.	3
We alleviate the missing labeled data of the latter by automatically generating high-quality question paraphrases.	3
We show that our model is on par and even outperforms supervised models on existing datasets.	4+5
----------
Humor plays an important role in human languages and it is essential to model humor when building intelligence systems.	1
Among different forms of humor, puns perform wordplay for humorous effects by employing words with double entendre and high phonetic similarity.	1
However, identifying and modeling puns are challenging as puns usually involved implicit semantic or phonological tricks.	1
In this paper, we propose Pronunciation-attentive Contextualized Pun Recognition (PCPR) to perceive human humor, detect if a sentence contains puns and locate them in the sentence.	2+3
PCPR derives contextualized representation for each word in a sentence by capturing the association between the surrounding context and its corresponding phonetic symbols.	2
Extensive experiments are conducted on two benchmark datasets.	3
Results demonstrate that the proposed approach significantly outperforms the state-of-the-art methods in pun detection and location tasks.	4+5
In-depth analyses verify the effectiveness and robustness of PCPR.	5
----------
Even though BERT has achieved successful performance improvements in various supervised learning tasks, BERT is still limited by repetitive inferences on unsupervised tasks for the computation of contextual language representations.	1
To resolve this limitation, we propose a novel deep bidirectional language model called a Transformer-based Text Autoencoder (T-TA).	2+3
The T-TA computes contextual language representations without repetition and displays the benefits of a deep bidirectional architecture, such as that of BERT.	3
In computation time experiments in a CPU environment, the proposed T-TA performs over six times faster than the BERT-like model on a reranking task and twelve times faster on a semantic similarity task.	3+4
Furthermore, the T-TA shows competitive or even better accuracies than those of BERT on the above tasks.	4+5
Code is available at https://github.com/joongbo/tta.	6
----------
Operational risk management is one of the biggest challenges nowadays faced by financial institutions.	1
There are several major challenges of building a text classification system for automatic operational risk prediction, including imbalanced labeled/unlabeled data and lacking interpretability.	2
To tackle these challenges, we present a semi-supervised text classification framework that integrates multi-head attention mechanism with Semi-supervised variational inference for Operational Risk Classification (SemiORC).	3
We empirically evaluate the framework on a real-world dataset.	3
The results demonstrate that our method can better utilize unlabeled data and learn visually interpretable document representations.	4
SemiORC also outperforms other baseline methods on operational risk classification.	4
----------
Identifying user geolocation in online social networks is an essential task in many location-based applications.	1
Existing methods rely on the similarity of text and network structure, however, they suffer from a lack of interpretability on the corresponding results, which is crucial for understanding model behavior.	1
In this work, we adopt influence functions to interpret the behavior of GNN-based models by identifying the importance of training users when predicting the locations of the testing users.	2+3
This methodology helps with providing meaningful explanations on prediction results.	3
Furthermore, it also initiates an attempt to uncover the so-called “black-box” GNN-based models by investigating the effect of individual nodes.	3
----------
Language modeling is the technique to estimate the probability of a sequence of words.	1
A bilingual language model is expected to model the sequential dependency for words across languages, which is difficult due to the inherent lack of suitable training data as well as diverse syntactic structure across languages.	1
We propose a bilingual attention language model (BALM) that simultaneously performs language modeling objective with a quasi-translation objective to model both the monolingual as well as the cross-lingual sequential dependency.	2+3
The attention mechanism learns the bilingual context from a parallel corpus.	3
BALM achieves state-of-the-art performance on the SEAME code-switch database by reducing the perplexity of 20.5% over the best-reported result.	4
We also apply BALM in bilingual lexicon induction, and language normalization tasks to validate the idea.	4
----------
Spelling error correction is an important yet challenging task because a satisfactory solution of it essentially needs human-level language understanding ability.	1
Without loss of generality we consider Chinese spelling error correction (CSC) in this paper.	2
A state-of-the-art method for the task selects a character from a list of candidates for correction (including non-correction) at each position of the sentence on the basis of BERT, the language representation model.	3
The accuracy of the method can be sub-optimal, however, because BERT does not have sufficient capability to detect whether there is an error at each position, apparently due to the way of pre-training it using mask language modeling.	3
In this work, we propose a novel neural architecture to address the aforementioned issue, which consists of a network for error detection and a network for error correction based on BERT, with the former being connected to the latter with what we call soft-masking technique.	2+3
Our method of using ‘Soft-Masked BERT’ is general, and it may be employed in other language detection-correction problems.	4
Experimental results on two datasets, including one large dataset which we create and plan to release, demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT.	4
----------
Chinese Spelling Check (CSC) is a task to detect and correct spelling errors in Chinese natural language.	1
Existing methods have made attempts to incorporate the similarity knowledge between Chinese characters.	1
However, they take the similarity knowledge as either an external input resource or just heuristic rules.	1
This paper proposes to incorporate phonological and visual similarity knowledge into language models for CSC via a specialized graph convolutional network (SpellGCN).	2+3
The model builds a graph over the characters, and SpellGCN is learned to map this graph into a set of inter-dependent character classifiers.	3
These classifiers are applied to the representations extracted by another network, such as BERT, enabling the whole network to be end-to-end trainable.	3
Experiments are conducted on three human-annotated datasets.	3
Our method achieves superior performance against previous models by a large margin.	4
----------
Sentence representation (SR) is the most crucial and challenging task in Machine Reading Comprehension (MRC).	1
MRC systems typically only utilize the information contained in the sentence itself, while human beings can leverage their semantic knowledge.	1
To bridge the gap, we proposed a novel Frame-based Sentence Representation (FSR) method, which employs frame semantic knowledge to facilitate sentence modelling.	2+3
Specifically, different from existing methods that only model lexical units (LUs), Frame Representation Models, which utilize both LUs in frame and Frame-to-Frame (F-to-F) relations, are designed to model frames and sentences with attention schema.	3
Our proposed FSR method is able to integrate multiple-frame semantic information to get much better sentence representations.	3+4
Our extensive experimental results show that it performs better than state-of-the-art technologies on machine reading comprehension task.	4+5
----------
In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data.	2+3
For this, we introduce an intermediate representation that is based on the logical query plan in a database, called Operation Trees (OT).	3
This representation allows us to invert the annotation process without loosing flexibility in the types of queries that we generate.	3+4
Furthermore, it allows for fine-grained alignment of the tokens to the operations.	4
Thus, we randomly generate OTs from a context free grammar and annotators just have to write the appropriate question and assign the tokens.	3
We compare our corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases, to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries.	3+4
Finally, we train a state-of-the-art semantic parsing model on our data and show that our dataset is a challenging dataset and that the token alignment can be leveraged to significantly increase the performance.	3+4
----------
Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models.	1
In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (Sparc).	2
Unlike previous sparse vectors that are term-frequency-based (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space.	3
By augmenting the previous phrase retrieval model (Seo et al., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.	3+4
Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed.	4
----------
Building general reading comprehension systems, capable of solving multiple datasets at the same time, is a recent aspirational goal in the research community.	1
Prior work has focused on model architecture or generalization to held out datasets, and largely passed over the particulars of the multi-task learning set up.	1
We show that a simple dynamic sampling strategy, selecting instances for training proportional to the multi-task model’s current performance on a dataset relative to its single task performance, gives substantive gains over prior multi-task sampling strategies, mitigating the catastrophic forgetting that is common in multi-task learning.	2+3
We also demonstrate that allowing instances of different tasks to be interleaved as much as possible between each epoch and batch has a clear benefit in multitask performance over forcing task homogeneity at the epoch or batch level.	3+4
Our final model shows greatly increased performance over the best model on ORB, a recently-released multitask reading comprehension benchmark.	4
----------
Multilingual pre-trained models could leverage the training data from a rich source language (such as English) to improve performance on low resource languages.	2+3
However, the transfer quality for multilingual Machine Reading Comprehension (MRC) is significantly worse than sentence classification tasks mainly due to the requirement of MRC to detect the word level answer boundary.	1
In this paper, we propose two auxiliary tasks in the fine-tuning stage to create additional phrase boundary supervision: (1) A mixed MRC task, which translates the question or passage to other languages and builds cross-lingual question-passage pairs; (2) A language-agnostic knowledge masking task by leveraging knowledge phrases mined from web.	2+3
Besides, extensive experiments on two cross-lingual MRC datasets show the effectiveness of our proposed approach.	4+5
----------
The goal of conversational machine reading is to answer user questions given a knowledge base text which may require asking clarification questions.	2
Existing approaches are limited in their decision making due to struggles in extracting question-related rules and reasoning about them.	1
In this paper, we present a new framework of conversational machine reading that comprises a novel Explicit Memory Tracker (EMT) to track whether conditions listed in the rule text have already been satisfied to make a decision.	2+3
Moreover, our framework generates clarification questions by adopting a coarse-to-fine reasoning strategy, utilizing sentence-level entailment scores to weight token-level distributions.	3
On the ShARC benchmark (blind, held-out) testset, EMT achieves new state-of-the-art results of 74.6% micro-averaged decision accuracy and 49.5 BLEU4.	4
We also show that EMT is more interpretable by visualizing the entailment-oriented reasoning process as the conversation flows.	4
Code and models are released at https://github.com/Yifan-Gao/explicit_memory_tracker.	6
----------
Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information.	1
However, high-level reasoning skills, such as numerical reasoning, are difficult to learn from a language-modeling objective only.	1
Consequently, existing models for numerical reasoning have used specialized architectures with limited flexibility.	1
In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup.	2+3
We show that pre-training our model, GenBERT, on this data, dramatically improves performance on DROP (49.3 –> 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture.	4
Moreover, GenBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks.	4+5
Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.	5
----------
Despite recent progress in conversational question answering, most prior work does not focus on follow-up questions.	1
Practical conversational question answering systems often receive follow-up questions in an ongoing conversation, and it is crucial for a system to be able to determine whether a question is a follow-up question of the current conversation, for more effective answer finding subsequently.	1+2
In this paper, we introduce a new follow-up question identification task.	1
We propose a three-way attentive pooling network that determines the suitability of a follow-up question by capturing pair-wise interactions between the associated passage, the conversation history, and a candidate follow-up question.	2+3
It enables the model to capture topic continuity and topic shift while scoring a particular candidate follow-up question.	3
Experiments show that our proposed three-way attentive pooling network outperforms all baseline systems by significant margins.	4
----------
Previous work on answering complex questions from knowledge bases usually separately addresses two types of complexity: questions with constraints and questions with multiple hops of relations.	1
In this paper, we handle both types of complexity at the same time.	2
Motivated by the observation that early incorporation of constraints into query graphs can more effectively prune the search space, we propose a modified staged query graph generation method with more flexible ways to generate query graphs.	3
Our experiments clearly show that our method achieves the state of the art on three benchmark KBQA datasets.	4+5
----------
Evaluating image captions is very challenging partially due to the fact that there are multiple correct captions for every single image.	1
Most of the existing one-to-one metrics operate by penalizing mismatches between reference and generative caption without considering the intrinsic variance between ground truth captions.	1
It usually leads to over-penalization and thus a bad correlation to human judgment.	1
Recently, the latest one-to-one metric BERTScore can achieve high human correlation in system-level tasks while some issues can be fixed for better performance.	2+3
In this paper, we propose a novel metric based on BERTScore that could handle such a challenge and extend BERTScore with a few new features appropriately for image captioning evaluation.	2+3
The experimental results show that our metric achieves state-of-the-art human judgment correlation.	4
----------
Existing approaches to mapping-based cross-lingual word embeddings are based on the assumption that the source and target embedding spaces are structurally similar.	1
The structures of embedding spaces largely depend on the co-occurrence statistics of each word, which the choice of context window determines.	1
Despite this obvious connection between the context window and mapping-based cross-lingual embeddings, their relationship has been underexplored in prior work.	1
In this work, we provide a thorough evaluation, in various languages, domains, and tasks, of bilingual embeddings trained with different context windows.	2
The highlight of our findings is that increasing the size of both the source and target window sizes improves the performance of bilingual lexicon induction, especially the performance on frequent nouns.	3+4
----------
A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training.	1
We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense.	2
The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding.	3+4
Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work.	4+5
This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.	5
----------
In this paper, we demonstrate how code-switching patterns can be utilised to improve various downstream NLP applications.	2+3
In particular, we encode various switching features to improve humour, sarcasm and hate speech detection tasks.	3
We believe that this simple linguistic observation can also be potentially helpful in improving other similar NLP applications.	5
----------
Recently, many methods discover effective evidence from reliable sources by appropriate neural networks for explainable claim verification, which has been widely recognized.	1
However, in these methods, the discovery process of evidence is nontransparent and unexplained.	1
Simultaneously, the discovered evidence is aimed at the interpretability of the whole sequence of claims but insufficient to focus on the false parts of claims.	1
In this paper, we propose a Decision Tree-based Co-Attention model (DTCA) to discover evidence for explainable claim verification.	2
Specifically, we first construct Decision Tree-based Evidence model (DTE) to select comments with high credibility as evidence in a transparent and interpretable way.	3
Then we design Co-attention Self-attention networks (CaSa) to make the selected evidence interact with claims, which is for 1) training DTE to determine the optimal decision thresholds and obtain more powerful evidence; and 2) utilizing the evidence to find the false parts in the claim.	3
Experiments on two public datasets, RumourEval and PHEME, demonstrate that DTCA not only provides explanations for the results of claim verification but also achieves the state-of-the-art performance, boosting the F1-score by more than 3.11%, 2.41%, respectively.	4
----------
User intent classification plays a vital role in dialogue systems.	1
Since user intent may frequently change over time in many realistic scenarios, unknown (new) intent detection has become an essential problem, where the study has just begun.	1
This paper proposes a semantic-enhanced Gaussian mixture model (SEG) for unknown intent detection.	2
In particular, we model utterance embeddings with a Gaussian mixture distribution and inject dynamic class semantic information into Gaussian means, which enables learning more class-concentrated embeddings that help to facilitate downstream outlier detection.	1+2
Coupled with a density-based outlier detection algorithm, SEG achieves competitive results on three real task-oriented dialogue datasets in two languages for unknown intent detection.	3+4
On top of that, we propose to integrate SEG as an unknown intent identifier into existing generalized zero-shot intent classification models to improve their performance.	3
A case study on a state-of-the-art method, ReCapsNet, shows that SEG can push the classification performance to a significantly higher level.	3+4
----------
Text generation from a knowledge base aims to translate knowledge triples to natural language descriptions.	2
Most existing methods ignore the faithfulness between a generated text description and the original table, leading to generated information that goes beyond the content of the table.	1
In this paper, for the first time, we propose a novel Transformer-based generation framework to achieve the goal.	2+3
The core techniques in our method to enforce faithfulness include a new table-text optimal-transport matching loss and a table-text embedding similarity loss based on the Transformer model.	3
Furthermore, to evaluate faithfulness, we propose a new automatic metric specialized to the table-to-text generation problem.	3+4
We also provide detailed analysis on each component of our model in our experiments.	4
Automatic and human evaluations show that our framework can significantly outperform state-of-the-art by a large margin.	4
----------
This paper proposes Dynamic Memory Induction Networks (DMIN) for few-short text classification.	2
The model develops a dynamic routing mechanism over static memory, enabling it to better adapt to unseen classes, a critical capability for few-short classification.	3+4
The model also expands the induction process with supervised learning weights and query information to enhance the generalization ability of meta-learning.	2+3
The proposed model brings forward the state-of-the-art performance significantly by 2~4% improvement on the miniRCV1 and ODIC datasets.	4
Detailed analysis is further performed to show how the proposed network achieves the new performance.	4
----------
Keyphrase generation (KG) aims to summarize the main ideas of a document into a set of keyphrases.	2
A new setting is recently introduced into this problem, in which, given a document, the model needs to predict a set of keyphrases and simultaneously determine the appropriate number of keyphrases to produce.	1
Previous work in this setting employs a sequential decoding process to generate keyphrases.	1
However, such a decoding method ignores the intrinsic hierarchical compositionality existing in the keyphrase set of a document.	1
Moreover, previous work tends to generate duplicated keyphrases, which wastes time and computing resources.	1
To overcome these limitations, we propose an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism.	1+2
The hierarchical decoding process is to explicitly model the hierarchical compositionality of a keyphrase set.	3
Both the soft and the hard exclusion mechanisms keep track of previously-predicted keyphrases within a window size to enhance the diversity of the generated keyphrases.	3
Extensive experiments on multiple KG benchmark datasets demonstrate the effectiveness of our method to generate less duplicated and more accurate keyphrases.	4
----------
Hierarchical text classification is an essential yet challenging subtask of multi-label text classification with a taxonomic hierarchy.	1
Existing methods have difficulties in modeling the hierarchical label structure in a global view.	1
Furthermore, they cannot make full use of the mutual interactions between the text feature space and the label space.	1
In this paper, we formulate the hierarchy as a directed graph and introduce hierarchy-aware structure encoders for modeling label dependencies.	2+3
Based on the hierarchy encoder, we propose a novel end-to-end hierarchy-aware global model (HiAGM) with two variants.	3
A multi-label attention variant (HiAGM-LA) learns hierarchy-aware label embeddings through the hierarchy encoder and conducts inductive fusion of label-aware text features.	3
A text feature propagation model (HiAGM-TP) is proposed as the deductive variant that directly feeds text features into hierarchy encoders.	3
Compared with previous works, both HiAGM-LA and HiAGM-TP achieve significant and consistent improvements on three benchmark datasets.	4
----------
Sequence-to-sequence models have lead to significant progress in keyphrase generation, but it remains unknown whether they are reliable enough to be beneficial for document retrieval.	1
This study provides empirical evidence that such models can significantly improve retrieval performance, and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models.	2+3
Using this framework, we point out and discuss the difficulties encountered with supplementing documents with -not present in text- keyphrases, and generalizing models across domains.	2+3
Our code is available at https://github.com/boudinfl/ir-using-kg	6
----------
There has been little work on modeling the morphological well-formedness (MWF) of derivatives, a problem judged to be complex and difficult in linguistics.	1
We present a graph auto-encoder that learns embeddings capturing information about the compatibility of affixes and stems in derivation.	2+3
The auto-encoder models MWF in English surprisingly well by combining syntactic and semantic information with associative information from the mental lexicon.	4
----------
Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis.	1
Yet, manually curated lexicons are only available for a handful of languages, leaving most languages of the world without such a precious resource for downstream applications.	1
Even worse, their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature.	1
In order to break this bottleneck, we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language.	2+3
Our approach requires nothing but a source language emotion lexicon, a bilingual word translation model, and a target language embedding model.	3
Fulfilling these requirements for 91 languages, we are able to generate representationally rich high-coverage lexicons comprising eight emotional variables with more than 100k lexical entries each.	4
We evaluated the automatically generated lexicons against human judgment from 26 datasets, spanning 12 typologically diverse languages, and found that our approach produces results in line with state-of-the-art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables.	4+5
Code and data are available at https://github.com/JULIELab/MEmoLon archived under DOI 10.5281/zenodo.3779901.	6
----------
Reliably evaluating Machine Translation (MT) through automated metrics is a long-standing problem.	1
One of the main challenges is the fact that multiple outputs can be equally valid.	1
Attempts to minimise this issue include metrics that relax the matching of MT output and reference strings, and the use of multiple references.	1
The latter has been shown to significantly improve the performance of evaluation metrics.	1
However, collecting multiple references is expensive and in practice a single reference is generally used.	1
In this paper, we propose an alternative approach: instead of modelling linguistic variation in human reference we exploit the MT model uncertainty to generate multiple diverse translations and use these: (i) as surrogates to reference translations; (ii) to obtain a quantification of translation variability to either complement existing metric scores or (iii) replace references altogether.	2+3
We show that for a number of popular evaluation metrics our variability estimates lead to substantial improvements in correlation with human judgements of quality by up 15%.	4
----------
We propose approaches to Quality Estimation (QE) for Machine Translation that explore both text and visual modalities for Multimodal QE.	2
We compare various multimodality integration and fusion strategies.	3
For both sentence-level and document-level predictions, we show that state-of-the-art neural and feature-based QE frameworks obtain better results when using the additional modality.	4
----------
We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph.	2
At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input sequence to abstract; and (2) where in the output graph to construct the new concept.	3
We show that the answers to these two questions are mutually causalities.	4
We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy.	2+3
Our experimental results significantly outperform all previously reported Smatch scores by large margins.	4
Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT.	4
With the help of BERT, we can push the state-of-the-art results to 80.2% on LDC2017T10 (AMR 2.0) and 75.4% on LDC2014T12 (AMR 1.0).	4
----------
Cross-lingual summarization aims at summarizing a document in one language (e.g., Chinese) into another language (e.g., English).	1
In this paper, we propose a novel method inspired by the translation pattern in the process of obtaining a cross-lingual summary.	2
We first attend to some words in the source text, then translate them into the target language, and summarize to get the final summary.	3
Specifically, we first employ the encoder-decoder attention distribution to attend to the source words.	3
Second, we present three strategies to acquire the translation probability, which helps obtain the translation candidates for each source word.	3
Finally, each summary word is generated either from the neural distribution or from the translation candidates of source words.	3
Experimental results on Chinese-to-English and English-to-Chinese summarization tasks have shown that our proposed method can significantly outperform the baselines, achieving comparable performance with the state-of-the-art.	4+5
----------
We study unsupervised multi-document summarization evaluation metrics, which require neither human-written reference summaries nor human annotations (e.g. preferences, ratings, etc.).	2
We propose SUPERT, which rates the quality of a summary by measuring its semantic similarity with a pseudo reference summary, i.e. selected salient sentences from the source documents, using contextualized embeddings and soft token alignment techniques.	3
Compared to the state-of-the-art unsupervised evaluation metrics, SUPERT correlates better with human ratings by 18- 39%.	4
Furthermore, we use SUPERT as rewards to guide a neural-based reinforcement learning summarizer, yielding favorable performance compared to the state-of-the-art unsupervised summarizers.	3+4
All source code is available at https://github.com/yg211/acl20-ref-free-eval.	6
----------
Copy module has been widely equipped in the recent abstractive summarization models, which facilitates the decoder to extract words from the source into the summary.	1
Generally, the encoder-decoder attention is served as the copy distribution, while how to guarantee that important words in the source are copied remains a challenge.	1
In this work, we propose a Transformer-based model to enhance the copy mechanism.	2
Specifically, we identify the importance of each source word based on the degree centrality with a directed graph built by the self-attention layer in the Transformer.	3
We use the centrality of each source word to guide the copy process explicitly.	3
Experimental results show that the self-attention graph provides useful guidance for the copy distribution.	4
Our proposed models significantly outperform the baseline methods on the CNN/Daily Mail dataset and the Gigaword dataset.	3+4
----------
Open Domain dialog system evaluation is one of the most important challenges in dialog research.	1
Existing automatic evaluation metrics, such as BLEU are mostly reference-based.	1
They calculate the difference between the generated response and a limited number of available references.	1
Likert-score based self-reported user rating is widely adopted by social conversational systems, such as Amazon Alexa Prize chatbots.	1
However, self-reported user rating suffers from bias and variance among different users.	1
To alleviate this problem, we formulate dialog evaluation as a comparison task.	2+3
We also propose an automatic evaluation model CMADE (Comparison Model for Automatic Dialog Evaluation) that automatically cleans self-reported user ratings as it trains on them.	2+3
Specifically, we first use a self-supervised method to learn better dialog feature representation, and then use KNN and Shapley to remove confusing samples.	3
Our experiments show that CMADE achieves 89.2% accuracy in the dialog comparison task.	4
----------
Human conversations contain many types of information, e.g., knowledge, common sense, and language habits.	1
In this paper, we propose a conversational word embedding method named PR-Embedding, which utilizes the conversation pairs <post, reply> to learn word embedding.	2+3
Different from previous works, PR-Embedding uses the vectors from two different semantic spaces to represent the words in post and reply.	1
To catch the information among the pair, we first introduce the word alignment model from statistical machine translation to generate the cross-sentence window, then train the embedding on word-level and sentence-level.	3
We evaluate the method on single-turn and multi-turn response selection tasks for retrieval-based dialog systems.	3
The experiment results show that PR-Embedding can improve the quality of the selected response.	4+5
----------
In this paper, we explore the slot tagging with only a few labeled support sentences (a.k.a. few-shot).	2
Few-shot slot tagging faces a unique challenge compared to the other fewshot classification problems as it calls for modeling the dependencies between labels.	1
But it is hard to apply previously learned label dependencies to an unseen domain, due to the discrepancy of label sets.	1
To tackle this, we introduce a collapsed dependency transfer mechanism into the conditional random field (CRF) to transfer abstract label dependency patterns as transition scores.	2+3
In the few-shot setting, the emission score of CRF can be calculated as a word’s similarity to the representation of each label.	3
To calculate such similarity, we propose a Label-enhanced Task-Adaptive Projection Network (L-TapNet) based on the state-of-the-art few-shot classification model – TapNet, by leveraging label name semantics in representing labels.	2+3
Experimental results show that our model significantly outperforms the strongest few-shot learning baseline by 14.64 F1 scores in the one-shot setting.	4
----------
Deep reinforcement learning is a promising approach to training a dialog manager, but current methods struggle with the large state and action spaces of multi-domain dialog systems.	1
Building upon Deep Q-learning from Demonstrations (DQfD), an algorithm that scores highly in difficult Atari games, we leverage dialog data to guide the agent to successfully respond to a user’s requests.	2+3
We make progressively fewer assumptions about the data needed, using labeled, reduced-labeled, and even unlabeled data to train expert demonstrators.	3
We introduce Reinforced Fine-tune Learning, an extension to DQfD, enabling us to overcome the domain gap between the datasets and the environment.	3
Experiments in a challenging multi-domain dialog system framework validate our approaches, and get high success rates even when trained on out-of-domain data.	4
----------
Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors.	1
The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation.	1
Motivated by this, we propose Pˆ2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding.	2+3
Specifically, Pˆ2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation.	3
Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.	3+4
----------
Most previous studies on bridging anaphora resolution (Poesio et al., 2004; Hou et al., 2013b; Hou, 2018a) use the pairwise model to tackle the problem and assume that the gold mention information is given.	1
In this paper, we cast bridging anaphora resolution as question answering based on context.	2
This allows us to find the antecedent for a given anaphor without knowing any gold mention information (except the anaphor itself).	4
We present a question answering framework (BARQA) for this task, which leverages the power of transfer learning.	2
Furthermore, we propose a novel method to generate a large amount of “quasi-bridging” training data.	2
We show that our model pre-trained on this dataset and fine-tuned on a small amount of in-domain dataset achieves new state-of-the-art results for bridging anaphora resolution on two bridging corpora (ISNotes (Markert et al., 2012) and BASHI (Ro ̈siger, 2018)).	4
----------
Recent dialogue coherence models use the coherence features designed for monologue texts, e.g. nominal entities, to represent utterances and then explicitly augment them with dialogue-relevant features, e.g., dialogue act labels.	1
It indicates two drawbacks, (a) semantics of utterances are limited to entity mentions, and (b) the performance of coherence models strongly relies on the quality of the input dialogue act labels.	1
We address these issues by introducing a novel approach to dialogue coherence assessment.	1
We use dialogue act prediction as an auxiliary task in a multi-task learning scenario to obtain informative utterance representations for coherence assessment.	2+3
Our approach alleviates the need for explicit dialogue act labels during evaluation.	4
The results of our experiments show that our model substantially (more than 20 accuracy points) outperforms its strong competitors on the DailyDialogue corpus, and performs on par with them on the SwitchBoard corpus for ranking dialogues concerning their coherence.	4+5
We release our source code.	6
----------
We propose a graph-based method to tackle the dependency tree linearization task.	2+3
We formulate the task as a Traveling Salesman Problem (TSP), and use a biaffine attention model to calculate the edge costs.	3
We facilitate the decoding by solving the TSP for each subtree and combining the solution into a projective tree.	3
We then design a transition system as post-processing, inspired by non-projective transition-based parsing, to obtain non-projective sentences.	3
Our proposed method outperforms the state-of-the-art linearizer while being 10 times faster in training and decoding.	4
----------
This paper proposes the problem of Deep Question Generation (DQG), which aims to generate complex questions that require reasoning over multiple pieces of information about the input passage.	2
In order to capture the global structure of the document and facilitate reasoning, we propose a novel framework that first constructs a semantic-level graph for the input document and then encodes the semantic graph by introducing an attention-based GGNN (Att-GGNN).	2+3
Afterward, we fuse the document-level and graph-level representations to perform joint training of content selection and question decoding.	3
On the HotpotQA deep-question centric dataset, our model greatly improves performance over questions requiring reasoning over multiple facts, leading to state-of-the-art performance.	4+5
The code is publicly available at https://github.com/WING-NUS/SG-Deep-Question-Generation.	6
----------
Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training.	1
When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain.	1
But what should one do when there is no hand-labelled data for the target domain?	1
This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision.	1+2
The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain.	3
These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions.	3
A sequence labelling model can finally be trained on the basis of this unified annotation.	3
We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F1 scores compared to an out-of-domain neural NER model.	4
----------
Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities.	1
However, effective aggregation of relevant information in the document remains a challenging research question.	1
Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies.	1
Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph.	2+3
We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning.	3+4
Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset.	4+5
Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.	5
----------
In this paper, we propose a new task of machine translation (MT), which is based on no parallel sentences but can refer to a ground-truth bilingual dictionary.	2
Motivated by the ability of a monolingual speaker learning to translate via looking up the bilingual dictionary, we propose the task to see how much potential an MT system can attain using the bilingual dictionary and large scale monolingual corpora, while is independent on parallel sentences.	3
We propose anchored training (AT) to tackle the task.	3
AT uses the bilingual dictionary to establish anchoring points for closing the gap between source language and target language.	3
Experiments on various language pairs show that our approaches are significantly better than various baselines, including dictionary-based word-by-word translation, dictionary-supervised cross-lingual word embedding transformation, and unsupervised MT.	4
On distant language pairs that are hard for unsupervised MT to perform well, AT performs remarkably better, achieving performances comparable to supervised SMT trained on more than 4M parallel sentences.	4+5
----------
Word alignment was once a core unsupervised learning task in natural language processing because of its essential role in training statistical machine translation (MT) models.	1
Although unnecessary for training neural MT models, word alignment still plays an important role in interactive applications of neural machine translation, such as annotation transfer and lexicon injection.	1
While statistical MT methods have been replaced by neural approaches with superior performance, the twenty-year-old GIZA++ toolkit remains a key component of state-of-the-art word alignment systems.	1
Prior work on neural word alignment has only been able to outperform GIZA++ by using its output during training.	1
We present the first end-to-end neural word alignment method that consistently outperforms GIZA++ on three data sets.	2
Our approach repurposes a Transformer model trained for supervised translation to also serve as an unsupervised word alignment model in a manner that is tightly integrated and does not affect translation quality.	3
----------
Most neural machine translation models only rely on pairs of parallel sentences, assuming syntactic information is automatically learned by an attention mechanism.	1
In this work, we investigate different approaches to incorporate syntactic knowledge in the Transformer model and also propose a novel, parameter-free, dependency-aware self-attention mechanism that improves its translation quality, especially for long sentences and in low-resource scenarios.	2+3
We show the efficacy of each approach on WMT English-German and English-Turkish, and WAT English-Japanese translation tasks.	4
----------
Massively multilingual models for neural machine translation (NMT) are theoretically attractive, but often underperform bilingual models and deliver poor zero-shot translations.	1
In this paper, we explore ways to improve them.	2+3
We argue that multilingual NMT requires stronger modeling capacity to support language pairs with varying typological characteristics, and overcome this bottleneck via language-specific components and deepening NMT architectures.	2
We identify the off-target translation issue (i.e. translating into a wrong target language) as the major source of the inferior zero-shot performance, and propose random online backtranslation to enforce the translation of unseen training language pairs.	2+3
Experiments on OPUS-100 (a novel multilingual dataset with 100 languages) show that our approach substantially narrows the performance gap with bilingual models in both one-to-many and many-to-many settings, and improves zero-shot performance by ~10 BLEU, approaching conventional pivot-based methods.	4
----------
The performance of neural machine translation systems is commonly evaluated in terms of BLEU.	1
However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model.	1
In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models.	2+3
XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task.	3+4
We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems.	4+5
Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.	6
----------
Multilingual neural machine translation (NMT) has led to impressive accuracy improvements in low-resource scenarios by sharing common linguistic information across languages.	1
However, the traditional multilingual model fails to capture the diversity and specificity of different languages, resulting in inferior performance compared with individual models that are sufficiently trained.	1
In this paper, we incorporate a language-aware interlingua into the Encoder-Decoder architecture.	2
The interlingual network enables the model to learn a language-independent representation from the semantic spaces of different languages, while still allowing for language-specific specialization of a particular language-pair.	3
Experiments show that our proposed method achieves remarkable improvements over state-of-the-art multilingual NMT baselines and produces comparable performance with strong individual models.	4
----------
Evaluation of cross-lingual encoders is usually performed either via zero-shot cross-lingual transfer in supervised downstream tasks or via unsupervised cross-lingual textual similarity.	1
In this paper, we concern ourselves with reference-free machine translation (MT) evaluation where we directly compare source texts to (sometimes low-quality) system translations, which represents a natural adversarial setup for multilingual encoders.	2+3
Reference-free evaluation holds the promise of web-scale comparison of MT systems.	1
We systematically investigate a range of metrics based on state-of-the-art cross-lingual semantic representations obtained with pretrained M-BERT and LASER.	3
We find that they perform poorly as semantic encoders for reference-free MT evaluation and identify their two key limitations, namely, (a) a semantic mismatch between representations of mutual translations and, more prominently, (b) the inability to punish “translationese”, i.e., low-quality literal translations.	4
We propose two partial remedies: (1) post-hoc re-alignment of the vector spaces and (2) coupling of semantic-similarity based metrics with target-side language modeling.	4
In segment-level MT evaluation, our best metric surpasses reference-based BLEU by 5.7 correlation points.	4
----------
We present a novel method to extract parallel sentences from two monolingual corpora, using neural machine translation.	2+3
Our method relies on translating sentences in one corpus, but constraining the decoding by a prefix tree built on the other corpus.	3
We argue that a neural machine translation system by itself can be a sentence similarity scorer and it efficiently approximates pairwise comparison with a modified beam search.	2
When benchmarked on the BUCC shared task, our method achieves results comparable to other submissions.	3+4
----------
Position encoding (PE), an essential part of self-attention networks (SANs), is used to preserve the word order information for natural language processing tasks, generating fixed position indices for input sequences.	1
However, in cross-lingual scenarios, machine translation, the PEs of source and target sentences are modeled independently.	1
Due to word order divergences in different languages, modeling the cross-lingual positional relationships might help SANs tackle this problem.	1
In this paper, we augment SANs with cross-lingual position representations to model the bilingually aware latent structure for the input sentence.	2+3
Specifically, we utilize bracketing transduction grammar (BTG)-based reordering information to encourage SANs to learn bilingual diagonal alignments.	3
Experimental results on WMT’14 English⇒German, WAT’17 Japanese⇒English, and WMT’17 Chinese⇔English translation tasks demonstrate that our approach significantly and consistently improves translation quality over strong baselines.	4+5
Extensive analyses confirm that the performance gains come from the cross-lingual information.	4
----------
We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages.	2+3
We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks.	3
We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia.	4
They actually equal or improve the current state of the art in tagging and parsing for all five languages.	4
In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.	4+5
----------
While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge.	1
Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations.	1
We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites.	2+3
We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures.	4
Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments.	4
Our results also reveal a dissociation between perplexity and syntactic generalization performance.	5
----------
With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful.	1
Several testing methodologies have been developed to probe models’ syntactic representations.	1
One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with different syntax.	1
We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.	2+3
----------
Suspense is a crucial ingredient of narrative fiction, engaging readers and making stories compelling.	1
While there is a vast theoretical literature on suspense, it is computationally not well understood.	1
We compare two ways for modelling suspense: surprise, a backward-looking measure of how unexpected the current state is given the story so far; and uncertainty reduction, a forward-looking measure of how unexpected the continuation of the story is.	1
Both can be computed either directly over story representations or over their probability distributions.	1
We propose a hierarchical language model that encodes stories and computes surprise and uncertainty reduction.	2+3
Evaluating against short stories annotated with human suspense judgements, we find that uncertainty reduction over representations is the best predictor, resulting in near human accuracy.	4
We also show that uncertainty reduction can be used to predict suspenseful events in movie synopses.	4+5
----------
Predicting reading time has been a subject of much previous work, focusing on how different words affect human processing, measured by reading time.	1
However, previous work has dealt with a limited number of participants as well as word level only predictions (i.e. predicting the time to read a single word).	1
We seek to extend these works by examining whether or not document level predictions are effective, given additional information such as subject matter, font characteristics, and readability metrics.	2
We perform a novel experiment to examine how different features of text contribute to the time it takes to read, distributing and collecting data from over a thousand participants.	2+3
We then employ a large number of machine learning methods to predict a user’s reading time.	3
We find that despite extensive research showing that word level reading time can be most effectively predicted by neural networks, larger scale text can be easily and most accurately predicted by one factor, the number of words.	4
----------
Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse.	1
A key to success in either task is parallel training data which is expensive to obtain at a large scale.	1
In this work, we propose a generative model which couples NLU and NLG through a shared latent variable.	2+3
This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG.	2+3
Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations.	3+4
We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.	3+4
----------
Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains.	1
However, this design lacks adaptation to individual domains.	1
To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing.	1
We first observe that words in a sentence are often related to multiple domains.	2
Hence, we assume each word has a domain proportion, which indicates its domain preference.	3
Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions.	3
We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions.	3+4
Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well.	4
Our experiments show that our proposed model outperforms existing ones in several NMT tasks.	4
----------
To address the challenge of policy learning in open-domain multi-turn conversation, we propose to represent prior information about dialog transitions as a graph and learn a graph grounded dialog policy, aimed at fostering a more coherent and controllable dialog.	1+2
To this end, we first construct a conversational graph (CG) from dialog corpora, in which there are vertices to represent “what to say” and “how to say”, and edges to represent natural transition between a message (the last utterance in a dialog context) and its response.	3
We then present a novel CG grounded policy learning framework that conducts dialog flow planning by graph traversal, which learns to identify a what-vertex and a how-vertex from the CG at each turn to guide response generation.	3
In this way, we effectively leverage the CG to facilitate policy learning as follows: (1) it enables more effective long-term reward design, (2) it provides high-quality candidate actions, and (3) it gives us more control over the policy.	3
Results on two benchmark corpora demonstrate the effectiveness of this framework.	4
----------
Abstract Meaning Representations (AMRs) are broad-coverage sentence-level semantic graphs.	1
Existing approaches to generating text from AMR have focused on training sequence-to-sequence or graph-to-sequence models on AMR annotated data only.	1
In this paper, we propose an alternative approach that combines a strong pre-trained language model with cycle consistency-based re-scoring.	2+3
Despite the simplicity of the approach, our experimental results show these models outperform all previous techniques on the English LDC2017T10 dataset, including the recent use of transformer architectures.	3+4
In addition to the standard evaluation metrics, we provide human evaluation experiments that further substantiate the strength of our approach.	5
----------
We formulate the novel task of automatically updating an existing natural language comment based on changes in the body of code it accompanies.	1
We propose an approach that learns to correlate changes across two distinct language representations, to generate a sequence of edits that are applied to the existing comment to reflect the source code modifications.	2+3
We train and evaluate our model using a dataset that we collected from commit histories of open-source software projects, with each example consisting of a concurrent update to a method and its corresponding comment.	3
We compare our approach against multiple baselines using both automatic metrics and human evaluation.	3
Results reflect the challenge of this task and that our model outperforms baselines with respect to making edits.	4
----------
Subword segmentation is widely used to address the open vocabulary problem in machine translation.	1
The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens.	1
While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors.	1
So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018).	1
In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word.	2
We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE.	2
It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework.	3
Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.	4
----------
Most general-purpose extractive summarization models are trained on news articles, which are short and present all important information upfront.	1
As a result, such models are biased on position and often perform a smart selection of sentences from the beginning of the document.	1
When summarizing long narratives, which have complex structure and present information piecemeal, simple position heuristics are not sufficient.	1
In this paper, we propose to explicitly incorporate the underlying structure of narratives into general unsupervised and supervised extractive summarization models.	2
We formalize narrative structure in terms of key narrative events (turning points) and treat it as latent in order to summarize screenplays (i.e., extract an optimal sequence of scenes).	3
Experimental results on the CSI corpus of TV screenplays, which we augment with scene-level summarization labels, show that latent turning points correlate with important aspects of a CSI episode and improve summarization performance over general extractive algorithms leading to more complete and diverse summaries.	3+4
----------
In recent years there has been a burgeoning interest in the use of computational methods to distinguish between elicited speech samples produced by patients with dementia, and those from healthy controls.	1
The difference between perplexity estimates from two neural language models (LMs) - one trained on transcripts of speech produced by healthy participants and one trained on those with dementia - as a single feature for diagnostic classification of unseen transcripts has been shown to produce state-of-the-art performance.	1
However, little is known about why this approach is effective, and on account of the lack of case/control matching in the most widely-used evaluation set of transcripts (DementiaBank), it is unclear if these approaches are truly diagnostic, or are sensitive to other variables.	1
In this paper, we interrogate neural LMs trained on participants with and without dementia by using synthetic narratives previously developed to simulate progressive semantic dementia by manipulating lexical frequency.	2+3
We find that perplexity of neural LMs is strongly and differentially associated with lexical frequency, and that using a mixture model resulting from interpolating control and dementia LMs improves upon the current state-of-the-art for models trained on transcript text exclusively.	4
----------
Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear.	1
There is accumulating evidence that neural models do not learn systematically.	1
We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour.	1
We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying.	2+3
As a case study, we perform a series of experiments in the setting of natural language inference (NLI).	3
We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.	4
----------
Human conversations naturally evolve around related concepts and hop to distant concepts.	1
This paper presents a new conversation generation model, ConceptFlow, which leverages commonsense knowledge graphs to explicitly model conversation flows.	2+3
By grounding conversations to the concept space, ConceptFlow represents the potential conversation flow as traverses in the concept space along commonsense relations.	1
The traverse is guided by graph attentions in the concept graph, moving towards more meaningful directions in the concept space, in order to generate more semantic and informative responses.	3
Experiments on Reddit conversations demonstrate ConceptFlow’s effectiveness over previous knowledge-aware conversation models and GPT-2 based models while using 70% fewer parameters, confirming the advantage of explicit modeling conversation structures.	4+5
All source codes of this work are available at https://github.com/thunlp/ConceptFlow.	6
----------
The Natural Language Understanding (NLU) component in task oriented dialog systems processes a user’s request and converts it into structured information that can be consumed by downstream components such as the Dialog State Tracker (DST).	1
This information is typically represented as a semantic frame that captures the and slot-labels provided by the user.	1
We first show that such a shallow representation is insufficient for complex dialog scenarios, because it does not capture the recursive nature inherent in many domains.	1
We propose a recursive, hierarchical frame-based representation and show how to learn it from data.	2+3
We formulate the frame generation task as a template-based tree decoding task, where the decoder recursively generates a template and then fills slot values into the template.	3
We extend local tree-based loss functions with terms that provide global supervision and show how to optimize them end-to-end.	2+3
We achieve a small improvement on the widely used ATIS dataset and a much larger improvement on a more complex dataset we describe here.	4
----------
We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications.	2
It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare.	1+2
However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods.	1
In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models.	2
Our proposed method can be used with any binary class calibration scheme and a neural network model.	3
Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements.	4
We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems.	4
We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets.	4
Our method improves the calibration and model performance on out-of-domain test scenarios as well.	5
----------
Imitation learning algorithms provide state-of-the-art results on many structured prediction tasks by learning near-optimal search policies.	1
Such algorithms assume training-time access to an expert that can provide the optimal action at any queried state; unfortunately, the number of such queries is often prohibitive, frequently rendering these approaches impractical.	1
To combat this query complexity, we consider an active learning setting in which the learning algorithm has additional access to a much cheaper noisy heuristic that provides noisy guidance.	2+3
Our algorithm, LEAQI, learns a difference classifier that predicts when the expert is likely to disagree with the heuristic, and queries the expert only when necessary.	3
We apply LEAQI to three sequence labelling tasks, demonstrating significantly fewer queries to the expert and comparable (or better) accuracies over a passive approach.	3+4
----------
Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text.	1
In this paper, we allow model developers to specify these types of inductive biases as natural language explanations.	2+3
We use BERT fine-tuned on MultiNLI to “interpret” these explanations with respect to the input sentence, producing explanation-guided representations of the input.	3
Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3–20x less labeled data and improves on the baseline by 3–10 F1 points with the same amount of labeled data.	4
----------
Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks.	1
However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples.	1
In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected.	1
One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks.	1
In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting.	2+3
Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks.	4
----------
Sequence labeling is a fundamental task for a range of natural language processing problems.	1
When used in practice, its performance is largely influenced by the annotation quality and quantity, and meanwhile, obtaining ground truth labels is often costly.	1
In many cases, ground truth labels do not exist, but noisy annotations or annotations from different domains are accessible.	1
In this paper, we propose a novel framework Consensus Network (ConNet) that can be trained on annotations from multiple sources (e.g., crowd annotation, cross-domain data).	2
It learns individual representation for every source and dynamically aggregates source-specific knowledge by a context-aware attention module.	3
Finally, it leads to a model reflecting the agreement (consensus) among multiple sources.	3
We evaluate the proposed framework in two practical settings of multi-source learning: learning with crowd annotations and unsupervised cross-domain model adaptation.	3
Extensive experimental results show that our model achieves significant improvements over existing methods in both settings.	4
We also demonstrate that the method can apply to various tasks and cope with different encoders.	5
----------
This paper presents MixText, a semi-supervised learning method for text classification, which uses our newly designed data augmentation method called TMix.	2+3
TMix creates a large amount of augmented training samples by interpolating text in hidden space.	3
Moreover, we leverage recent advances in data augmentation to guess low-entropy labels for unlabeled data, hence making them as easy to use as labeled data.	3
By mixing labeled, unlabeled and augmented data, MixText significantly outperformed current pre-trained and fined-tuned models and other state-of-the-art semi-supervised learning methods on several text classification benchmarks.	4
The improvement is especially prominent when supervision is extremely limited.	4
We have publicly released our code at https://github.com/GT-SALT/MixText.	6
----------
Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters.	1
However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resource-limited mobile devices.	1
In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model.	2+3
Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.	3
Basically, MobileBERT is a thin version of BERT_LARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks.	3
To train MobileBERT, we first train a specially designed teacher model, an inverted-bottleneck incorporated BERT_LARGE model.	3
Then, we conduct knowledge transfer from this teacher to MobileBERT.	3
Empirical studies show that MobileBERT is 4.3x smaller and 5.5x faster than BERT_BASE while achieving competitive results on well-known benchmarks.	4
On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERT_BASE), and 62 ms latency on a Pixel 4 phone.	4
On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERT_BASE).	4
----------
Transfer learning has fundamentally changed the landscape of natural language processing (NLP).	1
Many state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks.	1
However, due to limited data resources from downstream tasks and the extremely high complexity of pre-trained models, aggressive fine-tuning often causes the fine-tuned model to overfit the training data of downstream tasks and fail to generalize to unseen data.	1
To address such an issue in a principled manner, we propose a new learning framework for robust and efficient fine-tuning for pre-trained models to attain better generalization performance.	2+3
The proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the complexity of the model; 2. Bregman proximal point optimization, which is an instance of trust-region methods and can prevent aggressive updating.	3
Our experiments show that the proposed framework achieves new state-of-the-art performance on a number of NLP tasks including GLUE, SNLI, SciTail and ANLI.	4
Moreover, it also outperforms the state-of-the-art T5 model, which is the largest pre-trained model containing 11 billion parameters, on GLUE.	4
----------
Extracting lexico-semantic relations as graph-structured taxonomies, also known as taxonomy construction, has been beneficial in a variety of NLP applications.	1
Recently Graph Neural Network (GNN) has shown to be powerful in successfully tackling many tasks.	1
However, there has been no attempt to exploit GNN to create taxonomies.	1
In this paper, we propose Graph2Taxo, a GNN-based cross-domain transfer framework for the taxonomy construction task.	2+3
Our main contribution is to learn the latent features of taxonomy construction from existing domains to guide the structure learning of an unseen domain.	5
We also propose a novel method of directed acyclic graph (DAG) generation for taxonomy construction.	3
Specifically, our proposed Graph2Taxo uses a noisy graph constructed from automatically extracted noisy hyponym hypernym candidate pairs, and a set of taxonomies for some known domains for training.	3
The learned model is then used to generate taxonomy for a new unknown domain given a set of terms for that domain.	3
Experiments on benchmark datasets from science and environment domains show that our approach attains significant improvements correspondingly over the state of the art.	4
----------
Cross-lingual word embeddings (CLWE) are often evaluated on bilingual lexicon induction (BLI).	1
Recent CLWE methods use linear projections, which underfit the training dictionary, to generalize on BLI.	1
However, underfitting can hinder generalization to other downstream tasks that rely on words from the training dictionary.	1
We address this limitation by retrofitting CLWE to the training dictionary, which pulls training translation pairs closer in the embedding space and overfits the training dictionary.	2+3
This simple post-processing step often improves accuracy on two downstream tasks, despite lowering BLI test accuracy.	3
We also retrofit to both the training dictionary and a synthetic dictionary induced from CLWE, which sometimes generalizes even better on downstream tasks.	3
Our results confirm the importance of fully exploiting training dictionary in downstream tasks and explains why BLI is a flawed CLWE evaluation.	4+5
----------
Authorship attribution aims to identify the author of a text based on the stylometric analysis.	1
Authorship obfuscation, on the other hand, aims to protect against authorship attribution by modifying a text’s style.	1
In this paper, we evaluate the stealthiness of state-of-the-art authorship obfuscation methods under an adversarial threat model.	2
An obfuscator is stealthy to the extent an adversary finds it challenging to detect whether or not a text modified by the obfuscator is obfuscated – a decision that is key to the adversary interested in authorship attribution.	3
We show that the existing authorship obfuscation methods are not stealthy as their obfuscated texts can be identified with an average F1 score of 0.87.	4
The reason for the lack of stealthiness is that these obfuscators degrade text smoothness, as ascertained by neural language models, in a detectable manner.	4
Our results highlight the need to develop stealthy authorship obfuscation methods that can better protect the identity of an author seeking anonymity.	5
----------
Large-scale pre-trained language models such as BERT have brought significant improvements to NLP applications.	1
However, they are also notorious for being slow in inference, which makes them difficult to deploy in real-time applications.	1
We propose a simple but effective method, DeeBERT, to accelerate BERT inference.	2+3
Our approach allows samples to exit earlier without passing through the entire model.	3
Experiments show that DeeBERT is able to save up to ~40% inference time with minimal degradation in model quality.	4
Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy.	4
Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks.	4+5
Code is available at https://github.com/castorini/DeeBERT.	6
----------
In hierarchical text classification, we perform a sequence of inference steps to predict the category of a document from top to bottom of a given class taxonomy.	1
Most of the studies have focused on developing novels neural network architectures to deal with the hierarchical structure, but we prefer to look for efficient ways to strengthen a baseline model.	1
We first define the task as a sequence-to-sequence problem.	1
Afterwards, we propose an auxiliary synthetic task of bottom-up-classification.	2+3
Then, from external dictionaries, we retrieve textual definitions for the classes of all the hierarchy’s layers, and map them into the word vector space.	3
We use the class-definition embeddings as an additional input to condition the prediction of the next layer and in an adapted beam search.	3
Whereas the modified search did not provide large gains, the combination of the auxiliary task and the additional input of class-definitions significantly enhance the classification accuracy.	4
With our efficient approaches, we outperform previous studies, using a drastically reduced number of parameters, in two well-known English datasets.	4
----------
We address the task of automatically grading the language proficiency of spontaneous speech based on textual features from automatic speech recognition transcripts.	2+3
Motivated by recent advances in multi-task learning, we develop neural networks trained in a multi-task fashion that learn to predict the proficiency level of non-native English speakers by taking advantage of inductive transfer between the main task (grading) and auxiliary prediction tasks: morpho-syntactic labeling, language modeling, and native language identification (L1).	3
We encode the transcriptions with both bi-directional recurrent neural networks and with bi-directional representations from transformers, compare against a feature-rich baseline, and analyse performance at different proficiency levels and with transcriptions of varying error rates.	3
Our best performance comes from a transformer encoder with L1 prediction as an auxiliary task.	4
We discuss areas for improvement and potential applications for text-only speech scoring.	5
----------
Representation learning is a critical ingredient for natural language processing systems.	1
Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.	1
For applications on scientific documents, such as classification and recommendation, accurate embeddings of documents are a necessity.	1
We propose SPECTER, a new method to generate document-level embedding of scientific papers based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.	2+3
Unlike existing pretrained language models, Specter can be easily applied to downstream applications without task-specific fine-tuning.	3
Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.	3
We show that Specter outperforms a variety of competitive baselines on the benchmark.	4
----------
We propose a method for program generation based on semantic scaffolds, lightweight structures representing the high-level semantic and syntactic composition of a program.	2+3
By first searching over plausible scaffolds then using these as constraints for a beam search over programs, we achieve better coverage of the search space when compared with existing techniques.	3+4
We apply our hierarchical search method to the SPoC dataset for pseudocode-to-code generation, in which we are given line-level natural language pseudocode annotations and aim to produce a program satisfying execution-based test cases.	2+3
By using semantic scaffolds during inference, we achieve a 10% absolute improvement in top-100 accuracy over the previous state-of-the-art.	4
Additionally, we require only 11 candidates to reach the top-3000 performance of the previous best approach when tested against unseen problems, demonstrating a substantial improvement in efficiency.	4+5
----------
In this paper, we observe that semi-structured tabulated text is ubiquitous; understanding them requires not only comprehending the meaning of text fragments, but also implicit relationships between them.	1
We argue that such data can prove as a testing ground for understanding how we reason about information.	1
To study this, we introduce a new dataset called INFOTABS, comprising of human-written textual hypotheses based on premises that are tables extracted from Wikipedia info-boxes.	2+3
Our analysis shows that the semi-structured, multi-domain and heterogeneous nature of the premises admits complex, multi-faceted reasoning.	4
Experiments reveal that, while human annotators agree on the relationships between a table-hypothesis pair, several standard modeling strategies are unsuccessful at the task, suggesting that reasoning about tables can pose a difficult modeling challenge.	4
----------
Existing machine reading comprehension (MRC) models do not scale effectively to real-world applications like web-level information retrieval and question answering (QA).	1
We argue that this stems from the nature of MRC datasets: most of these are static environments wherein the supporting documents and all necessary information are fully observed.	1
In this paper, we propose a simple method that reframes existing MRC datasets as interactive, partially observable environments.	2
Specifically, we “occlude” the majority of a document’s text and add context-sensitive commands that reveal “glimpses” of the hidden text to a model.	3
We repurpose SQuAD and NewsQA as an initial case study, and then show how the interactive corpora can be used to train a model that seeks relevant information through sequential decision making.	3
We believe that this setting can contribute in scaling models to web-level QA scenarios.	5
----------
Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets.	1
We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage.	1
We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus.	2+3
The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set.	3+4
This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.	4+5
----------
Training objectives based on predictive coding have recently been shown to be very effective at learning meaningful representations from unlabeled speech.	1
One example is Autoregressive Predictive Coding (Chung et al., 2019), which trains an autoregressive RNN to generate an unseen future frame given a context such as recent past frames.	1
The basic hypothesis of these approaches is that hidden states that can accurately predict future frames are a useful representation for many downstream tasks.	1
In this paper we extend this hypothesis and aim to enrich the information encoded in the hidden states by training the model to make more accurate future predictions.	2
We propose an auxiliary objective that serves as a regularization to improve generalization of the future frame prediction task.	2+3
Experimental results on phonetic classification, speech recognition, and speech translation not only support the hypothesis, but also demonstrate the effectiveness of our approach in learning representations that contain richer phonetic content.	4+5
----------
Recent Transformer-based contextual word representations, including BERT and XLNet, have shown state-of-the-art performance in multiple disciplines within NLP.	1
Fine-tuning the trained contextual models on task-specific datasets has been the key to achieving superior performance downstream.	1
While fine-tuning these pre-trained models is straightforward for lexical applications (applications with only language modality), it is not trivial for multimodal language (a growing area in NLP focused on modeling face-to-face communication).	1
More specifically, this is due to the fact that pre-trained models don’t have the necessary components to accept two extra modalities of vision and acoustic.	1
In this paper, we proposed an attachment to BERT and XLNet called Multimodal Adaptation Gate (MAG).	2
MAG allows BERT and XLNet to accept multimodal nonverbal data during fine-tuning.	3
It does so by generating a shift to internal representation of BERT and XLNet; a shift that is conditioned on the visual and acoustic modalities.	3
In our experiments, we study the commonly used CMU-MOSI and CMU-MOSEI datasets for multimodal sentiment analysis.	3
Fine-tuning MAG-BERT and MAG-XLNet significantly boosts the sentiment analysis performance over previous baselines as well as language-only fine-tuning of BERT and XLNet.	3+4
On the CMU-MOSI dataset, MAG-XLNet achieves human-level multimodal sentiment analysis performance for the first time in the NLP community.	4
----------
We address a challenging and practical task of labeling questions in speech in real time during telephone calls to emergency medical services in English, which embeds within a broader decision support system for emergency call-takers.	1+2
We propose a novel multimodal approach to real-time sequence labeling in speech.	2
Our model treats speech and its own textual representation as two separate modalities or views, as it jointly learns from streamed audio and its noisy transcription into text via automatic speech recognition.	3
Our results show significant gains of jointly learning from the two modalities when compared to text or audio only, under adverse noise and limited volume of training data.	4
The results generalize to medical symptoms detection where we observe a similar pattern of improvements with multimodal learning.	4+5
----------
This paper presents an audio visual automatic speech recognition (AV-ASR) system using a Transformer-based architecture.	2
We particularly focus on the scene context provided by the visual information, to ground the ASR.	3
We extract representations for audio features in the encoder layers of the transformer and fuse video features using an additional crossmodal multihead attention layer.	3
Additionally, we incorporate a multitask training criterion for multiresolution ASR, where we train the model to generate both character and subword level transcriptions.	3
Experimental results on the How2 dataset, indicate that multiresolution training can speed up convergence by around 50% and relatively improves word error rate (WER) performance by upto 18% over subword prediction models.	4
Further, incorporating visual information improves performance with relative gains upto 3.76% over audio only models.	4
Our results are comparable to state-of-the-art Listen, Attend and Spell-based architectures.	4
----------
End-to-end models for speech translation (ST) more tightly couple speech recognition (ASR) and machine translation (MT) than a traditional cascade of separate ASR and MT models, with simpler model architectures and the potential for reduced error propagation.	1
Their performance is often assumed to be superior, though in many conditions this is not yet the case.	1
We compare cascaded and end-to-end models across high, medium, and low-resource conditions, and show that cascades remain stronger baselines.	2
Further, we introduce two methods to incorporate phone features into ST models.	3
We show that these features improve both architectures, closing the gap between end-to-end models and cascades, and outperforming previous academic work – by up to 9 BLEU on our low-resource setting.	5
----------
Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people.	1
Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication.	1
Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality.	1
We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier.	2+3
We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.	3
----------
To achieve the long-term goal of machines being able to engage humans in conversation, our models should captivate the interest of their speaking partners.	1
Communication grounded in images, whereby a dialogue is conducted based on a given photo, is a setup naturally appealing to humans (Hu et al., 2014).	1
In this work we study large-scale architectures and datasets for this goal.	2
We test a set of neural architectures using state-of-the-art image and text representations, considering various ways to fuse the components.	3
To test such models, we collect a dataset of grounded human-human conversations, where speakers are asked to play roles given a provided emotional mood or style, as the use of such traits is also a key factor in engagingness (Guo et al., 2019).	3
Our dataset, Image-Chat, consists of 202k dialogues over 202k images using 215 possible style traits.	3
Automatic metrics and human evaluations of engagingness show the efficacy of our approach; in particular, we obtain state-of-the-art performance on the existing IGC task, and our best performing model is almost on par with humans on the Image-Chat test set (preferred 47.7% of the time).	4
----------
Evaluating the quality of a dialogue interaction between two agents is a difficult task, especially in open-domain chit-chat style dialogue.	1
There have been recent efforts to develop automatic dialogue evaluation metrics, but most of them do not generalize to unseen datasets and/or need a human-generated reference response during inference, making it infeasible for online evaluation.	1
Here, we propose an unreferenced automated evaluation metric that uses large pre-trained language models to extract latent representations of utterances, and leverages the temporal transitions that exist between them.	2+3
We show that our model achieves higher correlation with human annotations in an online setting, while not requiring true responses for comparison during inference.	4
----------
The timings of spoken response offsets in human dialogue have been shown to vary based on contextual elements of the dialogue.	1
We propose neural models that simulate the distributions of these response offsets, taking into account the response turn as well as the preceding turn.	2
The models are designed to be integrated into the pipeline of an incremental spoken dialogue system (SDS).	3
We evaluate our models using offline experiments as well as human listening tests.	3
We show that human listeners consider certain response timings to be more natural based on the dialogue context.	4
The introduction of these models into SDS pipelines could increase the perceived naturalness of interactions.	4+5
----------
Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text.	1
Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task.	1
Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult.	1
To narrow this gap, we propose DualEnc, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text.	1+2
Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.	4
----------
We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.	1
While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling—a special case of infilling where text is predicted at the end of a document.	1
In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.	2
To this end, we train (or fine tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.	3
We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.	5
Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machine-generated in the domain of short stories.	5
----------
Missing sentence generation (or sentence in-filling) fosters a wide range of applications in natural language generation, such as document auto-completion and meeting note expansion.	1
This task asks the model to generate intermediate missing sentences that can syntactically and semantically bridge the surrounding context.	1
Solving the sentence infilling task requires techniques in natural language processing ranging from understanding to discourse-level planning to generation.	1
In this paper, we propose a framework to decouple the challenge and address these three aspects respectively, leveraging the power of existing large-scale pre-trained models such as BERT and GPT-2.	2+3
We empirically demonstrate the effectiveness of our model in learning a sentence representation for generation and further generating a missing sentence that fits the context.	3+4
----------
Auto-regressive text generation models usually focus on local fluency, and may cause inconsistent semantic meaning in long text generation.	1
Further, automatically generating words with similar semantics is challenging, and hand-crafted linguistic rules are difficult to apply.	1
We consider a text planning scheme and present a model-based imitation-learning approach to alleviate the aforementioned issues.	2
Specifically, we propose a novel guider network to focus on the generative process over a longer horizon, which can assist next-word prediction and provide intermediate rewards for generator optimization.	2
Extensive experiments demonstrate that the proposed method leads to improved performance.	5
----------
Retrieve-and-edit seq2seq methods typically retrieve an output from the training set and learn a model to edit it to produce the final output.	1
We propose to extend this framework with a simple and effective post-generation ranking approach.	2
Our framework (i) retrieves several potentially relevant outputs for each input, (ii) edits each candidate independently, and (iii) re-ranks the edited candidates to select the final output.	3
We use a standard editing model with simple task-specific re-ranking approaches, and we show empirically that this approach outperforms existing, significantly more complex methodologies.	3+4
Experiments on two machine translation (MT) datasets show new state-of-art results.	4
We also achieve near state-of-art performance on the Gigaword summarization dataset, where our analyses show that there is significant room for performance improvement with better candidate output selection in future work.	5
----------
Learning to follow instructions is of fundamental importance to autonomous agents for vision-and-language navigation (VLN).	1
In this paper, we study how an agent can navigate long paths when learning from a corpus that consists of shorter ones.	2
We show that existing state-of-the-art agents do not generalize well.	3
To this end, we propose BabyWalk, a new VLN agent that is learned to navigate by decomposing long instructions into shorter ones (BabySteps) and completing them sequentially.	3
A special design memory buffer is used by the agent to turn its past experiences into contexts for future steps.	3
The learning process is composed of two phases.	3
In the first phase, the agent uses imitation learning from demonstration to accomplish BabySteps.	3
In the second phase, the agent uses curriculum-based reinforcement learning to maximize rewards on navigation tasks with increasingly longer instructions.	3
We create two new benchmark datasets (of long navigation tasks) and use them in conjunction with existing ones to examine BabyWalk’s generalization ability.	3
Empirical results show that BabyWalk achieves state-of-the-art results on several metrics, in particular, is able to follow long instructions better.	4
The codes and the datasets are released on our project page: https://github.com/Sha-Lab/babywalk.	1
----------
We apply a generative segmental model of task structure, guided by narration, to action segmentation in video.	2+3
We focus on unsupervised and weakly-supervised settings where no action labels are known during training.	3
Despite its simplicity, our model performs competitively with previous work on a dataset of naturalistic instructional videos.	4
Our model allows us to vary the sources of supervision used in training, and we find that both task structure and narrative language provide large benefits in segmentation quality.	4+5
----------
Generating multi-sentence descriptions for videos is one of the most challenging captioning tasks due to its high requirements for not only visual relevance but also discourse-based coherence across the sentences in the paragraph.	1
Towards this goal, we propose a new approach called Memory-Augmented Recurrent Transformer (MART), which uses a memory module to augment the transformer architecture.	2+3
The memory module generates a highly summarized memory state from the video segments and the sentence history so as to help better prediction of the next sentence (w.r.t. coreference and repetition aspects), thus encouraging coherent paragraph generation.	3
Extensive experiments, human evaluations, and qualitative analyses on two popular datasets ActivityNet Captions and YouCookII show that MART generates more coherent and less repetitive paragraph captions than baseline methods, while maintaining relevance to the input video events.	4
----------
Visual features are a promising signal for learning bootstrap textual models.	1
However, blackbox learning models make it difficult to isolate the specific contribution of visual components.	1
In this analysis, we consider the case study of the Visually Grounded Neural Syntax Learner (Shi et al., 2019), a recent approach for learning syntax from a visual training signal.	2
By constructing simplified versions of the model, we isolate the core factors that yield the model’s strong performance.	3
Contrary to what the model might be capable of learning, we find significantly less expressive versions produce similar predictions and perform just as well, or even better.	4
We also find that a simple lexical signal of noun concreteness plays the main role in the model’s predictions as opposed to more complex syntactic reasoning.	4
----------
Variational Autoencoder (VAE) is widely used as a generative model to approximate a model’s posterior on latent variables by combining the amortized variational inference and deep neural networks.	1
However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as “posterior collapse”.	1
Previous approaches consider the Kullback–Leibler divergence (KL) individual for each datapoint.	1
We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL’s distribution positive.	2
Then we propose Batch Normalized-VAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior’s parameters.	3
Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently.	4
We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE).	4+5
Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.	5
----------
We study the settings for which deep contextual embeddings (e.g., BERT) give large improvements in performance relative to classic pretrained embeddings (e.g., GloVe), and an even simpler baseline—random word embeddings—focusing on the impact of the training set size and the linguistic properties of the task.	2
Surprisingly, we find that both of these simpler baselines can match contextual embeddings on industry-scale data, and often perform within 5 to 10% accuracy (absolute) on benchmark tasks.	4
Furthermore, we identify properties of data for which contextual embeddings give particularly large gains: language containing complex structure, ambiguous word usage, and words unseen in training.	4
----------
We study the potential for interaction in natural language classification.	2
We add a limited form of interaction for intent classification, where users provide an initial query using natural language, and the system asks for additional information using binary or multi-choice questions.	3
At each turn, our system decides between asking the most informative question or making the final classification pre-diction.	3
The simplicity of the model allows for bootstrapping of the system without interaction data, instead relying on simple crowd-sourcing tasks.	3
We evaluate our approach on two domains, showing the benefit of interaction and the advantage of learning to balance between asking additional questions and making the final prediction.	5
----------
Knowledge graph (KG) representation learning techniques that learn continuous embeddings of entities and relations in the KG have become popular in many AI applications.	1
With a large KG, the embeddings consume a large amount of storage and memory.	1
This is problematic and prohibits the deployment of these techniques in many real world settings.	1
Thus, we propose an approach that compresses the KG embedding layer by representing each entity in the KG as a vector of discrete codes and then composes the embeddings from these codes.	2+3
The approach can be trained end-to-end with simple modifications to any existing KG embedding technique.	3
We evaluate the approach on various standard KG embedding evaluations and show that it achieves 50-1000x compression of embeddings with a minor loss in performance.	4
The compressed embeddings also retain the ability to perform various reasoning tasks such as KG inference.	5
----------
This work revisits the task of training sequence tagging models with limited resources using transfer learning.	2+3
We investigate several proposed approaches introduced in recent works and suggest a new loss that relies on sentence reconstruction from normalized embeddings.	3
Specifically, our method demonstrates how by adding a decoding layer for sentence reconstruction, we can improve the performance of various baselines.	4
We show improved results on the CoNLL02 NER and UD 1.2 POS datasets and demonstrate the power of the method for transfer learning with low-resources achieving 0.6 F1 score in Dutch using only one sample from it.	4
----------
Pretrained masked language models (MLMs) require finetuning for most NLP tasks.	1
Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one.	1
We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks.	2
By rescoring ASR and NMT hypotheses, RoBERTa reduces an end-to-end LibriSpeech model’s WER by 30% relative and adds up to +1.7 BLEU on state-of-the-art baselines for low-resource translation pairs, with further gains from domain adaptation.	3+4
We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP).	4
One can finetune MLMs to give scores without masking, enabling computation in a single inference pass.	4
In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages.	4
We release our library for language model scoring at https://github.com/awslabs/mlm-scoring.	6
----------
Distance-based knowledge graph embeddings have shown substantial improvement on the knowledge graph link prediction task, from TransE to the latest state-of-the-art RotatE.	1
However, complex relations such as N-to-1, 1-to-N and N-to-N still remain challenging to predict.	1
In this work, we propose a novel distance-based approach for knowledge graph link prediction.	2
First, we extend the RotatE from 2D complex domain to high dimensional space with orthogonal transforms to model relations.	3
The orthogonal transform embedding for relations keeps the capability for modeling symmetric/anti-symmetric, inverse and compositional relations while achieves better modeling capacity.	3+4
Second, the graph context is integrated into distance scoring functions directly.	3
Specifically, graph context is explicitly modeled via two directed context representations.	3
Each node embedding in knowledge graph is augmented with two context representations, which are computed from the neighboring outgoing and incoming nodes/edges respectively.	4
The proposed approach improves prediction accuracy on the difficult N-to-1, 1-to-N and N-to-N cases.	4
Our experimental results show that it achieves state-of-the-art results on two common benchmarks FB15k-237 and WNRR-18, especially on FB15k-237 which has many high in-degree nodes.	4
----------
Most classification models work by first predicting a posterior probability distribution over all classes and then selecting that class with the largest estimated probability.	1
In many settings however, the quality of posterior probability itself (e.g., 65% chance having diabetes), gives more reliable information than the final predicted class alone.	1
When these methods are shown to be poorly calibrated, most fixes to date have relied on posterior calibration, which rescales the predicted probabilities but often has little impact on final classifications.	1
Here we propose an end-to-end training procedure called posterior calibrated (PosCal) training that directly optimizes the objective while minimizing the difference between the predicted and empirical posterior probabilities.	2
We show that PosCal not only helps reduce the calibration error but also improve task performance by penalizing drops in performance of both objectives.	4
Our PosCal achieves about 2.5% of task performance gain and 16.1% of calibration error reduction on GLUE (Wang et al., 2018) compared to the baseline.	4
We achieved the comparable task performance with 13.2% calibration error reduction on xSLUE (Kang and Hovy, 2019), but not outperforming the two-stage calibration baseline.	4
PosCal training can be easily extendable to any types of classification tasks as a form of regularization term.	5
Also, PosCal has the advantage that it incrementally tracks needed statistics for the calibration objective during the training process, making efficient use of large training sets.	5
----------
Text generation often requires high-precision output that obeys task-specific rules.	1
This fine-grained control is difficult to enforce with off-the-shelf deep learning models.	1
In this work, we consider augmenting neural generation models with discrete control states learned through a structured latent-variable approach.	2
Under this formulation, task-specific knowledge can be encoded through a range of rich, posterior constraints that are effectively trained into the model.	4
This approach allows users to ground internal model decisions based on prior knowledge, without sacrificing the representational power of neural generative models.	4
Experiments consider applications of this approach for text generation.	4
We find that this method improves over standard benchmarks, while also providing fine-grained control.	4
----------
Despite excellent performance on many tasks, NLP systems are easily fooled by small adversarial perturbations of inputs.	1
Existing procedures to defend against such perturbations are either (i) heuristic in nature and susceptible to stronger attacks or (ii) provide guaranteed robustness to worst-case attacks, but are incompatible with state-of-the-art models like BERT.	1
In this work, we introduce robust encodings (RobEn): a simple framework that confers guaranteed robustness, without making compromises on model architecture.	1+2
The core component of RobEn is an encoding function, which maps sentences to a smaller, discrete space of encodings.	3
Systems using these encodings as a bottleneck confer guaranteed robustness with standard training, and the same encodings can be used across multiple tasks.	3
We identify two desiderata to construct robust encoding functions: perturbations of a sentence should map to a small set of encodings (stability), and models using encodings should still perform well (fidelity).	3
We instantiate RobEn to defend against a large family of adversarial typos.	4
Across six tasks from GLUE, our instantiation of RobEn paired with BERT achieves an average robust accuracy of 71.3% against all adversarial typos in the family considered, while previous work using a typo-corrector achieves only 35.3% accuracy against a simple greedy attack.	4
----------
BERT (Bidirectional Encoder Representations from Transformers) and related pre-trained Transformers have provided large gains across many language understanding tasks, achieving a new state-of-the-art (SOTA).	1
BERT is pretrained on two auxiliary tasks: Masked Language Model and Next Sentence Prediction.	1
In this paper we introduce a new pre-training task inspired by reading comprehension to better align the pre-training from memorization to understanding.	1+2
Span Selection PreTraining (SSPT) poses cloze-like training instances, but rather than draw the answer from the model’s parameters, it is selected from a relevant passage.	3
We find significant and consistent improvements over both BERT-BASE and BERT-LARGE on multiple Machine Reading Comprehension (MRC) datasets.	4
Specifically, our proposed model has strong empirical evidence as it obtains SOTA results on Natural Questions, a new benchmark MRC dataset, outperforming BERT-LARGE by 3 F1 points on short answer prediction.	4
We also show significant impact in HotpotQA, improving answer prediction F1 by 4 points and supporting fact prediction F1 by 1 point and outperforming the previous best system.	4
Moreover, we show that our pre-training approach is particularly effective when training data is limited, improving the learning curve by a large amount.	5
----------
Recently, NLP has seen a surge in the usage of large pre-trained models.	1
Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice.	1
This raises the question of whether downloading untrusted pre-trained weights can pose a security threat.	1
In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.	2+3
We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure.	3+4
Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat.	4
Finally, we outline practical defenses against such attacks.	4
----------
Transformers have gradually become a key component for many state-of-the-art natural language representation models.	1
A recent Transformer based model- BERTachieved state-of-the-art results on various natural language processing tasks, including GLUE, SQuAD v1.1, and SQuAD v2.0.	1
This model however is computationally prohibitive and has a huge number of parameters.	1
In this work we revisit the architecture choices of BERT in efforts to obtain a lighter model.	2
We focus on reducing the number of parameters yet our methods can be applied towards other objectives such FLOPs or latency.	3
We show that much efficient light BERT models can be obtained by reducing algorithmically chosen correct architecture design dimensions rather than reducing the number of Transformer encoder layers.	3+4
In particular, our schuBERT gives 6.6% higher average accuracy on GLUE and SQuAD datasets as compared to BERT with three encoder layers while having the same number of parameters.	4
----------
We propose to train a non-autoregressive machine translation model to minimize the energy defined by a pretrained autoregressive model.	1+2
In particular, we view our non-autoregressive translation system as an inference network (Tu and Gimpel, 2018) trained to minimize the autoregressive teacher energy.	3
This contrasts with the popular approach of training a non-autoregressive model on a distilled corpus consisting of the beam-searched outputs of such a teacher model.	4
Our approach, which we call ENGINE (ENerGy-based Inference NEtworks), achieves state-of-the-art non-autoregressive results on the IWSLT 2014 DE-EN and WMT 2016 RO-EN datasets, approaching the performance of autoregressive models.	4
----------
Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged.	1
The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT.	1
The second direction employs monolingual data with self-supervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data.	1
In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT.	2
We offer three major results:	4
(i) Using monolingual data significantly boosts the translation quality of low-resource languages in multilingual models.	4
(ii) Self-supervision improves zero-shot translation quality in multilingual models.	4
(iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on ro-en translation without any parallel data or back-translation.	4
----------
Back-translation is a widely used data augmentation technique which leverages target monolingual data.	1
However, its effectiveness has been challenged since automatic metrics such as BLEU only show significant improvements for test examples where the source itself is a translation, or translationese.	1
This is believed to be due to translationese inputs better matching the back-translated training data.	1
In this work, we show that this conjecture is not empirically supported and that back-translation improves translation quality of both naturally occurring text as well as translationese according to professional human translators.	1+2
We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs.	4
BLEU cannot capture human preferences because references are translationese when source sentences are natural text.	4
We recommend complementing BLEU with a language model score to measure fluency.	4
----------
Adaptive policies are better than fixed policies for simultaneous translation, since they can flexibly balance the tradeoff between translation quality and latency based on the current context information.	1
But previous methods on obtaining adaptive policies either rely on complicated training process, or underperform simple fixed policies.	1
We design an algorithm to achieve adaptive policies via a simple heuristic composition of a set of fixed policies.	2+3
Experiments on Chinese -> English and German -> English show that our adaptive policies can outperform fixed ones by up to 4 BLEU points for the same latency, and more surprisingly, it even surpasses the BLEU score of full-sentence translation in the greedy mode (and very close to beam mode), but with much lower latency.	3+4
----------
Neural architectures are the current state of the art in Word Sense Disambiguation (WSD).	1
However, they make limited use of the vast amount of relational information encoded in Lexical Knowledge Bases (LKB).	1
We present Enhanced WSD Integrating Synset Embeddings and Relations (EWISER), a neural supervised architecture that is able to tap into this wealth of knowledge by embedding information from the LKB graph within the neural architecture, and to exploit pretrained synset embeddings, enabling the network to predict synsets that are not in the training set.	2
As a result, we set a new state of the art on almost all the evaluation settings considered, also breaking through, for the first time, the 80% ceiling on the concatenation of all the standard all-words English WSD evaluation benchmarks.	4
On multilingual all-words WSD, we report state-of-the-art results by training on nothing but English.	4
----------
Chinese NLP applications that rely on large text often contain huge amounts of vocabulary which are sparse in corpus.	1
We show that characters’ written form, Glyphs, in ideographic languages could carry rich semantics.	1
We present a multi-modal model, Glyph2Vec, to tackle Chinese out-of-vocabulary word embedding problem.	2
Glyph2Vec extracts visual features from word glyphs to expand current word embedding space for out-of-vocabulary word embedding, without the need of accessing any corpus, which is useful for improving Chinese NLP systems, especially for low-resource scenarios.	5
Experiments across different applications show the significant effectiveness of our model.	5
----------
We present a neural framework for learning associations between interrelated groups of words such as the ones found in Subject-Verb-Object (SVO) structures.	1+2
Our model induces a joint function-specific word vector space, where vectors of e.g. plausible SVO compositions lie close together.	3
The model retains information about word group membership even in the joint space, and can thereby effectively be applied to a number of tasks reasoning over the SVO structure.	4
We show the robustness and versatility of the proposed framework by reporting state-of-the-art results on the tasks of estimating selectional preference and event similarity.	5
The results indicate that the combinations of representations learned with our task-independent model outperform task-specific architectures from prior work, while reducing the number of parameters by up to 95%.	5
----------
While automatic term extraction is a well-researched area, computational approaches to distinguish between degrees of technicality are still understudied.	1
We semi-automatically create a German gold standard of technicality across four domains, and illustrate the impact of a web-crawled general-language corpus on technicality prediction.	2+3
When defining a classification approach that combines general-language and domain-specific word embeddings, we go beyond previous work and align vector spaces to gain comparative embeddings.	3
We suggest two novel models to exploit general- vs. domain-specific comparisons: a simple neural network model with pre-computed comparative-embedding information as input, and a multi-channel model computing the comparison internally.	3
Both models outperform previous approaches, with the multi-channel model performing best.	4
----------
Metaphor is a linguistic device in which a concept is expressed by mentioning another.	1
Identifying metaphorical expressions, therefore, requires a non-compositional understanding of semantics.	1
Multiword Expressions (MWEs), on the other hand, are linguistic phenomena with varying degrees of semantic opacity and their identification poses a challenge to computational models.	1
This work is the first attempt at analysing the interplay of metaphor and MWEs processing through the design of a neural architecture whereby classification of metaphors is enhanced by informing the model of the presence of MWEs.	1+2
To the best of our knowledge, this is the first “MWE-aware” metaphor identification system paving the way for further experiments on the complex interactions of these phenomena.	3
The results and analyses show that this proposed architecture reach state-of-the-art on two different established metaphor datasets.	4
----------
Multilingual representations embed words from many languages into a single semantic space such that words with similar meanings are close to each other regardless of the language.	1
These embeddings have been widely used in various settings, such as cross-lingual transfer, where a natural language processing (NLP) model trained on one language is deployed to another language.	1
While the cross-lingual transfer techniques are powerful, they carry gender bias from the source to target languages.	1
In this paper, we study gender bias in multilingual embeddings and how it affects transfer learning for NLP applications.	2
We create a multilingual dataset for bias analysis and propose several ways for quantifying bias in multilingual representations from both the intrinsic and extrinsic perspectives.	3
Experimental results show that the magnitude of bias in the multilingual representations changes differently when we align the embeddings to different target spaces and that the alignment direction can also have an influence on the bias in transfer learning.	4
We further provide recommendations for using the multilingual word representations for downstream tasks.	5
----------
Most NLP datasets are not annotated with protected attributes such as gender, making it difficult to measure classification bias using standard measures of fairness (e.g., equal opportunity).	1
However, manually annotating a large dataset with a protected attribute is slow and expensive.	1
Instead of annotating all the examples, can we annotate a subset of them and use that sample to estimate the bias?	1
While it is possible to do so, the smaller this annotated sample is, the less certain we are that the estimate is close to the true bias.	1
In this work, we propose using Bernstein bounds to represent this uncertainty about the bias estimate as a confidence interval.	1+2
We provide empirical evidence that a 95% confidence interval derived this way consistently bounds the true bias.	4
In quantifying this uncertainty, our method, which we call Bernstein-bounded unfairness, helps prevent classifiers from being deemed biased or unbiased when there is insufficient evidence to make either claim.	5
Our findings suggest that the datasets currently used to measure specific biases are too small to conclusively identify bias except in the most egregious cases.	5
For example, consider a co-reference resolution system that is 5% more accurate on gender-stereotypical sentences – to claim it is biased with 95% confidence, we need a bias-specific dataset that is 3.8 times larger than WinoBias, the largest available.	5
----------
We propose a deep and interpretable probabilistic generative model to analyze glyph shapes in printed Early Modern documents.	1+2
We focus on clustering extracted glyph images into underlying templates in the presence of multiple confounding sources of variance.	3
Our approach introduces a neural editor model that first generates well-understood printing phenomena like spatial perturbations from template parameters via interpertable latent variables, and then modifies the result by generating a non-interpretable latent vector responsible for inking variations, jitter, noise from the archiving process, and other unforeseen phenomena associated with Early Modern printing.	3
Critically, by introducing an inference network whose input is restricted to the visual residual between the observation and the interpretably-modified template, we are able to control and isolate what the vector-valued latent variable captures.	3
We show that our approach outperforms rigid interpretable clustering baselines (c.f. Ocular) and overly-flexible deep generative models (VAE) alike on the task of completely unsupervised discovery of typefaces in mixed-fonts documents.	4
----------
Pooling is an important technique for learning text representations in many neural NLP models.	1
In conventional pooling methods such as average, max and attentive pooling, text representations are weighted summations of the L1 or L∞ norm of input features.	1
However, their pooling norms are always fixed and may not be optimal for learning accurate text representations in different tasks.	1
In addition, in many popular pooling methods such as max and attentive pooling some features may be over-emphasized, while other useful ones are not fully exploited.	1
In this paper, we propose an Attentive Pooling with Learnable Norms (APLN) approach for text representation.	2
Different from existing pooling methods that use a fixed pooling norm, we propose to learn the norm in an end-to-end manner to automatically find the optimal ones for text representation in different tasks.	3
In addition, we propose two methods to ensure the numerical stability of the model training.	3
The first one is scale limiting, which re-scales the input to ensure non-negativity and alleviate the risk of exponential explosion.	3
The second one is re-formulation, which decomposes the exponent operation to avoid computing the real-valued powers of the input and further accelerate the pooling operation.	3
Experimental results on four benchmark datasets show that our approach can effectively improve the performance of attentive pooling.	4
----------
Multi-task learning (MTL) and transfer learning (TL) are techniques to overcome the issue of data scarcity when training state-of-the-art neural networks.	1
However, finding beneficial auxiliary datasets for MTL or TL is a time- and resource-consuming trial-and-error approach.	1
We propose new methods to automatically assess the similarity of sequence tagging datasets to identify beneficial auxiliary data for MTL or TL setups.	1+2
Our methods can compute the similarity between any two sequence tagging datasets, they do not need to be annotated with the same tagset or multiple labels in parallel.	4
Additionally, our methods take tokens and their labels into account, which is more robust than only using either of them as an information source, as conducted in prior work.	4
We empirically show that our similarity measures correlate with the change in test score of neural networks that use the auxiliary dataset for MTL to increase the main task performance.	4
We provide an efficient, open-source implementation.	6
----------
Self-attention networks (SANs) with selective mechanism has produced substantial improvements in various NLP tasks by concentrating on a subset of input words.	1
However, the underlying reasons for their strong performance have not been well explained.	1
In this paper, we bridge the gap by assessing the strengths of selective SANs (SSANs), which are implemented with a flexible and universal Gumbel-Softmax.	2+3
Experimental results on several representative NLP tasks, including natural language inference, semantic role labelling, and machine translation, show that SSANs consistently outperform the standard SANs.	4
Through well-designed probing experiments, we empirically validate that the improvement of SSANs can be attributed in part to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling.	4
Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence.	5
----------
Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers.	1
Could ordering the sublayers in a different pattern lead to better performance?	1
We generate randomly ordered transformers and train them with the language modeling objective.	2+3
We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top.	4
We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.	4
However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models.	4
Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.	4
----------
Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort.	1
In this study, we propose a novel method that replicates the effects of a model ensemble with a single model.	2
Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors.	3
Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.	4
----------
Zero-shot learning has been a tough problem since no labeled data is available for unseen classes during training, especially for classes with low similarity.	1
In this situation, transferring from seen classes to unseen classes is extremely hard.	1
To tackle this problem, in this paper we propose a self-training based method to efficiently leverage unlabeled data.	2
Traditional self-training methods use fixed heuristics to select instances from unlabeled data, whose performance varies among different datasets.	3
We propose a reinforcement learning framework to learn data selection strategy automatically and provide more reliable selection.	3
Experimental results on both benchmarks and a real-world e-commerce dataset show that our approach significantly outperforms previous methods in zero-shot text classification	4
----------
Multi-modal neural machine translation (NMT) aims to translate source sentences into a target language paired with images.	1
However, dominant multi-modal NMT models do not fully exploit fine-grained semantic correspondences between semantic units of different modalities, which have potential to refine multi-modal representation learning.	1
To deal with this issue, in this paper, we propose a novel graph-based multi-modal fusion encoder for NMT.	2+3
Specifically, we first represent the input sentence and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects).	3
We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations.	3
Finally, these representations provide an attention-based context vector for the decoder.	3
We evaluate our proposed encoder on the Multi30K datasets.	3
Experimental results and in-depth analysis show the superiority of our multi-modal NMT model.	4
----------
Recently unsupervised Bilingual Lexicon Induction(BLI) without any parallel corpus has attracted much research interest.	1
One of the crucial parts in methods for the BLI task is the matching procedure.	1
Previous works impose a too strong constraint on the matching and lead to many counterintuitive translation pairings.	1
Thus We propose a relaxed matching procedure to find a more precise matching between two languages.	2
We also find that aligning source and target language embedding space bidirectionally will bring significant improvement.	3
We follow the previous iterative framework to conduct experiments.	3
Results on standard benchmark demonstrate the effectiveness of our proposed method, which substantially outperforms previous unsupervised methods.	4
----------
This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units.	2
We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference.	3
A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability.	3
DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming.	3
Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences.	4
DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English <=> (German, Romanian, Estonian, Finnish, Hungarian).	4
----------
We propose a novel manifold based geometric approach for learning unsupervised alignment of word embeddings between the source and the target languages.	1+2
Our approach formulates the alignment learning problem as a domain adaptation problem over the manifold of doubly stochastic matrices.	3
This viewpoint arises from the aim to align the second order information of the two language spaces.	3
The rich geometry of the doubly stochastic manifold allows to employ efficient Riemannian conjugate gradient algorithm for the proposed formulation.	3
Empirically, the proposed approach outperforms state-of-the-art optimal transport based approach on the bilingual lexicon induction task across several language pairs.	4
The performance improvement is more significant for distant language pairs.	5
----------
Non-autoregressive neural machine translation (NAT) predicts the entire target sequence simultaneously and significantly accelerates inference process.	1
However, NAT discards the dependency information in a sentence, and thus inevitably suffers from the multi-modality problem: the target tokens may be provided by different possible translations, often causing token repetitions or missing.	1
To alleviate this problem, we propose a novel semi-autoregressive model RecoverSAT in this work, which generates a translation as a sequence of segments.	2
The segments are generated simultaneously while each segment is predicted token-by-token.	3
By dynamically determining segment length and deleting repetitive segments, RecoverSAT is capable of recovering from repetitive and missing token errors.	3
Experimental results on three widely-used benchmark datasets show that our proposed model achieves more than 4 times speedup while maintaining comparable performance compared with the corresponding autoregressive model.	4
----------
Confidence calibration, which aims to make model predictions equal to the true correctness measures, is important for neural machine translation (NMT) because it is able to offer useful indicators of translation errors in the generated output.	1
While prior studies have shown that NMT models trained with label smoothing are well-calibrated on the ground-truth training data, we find that miscalibration still remains a severe challenge for NMT during inference due to the discrepancy between training and inference.	1
By carefully designing experiments on three language pairs, our work provides in-depth analyses of the correlation between calibration and translation performance as well as linguistic properties of miscalibration and reports a number of interesting findings that might help humans better analyze, understand and improve NMT models.	2+3
Based on these observations, we further propose a new graduated label smoothing method that can improve both inference calibration and translation performance.	4
----------
We propose a Semi-supervIsed GeNerative Active Learning (SIGNAL) model to address the imbalance, efficiency, and text camouflage problems of Chinese text spam detection task.	1+2
A “self-diversity” criterion is proposed for measuring the “worthiness” of a candidate for annotation.	4
A semi-supervised variational autoencoder with masked attention learning approach and a character variation graph-enhanced augmentation procedure are proposed for data augmentation.	4
The preliminary experiment demonstrates the proposed SIGNAL model is not only sensitive to spam sample selection, but also can improve the performance of a series of conventional active learning models for Chinese spam detection task.	4+5
To the best of our knowledge, this is the first work to integrate active learning and semi-supervised generative learning for text spam detection.	5
----------
Legal Judgement Prediction (LJP) is the task of automatically predicting a law case’s judgment results given a text describing the case’s facts, which has great prospects in judicial assistance systems and handy services for the public.	1
In practice, confusing charges are often presented, because law cases applicable to similar law articles are easily misjudged.	1
To address this issue, existing work relies heavily on domain experts, which hinders its application in different law systems.	1
In this paper, we present an end-to-end model, LADAN, to solve the task of LJP.	1+2
To distinguish confusing charges, we propose a novel graph neural network, GDL, to automatically learn subtle differences between confusing law articles, and also design a novel attention mechanism that fully exploits the learned differences to attentively extract effective discriminative features from fact descriptions.	3
Experiments conducted on real-world datasets demonstrate the superiority of our LADAN.	4
----------
Writing a good job posting is a critical step in the recruiting process, but the task is often more difficult than many people think.	1
It is challenging to specify the level of education, experience, relevant skills per the company information and job description.	1
To this end, we propose a novel task of Job Posting Generation (JPG) which is cast as a conditional text generation problem to generate job requirements according to the job descriptions.	1+2
To deal with this task, we devise a data-driven global Skill-Aware Multi-Attention generation model, named SAMA.	3
Specifically, to model the complex mapping relationships between input and output, we design a hierarchical decoder that we first label the job description with multiple skills, then we generate a complete text guided by the skill labels.	3
At the same time, to exploit the prior knowledge about the skills, we further construct a skill knowledge graph to capture the global prior knowledge of skills and refine the generated results.	3
The proposed approach is evaluated on real-world job posting data.	4
Experimental results clearly demonstrate the effectiveness of the proposed method.	5
----------
The International Classification of Diseases (ICD) provides a standardized way for classifying diseases, which endows each disease with a unique code.	1
ICD coding aims to assign proper ICD codes to a medical record.	1
Since manual coding is very laborious and prone to errors, many methods have been proposed for the automatic ICD coding task.	1
However, most of existing methods independently predict each code, ignoring two important characteristics: Code Hierarchy and Code Co-occurrence.	1
In this paper, we propose a Hyperbolic and Co-graph Representation method (HyperCore) to address the above problem.	2
Specifically, we propose a hyperbolic representation method to leverage the code hierarchy.	2
Moreover, we propose a graph convolutional network to utilize the code co-occurrence.	2
Experimental results on two widely used datasets demonstrate that our proposed model outperforms previous state-of-the-art methods.	4
----------
Although deep neural networks are effective at extracting high-level features, classification methods usually encode an input into a vector representation via simple feature aggregation operations (e.g. pooling).	1
Such operations limit the performance.	1
For instance, a multi-label document may contain several concepts.	1
In this case, one vector can not sufficiently capture its salient and discriminative content.	1
Thus, we propose Hyperbolic Capsule Networks (HyperCaps) for Multi-Label Classification (MLC), which have two merits.	2
First, hyperbolic capsules are designed to capture fine-grained document information for each label, which has the ability to characterize complicated structures among labels and documents.	4
Second, Hyperbolic Dynamic Routing (HDR) is introduced to aggregate hyperbolic capsules in a label-aware manner, so that the label-level discriminative information can be preserved along the depth of neural networks.	4
To efficiently handle large-scale MLC datasets, we additionally present a new routing method to adaptively adjust the capsule number during routing.	4
Extensive experiments are conducted on four benchmark datasets.	4
Compared with the state-of-the-art methods, HyperCaps significantly improves the performance of MLC especially on tail labels.	4
----------
They typically contain user descriptions of the problem, the setup, and steps for attempted resolution.	1
Often they also contain various non-natural language text elements like outputs of commands, snippets of code, error messages or stack traces.	1
These elements contain potentially crucial information for problem resolution.	1
However, they cannot be correctly parsed by tools designed for natural language.	1
In this paper, we address the problem of segmentation for technical support questions.	2
We formulate the problem as a sequence labelling task, and study the performance of state of the art approaches.	3
We compare this against an intuitive contextual sentence-level classification baseline, and a state of the art supervised text-segmentation approach.	3
We also introduce a novel component of combining contextual embeddings from multiple language models pre-trained on different data sources, which achieves a marked improvement over using embeddings from a single pre-trained language model.	3+4
Finally, we also demonstrate the usefulness of such segmentation with improvements on the downstream task of answer retrieval.	4
----------
The automatic text-based diagnosis remains a challenging task for clinical use because it requires appropriate balance between accuracy and interpretability.	1
In this paper, we attempt to propose a solution by introducing a novel framework that stacks Bayesian Network Ensembles on top of Entity-Aware Convolutional Neural Networks (CNN) towards building an accurate yet interpretable diagnosis system.	2+3
The proposed framework takes advantage of the high accuracy and generality of deep neural networks as well as the interpretability of Bayesian Networks, which is critical for AI-empowered healthcare.	4
The evaluation conducted on the real Electronic Medical Record (EMR) documents from hospitals and annotated by professional doctors proves that, the proposed framework outperforms the previous automatic diagnosis methods in accuracy performance and the diagnosis explanation of the framework is reasonable.	4+5
----------
In recent years, a new interesting task, called emotion-cause pair extraction (ECPE), has emerged in the area of text emotion analysis.	1
It aims at extracting the potential pairs of emotions and their corresponding causes in a document.	1
To solve this task, the existing research employed a two-step framework, which first extracts individual emotion set and cause set, and then pair the corresponding emotions and causes.	1
However, such a pipeline of two steps contains some inherent flaws: 1) the modeling does not aim at extracting the final emotion-cause pair directly; 2) the errors from the first step will affect the performance of the second step.	1
To address these shortcomings, in this paper we propose a new end-to-end approach, called ECPE-Two-Dimensional (ECPE-2D), to represent the emotion-cause pairs by a 2D representation scheme.	2
A 2D transformer module and two variants, window-constrained and cross-road 2D transformers, are further proposed to model the interactions of different emotion-cause pairs.	3
The 2D representation, interaction, and prediction are integrated into a joint framework.	3
In addition to the advantages of joint modeling, the experimental results on the benchmark emotion cause corpus show that our approach improves the F1 score of the state-of-the-art from 61.28% to 68.89%.	4
----------
Emotion-cause pair extraction aims to extract all emotion clauses coupled with their cause clauses from a given document.	1
Previous work employs two-step approaches, in which the first step extracts emotion clauses and cause clauses separately, and the second step trains a classifier to filter out negative pairs.	1
However, such pipeline-style system for emotion-cause pair extraction is suboptimal because it suffers from error propagation and the two steps may not adapt to each other well.	1
In this paper, we tackle emotion-cause pair extraction from a ranking perspective, i.e., ranking clause pair candidates in a document, and propose a one-step neural approach which emphasizes inter-clause modeling to perform end-to-end extraction.	2+3
It models the interrelations between the clauses in a document to learn clause representations with graph attention, and enhances clause pair representations with kernel-based relative position embedding for effective ranking.	3
Experimental results show that our approach significantly outperforms the current two-step systems, especially in the condition of extracting multiple pairs in one document.	4
----------
We present a simple but effective method for aspect identification in sentiment analysis.	1+2
Our unsupervised method only requires word embeddings and a POS tagger, and is therefore straightforward to apply to new domains and languages.	3
We introduce Contrastive Attention (CAt), a novel single-head attention mechanism based on an RBF kernel, which gives a considerable boost in performance and makes the model interpretable.	3
Previous work relied on syntactic features and complex neural models.	3
We show that given the simplicity of current benchmark datasets for aspect extraction, such complex models are not needed.	4
The code to reproduce the experiments reported in this paper is available at https://github.com/clips/cat.	6
----------
Stance detection is an important task, which aims to classify the attitude of an opinionated text towards a given target.	1
Remarkable success has been achieved when sufficient labeled training data is available.	1
However, annotating sufficient data is labor-intensive, which establishes significant barriers for generalizing the stance classifier to the data with new targets.	1
In this paper, we proposed a Semantic-Emotion Knowledge Transferring (SEKT) model for cross-target stance detection, which uses the external knowledge (semantic and emotion lexicons) as a bridge to enable knowledge transfer across different targets.	1+2
Specifically, a semantic-emotion heterogeneous graph is constructed from external semantic and emotion lexicons, which is then fed into a graph convolutional network to learn multi-hop semantic connections between words and emotion tags.	3
Then, the learned semantic-emotion graph representation, which serves as prior knowledge bridging the gap between the source and target domains, is fully integrated into the bidirectional long short-term memory (BiLSTM) stance classifier by adding a novel knowledge-aware memory unit to the BiLSTM cell.	3
Extensive experiments on a large real-world dataset demonstrate the superiority of SEKT against the state-of-the-art baseline methods.	4
----------
Cross-domain sentiment analysis has received significant attention in recent years, prompted by the need to combat the domain gap between different applications that make use of sentiment analysis.	1
In this paper, we take a novel perspective on this task by exploring the role of external commonsense knowledge.	2
We introduce a new framework, KinGDOM, which utilizes the ConceptNet knowledge graph to enrich the semantics of a document by providing both domain-specific and domain-general background concepts.	3
These concepts are learned by training a graph convolutional autoencoder that leverages inter-domain concepts in a domain-invariant manner.	3
Conditioning a popular domain-adversarial baseline method with these learned concepts helps improve its performance over state-of-the-art approaches, demonstrating the efficacy of our proposed framework.	5
----------
The aspect-based sentiment analysis (ABSA) consists of two conceptual tasks, namely an aspect extraction and an aspect sentiment classification.	1
Rather than considering the tasks separately, we build an end-to-end ABSA solution.	1+2
Previous works in ABSA tasks did not fully leverage the importance of syntactical information.	1
Hence, the aspect extraction model often failed to detect the boundaries of multi-word aspect terms.	1
On the other hand, the aspect sentiment classifier was unable to account for the syntactical correlation between aspect terms and the context words.	1
This paper explores the grammatical aspect of the sentence and employs the self-attention mechanism for syntactical learning.	1+2
We combine part-of-speech embeddings, dependency-based embeddings and contextualized embeddings (e.g. BERT, RoBERTa) to enhance the performance of the aspect extractor.	3
We also propose the syntactic relative distance to de-emphasize the adverse effects of unrelated words, having weak syntactic connection with the aspect terms.	3
This increases the accuracy of the aspect sentiment classifier.	4
Our solutions outperform the state-of-the-art models on SemEval-2014 dataset in both two subtasks.	4
----------
Aspect-based sentiment analysis aims to determine the sentiment polarity towards a specific aspect in online reviews.	1
Most recent efforts adopt attention-based neural network models to implicitly connect aspects with opinion words.	1
However, due to the complexity of language and the existence of multiple aspects in a single sentence, these models often confuse the connections.	1
In this paper, we address this problem by means of effective encoding of syntax information.	2+3
Firstly, we define a unified aspect-oriented dependency tree structure rooted at a target aspect by reshaping and pruning an ordinary dependency parse tree.	3
Then, we propose a relational graph attention network (R-GAT) to encode the new tree structure for sentiment prediction.	3
Extensive experiments are conducted on the SemEval 2014 and Twitter datasets, and the experimental results confirm that the connections between aspects and opinion words can be better established with our approach, and the performance of the graph attention network (GAT) is significantly improved as a consequence.	3+4
----------
Aspect terms extraction and opinion terms extraction are two key problems of fine-grained Aspect Based Sentiment Analysis (ABSA).	1
The aspect-opinion pairs can provide a global profile about a product or service for consumers and opinion mining systems.	1
However, traditional methods can not directly output aspect-opinion pairs without given aspect terms or opinion terms.	1
Although some recent co-extraction methods have been proposed to extract both terms jointly, they fail to extract them as pairs.	1
To this end, this paper proposes an end-to-end method to solve the task of Pair-wise Aspect and Opinion Terms Extraction (PAOTE).	1+2
Furthermore, this paper treats the problem from a perspective of joint term and relation extraction rather than under the sequence tagging formulation performed in most prior works.	3
We propose a multi-task learning framework based on shared spans, where the terms are extracted under the supervision of span boundaries.	3
Meanwhile, the pair-wise relations are jointly identified using the span representations.	4
Extensive experiments show that our model consistently outperforms state-of-the-art methods.	4
----------
Opinion role labeling (ORL) is a fine-grained opinion analysis task and aims to answer “who expressed what kind of sentiment towards what?”.	1
Due to the scarcity of labeled data, ORL remains challenging for data-driven methods.	1
In this work, we try to enhance neural ORL models with syntactic knowledge by comparing and integrating different representations.	2+3
We also propose dependency graph convolutional networks (DEPGCN) to encode parser information at different processing levels.	3
In order to compensate for parser inaccuracy and reduce error propagation, we introduce multi-task learning (MTL) to train the parser and the ORL model simultaneously.	3
We verify our methods on the benchmark MPQA corpus.	3
The experimental results show that syntactic information is highly valuable for ORL, and our final MTL model effectively boosts the F1 score by 9.29 over the syntax-agnostic baseline.	4
In addition, we find that the contributions from syntactic knowledge do not fully overlap with contextualized word representations (BERT).	4
Our best model achieves 4.34 higher F1 score than the current state-ofthe-art.	4
----------
State-of-the-art argument mining studies have advanced the techniques for predicting argument structures.	1
However, the technology for capturing non-tree-structured arguments is still in its infancy.	1
In this paper, we focus on non-tree argument mining with a neural model.	2
We jointly predict proposition types and edges between propositions.	3
Our proposed model incorporates (i) task-specific parameterization (TSP) that effectively encodes a sequence of propositions and (ii) a proposition-level biaffine attention (PLBA) that can predict a non-tree argument consisting of edges.	3
Experimental results show that both TSP and PLBA boost edge prediction performance compared to baselines.	4
----------
We propose a novel linearization of a constituent tree, together with a new locally normalized model.	2
For each split point in a sentence, our model computes the normalizer on all spans ending with that split point, and then predicts a tree span from them.	3
Compared with global models, our model is fast and parallelizable.	4
Different from previous local models, our linearization method is tied on the spans directly and considers more local features when performing span prediction, which is more interpretable and effective.	4
Experiments on PTB (95.8 F1) and CTB (92.4 F1) show that our model significantly outperforms existing local models and efficiently achieves competitive results with global models.	4
----------
We propose a novel constituency parsing model that casts the parsing problem into a series of pointing tasks.	2+3
Specifically, our model estimates the likelihood of a span being a legitimate tree constituent via the pointing score corresponding to the boundary words of the span.	3
Our parsing model supports efficient top-down decoding and our learning objective is able to enforce structural consistency without resorting to the expensive CKY inference.	3
The experiments on the standard English Penn Treebank parsing task show that our method achieves 92.78 F1 without using pre-trained models, which is higher than all the existing methods with similar time complexity.	4
Using pre-trained BERT, our model achieves 95.48 F1, which is competitive with the state-of-the-art while being faster. Our approach also establishes new state-of-the-art in Basque and Swedish in the SPMRL shared tasks on multilingual constituency parsing.	4+5
----------
In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation.	1
As the most popular graph-based dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss.	1
This paper for the first time presents a second-order TreeCRF extension to the biaffine parser.	2
For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF.	3
To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation.	3
Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data.	4+5
We release our code at https://github.com/yzhangcs/crfpar.	6
----------
Sequence-based neural networks show significant sensitivity to syntactic structure, but they still perform less well on syntactic tasks than tree-based networks.	1
Such tree-based networks can be provided with a constituency parse, a dependency parse, or both.	1
We evaluate which of these two representational schemes more effectively introduces biases for syntactic structure that increase performance on the subject-verb agreement prediction task.	2
We find that a constituency-based network generalizes more robustly than a dependency-based one, and that combining the two types of structure does not yield further improvement.	4
Finally, we show that the syntactic robustness of sequential models can be substantially improved by fine-tuning on a small amount of constructed data, suggesting that data augmentation is a viable alternative to explicit constituency structure for imparting the syntactic biases that sequential models are lacking.	5
----------
Multilingual sequence labeling is a task of predicting label sequences using a single unified model for multiple languages.	1
Compared with relying on multiple monolingual models, using a multilingual model has the benefit of a smaller model size, easier in online serving, and generalizability to low-resource languages.	1
However, current multilingual models still underperform individual monolingual models significantly due to model capacity limitations.	1
In this paper, we propose to reduce the gap between monolingual models and the unified multilingual model by distilling the structural knowledge of several monolingual models (teachers) to the unified multilingual model (student).	2+3
We propose two novel KD methods based on structure-level information: (1) approximately minimizes the distance between the student’s and the teachers’ structure-level probability distributions, (2) aggregates the structure-level knowledge to local distributions and minimizes the distance between two local probability distributions.	3
Our experiments on 4 multilingual tasks with 25 datasets show that our approaches outperform several strong baselines and have stronger zero-shot generalizability than both the baseline model and teacher models.	4+5
----------
Trending topics in social media content evolve over time, and it is therefore crucial to understand social media users and their interpersonal communications in a dynamic manner.	1
Here we study dynamic online conversation recommendation, to help users engage in conversations that satisfy their evolving interests.	2
While most prior work assumes static user interests, our model is able to capture the temporal aspects of user interests, and further handle future conversations that are unseen during training time.	4
Concretely, we propose a neural architecture to exploit changes of user interactions and interests over time, to predict which discussions they are likely to enter.	3+4
We conduct experiments on large-scale collections of Reddit conversations, and results on three subreddits show that our model significantly outperforms state-of-the-art models that make a static assumption of user interests.	4
We further evaluate on handling “cold start”, and observe consistently better performance by our model when considering various degrees of sparsity of user’s chatting history and conversation contexts.	3+4
Lastly, analyses on our model outputs indicate user interest change, explaining the advantage and efficacy of our approach.	4
----------
In this paper, we study Multimodal Named Entity Recognition (MNER) for social media posts.	2
Existing approaches for MNER mainly suffer from two drawbacks: (1) despite generating word-aware visual representations, their word representations are insensitive to the visual context; (2) most of them ignore the bias brought by the visual context.	1
To tackle the first issue, we propose a multimodal interaction module to obtain both image-aware word representations and word-aware visual representations.	2
To alleviate the visual bias, we further propose to leverage purely text-based entity span detection as an auxiliary module, and design a Unified Multimodal Transformer to guide the final predictions with the entity span predictions.	3
Experiments show that our unified approach achieves the new state-of-the-art performance on two benchmark datasets.	4
----------
Previous works that integrated news articles to better process stock prices used a variety of neural networks to predict price movements.	1
The textual and price information were both encoded in the neural network, and it is therefore difficult to apply this approach in situations other than the original framework of the notoriously hard problem of price prediction.	1
In contrast, this paper presents a method to encode the influence of news articles through a vector representation of stocks called a stock embedding.	2+3
The stock embedding is acquired with a deep learning framework using both news articles and price history.	3
Because the embedding takes the operational form of a vector, it is applicable to other financial problems besides price prediction.	3
As one example application, we show the results of portfolio optimization using Reuters & Bloomberg headlines, producing a capital gain 2.8 times larger than that obtained with a baseline method using only stock price data.	4
This suggests that the proposed stock embedding can leverage textual financial semantics to solve financial prediction problems.	5
----------
Predicting the political bias and the factuality of reporting of entire news outlets are critical elements of media profiling, which is an understudied but an increasingly important research direction.	1
The present level of proliferation of fake, biased, and propagandistic content online has made it impossible to fact-check every single suspicious claim, either manually or automatically.	1
Thus, it has been proposed to profile entire news outlets and to look for those that are likely to publish fake or biased content.	1
This makes it possible to detect likely “fake news” the moment they are published, by simply checking the reliability of their source.	1
From a practical perspective, political bias and factuality of reporting have a linguistic aspect but also a social context.	1
Here, we study the impact of both, namely (i) what was written (i.e., what was published by the target medium, and how it describes itself in Twitter) vs. (ii) who reads it (i.e., analyzing the target medium’s audience on social media).	2
We further study (iii) what was written about the target medium (in Wikipedia).	2
The evaluation results show that what was written matters most, and we further show that putting all information sources together yields huge improvements over the current state-of-the-art.	4
----------
We explore the utilities of explicit negative examples in training neural language models.	1
Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks.	1
Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement.	1
In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity.	2
The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word.	3
We then provide a detailed analysis of the trained models.	3
One of our findings is the difficulty of object-relative clauses for RNNs.	4
We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause.	4
Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses.	4
Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.	5
----------
We conduct a thorough study to diagnose the behaviors of pre-trained language encoders (ELMo, BERT, and RoBERTa) when confronted with natural grammatical errors.	2
Specifically, we collect real grammatical errors from non-native speakers and conduct adversarial attacks to simulate these errors on clean text data.	3
We use this approach to facilitate debugging models on downstream applications.	4
Results confirm that the performance of all tested models is affected but the degree of impact varies.	4
To interpret model behaviors, we further design a linguistic acceptability task to reveal their abilities in identifying ungrammatical sentences and the position of errors.	4
We find that fixed contextual encoders with a simple classifier trained on the prediction of sentence correctness are able to locate error positions.	4
We also design a cloze test for BERT and discover that BERT captures the interaction between errors and specific tokens in context.	4
Our results shed light on understanding the robustness and behaviors of language encoders against grammatical errors.	5
----------
Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks.	1
The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture.	1
However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model.	1
For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA).	2+3
Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.	4+5
----------
Knowledge graph embedding methods often suffer from a limitation of memorizing valid triples to predict new ones for triple classification and search personalization problems.	1
To this end, we introduce a novel embedding model, named R-MeN, that explores a relational memory network to encode potential dependencies in relationship triples.	2+3
R-MeN considers each triple as a sequence of 3 input vectors that recurrently interact with a memory using a transformer self-attention mechanism.	3
Thus R-MeN encodes new information from interactions between the memory and each input vector to return a corresponding vector.	3
Consequently, R-MeN feeds these 3 returned vectors to a convolutional neural network-based decoder to produce a scalar score for the triple.	3
Experimental results show that our proposed R-MeN obtains state-of-the-art results on SEARCH17 for the search personalization task, and on WN11 and FB13 for the triple classification task.	4
----------
It has been a common approach to pre-train a language model on a large corpus and fine-tune it on task-specific data.	1
In practice, we observe that fine-tuning a pre-trained model on a small dataset may lead to over- and/or under-estimate problem.	1
In this paper, we propose MC-Tailor, a novel method to alleviate the above issue in text generation tasks by truncating and transferring the probability mass from over-estimated regions to under-estimated ones.	2
Experiments on a variety of text generation datasets show that MC-Tailor consistently and significantly outperforms the fine-tuning approach.	4
----------
Most Chinese pre-trained models take character as the basic unit and learn representation according to character’s external contexts, ignoring the semantics expressed in the word, which is the smallest meaningful utterance in Chinese.	1
Hence, we propose a novel word-aligned attention to exploit explicit word information, which is complementary to various character-based Chinese pre-trained language models.	1+2
Specifically, we devise a pooling mechanism to align the character-level attention to the word level and propose to alleviate the potential issue of segmentation error propagation by multi-source information fusion.	3
As a result, word and character information are explicitly integrated at the fine-tuning procedure.	3
Experimental results on five Chinese NLP benchmark tasks demonstrate that our method achieves significant improvements against BERT, ERNIE and BERT-wwm.	4
----------
Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling.	1
By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold.	1
We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them.	1+2
To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching.	3
We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy.	3
Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space.	4
We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.	4
----------
State-of-the-art NLP models can often be fooled by human-unaware transformations such as synonymous word substitution.	1
For security reasons, it is of critical importance to develop models with certified robustness that can provably guarantee that the prediction is can not be altered by any possible synonymous word substitution.	1
In this work, we propose a certified robust method based on a new randomized smoothing technique, which constructs a stochastic ensemble by applying random word substitutions on the input sentences, and leverage the statistical properties of the ensemble to provably certify the robustness.	2+3
Our method is simple and structure-free in that it only requires the black-box queries of the model outputs, and hence can be applied to any pre-trained models (such as BERT) and any types of models (world-level or subword-level).	4
Our method significantly outperforms recent state-of-the-art methods for certified robustness on both IMDB and Amazon text classification tasks.	4
To the best of our knowledge, we are the first work to achieve certified robustness on large systems such as BERT with practically meaningful certified accuracy.	5
----------
Unsupervised bilingual lexicon induction is the task of inducing word translations from monolingual corpora of two languages.	1
Recent methods are mostly based on unsupervised cross-lingual word embeddings, the key to which is to find initial solutions of word translations, followed by the learning and refinement of mappings between the embedding spaces of two languages.	1
However, previous methods find initial solutions just based on word-level information, which may be (1) limited and inaccurate, and (2) prone to contain some noise introduced by the insufficiently pre-trained embeddings of some words.	1
To deal with those issues, in this paper, we propose a novel graph-based paradigm to induce bilingual lexicons in a coarse-to-fine way.	2
We first build a graph for each language with its vertices representing different words.	3
Then we extract word cliques from the graphs and map the cliques of two languages.	3
Based on that, we induce the initial word translation solution with the central words of the aligned cliques.	3
This coarse-to-fine approach not only leverages clique-level information, which is richer and more accurate, but also effectively reduces the bad effect of the noise in the pre-trained embeddings.	4
Finally, we take the initial solution as the seed to learn cross-lingual embeddings, from which we induce bilingual lexicons.	3
Experiments show that our approach improves the performance of bilingual lexicon induction compared with previous methods.	4
----------
Neural machine translation systems tend to fail on less decent inputs despite its significant efficacy, which may significantly harm the credibility of these systems—fathoming how and when neural-based systems fail in such cases is critical for industrial maintenance.	1
Instead of collecting and analyzing bad cases using limited handcrafted error features, here we investigate this issue by generating adversarial examples via a new paradigm based on reinforcement learning.	2+3
Our paradigm could expose pitfalls for a given performance metric, e.g., BLEU, and could target any given neural machine translation architecture.	3
We conduct experiments of adversarial attacks on two mainstream neural machine translation architectures, RNN-search, and Transformer.	3
The results show that our method efficiently produces stable attacks with meaning-preserving adversarial examples.	4
We also present a qualitative and quantitative analysis for the preference pattern of the attack, demonstrating its capability of pitfall exposure.	5
----------
The commonly used framework for unsupervised machine translation builds initial translation models of both translation directions, and then performs iterative back-translation to jointly boost their translation performance.	1
The initialization stage is very important since bad initialization may wrongly squeeze the search space, and too much noise introduced in this stage may hurt the final performance.	1
In this paper, we propose a novel retrieval and rewriting based method to better initialize unsupervised translation models.	1+2
We first retrieve semantically comparable sentences from monolingual corpora of two languages and then rewrite the target side to minimize the semantic gap between the source and retrieved targets with a designed rewriting model.	3
The rewritten sentence pairs are used to initialize SMT models which are used to generate pseudo data for two NMT models, followed by the iterative back-translation.	3
Experiments show that our method can build better initial unsupervised translation models and improve the final translation performance by over 4 BLEU scores.	4
Our code is released at https://github.com/Imagist-Shuo/RRforUNMT.git.	6
----------
Open-domain dialogue generation has gained increasing attention in Natural Language Processing.	1
Its evaluation requires a holistic means.	1
Human ratings are deemed as the gold standard.	1
As human evaluation is inefficient and costly, an automated substitute is highly desirable.	1
In this paper, we propose holistic evaluation metrics that capture different aspects of open-domain dialogues.	2
Our metrics consist of (1) GPT-2 based context coherence between sentences in a dialogue, (2) GPT-2 based fluency in phrasing, (3) n-gram based diversity in responses to augmented queries, and (4) textual-entailment-inference based logical self-consistency.	3
The empirical validity of our metrics is demonstrated by strong correlations with human judgments.	4
We open source the code and relevant materials.	6
----------
The hypernymy detection task has been addressed under various frameworks.	1
Previously, the design of unsupervised hypernymy scores has been extensively studied.	1
In contrast, supervised classifiers, especially distributional models, leverage the global contexts of terms to make predictions, but are more likely to suffer from “lexical memorization”.	1
In this work, we revisit supervised distributional models for hypernymy detection.	1+2
Rather than taking embeddings of two terms as classification inputs, we introduce a representation learning framework named Bidirectional Residual Relation Embeddings (BiRRE).	3
In this model, a term pair is represented by a BiRRE vector as features for hypernymy classification, which models the possibility of a term being mapped to another in the embedding space by hypernymy relations.	3
A Latent Projection Model with Negative Regularization (LPMNR) is proposed to simulate how hypernyms and hyponyms are generated by neural language models, and to generate BiRRE vectors based on bidirectional residuals of projections.	3
Experiments verify BiRRE outperforms strong baselines over various evaluation frameworks.	4
----------
Biomedical named entities often play important roles in many biomedical text mining tools.	1
However, due to the incompleteness of provided synonyms and numerous variations in their surface forms, normalization of biomedical entities is very challenging.	1
In this paper, we focus on learning representations of biomedical entities solely based on the synonyms of entities.	2
To learn from the incomplete synonyms, we use a model-based candidate selection and maximize the marginal likelihood of the synonyms present in top candidates.	3
Our model-based candidates are iteratively updated to contain more difficult negative samples as our model evolves.	3
In this way, we avoid the explicit pre-selection of negative samples from more than 400K candidates.	3
On four biomedical entity normalization datasets having three different entity types (disease, chemical, adverse reaction), our model BioSyn consistently outperforms previous state-of-the-art models almost reaching the upper bound on each dataset.	4
----------
Hypernymy detection, a.k.a, lexical entailment, is a fundamental sub-task of many natural language understanding tasks.	1
Previous explorations mostly focus on monolingual hypernymy detection on high-resource languages, e.g., English, but few investigate the low-resource scenarios.	1
This paper addresses the problem of low-resource hypernymy detection by combining high-resource languages.	2
We extensively compare three joint training paradigms and for the first time propose applying meta learning to relieve the low-resource issue.	3
Experiments demonstrate the superiority of our method among the three settings, which substantially improves the performance of extremely low-resource languages by preventing over-fitting on small datasets.	5
----------
This paper presents an investigation on the distribution of word vectors belonging to a certain word class in a pre-trained word vector space.	2
To this end, we made several assumptions about the distribution, modeled the distribution accordingly, and validated each assumption by comparing the goodness of each model.	3
Specifically, we considered two types of word classes – the semantic class of direct objects of a verb and the semantic class in a thesaurus – and tried to build models that properly estimate how likely it is that a word in the vector space is a member of a given word class.	3
Our results on selectional preference and WordNet datasets show that the centroid-based model will fail to achieve good enough performance, the geometry of the distribution and the existence of subgroups will have limited impact, and also the negative instances need to be considered for adequate modeling of the distribution.	4
We further investigated the relationship between the scores calculated by each model and the degree of membership and found that discriminative learning-based models are best in finding the boundaries of a class, while models based on the offset between positive and negative instances perform best in determining the degree of membership.	3+4
----------
In the literature, existing studies always consider Aspect Sentiment Classification (ASC) as an independent sentence-level classification problem aspect by aspect, which largely ignore the document-level sentiment preference information, though obviously such information is crucial for alleviating the information deficiency problem in ASC.	1
In this paper, we explore two kinds of sentiment preference information inside a document, i.e., contextual sentiment consistency w.r.t. the same aspect (namely intra-aspect sentiment consistency) and contextual sentiment tendency w.r.t. all the related aspects (namely inter-aspect sentiment tendency).	2
On the basis, we propose a Cooperative Graph Attention Networks (CoGAN) approach for cooperatively learning the aspect-related sentence representation.	3
Specifically, two graph attention networks are leveraged to model above two kinds of document-level sentiment preference information respectively, followed by an interactive mechanism to integrate the two-fold preference.	3
Detailed evaluation demonstrates the great advantage of the proposed approach to ASC over the state-of-the-art baselines.	4
This justifies the importance of the document-level sentiment preference information to ASC and the effectiveness of our approach capturing such information.	5
----------
The current aspect extraction methods suffer from boundary errors.	1
In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth.	1
However, they hurt the performance severely.	1
In this paper, we propose to utilize a pointer network for repositioning the boundaries.	1+2
Recycling mechanism is used, which enables the training data to be collected without manual intervention.	4
We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant.	3
Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods.	4
----------
Aspect-based sentiment analysis (ABSA) involves three subtasks, i.e., aspect term extraction, opinion term extraction, and aspect-level sentiment classification.	1
Most existing studies focused on one of these subtasks only.	1
Several recent researches made successful attempts to solve the complete ABSA problem with a unified framework.	1
However, the interactive relations among three subtasks are still under-exploited.	1
We argue that such relations encode collaborative signals between different subtasks.	2
For example, when the opinion term is “delicious”, the aspect term must be “food” rather than “place”.	3
In order to fully exploit these relations, we propose a Relation-Aware Collaborative Learning (RACL) framework which allows the subtasks to work coordinately via the multi-task learning and relation propagation mechanisms in a stacked multi-layer network.	3
Extensive experiments on three real-world datasets demonstrate that RACL significantly outperforms the state-of-the-art methods for the complete ABSA task	4
----------
We propose SentiBERT, a variant of BERT that effectively captures compositional sentiment semantics.	1+2
The model incorporates contextualized representation with binary constituency parse tree to capture semantic composition.	3
Comprehensive experiments demonstrate that SentiBERT achieves competitive performance on phrase-level sentiment classification.	4
We further demonstrate that the sentiment composition learned from the phrase-level annotations on SST can be transferred to other sentiment analysis tasks as well as related tasks, such as emotion classification tasks.	5
Moreover, we conduct ablation studies and design visualization methods to understand SentiBERT.	5
We show that SentiBERT is better than baseline approaches in capturing negation and the contrastive relation and model the compositional sentiment semantics.	5
----------
Emotion-cause pair extraction aims to extract all potential pairs of emotions and corresponding causes from unannotated emotion text.	1
Most existing methods are pipelined framework, which identifies emotions and extracts causes separately, leading to a drawback of error propagation.	1
Towards this issue, we propose a transition-based model to transform the task into a procedure of parsing-like directed graph construction.	1+2
The proposed model incrementally generates the directed graph with labeled edges based on a sequence of actions, from which we can recognize emotions with the corresponding causes simultaneously, thereby optimizing separate subtasks jointly and maximizing mutual benefits of tasks interdependently.	3+4
Experimental results show that our approach achieves the best performance, outperforming the state-of-the-art methods by 6.71% (p<0.01) in F1 measure.	4
----------
End-to-end speech translation poses a heavy burden on the encoder because it has to transcribe, understand, and learn cross-lingual semantics simultaneously.	1
To obtain a powerful encoder, traditional methods pre-train it on ASR data to capture speech features.	1
However, we argue that pre-training the encoder only through simple speech recognition is not enough, and high-level linguistic knowledge should be considered.	1+2
Inspired by this, we propose a curriculum pre-training method that includes an elementary course for transcription learning and two advanced courses for understanding the utterance and mapping words in two languages.	3
The difficulty of these courses is gradually increasing.	4
Experiments show that our curriculum pre-training method leads to significant improvements on En-De and En-Fr speech translation benchmarks.	4
----------
In this work, we present a detailed analysis of how accent information is reflected in the internal representation of speech in an end-to-end automatic speech recognition (ASR) system.	2
We use a state-of-the-art end-to-end ASR system, comprising convolutional and recurrent layers, that is trained on a large amount of US-accented English speech and evaluate the model on speech samples from seven different English accents.	3
We examine the effects of accent on the internal representation using three main probing techniques: a) Gradient-based explanation methods, b) Information-theoretic measures, and c) Outputs of accent and phone classifiers.	3
We find different accents exhibiting similar trends irrespective of the probing technique used.	4
We also find that most accent information is encoded within the first recurrent layer, which is suggestive of how one could adapt such an end-to-end model to learn representations that are invariant to accents.	5
----------
Self-attentive neural syntactic parsers using contextualized word embeddings (e.g. ELMo or BERT) currently produce state-of-the-art results in joint parsing and disfluency detection in speech transcripts.	1
Since the contextualized word embeddings are pre-trained on a large amount of unlabeled data, using additional unlabeled data to train a neural model might seem redundant.	1
However, we show that self-training — a semi-supervised technique for incorporating unlabeled data — sets a new state-of-the-art for the self-attentive parser on disfluency detection, demonstrating that self-training provides benefits orthogonal to the pre-trained contextualized word representations.	2+3
We also show that ensembling self-trained parsers provides further gains for disfluency detection.	4
----------
Pre-trained language models have achieved huge improvement on many NLP tasks.	1
However, these methods are usually designed for written text, so they do not consider the properties of spoken language.	1
Therefore, this paper aims at generalizing the idea of language model pre-training to lattices generated by recognition systems.	2
We propose a framework that trains neural lattice language models to provide contextualized representations for spoken language understanding tasks.	3
The proposed two-stage pre-training approach reduces the demands of speech data and has better efficiency.	3+4
Experiments on intent detection and dialogue act recognition datasets demonstrate that our proposed method consistently outperforms strong baselines when evaluated on spoken inputs.	4
The code is available at https://github.com/MiuLab/Lattice-ELMo.	6
----------
An increasing number of people in the world today speak a mixed-language as a result of being multilingual.	1
However, building a speech recognition system for code-switching remains difficult due to the availability of limited resources and the expense and significant effort required to collect mixed-language data.	1
We therefore propose a new learning method, meta-transfer learning, to transfer learn on a code-switched speech recognition system in a low-resource setting by judiciously extracting information from high-resource monolingual datasets.	2+3
Our model learns to recognize individual languages, and transfer them so as to better recognize mixed-language speech by conditioning the optimization on the code-switching data.	3
Based on experimental results, our model outperforms existing baselines on speech recognition and language modeling tasks, and is faster to converge.	4
----------
Sarcasm is a sophisticated linguistic phenomenon to express the opposite of what one really means.	1
With the rapid growth of social media, multimodal sarcastic tweets are widely posted on various social platforms.	1
In multimodal context, sarcasm is no longer a pure linguistic phenomenon, and due to the nature of social media short text, the opposite is more often manifested via cross-modality expressions.	1
Thus traditional text-based methods are insufficient to detect multimodal sarcasm.	1
To reason with multimodal sarcastic tweets, in this paper, we propose a novel method for modeling cross-modality contrast in the associated context.	2+3
Our method models both cross-modality contrast and semantic association by constructing the Decomposition and Relation Network (namely D&R Net).	3
The decomposition network represents the commonality and discrepancy between image and text, and the relation network models the semantic association in cross-modality context.	3
Experimental results on a public dataset demonstrate the effectiveness of our model in multimodal sarcasm detection.	4
----------
In this work, we develop SimulSpeech, an end-to-end simultaneous speech to text translation system which translates speech in source language to text in target language concurrently.	2
SimulSpeech consists of a speech encoder, a speech segmenter and a text decoder, where 1) the segmenter builds upon the encoder and leverages a connectionist temporal classification (CTC) loss to split the input streaming speech in real time, 2) the encoder-decoder attention adopts a wait-k strategy for simultaneous translation.	3
SimulSpeech is more challenging than previous cascaded systems (with simultaneous automatic speech recognition (ASR) and simultaneous neural machine translation (NMT)).	4
We introduce two novel knowledge distillation methods to ensure the performance: 1) Attention-level knowledge distillation transfers the knowledge from the multiplication of the attention matrices of simultaneous NMT and ASR models to help the training of the attention mechanism in SimulSpeech; 2) Data-level knowledge distillation transfers the knowledge from the full-sentence NMT model and also reduces the complexity of data distribution to help on the optimization of SimulSpeech.	3
Experiments on MuST-C English-Spanish and English-German spoken language translation datasets show that SimulSpeech achieves reasonable BLEU scores and lower delay compared to full-sentence end-to-end speech to text translation (without simultaneous translation), and better performance than the two-stage cascaded simultaneous translation model in terms of BLEU scores and translation delay.	4
----------
Spoken language understanding tasks usually rely on pipelines involving complex processing blocks such as voice activity detection, speaker diarization and Automatic speech recognition (ASR).	1
We propose a novel framework for predicting utterance level labels directly from speech features, thus removing the dependency on first generating transcripts, and transcription free behavioral coding.	2
Our classifier uses a pretrained Speech-2-Vector encoder as bottleneck to generate word-level representations from speech features.	2
This pretrained encoder learns to encode speech features for a word using an objective similar to Word2Vec.	3
Our proposed approach just uses speech features and word segmentation information for predicting spoken utterance-level target labels.	3
We show that our model achieves competitive results to other state-of-the-art approaches which use transcribed text for the task of predicting psychotherapy-relevant behavior codes.	3+4
----------
Opinion prediction on Twitter is challenging due to the transient nature of tweet content and neighbourhood context.	1
In this paper, we model users’ tweet posting behaviour as a temporal point process to jointly predict the posting time and the stance label of the next tweet given a user’s historical tweet sequence and tweets posted by their neighbours.	2+3
We design a topic-driven attention mechanism to capture the dynamic topic shifts in the neighbourhood context.	2+3
Experimental results show that the proposed model predicts both the posting time and the stance labels of future tweets more accurately compared to a number of competitive baselines.	4+5
----------
Trust is implicit in many online text conversations—striking up new friendships, or asking for tech support.	1
But trust can be betrayed through deception.	1
We study the language and dynamics of deception in the negotiation-based game Diplomacy, where seven players compete for world domination by forging and breaking alliances with each other.	2+3
Our study with players from the Diplomacy community gathers 17,289 messages annotated by the sender for their intended truthfulness and by the receiver for their perceived truthfulness.	3
Unlike existing datasets, this captures deception in long-lasting relationships, where the interlocutors strategically combine truth with lies to advance objectives.	2+3
A model that uses power dynamics and conversational contexts can predict when a lie occurs nearly as well as human players.	4
----------
Generative feature matching network (GFMN) is an approach for training state-of-the-art implicit generative models for images by performing moment matching on features from pre-trained neural networks.	1
In this paper, we present new GFMN formulations that are effective for sequential data.	2
Our experimental results show the effectiveness of the proposed method, SeqGFMN, for three distinct generation tasks in English: unconditional text generation, class-conditional text generation, and unsupervised text style transfer.	4+5
SeqGFMN is stable to train and outperforms various adversarial approaches for text generation and text style transfer.	5
----------
A number of researchers have recently questioned the necessity of increasingly complex neural network (NN) architectures.	1
In particular, several recent papers have shown that simpler, properly tuned models are at least competitive across several NLP tasks.	1
In this work, we show that this is also the case for text generation from structured and unstructured data.	2
We consider neural table-to-text generation and neural question generation (NQG) tasks for text generation from structured and unstructured data, respectively.	3
Table-to-text generation aims to generate a description based on a given table, and NQG is the task of generating a question from a given passage where the generated question can be answered by a certain sub-span of the passage using NN models.	4
Experimental results demonstrate that a basic attention-based seq2seq model trained with the exponential moving average technique achieves the state of the art in both tasks.	4
Code is available at https://github.com/h-shahidi/2birds-gen.	6
----------
This paper presents the Bayesian Hierarchical Words Representation (BHWR) learning algorithm.	2
BHWR facilitates Variational Bayes word representation learning combined with semantic taxonomy modeling via hierarchical priors.	4
By propagating relevant information between related words, BHWR utilizes the taxonomy to improve the quality of such representations.	4
Evaluation of several linguistic datasets demonstrates the advantages of BHWR over suitable alternatives that facilitate Bayesian modeling with or without semantic priors.	4+5
Finally, we further show that BHWR produces better representations for rare words.	4+5
----------
Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks.	1
Most of the existing approaches rely on a randomly initialized classifier on top of such networks.	1+2
We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task.	1+2
In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase.	2
We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets.	2+3
By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches.	2+3
Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.	2+3
----------
In recent years, knowledge graph embedding becomes a pretty hot research topic of artificial intelligence and plays increasingly vital roles in various downstream applications, such as recommendation and question answering.	1
However, existing methods for knowledge graph embedding can not make a proper trade-off between the model complexity and the model expressiveness, which makes them still far from satisfactory.	1
To mitigate this problem, we propose a lightweight modeling framework that can achieve highly competitive relational expressiveness without increasing the model complexity.	2
Our framework focuses on the design of scoring functions and highlights two critical characteristics: 1) facilitating sufficient feature interactions; 2) preserving both symmetry and antisymmetry properties of relations.	2
It is noteworthy that owing to the general and elegant design of scoring functions, our framework can incorporate many famous existing methods as special cases.	3
Moreover, extensive experiments on public benchmarks demonstrate the efficiency and effectiveness of our framework.	3+4
Source codes and data can be found at https://github.com/Wentao-Xu/SEEK.	6
----------
Sparse models require less memory for storage and enable a faster inference by reducing the necessary number of FLOPs.	1
This is relevant both for time-critical and on-device computations using neural networks.	1
The stabilized lottery ticket hypothesis states that networks can be pruned after none or few training iterations, using a mask computed based on the unpruned converged model.	1
On the transformer architecture and the WMT 2014 English-to-German and English-to-French tasks, we show that stabilized lottery ticket pruning performs similar to magnitude pruning for sparsity levels of up to 85%, and propose a new combination of pruning techniques that outperforms all other techniques for even higher levels of sparsity.	2+3
----------
Neural models have achieved great success on machine reading comprehension (MRC), many of which typically consist of two components: an evidence extractor and an answer predictor.	2
The former seeks the most relevant information from a reference text, while the latter is to locate or generate answers from the extracted evidence.	3
Despite the importance of evidence labels for training the evidence extractor, they are not cheaply accessible, particularly in many non-extractive MRC tasks such as YES/NO question answering and multi-choice MRC.	3
To address this problem, we present a Self-Training method (STM), which supervises the evidence extractor with auto-generated evidence labels in an iterative process.	2
At each iteration, a base MRC model is trained with golden answers and noisy evidence labels.	3
The trained model will predict pseudo evidence labels as extra supervision in the next iteration.	3
We evaluate STM on seven datasets over three MRC tasks.	3+4
Experimental results demonstrate the improvement on existing MRC models, and we also analyze how and why such a self-training method works in MRC.	3
----------
While the recent tree-based neural models have demonstrated promising results in generating solution expression for the math word problem (MWP), most of these models do not capture the relationships and order information among the quantities well.	1
This results in poor quantity representations and incorrect solution expressions.	1
In this paper, we propose Graph2Tree, a novel deep learning architecture that combines the merits of the graph-based encoder and tree-based decoder to generate better solution expressions.	2
Included in our Graph2Tree framework are two graphs, namely the Quantity Cell Graph and Quantity Comparison Graph, which are designed to address limitations of existing methods by effectively representing the relationships and order information among the quantities in MWPs.	3
We conduct extensive experiments on two available datasets.	3
Our experiment results show that Graph2Tree outperforms the state-of-the-art baselines on two benchmark datasets significantly.	2+3
We also discuss case studies and empirically examine Graph2Tree’s effectiveness in translating the MWP text into solution expressions.	3
----------
In Ordinal Classification tasks, items have to be assigned to classes that have a relative ordering, such as “positive”, “neutral”, “negative” in sentiment analysis.	1
Remarkably, the most popular evaluation metrics for ordinal classification tasks either ignore relevant information (for instance, precision/recall on each of the classes ignores their relative ordering) or assume additional information (for instance, Mean Average Error assumes absolute distances between classes).	2+3
In this paper we propose a new metric for Ordinal Classification, Closeness Evaluation Measure, that is rooted on Measurement Theory and Information Theory.	2
Our theoretical analysis and experimental results over both synthetic data and data from NLP shared tasks indicate that the proposed metric captures quality aspects from different traditional tasks simultaneously.	2+3
In addition, it generalizes some popular classification (nominal scale) and error minimization (interval scale) metrics, depending on the measurement scale in which it is instantiated.	3+4
----------
Distributed representations of words have been an indispensable component for natural language processing (NLP) tasks.	1
However, the large memory footprint of word embeddings makes it challenging to deploy NLP models to memory-constrained devices (e.g., self-driving cars, mobile devices).	1+2
In this paper, we propose a novel method to adaptively compress word embeddings.	2+3
We fundamentally follow a code-book approach that represents words as discrete codes such as (8, 5, 2, 4).	2+3
However, unlike prior works that assign the same length of codes to all words, we adaptively assign different lengths of codes to each word by learning downstream tasks.	3+4
The proposed method works in two steps.	3
First, each word directly learns to select its code length in an end-to-end manner by applying the Gumbel-softmax tricks.	3
After selecting the code length, each word learns discrete codes through a neural network with a binary constraint.	3
To showcase the general applicability of the proposed method, we evaluate the performance on four different downstream tasks.	2+3
Comprehensive evaluation results clearly show that our method is effective and makes the highly compressed word embeddings without hurting the task accuracy.	3
Moreover, we show that our model assigns word to each code-book by considering the significance of tasks.	2+3
----------
Document clustering requires a deep understanding of the complex structure of long-text; in particular, the intra-sentential (local) and inter-sentential features (global).	1
Existing representation learning models do not fully capture these features.	1+2
To address this, we present a novel graph-based representation for document clustering that builds a graph autoencoder (GAE) on a Keyword Correlation Graph.	2+3
The graph is constructed with topical keywords as nodes and multiple local and global features as edges.	4
A GAE is employed to aggregate the two sets of features by learning a latent representation which can jointly reconstruct them.	3
Clustering is then performed on the learned representations, using vector dimensions as features for inducing document classes.	3
Extensive experiments on two datasets show that the features learned by our approach can achieve better clustering performance than other existing features, including term frequency-inverse document frequency and average embedding.	3
----------
Functional Distributional Semantics provides a linguistically interpretable framework for distributional semantics, by representing the meaning of a word as a function (a binary classifier), instead of a vector.	1
However, the large number of latent variables means that inference is computationally expensive, and training a model is therefore slow to converge.	1
In this paper, I introduce the Pixie Autoencoder, which augments the generative model of Functional Distributional Semantics with a graph-convolutional neural network to perform amortised variational inference.	2+3
This allows the model to be trained more effectively, achieving better results on two tasks (semantic similarity in context and semantic composition), and outperforming BERT, a large pre-trained language model.	3+4
----------
Pretraining deep language models has led to large performance gains in NLP.	1
Despite this success, Schick and Schütze (2020) recently showed that these models struggle to understand rare words.	1
For static word embeddings, this problem has been addressed by separately learning representations for rare words.	1
In this work, we transfer this idea to pretrained language models: We introduce BERTRAM, a powerful architecture based on BERT that is capable of inferring high-quality embeddings for rare words that are suitable as input representations for deep language models.	2
This is achieved by enabling the surface form and contexts of a word to interact with each other in a deep architecture.	3
Integrating BERTRAM into BERT leads to large performance increases due to improved representations of rare and medium frequency words on both a rare word probing task and three downstream tasks.	4
----------
Knowing the Most Frequent Sense (MFS) of a word has been proved to help Word Sense Disambiguation (WSD) models significantly.	1+2
However, the scarcity of sense-annotated data makes it difficult to induce a reliable and high-coverage distribution of the meanings in a language vocabulary.	1
To address this issue, in this paper we present CluBERT, an automatic and multilingual approach for inducing the distributions of word senses from a corpus of raw sentences.	1+2
Our experiments show that CluBERT learns distributions over English senses that are of higher quality than those extracted by alternative approaches.	2
When used to induce the MFS of a lemma, CluBERT attains state-of-the-art results on the English Word Sense Disambiguation tasks and helps to improve the disambiguation performance of two off-the-shelf WSD models.	3
Moreover, our distributions also prove to be effective in other languages, beating all their alternatives for computing the MFS on the multilingual WSD tasks.	2+3
We release our sense distributions in five different languages at https://github.com/SapienzaNLP/clubert.	3
----------
Cross-domain sentiment classification aims to address the lack of massive amounts of labeled data.	1+2
It demands to predict sentiment polarity on a target domain utilizing a classifier learned from a source domain.	1
In this paper, we investigate how to efficiently apply the pre-training language model BERT on the unsupervised domain adaptation.	2+3
Due to the pre-training task and corpus, BERT is task-agnostic, which lacks domain awareness and can not distinguish the characteristic of source and target domain when transferring knowledge.	1
To tackle these problems, we design a post-training procedure, which contains the target domain masked language model task and a novel domain-distinguish pre-training task.	2
The post-training procedure will encourage BERT to be domain-aware and distill the domain-specific features in a self-supervised way.	2+3
Based on this, we could then conduct the adversarial training to derive the enhanced domain-invariant features.	2+3
Extensive experiments on Amazon dataset show that our model outperforms state-of-the-art methods by a large margin.	3+4
The ablation study demonstrates that the remarkable improvement is not only from BERT but also from our method.	4
----------
Recently, sentiment analysis has seen remarkable advance with the help of pre-training approaches.	1+2
However, sentiment knowledge, such as sentiment words and aspect-sentiment pairs, is ignored in the process of pre-training, despite the fact that they are widely used in traditional sentiment analysis approaches.	1
In this paper, we introduce Sentiment Knowledge Enhanced Pre-training (SKEP) in order to learn a unified sentiment representation for multiple sentiment analysis tasks.	2
With the help of automatically-mined knowledge, SKEP conducts sentiment masking and constructs three sentiment knowledge prediction objectives, so as to embed sentiment information at the word, polarity and aspect level into pre-trained sentiment representation.	2+3
In particular, the prediction of aspect-sentiment pairs is converted into multi-label classification, aiming to capture the dependency between words in a pair.	3
Experiments on three kinds of sentiment tasks show that SKEP significantly outperforms strong pre-training baseline, and achieves new state-of-the-art results on most of the test datasets.	3
We release our code at https://github.com/baidu/Senta.	1
----------
Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces.	1
However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism.	1+2
In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages.	2
We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.	2+3
We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.	4
----------
Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences.	1+2
Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date.	1
In this paper, we show that these results can be improved by using an in-order linearization instead.	1
Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015)’s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers.	3+4
Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.	5
----------
A key problem in processing graph-based meaning representations is graph parsing, i.e. computing all possible derivations of a given graph according to a (competence) grammar.	1+2
We demonstrate, for the first time, that exact graph parsing can be efficient for large graphs and with large Hyperedge Replacement Grammars (HRGs).	2+3
The advance is achieved by exploiting locality as terminal edge-adjacency in HRG rules.	2+3
In particular, we highlight the importance of 1) a terminal edge-first parsing strategy, 2) a categorization of a subclass of HRG, i.e. what we call Weakly Regular Graph Grammar, and 3) distributing argument-structures to both lexical and phrasal rules.	3
----------
Incremental syntactic parsing has been an active research area both for cognitive scientists trying to model human sentence processing and for NLP researchers attempting to combine incremental parsing with language modelling for ASR and MT.	1
Most effort has been directed at designing the right transition mechanism, but less has been done to answer the question of what a probabilistic model for those transition parsers should look like.	1
A very incremental transition mechanism of a recently proposed CCG parser when trained in straightforward locally normalised discriminative fashion produces very bad results on English CCGbank.	1
We identify three biases as the causes of this problem: label bias, exposure bias and imbalanced probabilities bias.	2
While known techniques for tackling these biases improve results, they still do not make the parser state of the art.	1
Instead, we tackle all of these three biases at the same time using an improved version of beam search optimisation that minimises all beam search violations instead of minimising only the biggest violation.	4
The new incremental parser gives better results than all previously published incremental CCG parsers, and outperforms even some widely used non-incremental CCG parsers.	4
----------
Recent work has shown that neural rerankers can improve results for dependency parsing over the top k trees produced by a base parser.	1+2
However, all neural rerankers so far have been evaluated on English and Chinese only, both languages with a configurational word order and poor morphology.	1+2
In the paper, we re-assess the potential of successful neural reranking models from the literature on English and on two morphologically rich(er) languages, German and Czech.	2
In addition, we introduce a new variation of a discriminative reranker based on graph convolutional networks (GCNs).	2
We show that the GCN not only outperforms previous models on English but is the only model that is able to improve results over the baselines on German and Czech.	3
We explain the differences in reranking performance based on an analysis of a) the gold tree ratio and b) the variety in the k-best lists.	2+3
----------
To increase trust in artificial intelligence systems, a promising research direction consists of designing neural models capable of generating natural language explanations for their predictions.	1
In this work, we show that such models are nonetheless prone to generating mutually inconsistent explanations, such as ”Because there is a dog in the image.” and ”Because there is no dog in the [same] image.”, exposing flaws in either the decision-making process of the model or in the generation of the explanations.	2+3
We introduce a simple yet effective adversarial framework for sanity checking models against the generation of inconsistent natural language explanations.	1+2
Moreover, as part of the framework, we address the problem of adversarial attacks with full target sequences, a scenario that was not previously addressed in sequence-to-sequence attacks.	3
Finally, we apply our framework on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions.	3
Our framework shows that this model is capable of generating a significant number of inconsistent explanations.	4
----------
By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings).	1+2
The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge.	2
However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself.	3
Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT).	2
Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process.	3
Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines.	4
We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.	4+5
----------
Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence.	1+2
We investigate whether they also capture information beneficial for resolving pronominal anaphora in English.	1
We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus.	2+3
The Transformer outperforms the LSTM in all analyses.	1+2
Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world.	4+5
However, we find traces of the latter aspect, too.	4
----------
In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer.	1
Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed.	1
This makes attention weights unreliable as explanations probes.	1
In this paper, we consider the problem of quantifying this flow of information through self-attention.	2
We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens.	2+3
We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.	4
----------
Recent studies on interpretability of attention distributions have led to notions of faithful and plausible explanations for a model’s predictions.	1
Attention distributions can be considered a faithful explanation if a higher attention weight implies a greater impact on the model’s prediction.	1
They can be considered a plausible explanation if they provide a human-understandable justification for the model’s predictions.	1
In this work, we first explain why current attention mechanisms in LSTM based encoders can neither provide a faithful nor a plausible explanation of the model’s predictions.	2
We observe that in LSTM based encoders the hidden representations at different time-steps are very similar to each other (high conicity) and attention weights in these situations do not carry much meaning because even a random permutation of the attention weights does not affect the model’s predictions.	4
Based on experiments on a wide variety of tasks and datasets, we observe attention distributions often attribute the model’s predictions to unimportant words such as punctuation and fail to offer a plausible explanation for the predictions.	4
To make attention mechanisms more faithful and plausible, we propose a modified LSTM cell with a diversity-driven training objective that ensures that the hidden representations learned at different time steps are diverse.	2
We show that the resulting attention distributions offer more transparency as they (i) provide a more precise importance ranking of the hidden states (ii) are better indicative of words important for the model’s predictions (iii) correlate better with gradient-based attribution methods.	4
Human evaluations indicate that the attention distributions learned by our model offer a plausible explanation of the model’s predictions.	4
Our code has been made publicly available at https://github.com/akashkm99/Interpretable-Attention	6
----------
Multi-task Learning methods have achieved great progress in text classification.	1
However, existing methods assume that multi-task text classification problems are convex multiobjective optimization problems, which is unrealistic in real-world applications.	1+2
To address this issue, this paper presents a novel Tchebycheff procedure to optimize the multi-task classification problems without convex assumption.	2
The extensive experiments back up our theoretical analysis and validate the superiority of our proposals.	2+3
----------
This paper studies strategies to model word formation in NMT using rich linguistic information, namely a word segmentation approach that goes beyond splitting into substrings by considering fusional morphology.	2+3
Our linguistically sound segmentation is combined with a method for target-side inflection to accommodate modeling word formation.	3
The best system variants employ source-side morphological analysis and model complex target-side words, improving over a standard system.	5
----------
Existing approaches to active learning maximize the system performance by sampling unlabeled instances for annotation that yield the most efficient training.	1
However, when active learning is integrated with an end-user application, this can lead to frustration for participating users, as they spend time labeling instances that they would not otherwise be interested in reading.	1
In this paper, we propose a new active learning approach that jointly optimizes the seemingly counteracting objectives of the active learning system (training efficiently) and the user (receiving useful instances).	2
We study our approach in an educational application, which particularly benefits from this technique as the system needs to rapidly learn to predict the appropriateness of an exercise to a particular user, while the users should receive only exercises that match their skills.	2+3
We evaluate multiple learning strategies and user types with data from real users and find that our joint approach better satisfies both objectives when alternative methods lead to many unsuitable exercises for end users.	3+4
----------
This paper investigates how to effectively incorporate a pre-trained masked language model (MLM), such as BERT, into an encoder-decoder (EncDec) model for grammatical error correction (GEC).	2
The answer to this question is not as straightforward as one might expect because the previous common methods for incorporating a MLM into an EncDec model have potential drawbacks when applied to GEC.	1
For example, the distribution of the inputs to a GEC model can be considerably different (erroneous, clumsy, etc.) from that of the corpora used for pre-training MLMs; however, this issue is not addressed in the previous methods.	1
Our experiments show that our proposed method, where we first fine-tune a MLM with a given GEC corpus and then use the output of the fine-tuned MLM as additional features in the GEC model, maximizes the benefit of the MLM.	3+4
The best-performing model achieves state-of-the-art performances on the BEA-2019 and CoNLL-2014 benchmarks.	4
Our code is publicly available at: https://github.com/kanekomasahiro/bert-gec.	6
----------
With the explosion of news information, personalized news recommendation has become very important for users to quickly find their interested contents.	1
Most existing methods usually learn the representations of users and news from news contents for recommendation.	1
However, they seldom consider high-order connectivity underlying the user-news interactions.	1
Moreover, existing methods failed to disentangle a user’s latent preference factors which cause her clicks on different news.	1
In this paper, we model the user-news interactions as a bipartite graph and propose a novel Graph Neural News Recommendation model with Unsupervised Preference Disentanglement, named GNUD.	2
Our model can encode high-order relationships into user and news representations by information propagation along the graph.	2+3
Furthermore, the learned representations are disentangled with latent preference factors by a neighborhood routing algorithm, which can enhance expressiveness and interpretability.	3
A preference regularizer is also designed to force each disentangled subspace to independently reflect an isolated preference, improving the quality of the disentangled representations.	3+4
Experimental results on real-world news datasets demonstrate that our proposed model can effectively improve the performance of news recommendation and outperform state-of-the-art news recommendation methods.	4
----------
In this paper, we study the problem of identifying the principals and accessories from the fact description with multiple defendants in a criminal case.	1
We treat the fact descriptions as narrative texts and the defendants as roles over the narrative story.	2
We propose to model the defendants with behavioral semantic information and statistical characteristics, then learning the importances of defendants within a learning-to-rank framework.	2+3
Experimental results on a real-world dataset demonstrate the behavior analysis can effectively model the defendants’ impacts in a complex case.	3
----------
The rise of online communication platforms has been accompanied by some undesirable effects, such as the proliferation of aggressive and abusive behaviour online.	1
Aiming to tackle this problem, the natural language processing (NLP) community has experimented with a range of techniques for abuse detection.	1
While achieving substantial success, these methods have so far only focused on modelling the linguistic properties of the comments and the online communities of users, disregarding the emotional state of the users and how this might affect their language.	2+3
The latter is, however, inextricably linked to abusive behaviour.	3
In this paper, we present the first joint model of emotion and abusive language detection, experimenting in a multi-task learning framework that allows one task to inform the other.	3
Our results demonstrate that incorporating affective features leads to significant improvements in abuse detection performance across datasets.	3+4
----------
The key to effortless end-user programming is natural language.	1+2
We examine how to teach intelligent systems new functions, expressed in natural language.	1+2
As a first step, we collected 3168 samples of teaching efforts in plain English.	3
Then we built fuSE, a novel system that translates English function descriptions into code.	3+4
Our approach is three-tiered and each task is evaluated separately.	4
We first classify whether an intent to teach new functionality is present in the utterance (accuracy: 97.7% using BERT).	4
Then we analyze the linguistic structure and construct a semantic model (accuracy: 97.6% using a BiLSTM).	3+4
Finally, we synthesize the signature of the method, map the intermediate steps (instructions in the method body) to API calls and inject control structures (F1: 67.0% with information retrieval and knowledge-based methods).	3+4
In an end-to-end evaluation on an unseen dataset fuSE synthesized 84.6% of the method signatures and 79.2% of the API calls correctly.	4
----------
Moderation is crucial to promoting healthy online discussions.	1
Although several ‘toxicity’ detection datasets and models have been published, most of them ignore the context of the posts, implicitly assuming that comments may be judged independently.	1+2
We investigate this assumption by focusing on two questions: (a) does context affect the human judgement, and (b) does conditioning on context improve performance of toxicity detection systems?	3
We experiment with Wikipedia conversations, limiting the notion of context to the previous post in the thread and the discussion title.	3
We find that context can both amplify or mitigate the perceived toxicity of posts.	3
Moreover, a small but significant subset of manually labeled posts (5% in one of our experiments) end up having the opposite toxicity labels if the annotators are not provided with context.	4
Surprisingly, we also find no evidence that context actually improves the performance of toxicity classifiers, having tried a range of classifiers and mechanisms to make them context aware.	4
This points to the need for larger datasets of comments annotated in context.	5
We make our code and data publicly available.	6
----------
Answering natural language questions over tables is usually seen as a semantic parsing task.	1
To alleviate the collection cost of full logical forms, one popular approach focuses on weak supervision consisting of denotations instead of logical forms.	2
However, training semantic parsers from weak supervision poses difficulties, and in addition, the generated logical forms are only used as an intermediate step prior to retrieving the denotation.	1
In this paper, we present TaPas, an approach to question answering over tables without generating logical forms.	2+3
TaPas trains from weak supervision, and predicts the denotation by selecting table cells and optionally applying a corresponding aggregation operator to such selection.	3
TaPas extends BERT’s architecture to encode tables as input, initializes from an effective joint pre-training of text segments and tables crawled from Wikipedia, and is trained end-to-end.	3+4
We experiment with three different semantic parsing datasets, and find that TaPas outperforms or rivals semantic parsing models by improving state-of-the-art accuracy on SQA from 55.1 to 67.2 and performing on par with the state-of-the-art on WikiSQL and WikiTQ, but with a simpler model architecture.	3
We additionally find that transfer learning, which is trivial in our setting, from WikiSQL to WikiTQ, yields 48.7 accuracy, 4.2 points above the state-of-the-art.	4
----------
In argumentation, people state premises to reason towards a conclusion.	1
The conclusion conveys a stance towards some target, such as a concept or statement.	1
Often, the conclusion remains implicit, though, since it is self-evident in a discussion or left out for rhetorical reasons.	1
However, the conclusion is key to understanding an argument and, hence, to any application that processes argumentation.	1+2
We thus study the question to what extent an argument’s conclusion can be reconstructed from its premises.	2
In particular, we argue here that a decisive step is to infer a conclusion’s target, and we hypothesize that this target is related to the premises’ targets.	2+3
We develop two complementary target inference approaches: one ranks premise targets and selects the top-ranked target as the conclusion target, the other finds a new conclusion target in a learned embedding space using a triplet neural network.	3
Our evaluation on corpora from two domains indicates that a hybrid of both approaches is best, outperforming several strong baselines.	4
According to human annotators, we infer a reasonably adequate conclusion target in 89% of the cases.	4
----------
Multimodal Machine Translation (MMT) aims to introduce information from other modality, generally static images, to improve the translation quality.	1+2
Previous works propose various incorporation methods, but most of them do not consider the relative importance of multiple modalities.	1
Equally treating all modalities may encode too much useless information from less important modalities.	1
In this paper, we introduce the multimodal self-attention in Transformer to solve the issues above in MMT.	2
The proposed method learns the representation of images based on the text, which avoids encoding irrelevant information in images.	3
Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics.	4
----------
In this paper, we present a simple yet effective padding scheme that can be used as a drop-in module for existing convolutional neural networks.	2
We call it partial convolution based padding, with the intuition that the padded region can be treated as holes and the original input as non-holes.	2+3
Specifically, during the convolution operation, the convolution results are re-weighted near image borders based on the ratios between the padded area and the convolution sliding window area.	1
Extensive experiments with various deep network models on ImageNet classification and semantic segmentation demonstrate that the proposed padding scheme consistently outperforms standard zero padding with better accuracy.	4
----------
We introduce the "Energy-based Generative Adversarial Network" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions.	1
Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples.	1+2
Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output.	3
Among them, we show one instantiation of EBGAN framework as using an auto encoder architecture, with the energy being the reconstruction error, in place of the discriminator.	2
We show that this form of EBGAN exhibits more stable behavior than regular GANs during training.	3+4
We also show that a single-scale architecture can be trained to generate high-resolution images.	3+4
----------
Training modern deep learning models requires large amounts of computation, often provided by GPUs.	1+2
Scaling computation from one GPU to many can enable much faster training and research progress but entails two complications.	1+2
First, the training library must support inter-GPU communication.	3
Depending on the particular methods employed, this communication may entail anywhere from negligible to significant overhead.	3
Second, the user must modify his or her training code to take advantage of inter-GPU communication.	3
Depending on the training library's API, the modification required may be either significant or minimal.	3
Existing methods for enabling multi-GPU training under the TensorFlow library entail non-negligible communication overhead and require users to heavily modify their model-building code, leading many researchers to avoid the whole mess and stick with slower single-GPU training.	3
In this paper we introduce Horovod, an open source library that improves on both obstructions to scaling: it employs efficient inter-GPU communication via ring reduction and requires only a few lines of modification to user code, enabling faster, easier distributed training in TensorFlow.	4
Horovod is available under the Apache 2.0 license at this https URL	4
----------
Conditional GANs are at the forefront of natural image synthesis.	1
The main drawback of such models is the necessity for labeled data.	1
In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs.	2
In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game.	2
The role of self supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training.	2
We test empirically both the quality of the learned image representations, and the quality of the synthesized images.	3
Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts.	4
Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.	4
----------
One of the main barriers for deploying neural networks on embedded systems has been large memory and power consumption of existing neural networks.	1
In this work, we introduce SqueezeNext, a new family of neural network architectures whose design was guided by considering previous architectures such as SqueezeNet, as well as by simulation results on a neural network accelerator.	2
This new network is able to match AlexNet's accuracy on the ImageNet benchmark with 112× fewer parameters, and one of its deeper variants is able to achieve VGG-19 accuracy with only 4.4 Million parameters, (31× smaller than VGG-19).	2
SqueezeNext also achieves better top-5 classification accuracy with 1.3× fewer parameters as compared to MobileNet, but avoids using depthwise-separable convolutions that are inefficient on some mobile processor platforms.	2+3
This wide range of accuracy gives the user the ability to make speed accuracy tradeoffs, depending on the available resources on the target hardware.	3
Using hardware simulation results for power and inference speed on an embedded system has guided us to design variations of the baseline model that are 2.59×/8.26× faster and 2.25×/7.5× more energy efficient as compared to SqueezeNet/AlexNet without any accuracy degradation.	4
----------
We consider the problem of anomaly detection in images, and present a new detection technique.	1
Given a sample of images, all known to belong to a "normal" class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects).	2
The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images.	2
The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images.	2+3
We present extensive experiments using the proposed detector, which indicate that our algorithm improves state-of-the-art methods by a wide margin.	4
----------
Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available.	1
In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance.	2
Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient.	2
We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.	2+3
To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets.	3+4
In particular, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet.	4
Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.	4
Source code is at this https URL.	4
----------
Intersection over Union (IoU) is the most popular evaluation metric used in the object detection benchmarks.	1
However, there is a gap between optimizing the commonly used distance losses for regressing the parameters of a bounding box and maximizing this metric value.	1
The optimal objective for a metric is the metric itself.	2
In the case of axis-aligned 2D bounding boxes, it can be shown that IoU can be directly used as a regression loss.	2+3
However, IoU has a plateau making it infeasible to optimize in the case of non-overlapping bounding boxes.	3+4
In this paper, we address the weaknesses of IoU by introducing a generalized version as both a new loss and a new metric.	2
By incorporating this generalized IoU (GIoU) as a loss into the state-of-the art object detection frameworks, we show a consistent improvement on their performance using both the standard, IoU based, and new, GIoU based, performance measures on popular object detection benchmarks such as PASCAL VOC and MS COCO.	3+4
----------
Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.	1
We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.	1+2
When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples.	2+3
The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.	3+4
Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebTex	4
Samples from the model reflect these improvements and contain coherent paragraphs of text.	4
These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.	4+5
----------
A lot of the recent success in natural language processing (NLP) has been driven by distributed vector representations of words trained on large amounts of text in an unsupervised manner.	1
These representations are typically used as general purpose features for words across a range of NLP problems.	1
However, extending this success to learning representations of sequences of words, such as sentences, remains an open problem.	1
Recent work has explored unsupervised as well as supervised learning techniques with different training objectives to learn general purpose fixed-length sentence representations.	1+2
In this work, we present a simple, effective multi-task learning framework for sentence representations that combines the inductive biases of diverse training objectives in a single model.	2
We train this model on several data sources with multiple training objectives on over 100 million sentences.	3
Extensive experiments demonstrate that sharing a single recurrent sentence encoder across weakly related tasks leads to consistent improvements over previous methods.	3+4
We present substantial improvements in the context of transfer learning and low-resource settings using our learned general-purpose representations.	4
----------
We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models.	1+2
By end-to-end training, we mean flat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees.	2
We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks.	2+3
We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using significantly smaller models.	3+4
----------
Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase.	1
To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation --- which take into account all the information available in the raw audio signal, including the phase.	1+2
Although during the last decades end-to-end music source separation has been considered almost unattainable, our results confirm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model.	2+3
Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.	3+4
----------
We introduce a convolutional recurrent neural network (CRNN) for music tagging.	2
CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features.	3
We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample.	3
Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.	4
----------
We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error.	1+2
Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator.	2+3
In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance.	3+4
We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.	3+4
----------
Numerical studies on race car aerodynamics at wing in ground effect have been carried out using a steady 3d, double precision, pressure-based, and standard k-epsilon turbulence model.	1
Through various parametric analytical studies we have observed that at a particular speed and ground clearance of the wings a favorable negative lift was found high at a particular angle of attack for all the physical models considered in this paper.	2
The fact is that if the ground clearance height to chord length (h/c) is too small, the developing boundary layers from either side (the ground and the lower surface of the wing) can interact, leading to an altered variation of the aerodynamic characteristics at wing in ground effect.	4
Therefore a suitable ground clearance must be predicted throughout the racing for a better performance of the race car, which obviously depends upon the coupled effects of the topography, wing orientation with respect to the ground, the incoming flow features and/or the race car speed.	4
We have concluded that for the design of high performance and high speed race cars the adjustable wings capable to alter the ground clearance and the angles of attack is the best design option for any race car for racing safely with variable speeds.	4
----------
In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning.	1
Expressing these in a common form, we derive a novel algorithm, Retrace(λ), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies.	1+2
We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms.	3
We believe this is the first return-based off-policy control algorithm converging a.s. to Q∗ without the GLIE assumption (Greedy in the Limit with Infinite Exploration).	3
As a corollary, we prove the convergence of Watkins' Q(λ), which was an open problem since 1989.	4
We illustrate the benefits of Retrace(λ) on a standard suite of Atari 2600 games.	4+5
----------
Learning robust value functions given raw observations and rewards is now possible with model-free and model-based deep reinforcement learning algorithms.	1
There is a third alternative, called Successor Representations (SR), which decomposes the value function into two components -- a reward predictor and a successor map.	2+3
The successor map represents the expected future state occupancy from any given state and the reward predictor maps states to scalar rewards.	3
The value function of a state can be computed as the inner product between the successor map and the reward weights.	3
In this paper, we present DSR, which generalizes SR within an end-to-end deep reinforcement learning framework.	2+3
DSR has several appealing properties including: increased sensitivity to distal reward changes due to factorization of reward and world dynamics, and the ability to extract bottleneck states (subgoals) given successor maps trained under a random policy.	4
We show the efficacy of our approach on two diverse environments given raw pixel observations -- simple grid-world domains (MazeBase) and the Doom game engine.	4
----------
We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across observations.	1+2
Specifically, we focus on the problem of exploration in non-tabular reinforcement learning.	3
Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model.	3
This technique enables us to generalize count-based exploration algorithms to the non-tabular case.	4
We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels.	3+4
We transform these pseudo-counts into intrinsic rewards and obtain significantly improved exploration in a number of hard games, including the infamously difficult Montezuma's Revenge.	3+4
----------
Partially observed control problems are a challenging aspect of reinforcement learning.	1
We extend two related, model-free algorithms for continuous control -- deterministic policy gradient and stochastic value gradient -- to solve partially observed domains using recurrent neural networks trained with backpropagation through time.	2
We demonstrate that this approach, coupled with long-short term memory is able to solve a variety of physical control problems exhibiting an assortment of memory requirements.	4
These include the short-term integration of information from noisy sensors and the identification of system parameters, as well as long-term memory problems that require preserving information over many time steps.	3
We also demonstrate success on a combined exploration and memory problem in the form of a simplified version of the well-known Morris water maze task.	4
Finally, we show that our approach can deal with high-dimensional observations by learning directly from pixels.	4
We find that recurrent deterministic and stochastic policies are able to learn similarly good solutions to these tasks, including the water maze where the agent must learn effective search strategies.	4+5
----------
In this paper, we introduce a new set of reinforcement learning (RL) tasks in Minecraft (a flexible 3D world).	2
We then use these tasks to systematically compare and contrast existing deep reinforcement learning (DRL) architectures with our new memory-based DRL architectures.	3
These tasks are designed to emphasize, in a controllable manner, issues that pose challenges for RL methods including partial observability (due to first-person visual observations), delayed rewards, high-dimensional visual observations, and the need to use active perception in a correct manner so as to perform well in the tasks.	3
While these tasks are conceptually simple to describe, by virtue of having all of these challenges simultaneously they are difficult for current DRL architectures.	3
Additionally, we evaluate the generalization performance of the architectures on environments not used during training.	3
The experimental results show that our new architectures generalize to unseen environments better than existing DRL architectures.	4
----------
Deep Reinforcement Learning methods have achieved state of the art performance in learning control policies for the games in the Atari 2600 domain.	1
One of the important parameters in the Arcade Learning Environment (ALE) is the frame skip rate.	1
It decides the granularity at which agents can control game play.	1
A frame skip value of k allows the agent to repeat a selected action k number of times.	1+2
The current state of the art architectures like Deep Q-Network (DQN) and Dueling Network Architectures (DuDQN) consist of a framework with a static frame skip rate, where the action output from the network is repeated for a fixed number of frames regardless of the current state.	3
In this paper, we propose a new architecture, Dynamic Frame skip Deep Q-Network (DFDQN) which makes the frame skip rate a dynamic learnable parameter.	2+3
This allows us to choose the number of times an action is to be repeated based on the current state.	3
We show empirically that such a setting improves the performance on relatively harder games like Seaquest.	4
----------
This paper introduces an automated skill acquisition framework in reinforcement learning which involves identifying a hierarchical description of the given task in terms of abstract states and extended actions between abstract states.	2
Identifying such structures present in the task provides ways to simplify and speed up reinforcement learning algorithms.	3
These structures also help to generalize such algorithms over multiple tasks without relearning policies from scratch.	2+3
We use ideas from dynamical systems to find metastable regions in the state space and associate them with abstract states.	3
The spectral clustering algorithm PCCA+ is used to identify suitable abstractions aligned to the underlying structure.	3
Skills are defined in terms of the sequence of actions that lead to transitions between such abstract states.	4
The connectivity information from PCCA+ is used to generate these skills or options.	4
These skills are independent of the learning task and can be efficiently reused across a variety of tasks defined over the same model.	4+5
This approach works well even without the exact model of the environment by using sample trajectories to construct an approximate estimate.	4+5
We also present our approach to scaling the skill acquisition framework to complex tasks with large state spaces for which we perform state aggregation using the representation learned from an action conditional video prediction network and use the skill acquisition framework on the aggregated state space.	5
----------
Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms.	1
The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions.	1
Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems.	1
Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment.	1
We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning.	2
A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals.	3
h-DQN allows for flexible goal specifications, such as functions over entities and relations.	3
This provides an efficient space for exploration in complicated environments.	3
We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.	4
----------
We describe a learning-based approach to hand-eye coordination for robotic grasping from monocular images.	1
To learn hand-eye coordination for grasping, we trained a large convolutional neural network to predict the probability that task-space motion of the gripper will result in successful grasps, using only monocular camera images and independently of camera calibration or the current robot pose.	2+3
This requires the network to observe the spatial relationship between the gripper and objects in the scene, thus learning hand-eye coordination.	3
We then use this network to servo the gripper in real time to achieve successful grasps.	3
To train our network, we collected over 800,000 grasp attempts over the course of two months, using between 6 and 14 robotic manipulators at any given time, with differences in camera placement and hardware.	3+4
Our experimental evaluation demonstrates that our method achieves effective real-time control, can successfully grasp novel objects, and corrects mistakes by continuous servoing.	4
----------
Model-free reinforcement learning has been successfully applied to a range of challenging problems, and has recently been extended to handle large neural network policies and value functions.	1
However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems.	1
In this paper, we explore algorithms and representations to reduce the sample complexity of deep reinforcement learning for continuous control tasks.	2
We propose two complementary techniques for improving the efficiency of such algorithms.	3
First, we derive a continuous variant of the Q-learning algorithm, which we call normalized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods.	3
NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves performance on a set of simulated robotic control tasks.	3
To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learning.	3+4
We show that iteratively refitted local linear models are especially effective for this, and demonstrate substantially faster learning on domains where such models are applicable.	4
----------
Reinforcement learning can acquire complex behaviors from high-level specifications.	1
However, defining a cost function that can be optimized effectively and encodes the correct task is challenging in practice.	1
We explore how inverse optimal control (IOC) can be used to learn behaviors from demonstrations, with applications to torque control of high-dimensional robotic systems.	2
Our method addresses two key challenges in inverse optimal control: first, the need for informative features and effective regularization to impose structure on the cost, and second, the difficulty of learning the cost function under unknown dynamics for high-dimensional continuous systems.	2
To address the former challenge, we present an algorithm capable of learning arbitrary nonlinear cost functions, such as neural networks, without meticulous feature engineering.	2+3
To address the latter challenge, we formulate an efficient sample-based approximation for MaxEnt IOC.	3+4
We evaluate our method on a series of simulated tasks and real-world robotic manipulation problems, demonstrating substantial improvement over prior methods both in terms of task complexity and sample efficiency.	4
----------
Efficient exploration in complex environments remains a major challenge for reinforcement learning.	1
We propose bootstrapped DQN, a simple algorithm that explores in a computationally and statistically efficient manner through use of randomized value functions.	1+2
Unlike dithering strategies such as epsilon-greedy exploration, bootstrapped DQN carries out temporally-extended (or deep) exploration; this can lead to exponentially faster learning.	1
We demonstrate these benefits in complex stochastic MDPs and in the large-scale Arcade Learning Environment.	2+3
Bootstrapped DQN substantially improves learning times and performance across most Atari games.	3+4
----------
We introduce the value iteration network (VIN): a fully differentiable neural network with a `planning module' embedded within.	2
VINs can learn to plan, and are suitable for predicting outcomes that involve planning-based reasoning, such as policies for reinforcement learning.	2
Key to our approach is a novel differentiable approximation of the value-iteration algorithm, which can be represented as a convolutional neural network, and trained end-to-end using standard backpropagation.	2
We evaluate VIN based policies on discrete and continuous path-planning domains, and on a natural-language based search task.	3
We show that by learning an explicit planning computation, VIN policies generalize better to new, unseen domains.	4
----------
We propose deep distributed recurrent Q-networks (DDRQN), which enable teams of agents to learn to solve communication-based coordination tasks.	2
In these tasks, the agents are not given any pre-designed communication protocol.	3
Therefore, in order to successfully communicate, they must first automatically develop and agree upon their own communication protocol.	3
We present empirical results on two multi-agent learning problems based on well-known riddles, demonstrating that DDRQN can successfully solve such tasks and discover elegant communication protocols to do so.	4
To our knowledge, this is the first time deep reinforcement learning has succeeded in learning communication protocols.	4
In addition, we present ablation experiments that confirm that each of the main components of the DDRQN architecture are critical to its success.	4
----------
The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves.	1
Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves.	1+2
These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play.	3
Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play.	3
We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks.	3
Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0.	4+5
This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.	4+5
----------
State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available.	1
In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers.	1+2
Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora.	3+4
Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.	4
----------
In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding.	2
We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language.	3
We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark.	3
Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7.	4
We also release these models for the NLP and ML community to study and improve upon.	6
----------
Teaching machines to read natural language documents remains an elusive challenge.	1
Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation.	1
In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data.	1+2
This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.	3+4
----------
An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.	1
However, there has been little work exploring useful architectures for attention-based NMT.	1
This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.	1+2
We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions.	3
With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout.	3
Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.	4
----------
Pixel-level labelling tasks, such as semantic segmentation, play a central role in image understanding.	1
Recent approaches have attempted to harness the capabilities of deep learning techniques for image recognition to tackle pixel-level labelling tasks.	1
One central issue in this methodology is the limited capacity of deep learning techniques to delineate visual objects.	1
To solve this problem, we introduce a new form of convolutional neural network that combines the strengths of Convolutional Neural Networks (CNNs) and Conditional Random Fields (CRFs)-based probabilistic graphical modelling.	2
To this end, we formulate mean-field approximate inference for the Conditional Random Fields with Gaussian pairwise potentials as Recurrent Neural Networks.	3
This network, called CRF-RNN, is then plugged in as a part of a CNN to obtain a deep network that has desirable properties of both CNNs and CRFs.	3
Importantly, our system fully integrates CRF modelling with CNNs, making it possible to train the whole deep network end-to-end with the usual back-propagation algorithm, avoiding offline post-processing methods for object delineation.	3
We apply the proposed method to the problem of semantic image segmentation, obtaining top results on the challenging Pascal VOC 2012 segmentation benchmark.	3+4
----------
We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes.	1+2
The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.	3+4
Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.	4
----------
Neural machine translation is a recently proposed approach to machine translation.	1
Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance.	1
The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation.	1
In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly.	2
With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation.	4
Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.	4
----------
Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks.	1
Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences.	1
In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure.	2
Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector.	3
Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words.	4
Additionally, the LSTM did not have difficulty on long sentences.	4
For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset.	4
When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task.	3+4
The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice.	4
Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.	4+5
----------
In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN).	2
One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols.	3
The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence.	3
The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model.	3+4
Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.	4
----------
The ability to accurately represent sentences is central to language understanding.	1
We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences.	2
The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences.	2+3
The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations.	2+3
The network does not rely on a parse tree and is easily applicable to any language.	2+3
We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision.	3
The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.	2+3
----------
Many machine learning algorithms require the input to be represented as a fixed-length feature vector.	1
When it comes to texts, one of the most common fixed-length features is bag-of-words.	1
Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words.	1
For example, "powerful," "strong" and "Paris" are equally distant.	1
In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents.	2
Our algorithm represents each document by a dense vector which is trained to predict words in the document.	2+3
Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models.	3+4
Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations.	4
Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.	4
----------
The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.	1
In this paper we present several extensions that improve both the quality of the vectors and the training speed.	2
By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations.	3
We also describe a simple alternative to the hierarchical softmax called negative sampling.	3
An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.	4
For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada".	4
Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.	4+5
----------
We propose two novel model architectures for computing continuous vector representations of words from very large data sets.	2
The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks.	2+3
We observe large improvements in accuracy at much lower computational cost, i.e.it takes less than a day to learn high quality word vectors from a 1.6 billion words data set.	3
Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.	3+4
----------
This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time.	2
The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued).	2+3
It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence.	3
The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.	4
----------
Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs).	1
Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding.	1
We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level.	2
Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN.	3
For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames.	3
We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length.	3
Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.	3+4
----------
We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech--two vastly different languages.	2
Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages.	3
Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system.	2+3
Because of this efficiency, experiments that previously took weeks now run in days.	4
This enables us to iterate more quickly to identify superior architectures and algorithms.	4
As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets.	4+5
Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.	4+5
----------
We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition.	1+2
We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output.	2
The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error.	2+3
We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance.	3
Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.	4+5
----------
Policy search methods can allow robots to learn control policies for a wide range of tasks, but practical applications of policy search often require hand-engineered components for perception, state estimation, and low-level control.	1
In this paper, we aim to answer the following question: does training the perception and control systems jointly end-to-end provide better performance than training each component separately?	1+2
To this end, we develop a method that can be used to learn policies that map raw image observations directly to torques at the robot's motors.	2+3
The policies are represented by deep convolutional neural networks (CNNs) with 92,000 parameters, and are trained using a partially observed guided policy search method, which transforms policy search into supervised learning, with supervision provided by a simple trajectory-centric reinforcement learning method.	3+4
We evaluate our method on a range of real-world manipulation tasks that require close coordination between vision and control, such as screwing a cap onto a bottle, and present simulated comparisons to a range of prior policy search methods.	3+4
----------
We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain.	2
We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces.	2+3
Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving.	2+3
Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives.	4
We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.	4
----------
We consider the problem of detecting robotic grasps in an RGB-D view of a scene containing objects.	2
In this work, we apply a deep learning approach to solve this problem, which avoids time-consuming hand-design of features.	3
This presents two main challenges.	2+3
First, we need to evaluate a huge number of candidate grasps.	3
In order to make detection fast, as well as robust, we present a two-step cascaded structure with two deep networks, where the top detections from the first are re-evaluated by the second.	3
The first network has fewer features, is faster to run, and can effectively prune out unlikely candidate grasps.	3
The second, with more features, is slower but has to run only on the top few detections.	3
Second, we need to handle multimodal inputs well, for which we present a method to apply structured regularization on the weights based on multimodal group regularization.	3
We demonstrate that our method outperforms the previous state-of-the-art methods in robotic grasp detection, and can be used to successfully execute grasps on two different robotic platforms.	4
----------
We propose a deep learning method for single image super-resolution (SR).	2
Our method directly learns an end-to-end mapping between the low/high-resolution images.	2
The mapping is represented as a deep convolutional neural network (CNN) that takes the low-resolution image as the input and outputs the high-resolution one.	2+3
We further show that traditional sparse-coding-based SR methods can also be viewed as a deep convolutional network.	2+3
But unlike traditional methods that handle each component separately, our method jointly optimizes all layers.	2+3
Our deep CNN has a lightweight structure, yet demonstrates state-of-the-art restoration quality, and achieves fast speed for practical on-line usage.	3
We explore different network structures and parameter settings to achieve trade-offs between performance and speed.	3
Moreover, we extend our network to cope with three color channels simultaneously, and show better overall reconstruction quality.	3+4
----------
In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image.	1
Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities.	1
However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.	1
Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality.	2
The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images.	2
Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.	4
----------
We present a model that generates natural language descriptions of images and their regions.	2
Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data.	2
Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding.	2+3
We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions.	3+4
We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets.	3+4
We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.	4
----------
Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images.	2
We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound.	2+3
We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence.	3+4
We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.	4
----------
Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing.	1
In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image.	2
The model is trained to maximize the likelihood of the target description sentence given the training image.	2+3
Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions.	3+4
Our model is often quite accurate, which we verify both qualitatively and quantitatively.	4
For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69.	3+4
We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28.	4
Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.	4
----------
We consider the automated recognition of human actions in surveillance videos.	2
Most current methods build classifiers based on complex handcrafted features computed from the raw inputs.	1
Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs.	1
However, such models are currently limited to handling 2D inputs.	1
In this paper, we develop a novel 3D CNN model for action recognition.	2
This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames.	3
The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels.	3
To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models.	3
We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.	4
----------
Convolutional networks are powerful visual models that yield hierarchies of features.	1
We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation.	1+2
Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning.	2
We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models.	2+3
We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task.	3
We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.	3+4
Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.	4
----------
State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations.	1
Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck.	1
In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals.	1+2
An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.	3
The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection.	3
We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look.	3+4
For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image.	4
In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks.	4
Code has been made publicly available.	6
----------
Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years.	1
The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context.	1
In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012---achieving a mAP of 53.3%.	2
Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost.	2+3
Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features.	3
We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture.	3
We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset.	3+4
Source code for the complete system is available at this http URL.	4
----------
Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224x224) input image.	1
This requirement is "artificial" and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale.	1
In this work, we equip the networks with another pooling strategy, "spatial pyramid pooling", to eliminate the above requirement.	2
The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale.	2+3
Pyramid pooling is also robust to object deformations.	3
With these advantages, SPP-net should in general improve all CNN-based image classification methods.	3
On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs.	4
On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning.	3+4
The power of SPP-net is also significant in object detection.	4
Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors.	4
This method avoids repeatedly computing the convolutional features.	1
In processing test images, our method is 24-102x faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007.	1
In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams.	1
This manuscript also introduces the improvement made for this competition.	1
----------
Scene labeling consists of labeling each pixel in an image with the category of the object it belongs to.	1
We propose a method that uses a multiscale convolutional network trained from raw pixels to extract dense feature vectors that encode regions of multiple sizes centered on each pixel.	1+2
The method alleviates the need for engineered features, and produces a powerful representation that captures texture, shape, and contextual information.	3
We report results using multiple postprocessing methods to produce the final labeling.	3
Among those, we propose a technique to automatically retrieve, from a pool of segmentation components, an optimal set of components that best explain the scene; these components are arbitrary, for example, they can be taken from a segmentation tree or from any family of oversegmentations.	3
The system yields record accuracies on the SIFT Flow dataset (33 classes) and the Barcelona dataset (170 classes) and near-record accuracy on Stanford background dataset (eight classes), while being an order of magnitude faster than competing approaches, producing a 320×240 image labeling in less than a second, including feature extraction.	3+4
----------
Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks.	1
Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks.	1
Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios.	1
Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization.	2+3
We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.	2+3
With an ensemble of 4 models and multi-crop evaluation, we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.	4
----------
Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years.	1
One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost.	1
Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network.	1
This raises the question of whether there are any benefit in combining the Inception architecture with residual connections.	1+2
Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly.	2
There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin.	4
We also present several new streamlined architectures for both residual and non-residual Inception networks.	2
These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly.	4
We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks.	4
With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge.	4
----------
Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors.	1
In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation.	1+2
A series of ablation experiments support the importance of these identity mappings.	1
This motivates us to propose a new residual unit, which makes training easier and improves generalization.	1+2
We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.	3+4
Code is available at: this https URL	6
----------
Deeper neural networks are more difficult to train.	1
We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.	1+2
We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions.	2+3
We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth.	2+3
On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.	3
An ensemble of these residual nets achieves 3.57% error on the ImageNet test set.	3+4
This result won the 1st place on the ILSVRC 2015 classification task.	4
We also present analysis on CIFAR-10 with 100 and 1000 layers.	4
The depth of representations is of central importance for many visual recognition tasks.	4
Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset.	4
Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.	5
----------
Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner.	1
In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network.	2
This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process.	3
We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.	3+4
----------
We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014).	2+3
The main hallmark of this architecture is the improved utilization of the computing resources inside the network.	4
This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant.	4
To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing.	3+4
One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.	4
----------
In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting.	2
Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers.	3+4
These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively.	4
We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results.	4
We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.	6
----------
The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods.	1
Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector.	1
This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details.	2+3
We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance.	3
We also identify aspects of deep and shallow methods that can be successfully shared.	3
In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost.	5
Source code and models to reproduce the experiments in the paper is made publicly available.	6
----------
We present an integrated framework for using Convolutional Networks for classification, localization and detection.	2
We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet.	2
We also introduce a novel deep learning approach to localization by learning to predict object boundaries.	2
Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence.	4
We show that different tasks can be learned simultaneously using a single shared network.	4
This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks.	4
In post-competition work, we establish a new state of the art for the detection task.	5
Finally, we release a feature extractor from our best model called OverFeat.	4
----------
We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout.	2+3
We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique.	3
We empirically verify that the model successfully accomplishes both of these tasks.	3
We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.	3
----------
We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field.	2
The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input.	3
Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field.	3
We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator.	3
The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer.	3
Deep NIN can be implemented by stacking mutiple of the above described structure.	3
With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers.	4
We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.	4
----------
We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework.	2
We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic.	2
Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels.	2
Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN.	4
The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%.	4+5
We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.	4+5
----------
In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications.	1
Comparatively, unsupervised learning with CNNs has received less attention.	1
In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning.	1
We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning.	2+3
Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator.	3+4
Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.	2
----------
This paper introduces the Deep Recurrent Attentive Writer (DRAW) neural network architecture for image generation.	2
DRAW networks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images.	3
The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distinguished from real data with the naked eye.	4
----------
We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. 	2+3
The training procedure for G is to maximize the probability of D making a mistake.	3
This framework corresponds to a minimax two-player game.	4
In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere.	4
In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation.	4
There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples.	4
Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.	4+5
----------
We consider the problem of building high-level, class-specific feature detectors from only unlabeled data.	1
For example, is it possible to learn a face detector using only unlabeled images?	1
To answer this, we train a 9-layered locally connected sparse autoencoder with pooling and local contrast normalization on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200x200 pixel images downloaded from the Internet).	2+3
We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days.	2+3
Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not.	4
Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation.	4
We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies.	4
Starting with these learned features, we trained our network to obtain 15.8% accuracy in recognizing 20,000 object categories from ImageNet, a leap of 70% relative improvement over the previous state-of-the-art.	4+5
----------
Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success.	1
However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem.	1
Here we introduce a new architecture designed to overcome this.	1
Our so-called highway networks allow unimpeded information flow across many layers on information highways.	1
They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow.	2+3
Even with hundreds of layers, highway networks can be trained directly through simple gradient descent.	3
This enables the study of extremely deep and efficient architectures.	5
----------
Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change.	1
This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities.	1
We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.	2
Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch.	3+4
Batch Normalization allows us to use much higher learning rates and be less careful about initialization.	3+4
It also acts as a regularizer, in some cases eliminating the need for Dropout.	4
Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.	4
Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.	4+5
----------
Rectified activation units (rectifiers) are essential for state-of-the-art neural networks.	1
In this work, we study rectifier neural networks for image classification from two aspects.	2
First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit.	2
PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk.	4
Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities.	4
This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures.	5
Based on our PReLU networks (PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset.	4
This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66%).	4
To our knowledge, our result is the first to surpass human-level performance (5.1%, Russakovsky et al.) on this visual recognition challenge.	4+5
----------
When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data.	1
This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case.	2+3
This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors.	4
Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate.	3+4
Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.	3+4
----------
Training state-of-the-art, deep neural networks is computationally expensive.	1
One way to reduce the training time is to normalize the activities of the neurons.	1
A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case.	1
This significantly reduces the training time in feed-forward neural networks.	1
However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks.	1
In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case.	2+3
Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity.	2
Unlike batch normalization, layer normalization performs exactly the same computation at training and test times.	3
It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step.	3
Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks.	5
Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.	5
----------
The move from hand-designed features to learned features in machine learning has been wildly successful.	1
In spite of this, optimization algorithms are still designed by hand.	1
In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way.	2+3
Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure.	3
We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.	3
----------
Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result.	1
Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing.	1
In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network.	2+3
We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times.	2
The model automatically adjusts the output keeping all edits as realistic as possible.	3
All our manipulations are expressed in terms of constrained optimization and are applied in near-real time.	4
We evaluate our algorithm on the task of realistic photo manipulation of shape and color.	3
The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.	3+4
----------
Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example.	1
However, their methods requires a slow and memory-consuming optimization process.	1
We propose here an alternative approach that moves the computational burden to a learning stage.	2
Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image.	2+3
The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys~et~al., but hundreds of times faster.	4
More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.	5
----------
Recent research on deep neural networks has focused primarily on improving accuracy.	1
For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level.	1
With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training.	1
(2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car.	1
(3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory.	1
To provide all of these advantages, we propose a small DNN architecture called SqueezeNet.	2
SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.	4
Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).	4
The SqueezeNet architecture is available for download here: this https URL	6
----------
State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets.	1
While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power.	1
Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM.	1
This compression is achieved by pruning the redundant connections and having multiple connections share the same weight.	1
We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing.	2+3
Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.	4
Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression.	4
EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW.	4
It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively.	4
Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.	4
----------
We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time.	2
At training-time the binary weights and activations are used for computing the parameters gradients.	3
During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency.	3
To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks.	3
On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets.	4
Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy.	4
The code for training and running our BNNs is available on-line.	6
----------
Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering.	1
One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks.	1
However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images.	1
Based on an analysis of the DMN, we propose several improvements to its memory and input modules.	2
Together with these changes we introduce a novel input module for images in order to be able to answer visual questions.	1+2
Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision.	3+4
----------
This paper presents stacked attention networks (SANs) that learn to answer natural language questions from images.	2
SANs use semantic representation of a question as query to search for the regions in an image that are related to the answer.	1
We argue that image question answering (QA) often requires multiple steps of reasoning.	2
Thus, we develop a multiple-layer SAN in which we query an image multiple times to infer the answer progressively.	3
Experiments conducted on four image QA data sets demonstrate that the proposed SANs significantly outperform previous state-of-the-art approaches.	4
The visualization of the attention layers illustrates the progress that the SAN locates the relevant visual clues that lead to the answer of the question layer-by-layer.	4
----------
Batch Normalization is quite effective at accelerating and improving the training of deep models.	1
However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples.	1
We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference.	1
We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch.	2+3
Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches.	3+4
At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.	4
----------
Scale variation is one of the key challenges in object detection.	1
In this work, we first present a controlled experiment to investigate the effect of receptive fields for scale variation in object detection.	2
Based on the findings from the exploration experiments, we propose a novel Trident Network (TridentNet) aiming to generate scale-specific feature maps with a uniform representational power.	2
We construct a parallel multi-branch architecture in which each branch shares the same transformation parameters but with different receptive fields.	3
Then, we adopt a scale-aware training scheme to specialize each branch by sampling object instances of proper scales for training.	3
As a bonus, a fast approximation version of TridentNet could achieve significant improvements without any additional parameters and computational cost compared with the vanilla detector.	4
On the COCO dataset, our TridentNet with ResNet-101 backbone achieves state-of-the-art single-model results of 48.4 mAP.	4
Codes are available at this https URL.	6
----------
Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful.	1
Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function.	1
However, we found that this loss function may lead to the vanishing gradients problem during the learning process.	1
To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator.	2
We show that minimizing the objective function of LSGAN yields minimizing the Pearson χ2 divergence.	3+4
There are two benefits of LSGANs over regular GANs.	4
First, LSGANs are able to generate higher quality images than regular GANs.	4
Second, LSGANs perform more stable during the learning process.	4
We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs.	3+4
We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs.	3
----------
The existing machine translation systems, whether phrase-based or neural, have relied almost exclusively on word-level modelling with explicit segmentation.	1
In this paper, we ask a fundamental question: can neural machine translation generate a character sequence without any explicit segmentation?	1
To answer this question, we evaluate an attention-based encoder-decoder with a subword-level encoder and a character-level decoder on four language pairs--En-Cs, En-De, En-Ru and En-Fi-- using the parallel corpora from WMT'15.	2+3
Our experiments show that the models with a character-level decoder outperform the ones with a subword-level decoder on all of the four language pairs.	4
Furthermore, the ensembles of neural models with a character-level decoder outperform the state-of-the-art non-neural machine translation systems on En-Cs, En-De and En-Fi and perform comparably on En-Ru.	4
----------
Object category localization is a challenging problem in computer vision.	1
Standard supervised training requires bounding box annotations of object instances.	1
This time-consuming annotation process is sidestepped in weakly supervised learning.	1
In this case, the supervised information is restricted to binary labels that indicate the absence/presence of object instances in the image, without their locations.	1
We follow a multiple-instance learning approach that iteratively trains the detector and infers the object locations in the positive training images.	1
Our main contribution is a multi-fold multiple instance learning procedure, which prevents training from prematurely locking onto erroneous object locations.	2+3
This procedure is particularly important when using high-dimensional representations, such as Fisher vectors and convolutional neural network features.	3
We also propose a window refinement method, which improves the localization accuracy by incorporating an objectness prior.	3
We present a detailed experimental evaluation using the PASCAL VOC 2007 dataset, which verifies the effectiveness of our approach.	3+4
----------
This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output.	2+3
ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients.	3
Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers.	3
Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem.	3
We also present character-level language modelling results on the Hutter prize Wikipedia dataset.	3
In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences.	4+5
This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.	5
----------
Object detection performance, as measured on the canonical PASCAL VOC Challenge datasets, plateaued in the final years of the competition.	1
The best-performing methods were complex ensemble systems that typically combined multiple low-level image features with high-level context.	1
In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 50 percent relative to the previous best result on VOC 2012-achieving a mAP of 62.4 percent.	2+3
Our approach combines two ideas: (1) one can apply high-capacity convolutional networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data are scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, boosts performance significantly.	3+4
Since we combine region proposals with CNNs, we call the resulting model an R-CNN or Region-based Convolutional Network.	4
Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.	6
----------
Instance-level human analysis is common in real-life scenarios and has multiple manifestations, such as human part segmentation, dense pose estimation, human-object interactions, etc.	1
Models need to distinguish different human instances in the image panel and learn rich features to represent the details of each instance.	1
In this paper, we present an end-to-end pipeline for solving the instance-level human analysis, named Parsing R-CNN.	2+3
It processes a set of human instances simultaneously through comprehensive considering the characteristics of region-based approach and the appearance of a human, thus allowing representing the details of instances.	3
Parsing R-CNN is very flexible and efficient, which is applicable to many issues in human instance analysis.	4
Our approach outperforms all state-of-the-art methods on CIHP (Crowd Instance-level Human Parsing), MHP v2.0 (Multi-Human Parsing) and DensePose-COCO datasets.	4
Based on the proposed Parsing R-CNN, we reach the 1st place in the COCO 2018 Challenge DensePose Estimation task.	5
Code and models are public available.	6
----------
This paper presents KeypointNet, an end-to-end geometric reasoning framework to learn an optimal set of category-specific 3D keypoints, along with their detectors.	2
Given a single image, KeypointNet extracts 3D keypoints that are optimized for a downstream task.	3
We demonstrate this framework on 3D pose estimation by proposing a differentiable objective that seeks the optimal set of keypoints for recovering the relative pose between two views of an object.	2+3
Our model discovers geometrically and semantically consistent keypoints across viewing angles and instances of an object category.	4
Importantly, we find that our end-to-end framework using no ground-truth keypoint annotations outperforms a fully supervised baseline using the same neural network architecture on the task of pose estimation.	4
The discovered 3D keypoints on the car, chair, and plane categories of ShapeNet are visualized at this http URL.	4
----------
Multi-person pose estimation in the wild is challenging.	1
Although state-of-the-art human detectors have demonstrated good performance, small errors in localization and recognition are inevitable.	1
These errors can cause failures for a single-person pose estimator (SPPE), especially for methods that solely depend on human detection results.	1
In this paper, we propose a novel regional multi-person pose estimation (RMPE) framework to facilitate pose estimation in the presence of inaccurate human bounding boxes.	2
Our framework consists of three components: Symmetric Spatial Transformer Network (SSTN), Parametric Pose Non-Maximum-Suppression (NMS), and Pose-Guided Proposals Generator (PGPG).	3
Our method is able to handle inaccurate bounding boxes and redundant detections, allowing it to achieve a 17% increase in mAP over the state-of-the-art methods on the MPII (multi person) dataset.	4
Our model and source codes are publicly available.	6
----------
Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis.	1
The most successful architecture is StarGAN, that conditions GANs generation process with images of a specific domain, namely a set of images of persons sharing the same expression.	1
While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset.	1
To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression.	2+3
Our approach allows controlling the magnitude of activation of each AU and combine several of them.	4
Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions.	2+3
----------
Domain adaptation is critical for success in new, unseen environments.	1
Adversarial adaptation models applied in feature spaces discover domain invariant representations, but are difficult to visualize and sometimes fail to capture pixel-level and low-level domain shifts.	1
Recent work has shown that generative adversarial networks combined with cycle-consistency constraints are surprisingly effective at mapping images between domains, even without the use of aligned image pairs.	1
We propose a novel discriminatively-trained Cycle-Consistent Adversarial Domain Adaptation model.	2
CyCADA adapts representations at both the pixel-level and feature-level, enforces cycle-consistency while leveraging a task loss, and does not require aligned pairs.	4
Our model can be applied in a variety of visual recognition and prediction settings.	4
We show new state-of-the-art results across multiple adaptation tasks, including digit classification and semantic segmentation of road scenes demonstrating transfer from synthetic to real world domains.	4
----------
On the one hand, deep neural networks are effective in learning large datasets.	1
On the other, they are inefficient with their data usage.	1
They often require copious amount of labeled-data to train their scads of parameters.	1
Training larger and deeper networks is hard without appropriate regularization, particularly while using a small dataset.	1
Laterally, collecting well-annotated data is expensive, time-consuming and often infeasible.	1
A popular way to regularize these networks is to simply train the network with more data from an alternate representative dataset.	1
This can lead to adverse effects if the statistics of the representative dataset are dissimilar to our target.	1
This predicament is due to the problem of domain shift.	1
Data from a shifted domain might not produce bespoke features when a feature extractor from the representative domain is used.	1
Several techniques of domain adaptation have been proposed in the past to solve this problem.	1
In this paper, we propose a new technique (d-SNE) of domain adaptation that cleverly uses stochastic neighborhood embedding techniques and a novel modified-Hausdorff distance.	2+3
The proposed technique is learnable end-to-end and is therefore, ideally suited to train neural networks.	4
Extensive experiments demonstrate that d-SNE outperforms the current states-of-the-art and is robust to the variances in different datasets, even in the one-shot and semi-supervised learning settings.	4+5
d-SNE also demonstrates the ability to generalize to multiple domains concurrently.	5
----------
Video restoration tasks, including super-resolution, deblurring, etc, are drawing increasing attention in the computer vision community.	1
A challenging benchmark named REDS is released in the NTIRE19 Challenge.	1
This new benchmark challenges existing methods from two aspects: (1) how to align multiple frames given large motions, and (2) how to effectively fuse different frames with diverse motion and blur.	2+3
In this work, we propose a novel Video Restoration framework with Enhanced Deformable networks, termed EDVR, to address these challenges.	2
First, to handle large motions, we devise a Pyramid, Cascading and Deformable (PCD) alignment module, in which frame alignment is done at the feature level using deformable convolutions in a coarse-to-fine manner.	3
Second, we propose a Temporal and Spatial Attention (TSA) fusion module, in which attention is applied both temporally and spatially, so as to emphasize important features for subsequent restoration.	3
Thanks to these modules, our EDVR wins the champions and outperforms the second place by a large margin in all four tracks in the NTIRE19 video restoration and enhancement challenges.	4
EDVR also demonstrates superior performance to state-of-the-art published methods on video super-resolution and deblurring.	4
The code is available at this https URL.	6
----------
Recently, several models based on deep neural networks have achieved great success in terms of both reconstruction accuracy and computational performance for single image super-resolution.	1
In these methods, the low resolution (LR) input image is upscaled to the high resolution (HR) space using a single filter, commonly bicubic interpolation, before reconstruction.	1
This means that the super-resolution (SR) operation is performed in HR space.	1
We demonstrate that this is sub-optimal and adds computational complexity.	1
In this paper, we present the first convolutional neural network (CNN) capable of real-time SR of 1080p videos on a single K2 GPU.	2
To achieve this, we propose a novel CNN architecture where the feature maps are extracted in the LR space.	4
In addition, we introduce an efficient sub-pixel convolution layer which learns an array of upscaling filters to upscale the final LR feature maps into the HR output.	2+3
By doing so, we effectively replace the handcrafted bicubic filter in the SR pipeline with more complex upscaling filters specifically trained for each feature map, whilst also reducing the computational complexity of the overall SR operation.	4
We evaluate the proposed approach using images and videos from publicly available datasets and show that it performs significantly better (+0.15dB on Images and +0.39dB on Videos) and is an order of magnitude faster than previous CNN-based methods.	3+4
----------
Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors?	1
The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function.	1
Recent work has largely focused on minimizing the mean squared reconstruction error.	1
The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution.	1
In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR).	2
To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors.	4
To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss.	3
The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images.	3+4
In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space.	3
Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks.	4
An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN.	1
The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method.	1
----------
Non-local self-similarity is well-known to be an effective prior for the image denoising problem.	1
However, little work has been done to incorporate it in convolutional neural networks, which surpass non-local model-based methods despite only exploiting local information.	1
In this paper, we propose a novel end-to-end trainable neural network architecture employing layers based on graph convolution operations, thereby creating neurons with non-local receptive fields.	2+3
The graph convolution operation generalizes the classic convolution to arbitrary graphs.	1
In this work, the graph is dynamically computed from similarities among the hidden features of the network, so that the powerful representation learning capabilities of the network are exploited to uncover self-similar patterns.	2+3
We introduce a lightweight Edge-Conditioned Convolution which addresses vanishing gradient and over-parameterization issues of this particular graph convolution.	2
Extensive experiments show state-of-the-art performance with improved qualitative and quantitative results on both synthetic Gaussian noise and real noise.	4
----------
Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process.	1
Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments.	1
We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals.	2+3
Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink).	3
The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols.	4
In addition, the modular design enables various components to be easily usable independently in other projects.	4
We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.	3
----------
Learning compressed representations of multivariate time series (MTS) facilitates data analysis in the presence of noise and redundant information, and for a large number of variates and time steps.	1
However, classical dimensionality reduction approaches are designed for vectorial data and cannot deal explicitly with missing values.	2
In this work, we propose a novel autoencoder architecture based on recurrent neural networks to generate compressed representations of MTS.	2
The proposed model can process inputs characterized by variable lengths and it is specifically designed to handle missing data.	3
Our autoencoder learns fixed-length vectorial representations, whose pairwise similarities are aligned to a kernel function that operates in input space and that handles missing values.	3
This allows to learn good representations, even in the presence of a significant amount of missing data.	4
To show the effectiveness of the proposed approach, we evaluate the quality of the learned representations in several classification tasks, including those involving medical data, and we compare to other methods for dimensionality reduction.	2+3
Successively, we design two frameworks based on the proposed architecture: one for imputing missing data and another for one-class classification.	3
Finally, we analyze under what circumstances an autoencoder with recurrent layers can learn better compressed representations of MTS than feed-forward architectures.	4
----------
Localization of salient facial landmark points, such as eye corners or the tip of the nose, is still considered a challenging computer vision problem despite recent efforts.	1
This is especially evident in unconstrained environments, i.e., in the presence of background clutter and large head pose variations.	1
Most methods that achieve state-of-the-art accuracy are slow, and, thus, have limited applications.	1
We describe a method that can accurately estimate the positions of relevant facial landmarks in real-time even on hardware with limited processing power, such as mobile devices.	2
This is achieved with a sequence of estimators based on ensembles of regression trees.	3
The trees use simple pixel intensity comparisons in their internal nodes and this makes them able to process image regions very fast.	3
We test the developed system on several publicly available datasets and analyse its processing speed on various devices.	3
Experimental results show that our method has practical value.	5
----------
We describe efforts to adapt the Tesseract open source OCR engine for multiple scripts and languages.	2
Effort has been concentrated on enabling generic multi-lingual operation such that negligible customization is required for a new language beyond providing a corpus of text.	3
Although change was required to various modules, including physical layout analysis, and linguistic post-processing, no change was required to the character classifier beyond changing a few limits.	3
The Tesseract classifier has adapted easily to Simplified Chinese.	4
Test results on English, a mixture of European languages, and Russian, taken from a random sample of books, show a reasonably consistent word error rate between 3.72% and 5.78%, and Simplified Chinese has a character error rate of only 3.77%.	4
----------
Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time.	1
In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies.	2
Inspired by the classical non-local means method in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions.	3
This building block can be plugged into many computer vision architectures.	4
On the task of video classification, even without any bells and whistles, our non-local models can compete or outperform current competition winners on both Kinetics and Charades datasets.	4
In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks.	4
Code is available at this https URL .	6
----------
We present an approach that uses a multi-camera system to train fine-grained detectors for keypoints that are prone to occlusion, such as the joints of a hand.	2+3
We call this procedure multiview bootstrapping: first, an initial keypoint detector is used to produce noisy labels in multiple views of the hand.	4
The noisy detections are then triangulated in 3D using multiview geometry or marked as outliers.	4
Finally, the reprojected triangulations are used as new labeled training data to improve the detector.	4
We repeat this process, generating more labeled data in each iteration.	4
We derive a result analytically relating the minimum number of views to achieve target true and false positive rates for a given detector.	4
The method is used to train a hand keypoint detector for single images.	4
The resulting keypoint detector runs in realtime on RGB images and has accuracy comparable to methods that use depth sensors.	4
The single view detector, triangulated over multiple views, enables 3D markerless hand motion capture with complex object interactions.	4
----------
Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos.	1
In this work, we present a realtime approach to detect the 2D pose of multiple people in an image.	2
The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image.	2+3
This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image.	4
In previous work, PAFs and body part location estimation were refined simultaneously across training stages.	1
We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy.	4
We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released.	4
We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually.	4
This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.	4
----------
Pose Machines provide a sequential prediction framework for learning rich implicit spatial models.	1
In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation.	2+3
The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation.	3
We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference.	3
Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure.	3+4
We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.	3+4
----------
We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory.	2
After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings.	4
While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts.	4
Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy.	4
As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.	5
----------
In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks.	2
Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps.	1
In SAGAN, details can be generated using cues from all feature locations.	4
Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other.	4
Furthermore, recent work has shown that generator conditioning affects GAN performance.	1
Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics.	4
The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset.	4
Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.	4
----------
Similarity search finds application in specialized database systems handling complex data such as images or videos, which are typically represented by high-dimensional features and require specific indexing structures.	1
This paper tackles the problem of better utilizing GPUs for this task.	2
While GPUs excel at data-parallel tasks, prior approaches are bottlenecked by algorithms that expose less parallelism, such as k-min selection, or make poor use of the memory hierarchy.	3
We propose a design for k-selection that operates at up to 55% of theoretical peak performance, enabling a nearest neighbor implementation that is 8.5x faster than prior GPU state of the art.	4
We apply it in different similarity search scenarios, by proposing optimized design for brute-force, approximate and compressed-domain search based on product quantization.	3
In all these setups, we outperform the state of the art by large margins.	4
Our implementation enables the construction of a high accuracy k-NN graph on 95 million images from the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion vectors in less than 12 hours on 4 Maxwell Titan X GPUs.	3+4
We have open-sourced our approach for the sake of comparison and reproducibility.	3
----------
This paper presents the MAXQ approach to hierarchical reinforcement learning based on decomposing the target Markov decision process (MDP) into a hierarchy of smaller MDPs and decomposing the value function of the target MDP into an additive combination of the value functions of the smaller MDPs.	2+3
The paper defines the MAXQ hierarchy, proves formal results on its representational power, and establishes five conditions for the safe use of state abstractions.	2+3
The paper presents an online model-free learning algorithm, MAXQ-Q, and proves that it converges wih probability 1 to a kind of locally-optimal policy known as a recursively optimal policy, even in the presence of the five kinds of state abstraction.	3+4
The paper evaluates the MAXQ representation and MAXQ-Q through a series of experiments in three domains and shows experimentally that MAXQ-Q (with state abstractions) converges to a recursively optimal policy much faster than flat Q learning.	3+4
The fact that MAXQ learns a representation of the value function has an important benefit: it makes it possible to compute and execute an improved, non-hierarchical policy via a procedure similar to the policy improvement step of policy iteration.	4+5
The paper demonstrates the effectiveness of this non-hierarchical execution experimentally.	4
Finally, the paper concludes with a comparison to related work and a discussion of the design tradeoffs in hierarchical reinforcement learning.	4
----------
We present the first massively distributed architecture for deep reinforcement learning.	1+2
This architecture uses four main components: parallel actors that generate new behaviour; parallel learners that are trained from stored experience; a distributed neural network to represent the value function or behaviour policy; and a distributed store of experience.	3
We used our architecture to implement the Deep Q-Network algorithm (DQN).	3
Our distributed algorithm was applied to 49 games from Atari 2600 games from the Arcade Learning Environment, using identical hyperparameters.	3
Our performance surpassed non-distributed DQN in 41 of the 49 games and also reduced the wall-time required to achieve these results by an order of magnitude on most games.	4
----------
In this paper, we consider the task of learning control policies for text-based games.	2
In these games, all interactions in the virtual world are through text and the underlying state is not observed.	1
The resulting language barrier makes such environments challenging for automatic game players.	1
We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback.	3
This framework enables us to map text descriptions into vector representations that capture the semantics of the game states.	3
We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations.	3
Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.	4
----------
Deep Reinforcement Learning has yielded proficient controllers for complex tasks.	1
However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point.	1+2
To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM.	2+3
The resulting Deep Recurrent Q-Network(DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens.	4+5
Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability.	5
Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's.	5
Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.	4
----------
This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer.	2
Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition.	4
The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters.	3
Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.	3
----------
This paper introduces a machine learning based system for controlling a robotic manipulator with visual perception only.	2
The capability to autonomously learn robot controllers solely from raw-pixel images and without any prior knowledge of configuration is shown for the first time.	1
We build upon the success of recent deep reinforcement learning and develop a system for learning target reaching with a three-joint robot manipulator using external visual observation.	2+3
A Deep Q Network (DQN) was demonstrated to perform target reaching after training in simulation.	2+3
Transferring the network to real hardware and real observation in a naive approach failed, but experiments show that the network works when replacing camera images with synthetic images.	4
----------
Experience replay lets online reinforcement learning agents remember and reuse experiences from the past.	1
In prior work, experience transitions were uniformly sampled from a replay memory.	1
However, this approach simply replays transitions at the same frequency that they were originally experienced, regardless of their significance.	1
In this paper we develop a framework for prioritizing experience, so as to replay important transitions more frequently, and therefore learn more efficiently.	2+3
We use prioritized experience replay in Deep Q-Networks (DQN), a reinforcement learning algorithm that achieved human-level performance across many Atari games.	3
DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.	4+5
----------
In recent years there have been many successes of using deep representations in reinforcement learning.	1
Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders.	1
In this paper, we present a new neural network architecture for model-free reinforcement learning.	2
Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function.	2
The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm.	5
Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions.	5
Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.	5
----------
Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity.	1
Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN).	2+3
When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps.	4
When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments.	4
We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting.	4
We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.	4
----------
A deep learning approach to reinforcement learning led to a general learner able to train on visual input to play a variety of arcade games at the human and superhuman levels.	1
Its creators at the Google DeepMind's team called the approach: Deep Q-Network (DQN).	1
We present an extension of DQN by "soft" and "hard" attention mechanisms.	2
Tests of the proposed Deep Attention Recurrent Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of performance superior to that of DQN.	4
Moreover, built-in attention mechanisms allow a direct online monitoring of the training process by highlighting the regions of the game screen the agent is focusing on when making decisions.	4
----------
Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance.	1
In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient.	2+3
Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy.	3
We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.	4
----------
In recent years there is a growing interest in using deep representations for reinforcement learning.	1
In this paper, we present a methodology and tools to analyze Deep Q-networks (DQNs) in a non-blind matter.	2
Moreover, we propose a new model, the Semi Aggregated Markov Decision Process (SAMDP), and an algorithm that learns it automatically.	4
The SAMDP model allows us to identify spatio-temporal abstractions directly from features and may be used as a sub-goal detector in future work.	6
Using our tools we reveal that the features learned by DQNs aggregate the state space in a hierarchical fashion, explaining its success.	4
Moreover, we are able to understand and describe the policies learned by DQNs for three different Atari2600 games and suggest ways to interpret, debug and optimize deep neural networks in reinforcement learning.	4
----------
Many real-world applications can be described as large-scale games of imperfect information.	1
To deal with these challenging domains, prior work has focused on computing Nash equilibria in a handcrafted abstraction of the domain.	1
In this paper we introduce the first scalable end-to-end approach to learning approximate Nash equilibria without prior domain knowledge.	2
Our method combines fictitious self-play with deep reinforcement learning.	3
When applied to Leduc poker, Neural Fictitious Self-Play (NFSP) approached a Nash equilibrium, whereas common reinforcement learning methods diverged.	4
In Limit Texas Holdem, a poker game of real-world scale, NFSP learnt a strategy that approached the performance of state-of-the-art, superhuman algorithms based on significant domain expertise.	4
----------
We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement.	1
By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO).	2+3
This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks.	4
Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input.	4
Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.	4
----------
Model predictive control (MPC) is an effective method for controlling robotic systems, particularly autonomous aerial vehicles such as quadcopters.	1
However, application of MPC can be computationally demanding, and typically requires estimating the state of the system, which can be challenging in complex, unstructured environments.	1
Reinforcement learning can in principle forego the need for explicit state estimation and acquire a policy that directly maps sensor readings to actions, but is difficult to apply to unstable systems that are liable to fail catastrophically during training before an effective policy has been found.	1
We propose to combine MPC with reinforcement learning in the framework of guided policy search, where MPC is used to generate data at training time, under full state observations provided by an instrumented training environment.	2+3
This data is used to train a deep neural network policy, which is allowed to access only the raw observations from the vehicle's onboard sensors.	3
After training, the neural network policy can successfully control the robot without knowledge of the full state, and at a fraction of the computational cost of MPC.	4
We evaluate our method by learning obstacle avoidance policies for a simulated quadrotor, using simulated onboard sensors and no explicit state estimation at test time.	4
----------
Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks.	1
The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data.	2
We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).	3
We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.	3
Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground.	4
In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques.	4
Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.	4
----------
This paper proposes GProp, a deep reinforcement learning algorithm for continuous policies with compatible function approximation.	1+2
The algorithm is based on two innovations.	3
Firstly, we present a temporal-difference based method for learning the gradient of the value-function.	3
Secondly, we present the deviator-actor-critic (DAC) model, which comprises three neural networks that estimate the value function, its gradient, and determine the actor's policy respectively.	3
We evaluate GProp on two challenging tasks: a contextual bandit problem constructed from nonparametric regression datasets that is designed to probe the ability of reinforcement learning algorithms to accurately estimate gradients; and the octopus arm, a challenging reinforcement learning benchmark.	3
GProp is competitive with fully supervised methods on the bandit task and achieves the best performance to date on the octopus arm.	4
----------
Recent work has shown that deep neural networks are capable of approximating both value functions and policies in reinforcement learning domains featuring continuous state and action spaces.	1
However, to the best of our knowledge no previous work has succeeded at using deep neural networks in structured (parameterized) continuous action spaces.	1
To fill this gap, this paper focuses on learning within the domain of simulated RoboCup soccer, which features a small set of discrete action types, each of which is parameterized with continuous variables.	2+3
The best learned agent can score goals more reliably than the 2012 RoboCup champion agent.	4
As such, this paper represents a successful extension of deep reinforcement learning to the class of parameterized action space MDPs.	4
----------
Achieving efficient and scalable exploration in complex domains poses a major challenge in reinforcement learning.	1
While Bayesian and PAC-MDP approaches to the exploration problem offer strong formal guarantees, they are often impractical in higher dimensions due to their reliance on enumerating the state-action space.	1
Hence, exploration in complex domains is often performed with simple epsilon-greedy methods.	1
In this paper, we consider the challenging Atari games domain, which requires processing raw pixel inputs and delayed rewards.	1
We evaluate several more sophisticated exploration strategies, including Thompson sampling and Boltzman exploration, and propose a new exploration method based on assigning exploration bonuses from a concurrently learned model of the system dynamics.	2+3
By parameterizing our learned model with a neural network, we are able to develop a scalable and efficient approach to exploration bonuses that can be applied to tasks with complex, high-dimensional state spaces.	2
In the Atari domain, our method provides the most consistent improvement across a range of games that pose a major challenge for prior methods.	4
In addition to raw game-scores, we also develop an AUC-100 metric for the Atari Learning domain to evaluate the impact of exploration on this benchmark.	2
----------
Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future (image-)frames are dependent on control variables or actions as well as previous frames.	1
While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability.	1
We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks.	2+3
Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games.	4
To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.	4
----------
Data-efficient reinforcement learning (RL) in continuous state-action spaces using very high-dimensional observations remains a key challenge in developing fully autonomous systems.	1
We consider a particularly important instance of this challenge, the pixels-to-torques problem, where an RL agent learns a closed-loop control policy ("torques") from pixel information only.	1
We introduce a data-efficient, model-based reinforcement learning algorithm that learns such a closed-loop policy directly from pixel information.	2
The key ingredient is a deep dynamical model for learning a low-dimensional feature embedding of images jointly with a predictive model in this low-dimensional feature space.	3
Joint learning is crucial for long-term predictions, which lie at the core of the adaptive nonlinear model predictive control strategy that we use for closed-loop control.	4
Compared to state-of-the-art RL methods for continuous states and actions, our approach learns quickly, scales to high-dimensional state spaces, is lightweight and an important step toward fully autonomous end-to-end learning from pixels to torques.	4
----------
We present a unified framework for learning continuous control policies using backpropagation.	2+3
It supports stochastic control by treating stochasticity in the Bellman equation as a deterministic function of exogenous noise.	3
The product is a spectrum of general policy gradient algorithms that range from model-free methods with value functions to model-based methods without value functions.	3
We use learned models but only require observations from the environment in- stead of observations from model-predicted trajectories, minimizing the impact of compounded model errors.	3
We apply these algorithms first to a toy stochastic control problem and then to several physics-based control problems in simulation.	3
One of these variants, SVG(1), shows the effectiveness of learning models, value functions, and policies simultaneously in continuous domains.	4
----------
The ability to plan and execute goal specific actions in varied, unexpected settings is a central requirement of intelligent agents.	1
In this paper, we explore how an agent can be equipped with an internal model of the dynamics of the external world, and how it can use this model to plan novel actions by running multiple internal simulations ("visual imagination").	2+3
Our models directly process raw visual input, and use a novel object-centric prediction formulation based on visual glimpses centered on objects (fixations) to enforce translational invariance of the learned physical laws.	3
The agent gathers training data through random interaction with a collection of different environments, and the resulting model can then be used to plan goal-directed actions in novel environments that the agent has not seen before.	4
We demonstrate that our agent can accurately plan actions for playing a simulated billiards game, which requires pushing a ball into a target position or into collision with another ball.	4
----------
We present an active detection model for localizing objects in scenes.	1+2
The model is class-specific and allows an agent to focus attention on candidate regions for identifying the correct location of a target object.	1
This agent learns to deform a bounding box using simple transformation actions, with the goal of determining the most specific location of target objects following top-down reasoning.	3
The proposed localization agent is trained using deep reinforcement learning, and evaluated on the Pascal VOC 2007 dataset.	3
We show that agents guided by the proposed model are able to localize a single instance of an object after analyzing only between 11 and 25 regions in an image, and obtain the best detection results among systems that do not use object proposals for object localization.	4
----------
We introduce a novel schema for sequence to sequence learning with a Deep Q-Network (DQN), which decodes the output sequence iteratively.	2
The aim here is to enable the decoder to first tackle easier portions of the sequences, and then turn to cope with difficult parts.	2+3
Specifically, in each iteration, an encoder-decoder Long Short-Term Memory (LSTM) network is employed to, from the input sequence, automatically create features to represent the internal states of and formulate a list of potential actions for the DQN.	3
Take rephrasing a natural sentence as an example.	3
This list can contain ranked potential words.	4
Next, the DQN learns to make decision on which action (e.g., word) will be selected from the list to modify the current decoded sequence.	3
The newly modified output sequence is subsequently used as the input to the DQN for the next decoding iteration.	3
In each iteration, we also bias the reinforcement learning's attention to explore sequence portions which are previously difficult to be decoded.	3
For evaluation, the proposed strategy was trained to decode ten thousands natural sentences.	3
Our experiments indicate that, when compared to a left-to-right greedy beam search LSTM decoder, the proposed method performed competitively well when decoding sentences from the training set, but significantly outperformed the baseline when decoding unseen sentences, in terms of BLEU score obtained.	4
----------
We present a novel definition of the reinforcement learning state, actions and reward function that allows a deep Q-network (DQN) to learn to control an optimization hyperparameter.	2+3
Using Q-learning with experience replay, we train two DQNs to accept a state representation of an objective function as input and output the expected discounted return of rewards, or q-values, connected to the actions of either adjusting the learning rate or leaving it unchanged.	3
The two DQNs learn a policy similar to a line search, but differ in the number of allowed actions.	4
The trained DQNs in combination with a gradient-based update routine form the basis of the Q-gradient descent algorithms.	4
To demonstrate the viability of this framework, we show that the DQN's q-values associated with optimal action converge and that the Q-gradient descent algorithms outperform gradient descent with an Armijo or nonmonotone line search.	4
Unlike traditional optimization methods, Q-gradient descent can incorporate any objective statistic and by varying the actions we gain insight into the type of learning rate adjustment strategies that are successful for neural network optimization.	4
----------
The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents.	1
Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications.	1
This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning.	2
We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment.	2+3
Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.	2+3
----------
The recently introduced Deep Q-Networks (DQN) algorithm has gained attention as one of the first successful combinations of deep neural networks and reinforcement learning.	1
Its promise was demonstrated in the Arcade Learning Environment (ALE), a challenging framework composed of dozens of Atari 2600 games used to evaluate general competency in AI.	1
It achieved dramatically better results than earlier approaches, showing that its ability to learn good representations is quite robust and general.	1
This paper attempts to understand the principles that underlie DQN's impressive performance and to better contextualize its success.	2
We systematically evaluate the importance of key representational biases encoded by DQN's network by proposing simple linear representations that make use of these concepts.	2+3
Incorporating these characteristics, we obtain a computationally practical feature set that achieves competitive performance to DQN in the ALE.	3+4
Besides offering insight into the strengths and weaknesses of DQN, we provide a generic representation for the ALE, significantly reducing the burden of learning a representation for each game.	3+4
Moreover, we also provide a simple, reproducible benchmark for the sake of comparison to future work in the ALE.	3
----------
With the demand for machine learning increasing, so does the demand for tools which make it easier to use.	1
Automated machine learning (AutoML) tools have been developed to address this need, such as the Tree-Based Pipeline Optimization Tool (TPOT) which uses genetic programming to build optimal pipelines.	1
We introduce Layered TPOT, a modification to TPOT which aims to create pipelines equally good as the original, but in significantly less time.	2
This approach evaluates candidate pipelines on increasingly large subsets of the data according to their fitness, using a modified evolutionary algorithm to allow for separate competition between pipelines trained on different sample sizes.	2+3
Empirical evaluation shows that, on sufficiently large datasets, Layered TPOT indeed finds better models faster.	4
----------
Neural architecture search (NAS) has been proposed to automatically tune deep neural networks, but existing search algorithms, e.g., NASNet, PNAS, usually suffer from expensive computational cost.	1
Network morphism, which keeps the functionality of a neural network while changing its neural architecture, could be helpful for NAS by enabling more efficient training during the search.	1
In this paper, we propose a novel framework enabling Bayesian optimization to guide the network morphism for efficient neural architecture search.	2
The framework develops a neural network kernel and a tree-structured acquisition function optimization algorithm to efficiently explores the search space.	4
Intensive experiments on real-world benchmark datasets have been done to demonstrate the superior performance of the developed framework over the state-of-the-art methods.	3+4
Moreover, we build an open-source AutoML system based on our method, namely Auto-Keras.	2+3
The system runs in parallel on CPU and GPU, with an adaptive search strategy for different GPU memory limits.	3
----------
Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability.	1
The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge.	1
We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior.	1+2
We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input.	2+3
Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data.	4
We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.	4
----------
We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels.	2+3
We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes.	3
At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G.	3
We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.	5
----------
Machine learning models are powerful but fallible.	1
Generating adversarial examples - inputs deliberately crafted to cause model misclassification or other errors - can yield important insight into model assumptions and vulnerabilities.	1
Despite significant recent work on adversarial example generation targeting image classifiers, relatively little work exists exploring adversarial example generation for text classifiers; additionally, many existing adversarial example generation algorithms require full access to target model parameters, rendering them impractical for many real-world attacks.	1
In this work, we introduce DANCin SEQ2SEQ, a GAN-inspired algorithm for adversarial text example generation targeting largely black-box text classifiers.	2
We recast adversarial text example generation as a reinforcement learning problem, and demonstrate that our algorithm offers preliminary but promising steps towards generating semantically meaningful adversarial text examples in a real-world attack scenario.	3+4
----------
Convolutional Neural Networks have achieved significant success across multiple computer vision tasks.	1
However, they are vulnerable to carefully crafted, human-imperceptible adversarial noise patterns which constrain their deployment in critical security-sensitive systems.	1
This paper proposes a computationally efficient image enhancement approach that provides a strong defense mechanism to effectively mitigate the effect of such adversarial perturbations.	2
We show that deep image restoration networks learn mapping functions that can bring off-the-manifold adversarial samples onto the natural image manifold, thus restoring classification towards correct classes.	4
A distinguishing feature of our approach is that, in addition to providing robustness against attacks, it simultaneously enhances image quality and retains models performance on clean images.	4
Furthermore, the proposed method does not modify the classifier or requires a separate mechanism to detect adversarial images.	4
The effectiveness of the scheme has been demonstrated through extensive experiments, where it has proven a strong defense in gray-box settings.	3
The proposed scheme is simple and has the following advantages: (1) it does not require any model training or parameter optimization, (2) it complements other existing defense mechanisms, (3) it is agnostic to the attacked model and attack type and (4) it provides superior performance across all popular attack algorithms.	4+5
Our codes are publicly available at this https URL.	6
----------
Deep neural networks have achieved impressive experimental results in image classification, but can surprisingly be unstable with respect to adversarial perturbations, that is, minimal changes to the input image that cause the network to misclassify it.	1
With potential applications including perception modules and end-to-end controllers for self-driving cars, this raises concerns about their safety.	1
We develop a novel automated verification framework for feed-forward multi-layer neural networks based on Satisfiability Modulo Theory (SMT).	1+2
We focus on safety of image classification decisions with respect to image manipulations, such as scratches or changes to camera angle or lighting conditions that would result in the same class being assigned by a human, and define safety for an individual decision in terms of invariance of the classification within a small neighbourhood of the original image.	3
We enable exhaustive search of the region by employing discretisation, and propagate the analysis layer by layer.	3
Our method works directly with the network code and, in contrast to existing methods, can guarantee that adversarial examples, if they exist, are found for the given region and family of manipulations.	3
If found, adversarial examples can be shown to human testers and/or used to fine-tune the network.	3
We implement the techniques using Z3 and evaluate them on state-of-the-art networks, including regularised and deep learning networks.	3
We also compare against existing techniques to search for adversarial examples and estimate network robustness.	3
----------
In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research.	2
Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense).	3
Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a Macbook Pro notebook.	4
When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU.	4
In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like Arcade Learning Environment.	4
Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU and Batch Normalization coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS.	3+4
Strong performance is also achieved on the other two games.	4
In game replays, we show our agents learn interesting strategies.	4
ELF, along with its RL platform, is open-sourced at this https URL.	6
----------
Cooperative multi-agent systems can be naturally used to model many real world problems, such as network packet routing and the coordination of autonomous vehicles.	1
There is a great need for new reinforcement learning methods that can efficiently learn decentralised policies for such systems.	1
To this end, we propose a new multi-agent actor-critic method called counterfactual multi-agent (COMA) policy gradients.	2
COMA uses a centralised critic to estimate the Q-function and decentralised actors to optimise the agents' policies.	3
In addition, to address the challenges of multi-agent credit assignment, it uses a counterfactual baseline that marginalises out a single agent's action, while keeping the other agents' actions fixed.	3
COMA also uses a critic representation that allows the counterfactual baseline to be computed efficiently in a single forward pass.	3
We evaluate COMA in the testbed of StarCraft unit micromanagement, using a decentralised variant with significant partial observability.	3
COMA significantly improves average performance over other multi-agent actor-critic methods in this setting, and the best performing agents are competitive with state-of-the-art centralised controllers that get access to the full state.	5
----------
Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science.	1
Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature.	1
These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph.	1
At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach.	1
In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework.	2+3
Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.	4+5
----------
Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery.	1
This article aims to give a general overview of MTL, particularly in deep neural networks.	2
It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances.	3
In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.	2
----------
Docking is an important tool in computational drug discovery that aims to predict the binding pose of a ligand to a target protein through a combination of pose scoring and optimization.	2
A scoring function that is differentiable with respect to atom positions can be used for both scoring and gradient-based optimization of poses for docking.	3
In order to enrich libraries with molecules active towards a given biological target, we propose to fine-tune the model with small sets of molecules, which are known to be active against that target.	2+3
Against Staphylococcus aureus, the model reproduced 14% of 6051 hold-out test molecules that medicinal chemists designed, whereas against Plasmodium falciparum (Malaria) it reproduced 28% of 1240 test molecules.	4
When coupled with a scoring function, our model can perform the complete de novo drug design cycle to generate large sets of novel molecules for drug discovery.	3+4
----------
Recently, dense connections have attracted substantial attention in computer vision because they facilitate gradient flow and implicit deep supervision during training.	1
Particularly, DenseNet, which connects each layer to every other layer in a feed-forward fashion, has shown impressive performances in natural image classification tasks.	1
We propose HyperDenseNet, a 3D fully convolutional neural network that extends the definition of dense connectivity to multi-modal segmentation problems.	2+3
Each imaging modality has a path, and dense connections occur not only between the pairs of layers within the same path, but also between those across different paths.	3
This contrasts with the existing multi-modal CNN approaches, in which modeling several modalities relies entirely on a single joint layer (or level of abstraction) for fusion, typically either at the input or at the output of the network.	3
Therefore, the proposed network has total freedom to learn more complex combinations between the modalities, within and in-between all the levels of abstraction, which increases significantly the learning representation.	3
We report extensive evaluations over two different and highly competitive multi-modal brain tissue segmentation challenges, iSEG 2017 and MRBrainS 2013, with the former focusing on 6-month infant data and the latter on adult images.	3
HyperDenseNet yielded significant improvements over many state-of-the-art segmentation networks, ranking at the top on both benchmarks.	4
We further provide a comprehensive experimental analysis of features re-use, which confirms the importance of hyper-dense connections in multi-modal representation learning.	4+5
Our code is publicly available at this https URL.	6
----------
Spatial studies of transcriptome provide biologists with gene expression maps of heterogeneous and complex tissues.	1
However, most experimental protocols for spatial transcriptomics suffer from the need to select beforehand a small fraction of genes to be quantified over the entire transcriptome.	1
Standard single-cell RNA sequencing (scRNA-seq) is more prevalent, easier to implement and can in principle capture any gene but cannot recover the spatial location of the cells.	1
In this manuscript, we focus on the problem of imputation of missing genes in spatial transcriptomic data based on (unpaired) standard scRNA-seq data from the same biological tissue.	2+3
Building upon domain adaptation work, we propose gimVI, a deep generative model for the integration of spatial transcriptomic data and scRNA-seq data that can be used to impute missing genes.	4
After describing our generative model and an inference procedure for it, we compare gimVI to alternative methods from computational biology or domain adaptation on real datasets and outperform Seurat Anchors, Liger and CORAL to impute held-out genes.	2+3
----------
Interpretability of deep neural networks is a recently emerging area of machine learning research targeting a better understanding of how models perform feature selection and derive their classification decisions.	1
In this paper, two neural network architectures are trained on spectrogram and raw waveform data for audio classification tasks on a newly created audio dataset and layer-wise relevance propagation (LRP), a previously proposed interpretability method, is applied to investigate the models' feature selection and decision making.	2+3
It is demonstrated that the networks are highly reliant on feature marked as relevant by LRP through systematic manipulation of the input data.	4
Our results show that by making deep audio classifiers interpretable, one can analyze and compare the properties and strategies of different models beyond classification accuracy, which potentially opens up new ways for model improvements.	4+5
----------
Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales.	2+3
Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation.	4
In this paper we introduce WaveGAN, a first attempt at applying GANs to unsupervised synthesis of raw-waveform audio.	2
WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation.	4
Our experiments demonstrate that, without labels, WaveGAN learns to produce intelligible words when trained on a small-vocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano.	4
We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, finding both approaches to be promising.	4
----------
What is a good visual representation for autonomous agents?	1
We address this question in the context of semantic visual navigation, which is the problem of a robot finding its way through a complex environment to a target object, e.g. go to the refrigerator.	1+2
Instead of acquiring a metric semantic map of an environment and using planning for navigation, our approach learns navigation policies on top of representations that capture spatial layout and semantic contextual cues.	3
We propose to using high level semantic and contextual features including segmentation and detection masks obtained by off-the-shelf state-of-the-art vision as observations and use deep network to learn the navigation policy.	4
This choice allows using additional data, from orthogonal sources, to better train different parts of the model the representation extraction is trained on large standard vision datasets while the navigation component leverages large synthetic environments for training.	4
This combination of real and synthetic is possible because equitable feature representations are available in both (e.g., segmentation and detection masks), which alleviates the need for domain adaptation.	3
Both the representation and the navigation policy can be readily applied to real non-synthetic environments as demonstrated on the Active Vision Dataset [1].	4
Our approach gets successfully to the target in 54% of the cases in unexplored environments, compared to 46% for non-learning based approach, and 28% for the learning-based baseline.	4
----------
Audio tagging aims to predict one or several labels in an audio clip.	2
Many previous works use weakly labelled data (WLD) for audio tagging, where only presence or absence of sound events is known, but the order of sound events is unknown.	1
To use the order information of sound events, we propose sequential labelled data (SLD), where both the presence or absence and the order information of sound events are known.	4
To utilize SLD in audio tagging, we propose a Convolutional Recurrent Neural Network followed by a Connectionist Temporal Classification (CRNN-CTC) objective function to map from an audio clip spectrogram to SLD.	4
Experiments show that CRNN-CTC obtains an Area Under Curve (AUC) score of 0.986 in audio tagging, outperforming the baseline CRNN of 0.908 and 0.815 with Max Pooling and Average Pooling, respectively.	4
In addition, we show CRNN-CTC has the ability to predict the order of sound events in an audio clip.	4
----------
Environmental audio tagging aims to predict only the presence or absence of certain acoustic events in the interested acoustic scene.	2
In this paper we make contributions to audio tagging in two parts, respectively, acoustic modeling and feature learning.	3
We propose to use a shrinking deep neural network (DNN) framework incorporating unsupervised feature learning to handle the multi-label classification task.	4
For the acoustic modeling, a large set of contextual frames of the chunk are fed into the DNN to perform a multi-label classification for the expected tags, considering that only chunk (or utterance) level rather than frame-level labels are available.	4
Dropout and background noise aware training are also adopted to improve the generalization capability of the DNNs.	3
For the unsupervised feature learning, we propose to use a symmetric or asymmetric deep de-noising auto-encoder (sDAE or aDAE) to generate new data-driven features from the Mel-Filter Banks (MFBs) features.	4
The new features, which are smoothed against background noise and more compact with contextual information, can further improve the performance of the DNN baseline.	4
Compared with the standard Gaussian Mixture Model (GMM) baseline of the DCASE 2016 audio tagging challenge, our proposed method obtains a significant equal error rate (EER) reduction from 0.21 to 0.13 on the development set.	4
The proposed aDAE system can get a relative 6.7% EER reduction compared with the strong DNN baseline on the development set.	4
Finally, the results also show that our approach obtains the state-of-the-art performance with 0.15 EER on the evaluation set of the DCASE 2016 audio tagging task while EER of the first prize of this challenge is 0.17.	4
----------
Learning how objects sound from video is challenging, since they often heavily overlap in a single audio channel.	1
Current methods for visually-guided audio source separation sidestep the issue by training with artificially mixed video clips, but this puts unwieldy restrictions on training data collection and may even prevent learning the properties of "true" mixed sounds.	1
We introduce a co-separation training paradigm that permits learning object-level sounds from unlabeled multi-source videos.	2
Our novel training objective requires that the deep neural network's separated audio for similar-looking objects be consistently identifiable, while simultaneously reproducing accurate video-level audio tracks for each source training pair.	2
Our approach disentangles sounds in realistic test videos, even in cases where an object was not observed individually during training.	3
We obtain state-of-the-art results on visually-guided audio source separation and audio denoising for the MUSIC, AudioSet, and AV-Bench datasets.	4
----------
Stereo matching algorithms usually consist of four steps, including matching cost calculation, matching cost aggregation, disparity calculation, and disparity refinement.	1
Existing CNN-based methods only adopt CNN to solve parts of the four steps, or use different networks to deal with different steps, making them difficult to obtain the overall optimal solution.	1
In this paper, we propose a network architecture to incorporate all steps of stereo matching.	2
The network consists of three parts.	3
The first part calculates the multi-scale shared features.	3
The second part performs matching cost calculation, matching cost aggregation and disparity calculation to estimate the initial disparity using shared features.	3
The initial disparity and the shared features are used to calculate the feature constancy that measures correctness of the correspondence between two input images.	3
The initial disparity and the feature constancy are then fed to a sub-network to refine the initial disparity.	3
The proposed method has been evaluated on the Scene Flow and KITTI datasets.	3
It achieves the state-of-the-art performance on the KITTI 2012 and KITTI 2015 benchmarks while maintaining a very fast running time.	4
----------
Depth prediction is one of the fundamental problems in computer vision.	1
In this paper, we propose a simple yet effective convolutional spatial propagation network (CSPN) to learn the affinity matrix for various depth estimation tasks.	2
Specifically, it is an efficient linear propagation model, in which the propagation is performed with a manner of recurrent convolutional operation, and the affinity among neighboring pixels is learned through a deep convolutional neural network (CNN).	4
We can append this module to any output from a state-of-the-art (SOTA) depth estimation networks to improve their performances.	4
In practice, we further extend CSPN in two aspects: 1) take sparse depth map as additional input, which is useful for the task of depth completion; 2) similar to commonly used 3D convolution operation in CNNs, we propose 3D CSPN to handle features with one additional dimension, which is effective in the task of stereo matching using 3D cost volume.	3+4
For the tasks of sparse to dense, a.k.a depth completion.	4
We experimented the proposed CPSN conjunct algorithms over the popular NYU v2 and KITTI datasets, where we show that our proposed algorithms not only produce high quality (e.g., 30% more reduction in depth error), but also run faster (e.g., 2 to 5x faster) than previous SOTA spatial propagation network.	4
We also evaluated our stereo matching algorithm on the Scene Flow and KITTI Stereo datasets, and rank 1st on both the KITTI Stereo 2012 and 2015 benchmarks, which demonstrates the effectiveness of the proposed module.	4
The code of CSPN proposed in this work will be released at this https URL.	6
----------
This paper presents StereoNet, the first end-to-end deep architecture for real-time stereo matching that runs at 60 fps on an NVidia Titan X, producing high-quality, edge-preserved, quantization-free disparity maps.	2+3
A key insight of this paper is that the network achieves a sub-pixel matching precision than is a magnitude higher than those of traditional stereo matching approaches.	3
This allows us to achieve real-time performance by using a very low resolution cost volume that encodes all the information needed to achieve high disparity precision.	3+4
Spatial precision is achieved by employing a learned edge-aware upsampling function.	3
Our model uses a Siamese network to extract features from the left and right image.	3
A first estimate of the disparity is computed in a very low resolution cost volume, then hierarchically the model re-introduces high-frequency details through a learned upsampling function that uses compact pixel-to-pixel refinement networks.	3
Leveraging color input as a guide, this function is capable of producing high-quality edge-aware output.	4
We achieve compelling results on multiple benchmarks, showing how the proposed method offers extreme flexibility at an acceptable computational budget.	5
----------
Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures.	1
Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models.	1
We introduce StructVAE, a variational auto-encoding model for semisupervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances.	2+3
StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables.	2
Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.	4
----------
We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimate.	1
MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization tasks.	1
We express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside the memory buffer, and a separate expectation over trajectories outside the buffer.	1
To make an efficient algorithm of MAPO, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to scale up training.	2+3
MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards.	4
We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing).	4
On the WikiTableQuestions benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%.	4
On the WikiSQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision.	4
Our source code is available at this https URL	6
----------
Cross-correlator plays a significant role in many visual perception tasks, such as object detection and tracking.	1
Beyond the linear cross-correlator, this paper proposes a kernel cross-correlator (KCC) that breaks traditional limitations.	2
First, by introducing the kernel trick, the KCC extends the linear cross-correlation to non-linear space, which is more robust to signal noises and distortions.	3+4
Second, the connection to the existing works shows that KCC provides a unified solution for correlation filters.	3+4
Third, KCC is applicable to any kernel function and is not limited to circulant structure on training data, thus it is able to predict affine transformations with customized properties.	3+4
Last, by leveraging the fast Fourier transform (FFT), KCC eliminates direct calculation of kernel vectors, thus achieves better performance yet still with a reasonable computational cost.	3+4
Comprehensive experiments on visual tracking and human activity recognition using wearable devices demonstrate its robustness, flexibility, and efficiency.	3+4
The source codes of both experiments are released at this https URL	6
----------
